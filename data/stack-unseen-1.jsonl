{"questionId":"a154d96fe44e4fc5b3a4912ca3af198f","question":"Compilation error after upgrading to JDK 21 - \"NoSuchFieldError: JCImport does not have member field JCTree qualid\"\nAfter upgrading to JDK 21, I have the following compilation error in my Spring Boot project:\n\n\n\n```\nFatal error compiling: java.lang.NoSuchFieldError:\nClass com.sun.tools.javac.tree.JCTree$JCImport does not have member field 'com.sun.tools.javac.tree.JCTree qualid'","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"java"},"answer":"The culprit is Lombok. The minimal Lombok version compatible with JDK 21 is `1.18.30`.\n\n\nThis implies that the minimal Spring Boot version is `3.1.4`, unless you want to meddle with the Spring Boot autoconfiguration and set the Lombok version in your project differently from the Spring Boot [BOM](https:\/\/www.baeldung.com\/spring-maven-bom) defined in `spring-boot-dependencies`.\n\n\nSee: *[[BUG] Lombok 1.8.26 incompatible with JDK 21 #3393](https:\/\/github.com\/projectlombok\/lombok\/issues\/3393)*"}
{"questionId":"41d7fc57c785419ea2e0e3000849faf8","question":"Why did Flask start failing with \"ImportError: cannot import name 'url\\_quote' from 'werkzeug.urls'\"?\nEnvironment:\n\n\n\n```\nPython 3.10.11\nFlask==2.2.2\n\n```\n\nI run my Flask backend code in docker container, with BASE Image:\n`FROM pytorch\/pytorch:2.0.1-cuda11.7-cudnn8-runtime`\n\n\nBut when I run the pytest with version `pytest 7.4.2`,\n\n\n\n```\npip install pytest\npytest\n\n```\n\nit raised an Error, with logs:\n\n\n\n```\n==================================== ERRORS ====================================\n_____________ ERROR collecting tests\/test_fiftyone_utils_utils.py ______________\nImportError while importing test module '\/builds\/kw\/data-auto-analysis-toolkit-backend\/tests\/test_fiftyone_utils_utils.py'.\nHint: make sure your test modules\/packages have valid Python names.\nTraceback:\n\/opt\/conda\/lib\/python3.10\/importlib\/__init__.py:126: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\ntests\/test_fiftyone_utils_utils.py:2: in <module>\n    import daat  # noqa: F401\n\/opt\/conda\/lib\/python3.10\/site-packages\/daat-1.0.0-py3.10.egg\/daat\/__init__.py:1: in <module>\n    from daat.app import app\n\/opt\/conda\/lib\/python3.10\/site-packages\/daat-1.0.0-py3.10.egg\/daat\/app\/__init__.py:6: in <module>\n    from flask import Flask, jsonify, request\n\/opt\/conda\/lib\/python3.10\/site-packages\/flask\/__init__.py:5: in <module>\n    from .app import Flask as Flask\n\/opt\/conda\/lib\/python3.10\/site-packages\/flask\/app.py:30: in <module>\n    from werkzeug.urls import url_quote\nE   ImportError: cannot import name 'url_quote' from 'werkzeug.urls' (\/opt\/conda\/lib\/python3.10\/site-packages\/werkzeug\/urls.py)\n\n```\n\nMy codes works well when I directly run it with `python run.py`\n\n\n`run.py` shown below\n\n\n\n```\nfrom daat import app\n\napp.run(host='0.0.0.0')\n\n```\n\nI guess it should be the pytest versions issue, because it used to work well without changing any related code, and I use `pip install pytest` without defined a specific version.\n\n\nAnd my backend runs well without pytest.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"I had the same problem. It is because `Werkzeug 3.0.0` was released and Flask doesn't specify the dependency correctly (requirements says `Werkzeug>=2.2.0`). This is why, `Werkzeug 3.0.0` is still installed and `Flask 2.2.2` isn't made for `Werkzeug 3.0.0`.\n\n\n**Solution**: Just set a fix version for Werkzeug such as `Werkzeug==2.2.2` in your `requirements.txt` and it should work."}
{"questionId":"daa3d402039e4f4688534bc38c0f25ab","question":"Disable macOS Sonoma Text Insertion Point (Cursor) \/ Caps-lock Indicator\nIs there any way to disable the new text cursor in macOS Sonoma? The caps lock indicator is kind of distracting.","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"other"},"answer":"Thanks to stephancasas for the explanation and the solution.\nAnother way of implementing it, from Terminal:\n\n\n\n```\nsudo mkdir -p \/Library\/Preferences\/FeatureFlags\/Domain\nsudo \/usr\/libexec\/PlistBuddy -c \"Add 'redesigned_text_cursor:Enabled' bool false\" \/Library\/Preferences\/FeatureFlags\/Domain\/UIKit.plist\n\n```\n\nReboot for the change to take effect."}
{"questionId":"b8bdbbb9004c4416816eb8fdeb5c82cd","question":"AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\nEarlier I installed some packages like [Matplotlib](https:\/\/en.wikipedia.org\/wiki\/Matplotlib), [NumPy](https:\/\/en.wikipedia.org\/wiki\/NumPy), pip (version 23.3.1), wheel (version 0.41.2), etc., and did some programming with those. I used the command `C:\\Users\\UserName>pip list` to find the list of packages that I have installed, and I am using Python 3.12.0 (by employing code `C:\\Users\\UserName>py -V`).\n\n\nI need to use [pyspedas](https:\/\/github.com\/spedas\/pyspedas) to analyse some data. I am following the instruction that that I received from site to install the package, with a variation (I am not sure whether it matters or not: I am using `py`, instead of `python`). The commands that I use, in the order, are:\n\n\n\n```\npy -m venv pyspedas\n.\\pyspedas\\Scripts\\activate\npip install pyspedas\n\n```\n\nAfter the last step, I am getting the following output:\n\n\n\n```\nCollecting pyspedas\n  Using cached pyspedas-1.4.47-py3-none-any.whl.metadata (14 kB)\nCollecting numpy>=1.19.5 (from pyspedas)\n  Using cached numpy-1.26.1-cp312-cp312-win_amd64.whl.metadata (61 kB)\nCollecting requests (from pyspedas)\n  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\nCollecting geopack>=1.0.10 (from pyspedas)\n  Using cached geopack-1.0.10-py3-none-any.whl (114 kB)\nCollecting cdflib<1.0.0 (from pyspedas)\n  Using cached cdflib-0.4.9-py3-none-any.whl (72 kB)\nCollecting cdasws>=1.7.24 (from pyspedas)\n  Using cached cdasws-1.7.43.tar.gz (21 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting netCDF4>=1.6.2 (from pyspedas)\n  Using cached netCDF4-1.6.5-cp312-cp312-win_amd64.whl.metadata (1.8 kB)\nCollecting pywavelets (from pyspedas)\n  Using cached PyWavelets-1.4.1.tar.gz (4.6 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... error\n  error: subprocess-exited-with-error\n\n  \u00d7 Getting requirements to build wheel did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [33 lines of output]\n      Traceback (most recent call last):\n        File \"C:\\Users\\UserName\\pyspedas\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n          main()\n        File \"C:\\Users\\UserName\\pyspedas\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n          json_out['return_val'] = hook(**hook_input['kwargs'])\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"C:\\Users\\UserName\\pyspedas\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 112, in get_requires_for_build_wheel\n          backend = _build_backend()\n                    ^^^^^^^^^^^^^^^^\n        File \"C:\\Users\\UserName\\pyspedas\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 77, in _build_backend\n          obj = import_module(mod_path)\n                ^^^^^^^^^^^^^^^^^^^^^^^\n        File \"C:\\Users\\UserName\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py\", line 90, in import_module\n          return _bootstrap._gcd_import(name[level:], package, level)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"<frozen importlib._bootstrap>\", line 1381, in _gcd_import\n        File \"<frozen importlib._bootstrap>\", line 1354, in _find_and_load\n        File \"<frozen importlib._bootstrap>\", line 1304, in _find_and_load_unlocked\n        File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n        File \"<frozen importlib._bootstrap>\", line 1381, in _gcd_import\n        File \"<frozen importlib._bootstrap>\", line 1354, in _find_and_load\n        File \"<frozen importlib._bootstrap>\", line 1325, in _find_and_load_unlocked\n        File \"<frozen importlib._bootstrap>\", line 929, in _load_unlocked\n        File \"<frozen importlib._bootstrap_external>\", line 994, in exec_module\n        File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n        File \"C:\\Users\\UserName\\AppData\\Local\\Temp\\pip-build-env-_lgbq70y\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 16, in <module>\n          import setuptools.version\n        File \"C:\\Users\\UserName\\AppData\\Local\\Temp\\pip-build-env-_lgbq70y\\overlay\\Lib\\site-packages\\setuptools\\version.py\", line 1, in <module>\n          import pkg_resources\n        File \"C:\\Users\\UserName\\AppData\\Local\\Temp\\pip-build-env-_lgbq70y\\overlay\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2191, in <module>\n          register_finder(pkgutil.ImpImporter, find_on_path)\n                          ^^^^^^^^^^^^^^^^^^^\n      AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n\u00d7 Getting requirements to build wheel did not run successfully.\n\u2502 exit code: 1\n\u2570\u2500> See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n\n```\n\nAfter little bit of googling, I came to know that this issues was reported at multiple places, but none for this package. I did install wheel in the new environment as mentioned in the answer [here](https:\/\/stackoverflow.com\/a\/56504270\/6323020), but the problem still persists.\n\n\nInstead of setting up a virtual environment, I simply executed the command `py -m pip install pyspedas`. But I am still getting the error.\n\n\nWhat I could gather is that the program has an issue with\n\n\n\n```\nCollecting pywavelets (from pyspedas)\n  Using cached PyWavelets-1.4.1.tar.gz (4.6 MB)\n  Installing build dependencies ... done\n\n```\n\nI am using [IDLE](https:\/\/en.wikipedia.org\/wiki\/IDLE) in Windows 11.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"###  If you can downgrade to Python 3.11 then you will probably not face any issue.\n\n\n\n\n---\n\n\n#### If you must use only Python 3.12, then here is an attempt to solve the issue:\n\n\n\n\n---\n\n\n\n> \n> AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n> \n> \n> \n\n\noccurs due to using Python 3.12.\n\n\nDue to the removal of the long-deprecated pkgutil.ImpImporter class, the pip command may not work for Python 3.12.\n\n\nHere is the link to a post which describes it: <https:\/\/ubuntuhandbook.org\/index.php\/2023\/10\/fix-broken-pip-python-312-ubuntu\/>\n\n\n**You just have to manually install pip for Python 3.12**\n\n\nThere are a couple of methods to fix this. In your virtual environment:\n\n\n\n```\npip install --upgrade setuptools\n\n```\n\n**Task 1:**\n\n\nPython comes with an `ensurepip`, which can install pip in a Python environment.\n\n\n<https:\/\/pip.pypa.io\/en\/stable\/installation\/>\n\n\nOn **Linux\/macOS** terminal:\n\n\n\n```\npython -m ensurepip --upgrade\n\n```\n\nOn **Windows**:\n\n\n\n```\npy -m ensurepip --upgrade\n\n```\n\n**Task 2:**\n\n\nYou need to install the [virtualenv](https:\/\/virtualenv.pypa.io\/en\/latest\/installation.html) package via:\n\n\n\n```\npython -m pip install --user virtualenv\n\n```\n\nIf it is already present then:\n\n\n\n```\npip install --upgrade virtualenv\n\n```\n\nThen create a new virtual environment by:\n\n\n\n```\nvirtualenv your_virtual_environment --python=python3.12\n\n```\n\nActivate `your_virtual_environment` and you will be ready to install packages.\n\n\n*Other points to note*:\n\n\n`virtualenv` likes to cache packages to save on loading up environments, including caching old versions of pip\n\n\nYou may have an old cached version of pip in your computer.\n\n\nYou may need to run:\n\n\n\n```\nvirtualenv --upgrade-embed-wheels\n\n```\n\nand\n\n\n\n```\nvirtualenv --reset-app-data <path_to_your_venv>\n\n```\n\nBetter upgrade `setuptools` as well:\n\n\n\n```\npython3.12 -m pip install --upgrade setuptools\n\n```\n\n<https:\/\/pythontest.com\/posts\/2023\/2023-10-02-py312-impimporter\/>\n\n\n\n\n---\n\n\nQuote from: <https:\/\/github.com\/readthedocs\/readthedocs.org\/pull\/10844>\n\n\n\n> \n> Having an outdated version of pip is still the main problem, but the outdated version of pip didn't come from the python installation, but from the virtualenv creation.\n> \n> \n> When creating the environment, virtualenv installs some specfic\/outdated [sic] versions of pip\/setuptools\/wheels.\n> \n> \n> \n\n\n[virtualenv](https:\/\/github.com\/pypa\/virtualenv\/blob\/20.7.2\/src\/virtualenv\/seed\/wheels\/embed\/__init__.py)\n\n\n\n\n---\n\n\nRelated gits:\n\n\n<https:\/\/github.com\/pypa\/pip\/issues\/11501>\n\n\n<https:\/\/github.com\/xlwings\/xlwings\/issues\/2342>\n\n\n<https:\/\/github.com\/pypa\/pip\/issues\/12179>\n\n\n<https:\/\/github.com\/readthedocs\/readthedocs.org\/issues\/10832>\n\n\n<https:\/\/github.com\/pypa\/setuptools\/issues\/3935>\n\n\n<https:\/\/github.com\/googleapis\/gapic-generator-python\/issues\/1824>"}
{"questionId":"dba3190467d046c494d2a49913e3bdae","question":"Flutter or React Native Xcode 15 Error (Xcode): DT\\_TOOLCHAIN\\_DIR cannot be used to evaluate LIBRARY\\_SEARCH\\_PATHS\nupon upgrading my Xcode today to version 15, I receive the following error when building my app for IOS:\n\n\n\n```\nError (Xcode): DT_TOOLCHAIN_DIR cannot be used to evaluate LIBRARY_SEARCH_PATHS, use TOOLCHAIN_DIR instead\n\n```\n\nIt appears some specific dependencies are causing this issue, in my case:\n\n\n- objectbox\\_flutter\\_libs: ^2.2.1\n- firebase\\_core: ^2.16.0\n\n\nI have no idea how to solve this issue, and I'll really appreciate any advice!\n\n\nThanks!","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"swift"},"answer":"***UPDATE: This issue is resolved in the latest [cocaopod version](https:\/\/github.com\/CocoaPods\/CocoaPods\/releases\/tag\/1.13.0).***\n\n\n\n> \n> Command to upgrade: brew upgrade cocoapods\n> \n> \n> \n\n\n***so if you install that you shouldn't need the solutions below:***\n\n\n**Solution 1:**\nAdd this code to your podfile\n\n\n\n```\npost_install do |installer|\n  installer.pods_project.targets.each do |target|\n     flutter_additional_ios_build_settings(target)\n      target.build_configurations.each do |config|\n        xcconfig_path = config.base_configuration_reference.real_path\n        xcconfig = File.read(xcconfig_path)\n        xcconfig_mod = xcconfig.gsub(\/DT_TOOLCHAIN_DIR\/, \"TOOLCHAIN_DIR\")\n        File.open(xcconfig_path, \"w\") { |file| file << xcconfig_mod }\n      end\n  end\nend\n\n```\n\nAfter that run a pod update (make sure that the iOS folder is the current directory before running pod update).\n\n\n\n> \n> pod update\n> \n> \n> \n\n\n  \n\n**Solution 2:** [Issues while building iOS project with flutter](https:\/\/stackoverflow.com\/questions\/76791937\/issues-while-building-ios-project-with-flutter)\n\n\nCheck out this for multi flavour config.\n\n\n  \n\n**Solution 3:**\n**if you use [inAppWebview](https:\/\/pub.dev\/packages\/flutter_inappwebview) for flutter then come error like this :**\n\n\nParse Issue (Xcode): Could not build module 'WebKit'\n\n\nSo you can add in Your pubspec.yaml file Like this:\n\n\n\n```\nflutter_inappwebview:\n    git:\n      url: https:\/\/github.com\/Estrelio\/flutter_inappwebview.git\n      ref: fix-xcode-17  \n\n```\n\n**Solution 4 (For non-flutter users):**\n\n\nRemove the below line from the script.\n\n\n\n> \n> flutter\\_additional\\_ios\\_build\\_settings(target)\n> \n> \n>"}
{"questionId":"9a51c67b6707418bb9a47c6d9f8bdbde","question":"Why is the simpler loop slower?\nCalled with `n = 10**8`, the simple loop is consistently significantly slower for me than the complex one, and I don't see why:\n\n\n\n```\ndef simple(n):\n    while n:\n        n -= 1\n\ndef complex(n):\n    while True:\n        if not n:\n            break\n        n -= 1\n\n```\n\nSome times in seconds:\n\n\n\n```\nsimple 4.340795516967773\ncomplex 3.6490490436553955\nsimple 4.374553918838501\ncomplex 3.639145851135254\nsimple 4.336690425872803\ncomplex 3.624480724334717\nPython: 3.11.4 (main, Sep  9 2023, 15:09:21) [GCC 13.2.1 20230801]\n\n```\n\nHere's the looping part of the bytecode as shown by `dis.dis(simple)`:\n\n\n\n```\n  6     >>    6 LOAD_FAST                0 (n)\n              8 LOAD_CONST               1 (1)\n             10 BINARY_OP               23 (-=)\n             14 STORE_FAST               0 (n)\n\n  5          16 LOAD_FAST                0 (n)\n             18 POP_JUMP_BACKWARD_IF_TRUE     7 (to 6)\n\n```\n\nAnd for `complex`:\n\n\n\n```\n 10     >>    4 LOAD_FAST                0 (n)\n              6 POP_JUMP_FORWARD_IF_TRUE     2 (to 12)\n\n 11           8 LOAD_CONST               0 (None)\n             10 RETURN_VALUE\n\n 12     >>   12 LOAD_FAST                0 (n)\n             14 LOAD_CONST               2 (1)\n             16 BINARY_OP               23 (-=)\n             20 STORE_FAST               0 (n)\n\n  9          22 JUMP_BACKWARD           10 (to 4)\n\n```\n\nSo it looks like the complex one does more work per iteration (two jumps instead of one). Then why is it faster?\n\n\nSeems to be a Python 3.11 phenomenon, see the comments.\n\n\nBenchmark script ([Attempt This Online!](https:\/\/ato.pxeger.com\/run?1=ZZBBDoIwEEXjtocws6MQIBI3hIQ7uHBnTIPahkaYklJUzuKGjR7K0wgUotG_mt--_E7__Vm1JlfYdY_GiCB-LZZCqxKMLDnIslLajDOZ5rqtCTlxAXV_UHCKbkKg1zWXBQe0ZhBCkEJk2aMa2NsPvNUN__BSACrznTDooHl2_ssUSoMAibCzW_jzC3vwYG0TDKTj4tQdraDRyvNiayot0VARMoZZyRnzJxICMC4h9trZjMUkjj98OrxwXUuFrq1pamtu7Q0)):\n\n\n\n```\nfrom time import time\nimport sys\n\ndef simple(n):\n    while n:\n        n -= 1\n\ndef complex(n):\n    while True:\n        if not n:\n            break\n        n -= 1\n\nfor f in [simple, complex] * 3:\n    t = time()\n    f(10**8)\n    print(f.__name__, time() - t)\n\nprint('Python:', sys.version)","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"python"},"answer":"I checked the source code of the bytecode (python 3.11.6) and found that in the decompiled bytecode, it seems that only `JUMP_BACKWARD` will execute a warmup function, which will trigger [specialization](https:\/\/docs.python.org\/3.11\/whatsnew\/3.11.html#pep-659-specializing-adaptive-interpreter) in python 3.11 when executed enough times:\n\n\n\n```\nPyObject* _Py_HOT_FUNCTION\n_PyEval_EvalFrameDefault(PyThreadState *tstate, _PyInterpreterFrame *frame, int throwflag)\n{\n    \/* ... *\/\n        TARGET(JUMP_BACKWARD) {\n            _PyCode_Warmup(frame->f_code);\n            JUMP_TO_INSTRUCTION(JUMP_BACKWARD_QUICK);\n        }\n    \/* ... *\/\n}\n\n```\n\n\n```\nstatic inline void\n_PyCode_Warmup(PyCodeObject *code)\n{\n    if (code->co_warmup != 0) {\n        code->co_warmup++;\n        if (code->co_warmup == 0) {\n            _PyCode_Quicken(code);\n        }\n    }\n}\n\n```\n\nAmong **all** bytecodes, only `JUMP_BACKWARD` and `RESUME` will call `_PyCode_Warmup()`.\n\n\nSpecialization appears to speed up multiple bytecodes used, resulting in a significant increase in speed:\n\n\n\n```\nvoid\n_PyCode_Quicken(PyCodeObject *code)\n{\n    \/* ... *\/\n            switch (opcode) {\n                case EXTENDED_ARG:  \/* ... *\/\n                case JUMP_BACKWARD: \/* ... *\/\n                case RESUME:        \/* ... *\/\n                case LOAD_FAST:     \/* ... *\/\n                case STORE_FAST:    \/* ... *\/\n                case LOAD_CONST:    \/* ... *\/\n            }\n    \/* ... *\/\n}\n\n```\n\nAfter executing once, the bytecode of `complex` changed, while `simple` did not:\n\n\n\n```\nIn [_]: %timeit -n 1 -r 1 complex(10 ** 8)\n2.7 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n\nIn [_]: dis(complex, adaptive=True)\n  5           0 RESUME_QUICK             0\n\n  6           2 NOP\n\n  7           4 LOAD_FAST                0 (n)\n              6 POP_JUMP_FORWARD_IF_TRUE     2 (to 12)\n\n  8           8 LOAD_CONST               0 (None)\n             10 RETURN_VALUE\n\n  9     >>   12 LOAD_FAST__LOAD_CONST     0 (n)\n             14 LOAD_CONST               2 (1)\n             16 BINARY_OP_SUBTRACT_INT    23 (-=)\n             20 STORE_FAST               0 (n)\n\n  6          22 JUMP_BACKWARD_QUICK     10 (to 4)\n\n\n```\n\n\n```\nIn [_]: %timeit -n 1 -r 1 simple(10 ** 8)\n4.78 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n\nIn [_]: dis(simple, adaptive=True)\n  1           0 RESUME                   0\n\n  2           2 LOAD_FAST                0 (n)\n              4 POP_JUMP_FORWARD_IF_FALSE     9 (to 24)\n\n  3     >>    6 LOAD_FAST                0 (n)\n              8 LOAD_CONST               1 (1)\n             10 BINARY_OP               23 (-=)\n             14 STORE_FAST               0 (n)\n\n  2          16 LOAD_FAST                0 (n)\n             18 POP_JUMP_BACKWARD_IF_TRUE     7 (to 6)\n             20 LOAD_CONST               0 (None)\n             22 RETURN_VALUE\n        >>   24 LOAD_CONST               0 (None)\n             26 RETURN_VALUE"}
{"questionId":"2a8c6d4071394d7db3c47fe57c3c9ef9","question":"VSCode workspace settings change on its own\nI am using eslint, prettier, vite for a React project.\n\n\nI have set my VSCode workspace settings for `\"source.fixAll.eslint\"` and `\"source.organizeImports\"` to `true`. But it keeps changing to `\"explicit\"` automatically, for example when I open VSCode or when I'm coding.\n\n\nWhat might be the cause?\n\n\n\n```\n\/\/ .vscode\/settings.json (workspace settings)\n\n{\n  \"editor.codeActionsOnSave\": {\n    \"source.fixAll.eslint\": true, \/\/ keeps automatically changing to \"explicit\"\n    \"source.organizeImports\": true \/\/ keeps automatically changing to \"explicit\"\n  },\n  \"editor.formatOnSave\": true,\n  \"editor.wordWrap\": \"on\",\n  \"editor.defaultFormatter\": \"esbenp.prettier-vscode\"\n}\n\n```\n\n\n```\n\/\/ package.json devDependencies\n\n\"devDependencies\": {\n  \"@testing-library\/react\": \"^14.0.0\",\n  \"@types\/jest\": \"^29.5.6\",\n  \"@types\/react\": \"^18.2.15\",\n  \"@types\/react-dom\": \"^18.2.7\",\n  \"@typescript-eslint\/eslint-plugin\": \"^6.0.0\",\n  \"@typescript-eslint\/parser\": \"^6.0.0\",\n  \"@vitejs\/plugin-react\": \"^4.0.3\",\n  \"eslint\": \"^8.45.0\",\n  \"eslint-plugin-prettier\": \"^5.0.1\",\n  \"eslint-plugin-react\": \"^7.33.2\",\n  \"eslint-plugin-react-hooks\": \"^4.6.0\",\n  \"eslint-plugin-react-refresh\": \"^0.4.3\",\n  \"husky\": \"^8.0.3\",\n  \"jest\": \"^29.7.0\",\n  \"jest-environment-jsdom\": \"^29.7.0\",\n  \"lint-staged\": \"^15.0.2\",\n  \"msw\": \"^1.3.2\",\n  \"prettier\": \"^3.0.3\",\n  \"ts-jest\": \"^29.1.1\",\n  \"typescript\": \"^5.0.2\",\n  \"vite\": \"^4.4.5\",\n  \"vite-tsconfig-paths\": \"^4.2.1\"\n},\n\n```\n\n\n```\n\/\/ .eslintrc.cjs\n\nmodule.exports = {\n  root: true,\n  env: { browser: true, es2020: true },\n  extends: [\n    \"eslint:recommended\",\n    \"plugin:@typescript-eslint\/recommended\",\n    \"plugin:react-hooks\/recommended\",\n  ],\n  ignorePatterns: [\"dist\", \".eslintrc.cjs\"],\n  parser: \"@typescript-eslint\/parser\",\n  plugins: [\"react-refresh\", \"eslint-plugin-react\", \"prettier\"],\n  rules: {\n    \"react-refresh\/only-export-components\": [\n      \"warn\",\n      { allowConstantExport: true },\n    ],\n    \"react\/self-closing-comp\": [\n      \"error\",\n      {\n        component: true,\n        html: true,\n      },\n    ],\n    \"prettier\/prettier\": \"error\",\n    \"no-console\": \"error\",\n  },\n};\n\n```\n\n\n```\n\/\/ .prettierrc.json\n\n{\n  \"singleQuote\": false,\n  \"quoteProps\": \"consistent\",\n  \"printWidth\": 80,\n  \"tabWidth\": 2,\n  \"bracketSameLine\": true,\n  \"bracketSpacing\": true,\n  \"arrowParens\": \"always\",\n  \"trailingComma\": \"es5\",\n  \"semi\": true,\n  \"endOfLine\": \"auto\",\n  \"eslintIntegration\": false\n}","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"javascript"},"answer":"Turns out, VSCode `true` is set to `\"explicit\"` since VSCode 1.85.0. It seems boolean values were supported until 1.84.0.\n\n\nReferences\n\n\n<https:\/\/code.visualstudio.com\/updates\/v1_85#_code-actions-on-save-and-auto>\n<https:\/\/code.visualstudio.com\/updates\/v1_83#_code-actions-on-save-and-auto-save>"}
{"questionId":"3f357b97b7384032b5c85ce01486d575","question":"Why doesn't App Module exist in Angular 17?\nI have downloaded with npm the new version of Angular and I haven't seen the *app.module.ts* file.\n\n\nHas it been eliminated from the project structure?\n\n\n[Visual Studio Code files](https:\/\/i.stack.imgur.com\/4CpjW.png).\n\n\nI have tried creating a new one. It seems odd.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"typescript"},"answer":"From Angular v17 onwards, Standalone is now the new default for the CLI.\n\n\nSo when you create a new project, you won't have any modules in it if you don't specify anything.\n\n\nHowever, it is still possible to create a module-based app by using the `--no-standalone` flag : `ng new --no-standalone`\n\n\nStandalone components are a feature introduced in v14. With the change in v17, the Angular team [strongly recommends to use them](https:\/\/blog.angular.io\/introducing-angular-v17-4d7033312e4b#586d) as they are easier to use, understand and are require less boilerplate.\n\n\nFor tutorials using standalone components, I advise you to get a look at [the new documentation site](https:\/\/angular.dev)."}
{"questionId":"ccbaec9a8ac5469391309cdef8ca5dca","question":"After updating Cocoapods to 1.13.0 it throws error\nI updated CocoaPods to version `1.13.0`. Now, when I run `pod install`, it throws the following error:\n\n\n\n```\nconversions.rb:108:in '<class:Array>': undefined method 'deprecator' for ActiveSupport:Module (NoMethodError)\n\n```\n\nHow can I fix it?\n\n\nHere's the full error:\n\n\n\n```\nbundler: failed to load command: pod (\/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/bin\/pod)\n\/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/gems\/3.2.0\/gems\/activesupport-7.1.0\/lib\/active_support\/core_ext\/array\/conversions.rb:108:in `<class:Array>': undefined method `deprecator' for ActiveSupport:Module (NoMethodError)\n\n  deprecate to_default_s: :to_s, deprecator: ActiveSupport.deprecator\n                                                          ^^^^^^^^^^^\nDid you mean?  deprecate_constant\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/gems\/3.2.0\/gems\/activesupport-7.1.0\/lib\/active_support\/core_ext\/array\/conversions.rb:8:in `<top (required)>'\n    from <internal:\/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/3.2.0\/rubygems\/core_ext\/kernel_require.rb>:37:in `require'\n    from <internal:\/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/3.2.0\/rubygems\/core_ext\/kernel_require.rb>:37:in `require'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/gems\/3.2.0\/gems\/cocoapods-1.13.0\/lib\/cocoapods.rb:9:in `<top (required)>'\n    from <internal:\/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/3.2.0\/rubygems\/core_ext\/kernel_require.rb>:37:in `require'\n    from <internal:\/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/3.2.0\/rubygems\/core_ext\/kernel_require.rb>:37:in `require'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/gems\/3.2.0\/gems\/cocoapods-1.13.0\/bin\/pod:36:in `<top (required)>'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/bin\/pod:25:in `load'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/bin\/pod:25:in `<top (required)>'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/3.2.0\/bundler\/cli\/exec.rb:58:in `load'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/3.2.0\/bundler\/cli\/exec.rb:58:in `kernel_load'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/3.2.0\/bundler\/cli\/exec.rb:23:in `run'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/3.2.0\/bundler\/cli.rb:492:in `exec'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/3.2.0\/bundler\/vendor\/thor\/lib\/thor\/command.rb:27:in `run'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/3.2.0\/bundler\/vendor\/thor\/lib\/thor\/invocation.rb:127:in `invoke_command'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/3.2.0\/bundler\/vendor\/thor\/lib\/thor.rb:392:in `dispatch'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/3.2.0\/bundler\/cli.rb:34:in `dispatch'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/3.2.0\/bundler\/vendor\/thor\/lib\/thor\/base.rb:485:in `start'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/3.2.0\/bundler\/cli.rb:28:in `start'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/gems\/3.2.0\/gems\/bundler-2.4.10\/libexec\/bundle:45:in `block in <top (required)>'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/3.2.0\/bundler\/friendly_errors.rb:117:in `with_friendly_errors'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/lib\/ruby\/gems\/3.2.0\/gems\/bundler-2.4.10\/libexec\/bundle:33:in `<top (required)>'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/bin\/bundle:25:in `load'\n    from \/opt\/homebrew\/opt\/bin\/rbenv\/versions\/3.2.2\/bin\/bundle:25:in `<main>'","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"other"},"answer":"This happens with the installation of CocoaPods version `1.13.0`. The issue has been reported in the CocoaPods project [here](https:\/\/%20https:\/\/github.com\/CocoaPods\/CocoaPods\/issues\/12080).\n\n\nYou can fix the issue by downgrading the [activesupport](https:\/\/rubygems.org\/gems\/activesupport) gem to version `7.0.8`. Run the following command in Terminal to begin with:\n\n\n\n```\nsudo gem uninstall activesupport\n\n```\n\nThis may ask you to select the version to uninstall. Select the version if so.\n\n\nThen install the version which works by running the following command in Terminal:\n\n\n\n```\nsudo gem install activesupport -v 7.0.8"}
{"questionId":"0b558293aa8c45a89f71efaec6798a03","question":"Why list comprehensions create a function internally?\nThis is disassembly of a list comprehension in [python-3.10](\/questions\/tagged\/python-3.10 \"show questions tagged 'python-3.10'\"):\n\n\n\n```\nPython 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import dis\n>>> \n>>> dis.dis(\"[True for _ in ()]\")\n  1           0 LOAD_CONST               0 (<code object <listcomp> at 0x7fea68e0dc60, file \"<dis>\", line 1>)\n              2 LOAD_CONST               1 ('<listcomp>')\n              4 MAKE_FUNCTION            0\n              6 LOAD_CONST               2 (())\n              8 GET_ITER\n             10 CALL_FUNCTION            1\n             12 RETURN_VALUE\n\nDisassembly of <code object <listcomp> at 0x7fea68e0dc60, file \"<dis>\", line 1>:\n  1           0 BUILD_LIST               0\n              2 LOAD_FAST                0 (.0)\n        >>    4 FOR_ITER                 4 (to 14)\n              6 STORE_FAST               1 (_)\n              8 LOAD_CONST               0 (True)\n             10 LIST_APPEND              2\n             12 JUMP_ABSOLUTE            2 (to 4)\n        >>   14 RETURN_VALUE\n\n```\n\nFrom what I understand it creates a code object called `listcomp` which does the actual iteration and return the result list, and immediately call it.\nI can't figure out the need to create a separate function to execute this job. Is this kind of an optimization trick?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"python"},"answer":"The main logic of creating a function is to [isolate the comprehension\u2019s iteration variable](https:\/\/peps.python.org\/pep-0709\/)peps.python.org.\n\n\nBy creating a function:\n\n\n\n> \n> Comprehension iteration variables remain **isolated** and don\u2019t overwrite\n> a variable of the same name in the outer scope, nor are they visible\n> after the comprehension\n> \n> \n> \n\n\nHowever, this is *inefficient at runtime*. Due to this reason, [python-3.12](\/questions\/tagged\/python-3.12 \"show questions tagged 'python-3.12'\") implemented an optimization called [comprehension inlining(PEP 709)](https:\/\/docs.python.org\/3\/whatsnew\/3.12.html#pep-709-comprehension-inlining)peps.python.org which will no longer [create a separate code object](https:\/\/peps.python.org\/pep-0709\/#specification)peps.python.org.\n\n\n\n> \n> Dictionary, list, and set comprehensions are now inlined, **rather than\n> creating a new single-use function object for each execution of the\n> comprehension.** This **speeds up execution of a comprehension by up to\n> two times**. See *PEP 709* for further details.\n> \n> \n> \n\n\nHere is the output for the same code disassembled with [python-3.12](\/questions\/tagged\/python-3.12 \"show questions tagged 'python-3.12'\"):\n\n\n\n```\n>>> import dis\n>>> \n>>> dis.dis(\"[True for _ in ()]\")\n  0           0 RESUME                   0\n\n  1           2 LOAD_CONST               0 (())\n              4 GET_ITER\n              6 **LOAD\\_FAST\\_AND\\_CLEAR**      0 (_)\n              8 SWAP                     2\n             10 BUILD_LIST               0\n             12 SWAP                     2\n        >>   14 FOR_ITER                 4 (to 26)\n             18 STORE_FAST               0 (_)\n             20 LOAD_CONST               1 (True)\n             22 LIST_APPEND              2\n             24 JUMP_BACKWARD            6 (to 14)\n        >>   26 END_FOR\n             28 SWAP                     2\n             30 **STORE\\_FAST**               0 (_)\n             32 RETURN_VALUE\n        >>   34 SWAP                     2\n             36 POP_TOP\n             38 SWAP                     2\n             40 STORE_FAST               0 (_)\n             42 RERAISE                  0\nExceptionTable:\n  10 to 26 -> 34 [2]\n```\n\nAs you can see, there is no longer a `MAKE_FUNCTION` opcode nor a separate *code object.* Instead [python-3.12](\/questions\/tagged\/python-3.12 \"show questions tagged 'python-3.12'\") uses [`LOAD_FAST_AND_CLEAR`](https:\/\/docs.python.org\/3\/library\/dis.html#opcode-LOAD_FAST_AND_CLEAR)docs.python.org(at offset `6`) and `STORE_FAST`(at offset `30`) opcodes to provide the isolation for the iteration variable.\n\n\nQuoting from the [*Specification* section](https:\/\/peps.python.org\/pep-0709\/#specification)peps.python.org of the PEP 709:\n\n\n\n> \n> Isolation of the `x` iteration variable is achieved by the combination\n> of the new `LOAD_FAST_AND_CLEAR` opcode at offset `6`, which saves any\n> outer value of `x` on the stack before running the comprehension, and `30`\n> `STORE_FAST`, which restores the outer value of `x` (if any) after running\n> the comprehension.\n> \n> \n> \n\n\nIn addition to that, in [python-3.12](\/questions\/tagged\/python-3.12 \"show questions tagged 'python-3.12'\") [*there is no longer a separate frame for the comprehension in tracebacks*](https:\/\/docs.python.org\/3\/whatsnew\/3.12.html#pep-709-comprehension-inlining).\n\n\n\n\n\n| Traceback in <[python-3.12](\/questions\/tagged\/python-3.12 \"show questions tagged 'python-3.12'\") | Traceback in [python-3.12](\/questions\/tagged\/python-3.12 \"show questions tagged 'python-3.12'\") |\n| --- | --- |\n| \n```\n>>> [1 \/ 0 for i in range(10)]Traceback (most recent call last): File \"<stdin>\", line 1, in <module> *File \"<stdin>\", line 1, in <listcomp>*ZeroDivisionError: division by zero\n```\n | \n```\n>>> [1 \/ 0 for i in range(10)]Traceback (most recent call last): File \"<stdin>\", line 1, in <module>ZeroDivisionError: division by zero\n```\n |\n\n\n\nAnd here is the [benchmark results](https:\/\/peps.python.org\/pep-0709\/#reference-implementation)peps.python.org(measured with MacOS M2):\n\n\n\n```\n$ python3.10 -m pyperf timeit -s 'l = [1]' '[x for x in l]'\nMean +- std dev: 108 ns +- 3 ns\n$ python3.12 -m pyperf timeit -s 'l = [1]' '[x for x in l]'\nMean +- std dev: 60.9 ns +- 0.3 ns"}
{"questionId":"f01f263f242c40a1977e9b6384e5ad33","question":"One of RECEIVER\\_EXPORTED or RECEIVER\\_NOT\\_EXPORTED should be specified when a receiver isn't being registered exclusively for system broadcasts\nIn lib com.thingclips.smart of tuya have 1 error One of RECEIVER\\_EXPORTED or RECEIVER\\_NOT\\_EXPORTED should be specified when a receiver isn't being registered exclusively for system broadcasts of but I can't fix it.This is version 5.1.0 .\nI encountered this problem on Android 14 During runtime.Can you help me?\n\n\n+I found the latest version which is 5.1.0 but it didn't solve the problem.\n+\nTuya library also does not mention this on Android 14.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"java"},"answer":"As discussed at Google I\/O 2023, registering receivers with intention using the RECEIVER\\_EXPORTED \/ RECEIVER\\_NOT\\_EXPORTED flag was introduced as part of Android 13 and is now a requirement for apps running on Android 14 or higher (U+).\n\n\nIf you do not implement this, the system will throw a security exception.\n\n\nTo allow the broadcast receiver to receive broadcasts from other apps, register the receiver using the following code:\n\n\n\n```\ncontext.registerReceiver(broadcastReceiver, intentFilter, RECEIVER_EXPORTED);\n\n```\n\nTo register a broadcast receiver that does not receive broadcasts from other apps, including system apps, register the receiver using the following code:\n\n\n\n```\ncontext.registerReceiver(broadcastReceiver, intentFilter, RECEIVER_NOT_EXPORTED);\n\n```\n\nNote: That call will need minSdkVersion to be 26 (Android 8) al least\n\n\nCheck [https:\/\/www.delasign.com\/blog\/android-studio-kotlin-broadcast-recievers-export-or-not\/#:~:text=As%20discussed%20at%20Google%20I,will%20throw%20a%20security%20exception](https:\/\/www.delasign.com\/blog\/android-studio-kotlin-broadcast-recievers-export-or-not\/#:%7E:text=As%20discussed%20at%20Google%20I,will%20throw%20a%20security%20exception)."}
{"questionId":"25fc62228bfd414ab9373122df8ad0c9","question":"Why is a stray semicolon no longer detected by `-pedantic` modern compilers?\nThe following snippet generates compilation errors on adding [`-pedantic`](https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/Warning-Options.html#index-pedantic-1) and [`-Werror`](https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/Warning-Options.html#index-Werror) on compilers that are a bit old.\n\n\n\n```\n#include <cstdint>\n#include <iostream>\n\nint add(int a, int b){\n    return a + b;\n}; \/\/ <-- stray semicolon\n\nint main (){\n    return 0;\n}\n\n\n```\n\nHowever this does not happen newer compiler versions. Please find a matrix of GCC ([10.x](https:\/\/gcc.gnu.org\/releases.html) and [11.x](https:\/\/gcc.gnu.org\/gcc-11\/)) and Clang (5.x, 6.x) demonstrating the difference at <https:\/\/godbolt.org\/z\/KWeb8WTxz>.\n\n\nI have two parts to my question:\n\n\n1. Why is this not triggered in recent compilers?\n2. Is it possible to enable the old behaviour in recent versions of Clang or GCC?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"cpp"},"answer":"Starting in C++11, extra semicolons `;` (aka *[empty-declaration](https:\/\/eel.is\/c++draft\/dcl.pre#nt:empty-declaration)s*) at the global level are valid. I believe this is occasionally useful for writing macros.\n\n\nAs such, GCC 11 removed `-pedantic` diagnostics for an extra `;` when [`-std=c++11`](https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/C-Dialect-Options.html#index-std-1) or later is used. See:\n\n\n- [[GCC Bug 96068] Extra semicolon outside of a function should be allowed after c++11?](https:\/\/gcc.gnu.org\/bugzilla\/show_bug.cgi?id=96068).\n- [[GCC Bugs Mailing list] -Wpedantic doesn't warn about extra semicolons anymore](https:\/\/gcc.gnu.org\/pipermail\/gcc-bugs\/2021-November\/763193.html)\n- [[CWG 569] Spurious semicolons at namespace scope should be allowed](https:\/\/cplusplus.github.io\/CWG\/issues\/569.html)\n\n\nYou can restore the old behavior by using a C++ standard older than C++11. Both GCC 11 and clang 6 will emit the old diagnostics if you pass `-std=c++03`.\n\n\nAlternatively, recent versions of both GCC and Clang support the warning option [`-Wextra-semi`](https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/C_002b_002b-Dialect-Options.html#index-Wextra-semi) which specifically warns about redundant semicolons. Thanks to HolyBlackCat for mentioning this."}
{"questionId":"7130df395ecd46c891ceebc192c00520","question":"Why are all of my \"this\" pointers the same value?\nI'm working on the following code snippet:\n\n\n\n```\n#include <iostream>                                                                                                                                                                                            \n#include <vector>\n\nclass myclass\n{\n    public:\n        myclass()\n        {\n            std::cout << this << std::endl;\n        }\n};\n\nint main()\n{\n    std::vector<myclass> v;\n    for(uint32_t i = 0; i < 10; i++)\n        v.push_back(myclass());\n    return 0;\n}\n\n```\n\nI'm compiling the code using `g++ main.cpp`. When I execute the compiled binary, I get:\n\n\n\n```\n0x7ffebb8f8cab\n0x7ffebb8f8cab\n0x7ffebb8f8cab\n0x7ffebb8f8cab\n0x7ffebb8f8cab\n0x7ffebb8f8cab\n0x7ffebb8f8cab\n0x7ffebb8f8cab\n0x7ffebb8f8cab\n0x7ffebb8f8cab\n\n```\n\nMy question is why are all the `this` pointers identical? If I'm creating 10 different objects of the same class, there should be 10 distinct `this` pointers. Right?\n\n\nAs far as I understand, my code is currently using the references of the same objects to populate the vector `v`. However, what I want are 10 distinct objects of `myclass`. How can I get this? This code is a part of a larger project and that project has some issues with `new` and `delete`. So I cannot use that API. What am I doing wrong and how can I fix this?","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"cpp"},"answer":"As noted in comments, you're seeing the `this` pointer for a temporary and then copying the temporary into the vector. Since the temporary is *temporary* your system is reusing the same memory location on each loop iteration.\n\n\nHowever, if you print the `this` pointer for the vector elements, you will notice they are placed in sequential memory locations as you would expect for a vector.\n\n\n\n```\n#include <iostream>                                                                                                                                                                                            \n#include <vector>\n\nstruct myclass {\n    myclass() {\n        std::cout << this << std::endl;\n    }\n\n    void print() const { \n        std::cout << this << std::endl; \n    }\n};\n\nint main() {\n    std::vector<myclass> v;\n    for (uint32_t i = 0; i < 10; i++)\n        v.push_back(myclass());\n\n    std::cout << \"\\n\";\n\n    for (auto &x : v) \n        x.print();\n        \n    return 0;\n}\n\n```\n\nOutput:\n\n\n\n```\n0x7ff7bfe6f5a8\n0x7ff7bfe6f5a8\n0x7ff7bfe6f5a8\n0x7ff7bfe6f5a8\n0x7ff7bfe6f5a8\n0x7ff7bfe6f5a8\n0x7ff7bfe6f5a8\n0x7ff7bfe6f5a8\n0x7ff7bfe6f5a8\n0x7ff7bfe6f5a8\n\n0x600001f4c030\n0x600001f4c031\n0x600001f4c032\n0x600001f4c033\n0x600001f4c034\n0x600001f4c035\n0x600001f4c036\n0x600001f4c037\n0x600001f4c038\n0x600001f4c039\n\n```\n\nThe [`emplace_back`](https:\/\/en.cppreference.com\/w\/cpp\/container\/vector\/emplace_back) member function of `std::vector` may be used to construct objects in-place in a vector.\n\n\nIn the following we construct 5 temporaries which we can see reuse the same memory, and then we use `push_back` to place them in the vector.\n\n\nThen we use `emplace_back` to construct five objects directly in the vector.\n\n\nWe can see that the memory address printed when the objects are cerated are the same as those printed later.\n\n\nNote: vectors may reallocate and move their contents if size exceeds capacity. To avoid this in this case I've reserved a capacity of 10 for the vector `v`.\n\n\n\n```\n#include <iostream>                                                                                                                                                                                            \n#include <vector>\n\nstruct myclass {\n    myclass() {\n        std::cout << this << std::endl;\n    }\n\n    void print() const { \n        std::cout << this << std::endl; \n    }\n};\n\nint main() {\n    std::vector<myclass> v;\n\n    v.reserve(10);\n\n    for (uint32_t i = 0; i < 5; i++)\n        v.push_back(myclass());\n\n    for (uint32_t i = 0; i < 5; i++) \n        v.emplace_back();\n\n    std::cout << \"\\n\";\n\n    for (auto &x : v) \n        x.print();\n        \n    return 0;\n}\n\n```\n\nOutput:\n\n\n\n```\n0x7ff7bf57b598\n0x7ff7bf57b598\n0x7ff7bf57b598\n0x7ff7bf57b598\n0x7ff7bf57b598\n0x600002610035\n0x600002610036\n0x600002610037\n0x600002610038\n0x600002610039\n\n0x600002610030\n0x600002610031\n0x600002610032\n0x600002610033\n0x600002610034\n0x600002610035\n0x600002610036\n0x600002610037\n0x600002610038\n0x600002610039"}
{"questionId":"abe1768cba83458b9464446daccb0d68","question":"man sscanf: %d is deprecated in C or glibc?\nI was just reading the glibc `sscanf` [man page](https:\/\/man7.org\/linux\/man-pages\/man3\/sscanf.3.html) (from the Linux man-pages package) and I found the following:\n\n\n\n> \n> The following conversion specifiers are available:  \n> \n> (...)\n> \n> \n> `d` \u00a0\u00a0 *Deprecated*. Matches an optionally signed decimal integer; the\n> next pointer must be a pointer to `int`.\n> \n> \n> `i` \u00a0\u00a0 *Deprecated*. Matches an optionally signed integer; the next\n> pointer must be a pointer to `int`. The integer is read in base\n> 16 if it begins with `0x` or `0X`, in base 8 if it begins with `0`,\n> and in base 10 otherwise. Only characters that correspond to\n> the base are used.\n> \n> \n> `o` \u00a0\u00a0 *Deprecated*. Matches an unsigned octal integer; the next pointer\n> must be a pointer to `unsigned int`.\n> \n> \n> (...)\n> \n> \n> \n\n\n- How come `%d` is deprecated? It seem that all `int` specifiers are deprecated.\n- What does it mean and what is there to replace them?","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"c"},"answer":"As pointed in the comments (thanks to @JeffHolt, @Eugene-sh, @DanielWalker, @Barmar, @DanielWalker) , the answer is indeed in the Bugs section:\n\n\n\n```\nBUGS\n   Numeric conversion specifiers\n       Use of the numeric conversion specifiers produces Undefined\n       Behavior for invalid input.  See C11 7.21.6.2\/10 \n       \u27e8https:\/\/port70.net\/%7Ensz\/c\/c11\/n1570.html#7.21.6.2p10\u27e9.  This is\n       a bug in the ISO C standard, and not an inherent design issue\n       with the API.  However, current implementations are not safe from\n       that bug, so it is not recommended to use them.  Instead,\n       programs should use functions such as strtol(3) to parse numeric\n       input.  This manual page deprecates use of the numeric conversion\n       specifiers until they are fixed by ISO C.\n\n```\n\nI do agree that of \"deprecate\" means here \"express disapproval of\" (as from @Barmar's comment)."}
{"questionId":"674b4f0db34d42209b51dba8c801a0a4","question":"What is the correct output of sizeof(\"string\")?\nOn a microcontroller, in order to avoid loading settings from a previous firmware build, I also store the compilation time, which is checked at loading.\n\n\nThe microcontroller project is build with **'mikroC PRO for ARM'** from **MikroElektronika**.\n\n\nBeing easier to debug, I programmed the code with **minGW** on my PC and, after checking it left and right put, it into microC.\n\n\nThe code using that check failed to work properly. After an evening of frustrating debugging I, found ***`sizeof(\"...\")` yielding different values on the two platforms*** and causing a buffer overflow as a consequence.\n\n\nBut now I don't know whose fault is it.\n\n\nTo re-create the problem, use following code:\n\n\n\n```\n#define SAVEFILECHECK_COMPILE_DATE __DATE__ \" \" __TIME__\n\nchar strA[sizeof(SAVEFILECHECK_COMPILE_DATE)];\nchar strB[] = SAVEFILECHECK_COMPILE_DATE;\n\nprintf(\"sizeof(#def): %d\\n\", (int)sizeof(SAVEFILECHECK_COMPILE_DATE));\nprintf(\"sizeof(strA): %d\\n\", (int)sizeof(strA));\nprintf(\"sizeof(strB): %d\\n\", (int)sizeof(strB));\n\n```\n\nOn MinGW it returns (as expected):\n\n\n\n```\nsizeof(#def): 21\nsizeof(strA): 21\nsizeof(strB): 21\n\n```\n\nHowever, on 'mikroC PRO for ARM' it returns:\n\n\n\n```\nsizeof(#def): 20\nsizeof(strA): 20\nsizeof(strB): 21\n\n```\n\nThis difference caused a buffer overflow down the line (overwriting byte zero of a pointer \u2013 ouch).\n\n\n21 is the answer I expect: 20 chars and the '\\0' terminator.\n\n\nIs this one of the 'it depends' things in C or is there a violation of the `sizeof` operator behavior?","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"c"},"answer":"This is all 100% standardized. C17 6.10.8.1:\n\n\n\n> \n> `__DATE__` The date of translation of the preprocessing translation unit: a character string literal of the form `\"Mmm dd yyyy\"` ... and the first character of `dd` is a space character if the value is\n> less than 10.  \n> \n> ...  \n> \n> `__TIME__` The time of translation of the preprocessing translation unit: a character string literal of\n> the form `\"hh:mm:ss\"`\n> \n> \n> \n\n\n- \"Mmm dd yyyy\" = 11\n- \"hh:mm:ss\" = 8\n- `\" \"` (the space you used for string literal concatenation) = 1\n- Null termination = 1\n\n\n11 + 8 + 1 + 1 = 21\n\n\nAs for `sizeof`, a string literal is an array. Whenever you pass a declared array to `sizeof`, the array does not \"decay\" into a pointer to the first element, so `sizeof` will report the size of the array in bytes. In case of string literals, this includes the null termination, C17 6.4.5:\n\n\n\n> \n> In translation phase 7, a byte or code of value zero is appended to each multibyte character sequence that results from a string literal or literals. The multibyte character sequence is then used to\n> initialize an array of static storage duration and length just sufficient to contain the sequence. For character string literals, the array elements have type `char`, and are initialized with the individual\n> bytes of the multibyte character sequence.\n> \n> \n> \n\n\n(Translation phase 6 is also mentioned, which is the string literal concatenation phase. I.e string literal concatenation is guaranteed to happen before null termination is added.)\n\n\nSo it would appear that mikroC PRO is non-conforming\/bugged. There's lots of questionable embedded systems compilers out there for sure."}
{"questionId":"a81d7d0fa3ce4f6587e2f3649025803a","question":"Is there a possibility when calling .ToUpper() that the new string requires more memory?\nI want to use the the following function in the MemoryExtensions namespace\n\n\n\n```\npublic static int ToUpper(this ReadOnlySpan<char> source, Span<char> destination, CultureInfo? culture)\n\n```\n\nMy question now is: am I always safe when destination Span has the length of the source span? e.g.\n\n\n\n```\ndestination = stackalloc char[source.Length];\n\n```\n\nIf no, can someone provide an example which string converts to a larger string when calling ToUpper on in (including which culture)?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"csharp"},"answer":"[`MemoryExtensions.ToUpper`](https:\/\/learn.microsoft.com\/en-us\/dotnet\/api\/system.memoryextensions.toupper?view=net-7.0#returns) returns `-1` if the destination is too small.\n\n\nThe [source code for `ToUpper`](https:\/\/github.com\/dotnet\/runtime\/blob\/a5b75b8179d7db4ff720e19233d2b99fd15b94f0\/src\/libraries\/System.Private.CoreLib\/src\/System\/MemoryExtensions.Globalization.cs#L249) has this gem:\n\n\n\n```\n\/\/ Assuming that changing case does not affect length\nif (destination.Length < source.Length)\n    return -1;\n\n```\n\nThere is no other point where `-1` is returned, the function finishes with `return source.Length;`\n\n\nSo they've assumed it can't happen. Whether they're right is another question: if you find a counter-example I suggest you file a bug report on GitHub.\n\n\nThe [docs for `TextInfo`](https:\/\/learn.microsoft.com\/en-us\/dotnet\/api\/system.globalization.textinfo.toupper?view=net-7.0#system-globalization-textinfo-toupper(system-string)) (used later on in the code) say:\n\n\n\n> \n> The returned string might differ in length from the input string. For more information on casing, refer to the Unicode Technical Report #21 \"Case Mappings,\" published by the Unicode Consortium (<https:\/\/www.unicode.org\/>). **The current implementation preserves the length of the string.** However, this behavior is not guaranteed and could change in future implementations.\n> \n> \n> \n\n\n\n\n---\n\n\nTo clarify further, we are talking about code points in UTF-16. We have not considered UTF-8 or UTF-32, as `char` is strictly UTF-16.\n\n\nUnicode defines Case Mapping as follows:\n\n\n\n> \n> - **Simple (Single-Character) Case Mapping**\n> \n> \n> The general case mapping in ICU is non-language based and a 1 to 1 generic character map.\n> \n> \n> A character is considered to have a lowercase, uppercase, or title case equivalent if there is a respective \u201csimple\u201d case mapping specified for the character in the Unicode Character Database (UnicodeData.txt). If a character has no mapping equivalent, the result is the character itself.\n> \n> \n> The APIs provided for the general case mapping, located in uchar.h file, handles only single characters of type UChar32 and returns only single characters. To convert a string to a non-language based specific case, use the APIs in either the unistr.h or ustring.h files with a NULL argument locale.\n> - **Full (Language-Specific) Case Mapping**\n> \n> \n> There are different case mappings for different locales. For instance, unlike English, the character Latin small letter \u2018i\u2019 in Turkish has an equivalent Latin capital letter \u2018I\u2019 with dot above ( \\u0130 \u2018\u0130\u2019).\n> \n> \n> Similar to the simple case mapping API, a character is considered to have a lowercase, uppercase or title case equivalent if there is a respective mapping specified for the character in the Unicode Character database (UnicodeData.txt). In the case where a character has no mapping equivalent, the result is the character itself.\n> \n> \n> To convert a string to a language based specific case, use the APIs in ustring.h and unistr.h with an intended argument locale.\n> \n> \n> ICU implements full Unicode string case mappings.\n> \n> \n> In general:\n> \n> \n> \t- **case mapping can change the number of code points and\/or code units of a string,**\n> \t- is language-sensitive (results may differ depending on language), and\n> \t- is context-sensitive (a character in the input string may map differently depending on surrounding characters).\n> \n> \n> \n\n\n## TL;DR;\n\n\nIn theory, the number of code points could change (this is separate from the number of bytes). But .NET does not currently implement this. That could change without notice, but that's unlikely until there is a way to calculate the number of code points, due to interdependencies on `Span`."}
{"questionId":"dd30fafac00d4f69a0336b3ce7156915","question":"Deprecation warning when installing nodejs on docker container using nodesource install script\nI used to install nodejs on Debian based container using the following in the Dockerfile:\n\n\n\n```\nRUN apt-get update -yq && apt-get upgrade -yq && apt-get install -yq curl git nano\nRUN curl -sL https:\/\/deb.nodesource.com\/setup_18.x | bash - \nRUN apt-get install nodejs -y \n\n```\n\nBut I recently started getting the following message:\n\n\n\n```\n    => [base 3\/7] RUN curl -sL https:\/\/deb.nodesource.com\/setup_18.x | bash -\n                               SCRIPT DEPRECATION WARNING\n    ================================================================================ \nTO AVOID THIS WAIT MIGRATE THE SCRIPT Continuing in 60 seconds (press Ctrl-C to abort) ...\n\n```\n\nHow do I fix it?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"javascript"},"answer":"The notice from the [script](https:\/\/deb.nodesource.com\/setup_18.x) is\n\n\n\n```\n  This script, located at https:\/\/deb.nodesource.com\/setup_X, used to\n  install Node.js is deprecated now and will eventually be made inactive.\n\n  Please visit the NodeSource distributions Github and follow the\n  instructions to migrate your repo.\n  https:\/\/github.com\/nodesource\/distributions \n\n  The NodeSource Node.js Linux distributions GitHub repository contains\n  information about which versions of Node.js and which Linux distributions\n  are supported and how to install it.\n  https:\/\/github.com\/nodesource\/distributions\n\n```\n\nThe [instructions on github](https:\/\/github.com\/nodesource\/distributions) amount to a Dockerfile `RUN`\n\n\n\n```\nFROM docker.io\/debian:12-slim\nRUN set -uex; \\\n    apt-get update; \\\n    apt-get install -y ca-certificates curl gnupg; \\\n    mkdir -p \/etc\/apt\/keyrings; \\\n    curl -fsSL https:\/\/deb.nodesource.com\/gpgkey\/nodesource-repo.gpg.key \\\n     | gpg --dearmor -o \/etc\/apt\/keyrings\/nodesource.gpg; \\\n    NODE_MAJOR=18; \\\n    echo \"deb [signed-by=\/etc\/apt\/keyrings\/nodesource.gpg] https:\/\/deb.nodesource.com\/node_$NODE_MAJOR.x nodistro main\" \\\n     > \/etc\/apt\/sources.list.d\/nodesource.list; \\\n    apt-get -qy update; \\\n    apt-get -qy install nodejs;\n\n```\n\nThe `docker.io\/node:18` image maintained by the Node.js project is Debian based if you want to save some time.\n\n\n\n```\nFROM docker.io\/node:18-bookworm-slim"}
{"questionId":"1eb236ffe060475eaf09e122ddb6ffce","question":"Why isn't the keyword false an integer constant expression in gcc C23?\nLatest gcc 13.x (trunk) gives a compiler error (`gcc -std=c23`) for this code:\n\n\n\n```\nint* p = false;\n\n```\n\n\n> \n> error: incompatible types when initializing type '`int *`' using type '`_Bool`'\n> \n> \n> \n\n\nHow can this be correct?\n\n\n\n\n---\n\n\nC23 6.2.5 \u00a78-9 (Types - the definition of integer types):\n\n\n\n> \n> The type `bool` and the unsigned integer types that correspond to the standard signed integer types are the *standard unsigned integer types*.\n> \n> \n> The standard signed integer types and standard unsigned integer types are collectively called the *standard integer types*;\n> \n> \n> \n\n\nC23 6.6 \u00a78 (Constant expressions - the definition of an integer constant expression):\n\n\n\n> \n> An *integer constant expression* shall have integer type...\n> \n> \n> \n\n\nC23 6.3.2.3 (Pointers - the definition of null pointer constant)\n\n\n\n> \n> An integer constant expression with the value 0, such an expression cast to type `void *`, or the predefined constant `nullptr` is called a *null pointer constant*\n> \n> \n> \n\n\nC23 6.5.16.1 (Simple assignment):\n\n\n\n> \n> **Constraints**  \n> \n> \/--\/\n> \n> \n> - the left operand is an atomic, qualified, or unqualified pointer, and the right operand is a null pointer constant\n> \n> \n> \n\n\nConclusion:\n\n\n- `int* p = false;` is a valid form of assignment,\n- because `false` is a null pointer constant,\n- since it is an integer constant expression,\n- since it has type `bool`, which is one of the standard integer types.\n\n\nDid I misunderstand anything in the standard or is this a gcc bug? clang gives a warning but still produces an executable. I don't really see how any of the above has changed in C23 either.\n\n\nIncluding `stdbool.h` and compiling under `gcc -std=c17 -pedantic-errors` makes it compile cleanly as expected.","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"c"},"answer":"In addition to the paragraphs quoted in the question, there is paragraph 6.4.4.5\/3:\n\n\n\n> \n> The keywords `false` and `true` are constants of type `bool` with a value of\n> 0 for `false` and 1 for `true`.\n> \n> \n> \n\n\nand 6.6\/7:\n\n\n\n> \n> An identifier that is:  \n> \n> [...]  \n> \n> \u2014 a predefined constant;  \n> \n> [...]  \n> \n> is a *named constant*\n> \n> \n> \n\n\nThat makes `false` a named constant, which is relevant because a fuller quotation of 6.6\/8 is\n\n\n\n> \n> An *integer constant expression* shall have integer type and shall only have operands that are\n> integer constants, **named and compound literal constants of integer type** [...]\n> \n> \n> \n\n\n(emphasis added).\n\n\nRegardless, then, of GCC implementation details surrounding the constant `false`, when taken as an expression, `false` meets C23's criteria for an integer constant expression. Its value is specified to be 0. It is therefore a valid null pointer constant.\n\n\n**GCC's behavior here is contrary to the language spec.**\n\n\n\n\n---\n\n\n\n> \n> I don't really see how any of the above has changed in C23 either.\n> \n> \n> \n\n\nNone of the points you were referring to changed in C23, but what *did* change is the type of `false` and `true`. In C17, these had type `int`. In C23, they have type `bool` (a.k.a. `_Bool`). As you observe, however, `bool` is an integer type, so GCC not allowing that as the type of a null pointer constant is non-conforming.\n\n\nI note that GCC 8.5 does not accept `_Bool` as a valid type for a null pointer constant, either. Given this source:\n\n\n\n```\nvoid *test = (_Bool)0;\n\nint main(void) {\n}\n\n```\n\n, it fails with the same error you observed:\n\n\n\n> \n> npctest.c:1:14: error: incompatible types when initializing type \u2018void \\*\u2019 using type \u2018\\_Bool\u2019\n> \n> \n> \n\n\nThus, your observation appears to be a manifestation of a larger, relatively longstanding issue that is not specific to C23.\n\n\nThis issue is mentioned in the GCC bug tracker in comments on a related issue: <https:\/\/gcc.gnu.org\/bugzilla\/show_bug.cgi?id=112556>."}
{"questionId":"00f231f8e0a04eb1b3e4d7a58876256b","question":"Can't add ProgressIndicators (app crashes)\nWhen I use any kind of Progress Indicators like `CircularProgressIndicator()` or `LinearProgressIndicator()` my app crashes:\n\n\n\n```\njava.lang.NoSuchMethodError: No virtual method at(Ljava\/lang\/Object;I)\n  Landroidx\/compose\/animation\/core\/KeyframesSpec$KeyframeEntity; in class\n  Landroidx\/compose\/animation\/core\/KeyframesSpec$KeyframesSpecConfig; or its super classes\n  (declaration of 'androidx.compose.animation.core.KeyframesSpec$KeyframesSpecConfig' appears in\n  \/data\/app\/~~rlFP4ThDS2yjsD1Xsh1cWQ==\/es.jvbabi.vplanplus-6LDitHOwZpwGZt8Ywx6lFQ==\/base.apk)\n    at androidx.compose.material3.ProgressIndicatorKt$CircularProgressIndicator$endAngle$1.invoke(ProgressIndicator.kt:302)\n    at androidx.compose.material3.ProgressIndicatorKt$CircularProgressIndicator$endAngle$1.invoke(ProgressIndicator.kt:300)\n\n```\n\nThese are my dependencies relevant to this issue:\n\n\n\n```\nimplementation(\"androidx.core:core-ktx:1.12.0\")\nimplementation(\"androidx.lifecycle:lifecycle-runtime-ktx:2.6.2\")\nimplementation(\"androidx.activity:activity-compose:1.8.0\")\nimplementation(\"androidx.compose.ui:ui\")\nimplementation(\"androidx.compose.material3:material3\")\n\n```\n\nNo matter where I put them, they cause the app to crash.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"java"},"answer":"Using this version (confirmed it works) or a newer one should fix the issue.\n[See all versions on mvnrepository](https:\/\/mvnrepository.com\/artifact\/androidx.compose.material3\/material3)\n\n\n\n```\nimplementation(\"androidx.compose.material3:material3-android:1.2.0-alpha10\")"}
{"questionId":"6eccf760d01d461c949df4b65316dfc9","question":"vscode + vim plugin: CTRL+P no longer works\nI've used vscode with the vim plugin for years. All of a sudden I can no longer open files using CTRL+P. Looks like the plug-in intercepts this combination and moves the cursor up instead.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"It looks like this [issue was introduced in v1.26.0](https:\/\/github.com\/VSCodeVim\/Vim\/issues\/8574) a few days ago. If you don't use Ctrl+P to navigate in the filepicker, you can disable or change the vim shortcut back to the default VSCode behavior, as noted in the issue:\n\n\n\n```\n\/\/ keybindings.json\n{\n    \"key\": \"ctrl+p\",\n    \"command\": \"workbench.action.quickOpen\"\n}"}
{"questionId":"950b520c11f94fad82f342d9aeed81e0","question":"Manifest merger failed with AGP 8.3.0\nI'm trying to upgrade my project to AGP 8.3.0 but I'm getting the error:\n\n\n\n```\nAttribute property#android.adservices.AD_SERVICES_CONFIG@resource value=(@xml\/ga_ad_services_config) from [com.google.android.gms:play-services-measurement-api:21.5.1] AndroidManifest.xml:32:13-58\n    is also present at [com.google.android.gms:play-services-ads-lite:22.6.0] AndroidManifest.xml:92:13-59 value=(@xml\/gma_ad_services_config).\n    Suggestion: add 'tools:replace=\"android:resource\"' to <property> element at AndroidManifest.xml to override.\n\n```\n\nAny idea on how to fix this?\n\n\nI don't have anything related to `AD_SERVICES_CONFIG` in my manifest file.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"java"},"answer":"Problem is definitely not with your `AndroidManifest.xml` file, but rather the files that are bundled with the external Google services libraries that you have implemented in your application. Yes, sometimes they may conflict with each other (as all manifests are merged during app build) and this seems to be the case.\n\n\nEventually, Google will address this issue, but for now you can either downgrade your dependencies (need to figure out which ones, but AGP 8.2.2 didn't have this problem AFAIK).\n\n\nOr, just do as suggested by the error log, and solve the conflict by adding this block to your `AndroidManifest.xml` file:\n\n\n\n```\n<manifest\n    ...\n\n    <application\n        ...\n\n        <property\n            android:name=\"android.adservices.AD_SERVICES_CONFIG\"\n            android:resource=\"@xml\/gma_ad_services_config\"\n            tools:replace=\"android:resource\" \/>\n        \n        ...\n    <\/application>\n\n    ...\n<\/manifest>\n\n```\n\n**Note:** I would still recommend going back to AGP 8.2.2 if your project is important, since new releases are always risky and this might not be the only problem in the updated Gradle plugin"}
{"questionId":"680c6943019b4c5c85f81aa41886c678","question":"Linux memcpy restrict keyword syntax\nI know that the `restrict` qualifier in C specifies that the memory region pointed by two pointers should not overlap. It was my understanding that the Linux (not SUS) prototype for `memcpy` looks like -\n\n\n\n```\nvoid* memcpy(void *restrict dest, const void *restrict src, size_t count);\n\n```\n\nHowever, when I looked at [man7.org\/memcpy](https:\/\/man7.org\/linux\/man-pages\/man3\/memcpy.3.html) it seems that the declarations is -\n\n\n\n```\nvoid *memcpy(void dest[restrict .n], const void src[restrict .n], size_t n);\n\n```\n\nMy questions are -\n\n\n1. When did this syntax get introduced? C99 or later or is this some GNU extension?\n2. What does the `.` before `n` signify? I am familiar with the variable length array declaration. Is the `.` for the variable appearing after the array specification? Is this part of the standard?","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"c"},"answer":"**TLDR**: It's an ad hoc syntax created in a discussion in a Linux mailing list that is used to express the size of VLA before the variable is declared, the `.` in `.n` means `n` refers to a parameter in the current function declaration, but `n` may appear after the currently declared parameter. They have also extended the usual `int a[restrict n]` parameter declaration to `void` type. I have no idea where such syntax can be found in the official documentation, but the mailing list has all the details.\n\n\n\n\n---\n\n\nThe change to the `memcpy` syntax in the Linux library functions manual was introduced by [commit c64cd13e](https:\/\/git.kernel.org\/pub\/scm\/docs\/man-pages\/man-pages.git\/commit\/man3\/memcpy.3?id=c64cd13e002561c6802c6a1a1a8a640f034fea70). The commit message is copied here verbatim for reference.\n\n\n\n> \n> **Various pages: SYNOPSIS: Use VLA syntax in 'void \\*' function parameters**\n> \n> \n> Use VLA syntax also for void \\*, even if it's a bit more weird.\n> \n> \n> \n\n\nAdmittedly, it is weird enough from the C language perspective, because while `void f(int n, int[restrict n])` is valid VLA syntax, `void f(int n, void[restrict n])` is not because we are not allowed to have arrays of `void`.\n\n\nFor the `.` before `n`, if we dig deeper we can find [this thread](https:\/\/marc.info\/?l=linux-man&m=166220937010535&w=2) from the [`linux-man` mailing list](https:\/\/www.kernel.org\/doc\/man-pages\/linux-man-ml.html).\n\n\n\n> \n> \n> > \n> > Let's take an example:\n> > \n> > \n> > \n> > ```\n> >     int getnameinfo(const struct sockaddr *restrict addr,\n> >                     socklen_t addrlen,\n> >                     char *restrict host, socklen_t hostlen,\n> >                     char *restrict serv, socklen_t servlen,\n> >                     int flags);\n> > \n> > ```\n> > \n> > and some transformations:\n> > \n> > \n> > \n> > ```\n> >     int getnameinfo(const struct sockaddr *restrict addr,\n> >                     socklen_t addrlen,\n> >                     char host[restrict hostlen], socklen_t hostlen,\n> >                     char serv[restrict servlen], socklen_t servlen,\n> >                     int flags);\n> > \n> > \n> >     int getnameinfo(socklen_t hostlen;\n> >                     socklen_t servlen;\n> >                     const struct sockaddr *restrict addr,\n> >                     socklen_t addrlen,\n> >                     char host[restrict hostlen], socklen_t hostlen,\n> >                     char serv[restrict servlen], socklen_t servlen,\n> >                     int flags);\n> > \n> > ```\n> > \n> > (I'm not sure if I used correct GNU syntax, since I never used that\n> > extension myself.)\n> > \n> > \n> > The first transformation above is non-ambiguous, as concise as possible,\n> > and its only issue is that it might complicate the implementation a bit\n> > too much. I don't think forward-using a parameter's size would be too\n> > much of a parsing problem for human readers.\n> > \n> > \n> > \n> \n> \n> I personally find the second form not terrible. Being able to read\n> code left-to-right, top-down is helpful in more complicated examples.\n> \n> \n> \n> > \n> > The second one is unnecessarily long and verbose, and semicolons are not\n> > very distinguishable from commas, for human readers, which may be very\n> > confusing.\n> > \n> > \n> > \n> > ```\n> >     int foo(int a; int b[a], int a);\n> >     int foo(int a, int b[a], int o);\n> > \n> > ```\n> > \n> > Those two are very different to the compiler, and yet very similar to\n> > the human eye. I don't like it. The fact that it allows for simpler\n> > compilers isn't enough to overcome the readability issues.\n> > \n> > \n> > \n> \n> \n> This is true, I would probably use it with a comma and\/or syntax\n> highlighting.\n> \n> \n> \n> > \n> > I think I'd prefer having the forward-using syntax as a non-standard\n> > extension --or a standard but optional language feature-- to avoid\n> > forcing small compilers to implement it, rather than having the GNU\n> > extension standardized in all compilers.\n> > \n> > \n> > \n> \n> \n> The problems with the second form are:\n> \n> \n> - it is not 100% backwards compatible (which maybe ok though) as the semantics of the following code changes:\n> \n> \n> int n; int foo(int a[n], int n); \/\/ refers to different n!\n> \n> \n> Code written for new compilers could then be misunderstood by old\n> compilers when a variable with 'n' is in scope.\n> \n> \n> - it would generally be fundamentally new to C to have backwards references and parser might need to be changes to allow this\n> - a compiler or tool then has to deal also with ugly corner cases such as mutual references:\n> \n> \n> int foo(int (\\*a)[sizeof(\\*b)], int (\\*b)[sizeof(\\*a)]);\n> \n> \n> We could consider new syntax such as\n> \n> \n> int foo(char buf[.n], int n);\n> \n> \n> Personally, I would prefer the conceptual simplicity of forward\n> declarations and the fact that these exist already in GCC over any\n> alternative. I would also not mind new syntax, but then one has to\n> define the rules more precisely to avoid the aforementioned problems.\n> \n> \n> \n\n\nAccording to my understanding, this basically means the `.` is a way to refer to a VLA array size parameter that is used before declaration, and one use case is to handle mutual references.\n\n\nThere is a [follow-up thread](https:\/\/marc.info\/?l=linux-man&m=166221546212525&w=2) that states,\n\n\n\n> \n> I am ok with the syntax, but I am not sure how this would work. If the\n> type is determined only later you would still have to change parsers\n> (some C compilers do type checking and folding during parsing, so\n> need the types to be known during parsing) and you also still have the\n> problem with the mutual dependencies.\n> \n> \n> We thought about using this syntax\n> \n> \n> int foo(char buf[.n], int n);\n> \n> \n> because it is new syntax which means we can restrict the size to be\n> the name of a parameter instead of allowing arbitrary expressions,\n> which then makes forward references less problematic. It is also\n> consistent with designators in initializers and could also be extend\n> to annotate flexible array members or for storing pointers to arrays\n> in structures:\n> \n> \n> struct { int n; char buf[.n]; };\n> \n> \n> struct { int n; char (\\*buf)[.n]; };\n> \n> \n> \n\n\nOf course, there was also [objection](https:\/\/marc.info\/?l=linux-man&m=166221147411176&w=2), which I think many people in the SO community would agree with,\n\n\n\n> \n> the only point i strongly care about is this one:\n> \n> \n> Manual pages should not use\n> \n> \n> - non-standard syntax\n> - non-portable syntax\n> - ambiguous syntax (i.e. syntax that might have different meanings with different compilers or in different contexts)\n> - syntax that might be invalid or dangerous with some widely used compiler collections like GCC or LLVM\n> \n> \n>"}
{"questionId":"cccd6c3ae2ff46c78b8741de480a599b","question":"Overload resolution and template argument deduction - why is 0 special?\nIn the following example, `0` behaves in a special way: it chooses a different overload than one would expect for one example function call. I would like to know why. My understanding is also below.\n\n\n\n```\n#include <iostream>\n\ntemplate<typename T>\nvoid f(T a) {\n    std::cout << \"first\" << std::endl;\n}\n\ntemplate<typename T>\nvoid f(T* a) {\n    std::cout << \"second\" << std::endl;\n}\n\nint main()\n{\n    f(0);\n    f<size_t>(0);\n    f<size_t>(0UL);\n    \n    f(1);\n    f<size_t>(1);\n}\n\n```\n\nOutput:\n\n\n\n```\nfirst\nsecond\nfirst\nfirst\nfirst\n\n```\n\nMy understanding:\n\n\n`f(0)` - template argument deduction, integer literal `0` is `int` type, hence first `f` is chosen with `T=int`\n\n\n`f<size_t>(0)` - explicit template instantiation with **integer promotion**, chosen type is `T=size_t`, first function is chosen and `0` is **promoted** from `int` to `size_t` (**I AM WRONG HERE**)\n\n\n`f<size_t>(0UL)` - same as above but with **no promotion** (0 is already type `size_t`)\n\n\n`f(1)` - same as 1.\n\n\n`f<size_t>(1)` - same as 2. (I am right here for some reason??)\n\n\n**NOTE:**\n\n\nI know that `0` is implicitly convertible to a null pointer:\n\n\n\n> \n> A null pointer constant is an integer literal (5.13.2) with value zero or a prvalue of type std::nullptr\\_t\n> \n> \n> \n\n\nHowever, I also know from the standard that promotion has higher priority than conversion:\n\n\n\n> \n> Each type of standard conversion sequence is assigned one of three\n> ranks:\n> \n> \n> 1. Exact match: no conversion required, lvalue-to-rvalue\n> conversion, qualification conversion, function pointer\n> conversion,(since C++17) user-defined conversion of class type to the\n> same class\n> 2. Promotion: integral promotion, floating-point promotion\n> 3. Conversion: integral conversion, floating-point conversion,\n> floating-integral conversion, pointer conversion, pointer-to-member\n> conversion, boolean conversion, user-defined conversion of a derived\n> class to its base\n> \n> \n>","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"cpp"},"answer":"> \n> `f<size_t>(0)` - explicit template instantiation with integer promotion, chosen type is `T=size_t`, first function is chosen and 0 is promoted from int to `size_t`\n> \n> \n> \n\n\nThe reason `f<size_t>(0)` calls\/uses the second overload `f(T*)` is because of [partial ordering rules](https:\/\/stackoverflow.com\/questions\/17005985\/what-is-the-partial-ordering-procedure-in-template-deduction). In particular, the second overload `f(T* a)` is **more specialized** than the first overload `f(T a)`.\n\n\nNote that **if** we were to have ordinary(non-template) functions say `void f(std::size_t*)` and `void f(std::size_t)` then the call `f(0)` would've been ambiguous but with function templates(as in your case) the call `f(0)` prefers\/selects the `void f(T*)` version because it is more specialized than `void f(T)`\n\n\n\n\n---\n\n\n\n> \n> why `f(0)` calls first overload. I know that 0 is **implicitly convertible** to a null pointer:\n> \n> \n> \n\n\nBecause [implicit conversions are not considered during template argument deduction](https:\/\/stackoverflow.com\/questions\/53227713\/why-does-the-implicit-type-conversion-not-work-in-template-deduction). This means that for the call `f(0)` only the first overload `void f(T a)` is **viable**. In other words, **the second overload `void f(T* a)` is not even viable for the call `f(0)`** .You can verify this in this [demo](https:\/\/godbolt.org\/z\/Gd1oPzE8Y):\n\n\n\n```\ntemplate<typename T>\nvoid f(T* a) {\n    std::cout << \"second\" << std::endl;\n}\nint main()\n{\n    f(0);\/\/FAILS because implicit conversion are not considered during deduction   \n}"}
{"questionId":"fac6badbb6fe43cdaf427854ccb109a9","question":"Why performance for this index-of-max function over many arrays of 256 bytes is so slow on Intel i3-N305 compared to AMD Ryzen 7 3800X?\nI've run the same binaries compiled with gcc-13 (<https:\/\/godbolt.org\/z\/qq5WrE8qx>) on Intel i3-N305 3.8GHz and AMD Ryzen 7 3800X 3.9GHz PCs. This code uses VCL library (<https:\/\/github.com\/vectorclass\/version2>):\n\n\n\n```\nint loop_vc_nested(const array<uint8_t, H*W> &img, const array<Vec32uc, 8> &idx) {\n  int sum = 0;\n  Vec32uc vMax, iMax, vCurr, iCurr;\n\n  for (int i=0; i<H*W; i+=W) {\n    iMax.load(&idx[0]);\n    vMax.load(&img[i]);\n\n    for (int j=1; j<8; j++) {\n      iCurr.load(&idx[j]);\n      vCurr.load(&img[i+j*32]);\n      iMax = select(vCurr > vMax, iCurr, iMax);\n      vMax = max(vMax, vCurr);\n    }\n\n    Vec32uc vMaxAll{horizontal_max(vMax)};\n    sum += iMax[horizontal_find_first(vMax == vMaxAll)];\n  }\n\n  return sum;\n}\n\n```\n\nFull benchmark source is here: <https:\/\/github.com\/pauljurczak\/simd-benchmarks\/blob\/main\/main-5-vcl-eve.cpp>. Here is the timing:\n\n\n\n```\nUbuntu 22.04.3 LTS on AMD Ryzen 7 3800X 8-Core Processor\ngcc    v13.1   __cplusplus=202100\nloop_vc_nested(): 3.597  3.777 [us]  108834\n\nUbuntu 23.10 on Intel(R) Core(TM) i3-N305\ngcc    v13.1   __cplusplus=202100\nloop_vc_nested(): 11.804  11.922 [us]  108834\n\n```\n\nThere is an unexpected slowdown of 3.2x. AFAIK, these CPUs have similar SIMD capabilities for a single thread program. Performance on 7-zip benchmark is very close. Why such a big gap?\n\n\n\n\n---\n\n\nHere is an output from `perf`. AMD Ryzen 7 3800X:\n\n\n\n```\n          3,841.61 msec task-clock                       #    1.000 CPUs utilized             \n                20      context-switches                 #    5.206 \/sec                      \n                 0      cpu-migrations                   #    0.000 \/sec                      \n             2,191      page-faults                      #  570.333 \/sec                      \n    14,909,837,582      cycles                           #    3.881 GHz                         (83.34%)\n         3,509,824      stalled-cycles-frontend          #    0.02% frontend cycles idle        (83.34%)\n     9,865,497,290      stalled-cycles-backend           #   66.17% backend cycles idle         (83.34%)\n    42,856,816,868      instructions                     #    2.87  insn per cycle            \n                                                  #    0.23  stalled cycles per insn     (83.34%)\n     1,718,672,677      branches                         #  447.383 M\/sec                       (83.34%)\n         2,409,251      branch-misses                    #    0.14% of all branches             (83.29%)\n\n```\n\nIntel i3-N305:\n\n\n\n```\n         12,015.18 msec task-clock                       #    1.000 CPUs utilized             \n                57      context-switches                 #    4.744 \/sec                      \n                 0      cpu-migrations                   #    0.000 \/sec                      \n             2,196      page-faults                      #  182.769 \/sec                      \n    45,432,594,158      cycles                           #    3.781 GHz                         (74.97%)\n    42,847,054,707      instructions                     #    0.94  insn per cycle              (87.48%)\n     1,714,003,765      branches                         #  142.653 M\/sec                       (87.48%)\n         4,254,872      branch-misses                    #    0.25% of all branches             (87.51%)\n                        TopdownL1                 #      0.2 %  tma_bad_speculation    \n                                                  #     45.5 %  tma_retiring             (87.52%)\n                                                  #     53.8 %  tma_backend_bound      \n                                                  #     53.8 %  tma_backend_bound_aux  \n                                                  #      0.5 %  tma_frontend_bound       (87.52%)\n\n```\n\nCompiler options: `-O3 -Wno-narrowing -ffast-math -fno-trapping-math -fno-math-errno -ffinite-math-only -march=alderlake`\n\n\n\n\n---\n\n\nAdditional cache use information from `perf stat -d` on i3-N305:\n\n\n\n```\n    15,615,324,576      L1-dcache-loads                  #    1.294 G\/sec                       (54.50%)\n   <not supported>      L1-dcache-load-misses                                                 \n            60,909      LLC-loads                        #    5.048 K\/sec                       (54.50%)\n             5,231      LLC-load-misses                  #    8.59% of all L1-icache accesses   (54.50%)\n\n```\n\n\n\n---\n\n\nI installed the newest Intel C++ compiler, in order to get `-march=gracemont` working. Performance did not improve, since Intel compiler is based on clang, which performed worse than gcc in this benchmark. Here are the timings:\n\n\n\n```\nUbuntu 23.10 on Intel(R) Core(TM) i3-N305\nclang v17.0.0 (icx 2024.0.2.20231213) C++\nloop_vc_nested(): 12.311  12.397 [us]  108834  # -march=native\nloop_vc_nested(): 12.773  12.847 [us]  108834  # -march=alderlake\nloop_vc_nested(): 12.418  12.519 [us]  108834  # -march=gracemont\nloop_vc_unrolled(): 10.388  12.406 [us]  108834  # -march=gracemont\nloop_vc_nested_noselect_2chains(): 6.686  10.454 [us]  109599  # -march=gracemont","questionMetadata":{"Type":"Optimization","Level":"Advanced","Tag":"cpp"},"answer":"The AVX encoding of `vpblendvb` has 4 operands (3 sources and a separate destination), and is multi-uop even on Intel P-cores (unlike the legacy-SSE 128-bit encoding), but is single-uop on Zen. A different algorithm can avoid it.\n\n\nAlder Lake E-cores (Gracemont) are 5-wide out-of-order with reasonable out-of-order exec capability, but they're not great at 256-bit SIMD in general, and choke badly on 8-uop `vpblendvb ymm` in particular, including a front-end bottleneck it looks like. But your inner loop uses it every 4th instruction in a dependency chain (short enough for OoO exec to maybe partly hide, so we might just be getting the effects of the back-end-throughput or front-end bottleneck).\n\n\n**Your implementation strategy \/ algorithm is something Zen 2 is great at but which is a stumbling block for Gracemont,** amplifying the difference between 256-bit vs. 128-bit SIMD execution units.\n\n\n\n\n---\n\n\nYour [i3-N305](https:\/\/www.intel.com\/content\/www\/us\/en\/products\/sku\/231805\/intel-core-i3n305-processor-6m-cache-up-to-3-80-ghz\/specifications.html) is Alder Lake-N series. Like earlier Celeron \/ Pentium CPUs with N in their model number, **the cores are low-power Silvermont-family**. In this case Gracemont, the E-cores found in full Alder Lake chips. (Which are significantly beefier than Tremont or especially earlier generations like Goldmont Plus.) And it has AVX2+FMA which I guess is what justifies selling it as an i3.\n\n\n<https:\/\/chipsandcheese.com\/2021\/12\/21\/gracemont-revenge-of-the-atom-cores\/> is a good deep-dive on the CPU microarchitecture, with some comparisons to Zen 2, and microbenchmarks of cache bandwidth and latency (as part of an i9-12900k, IDK if the interconnect or L3 would be different in an i3-N series, but your benchmark fits in its 2M L2 cache; with a single core active, read bandwidth from L2 is about the same as L1d for sequential access.) No mention about how the decoders handle instructions that are more than 3 uops, but it does have a diagram showing the pair of 3-wide decode clusters. (If it's like previous Intel, any instruction more than 1 uop can only decode in the first decoder of a cluster, so that probably limits front-end throughput to two YMM vector instructions per clock even if they're the minimum 2 uops.)\n\n\n**Your Ryzen 3800X is a Zen 2, a full-fledged big core with good 256-bit SIMD load and ALU throughput** (up from 128-bit in Zen 1, Ryzen 1xxx and 2xxx series). And single-uop `vpblendvb`.\n\n\nThe most important factors are:\n\n\n- **Vector ALU and memory ports are 128-bit wide, and every 256-bit instruction decodes to (at least) 2 uops**, except a few like `vextracti128` and `vpmovmskb`. (So it's like Zen 1 and Bulldozer-family). So uops per clock is about twice the IPC, when running code that's mostly vector instructions with a bit of scalar overhead. 2\/clock load bandwidth only goes half as far when each load is only 128-bit.\n- **That `select` compiles to a `vpblendvb`**. Unfortunately that's very slow on Gracemont, see <https:\/\/uops.info\/> - VEX encodings of variable blends are 4 uops per 128-bit lane, so the YMM version is 8 uops with a measured throughput of one per 3.86 cycles. (Or 3.2 cycles for a memory source instead of register, surprisingly.) Zen family runs the 4-operand `vpblendvb` as a single uop (with a choice of ports even).\n\n\nThe legacy-SSE encoding only has 3 operands, one of them implicitly XMM0, and Gracemont runs that as a single uop. Even Alder Lake P-cores run `vpblendvb x\/ymm` as 3 uops, up from 2 in Ice Lake, while SSE4.1 `pblendvb xmm, xmm` is single uop on modern Intel P-cores, too.\n\n\n**Gracemont `vpblendvb ymm` also has 6 to 7 cycle latency**, or 5c for the XMM version (vs. 2 to 3 on P cores), depending on data vs. control inputs being the critical path, vs. 1 cycle on Zen. Even worse than its throughput even with the front-end bottleneck. Out-of-order exec buffers (scheduler and ROB) are probably big enough to hide this over a chain of 7 of them, since you start a new dep chain every 256 bytes, but it's not great and would be a bottleneck in a loop that ran more iterations.\n\n\nIt seems Intel goofed when designing the AVX1 encoding of it (with a 4th register number in an immediate byte!) while Sandybridge-family was still being designed, not anticipating that their later CPUs would be able to handle 3-operand instructions as a single uop. (Motivated by FMA in Haswell, but benefiting others in Broadwell and later.) And that mov-elimination would remove the back-end execution port cost of copying a register if needed (unlike here) if the original value is needed after an instruction that modifies a R+W destination in-place. FMA3 and later 3-input instructions like AVX-512 [`vpternlogd`](https:\/\/www.felixcloutier.com\/x86\/vpternlogd:vpternlogq) and [`vpermi\/t2d`](https:\/\/www.felixcloutier.com\/x86\/vpermt2w:vpermt2d:vpermt2q:vpermt2ps:vpermt2pd) have an R+W source\/destination as the first operand. (`k` mask inputs to AVX-512 instructions are a separate forwarding network and a separate domain to track dependencies in, so they don't count.)\n\n\n8 uops inherently contributes to low IPC for the same uops\/clock throughput, but probably also stalls the front-end some, reducing uops\/clock. Even Gracemont's 4-uop `vpblendvb xmm` has about the same bad throughput if running *just* that back-to-back, which is consistent with some kind of decode stall or having to switch to a microcode ROM on >3 uop instructions.\n\n\n\n\n---\n\n\nYou could try to blend manually with `_mm256_and_si256` \/ `andnot` \/ `or`, which would be 6 uops but avoid front-end stalls for a total throughput cost of 1.33 cycles on the vector ALU ports. But clang will \"optimize\" those intrinsics to a `vpblendvb` since it knows the blend-control is a compare result, with all bits matching the sign bit.\n\n\nClang trunk's `-mtune=gracemont` or `-march=gracemont` doesn't know it's slow on that uarch, at least not splitting `select` into those. MSVC, or classic ICC, are a lot more literal about intrinsics. GCC does optimize some, but in this case it does use actual `vpand`\/`vpandn`\/`vpor` instructions (<https:\/\/godbolt.org\/z\/3fc1jo9r4>), so you could make a version that's worse on Ryzen, less bad on Gracemont, but not optimal anywhere. I think it's still worse on Gracemont than the `noselect` version below.\n\n\nYour original is fairly good for Ryzen, but there's room for improvement in the cleanup, and in maybe scanning backwards to avoid inverting the compare to feed the blend. Or the branchy strategy might be best if an instance of the max element is often seen within the first 64 bytes so it's predictable. Just load + 7x `vpmaxub ymm, mem`, then reduce and scan.\n\n\n\n\n---\n\n\n## Avoiding variable-blend\n\n\nYour actual problem could be done other ways, for example unpacking your data with indices as chtz suggested in [Looking for an efficient function to find an index of max element in SIMD vector using a library](https:\/\/stackoverflow.com\/questions\/77707238\/looking-for-an-efficient-function-to-find-an-index-of-max-element-in-simd-vector) , so the max `u16` element contains the data and the index. (And instead of loading, the index can come from `idx = _mm256_add_epi8(idx, _mm256_set1_epi8(32));`.\nOf maybe that inner loop over 256 bytes can get fully unrolled so you have 8 registers holding index data.)\n\n\nSince you'd probably want to use that improved reduction anyway, unpacking even earlier saves some cleanup work, and your loop is only 8 vectors.\n\n\nFor a sum of indices, I guess it's important that you get the first occurrence of a match? So you'd want to invert your indices so the max of data:index packed as a u16 picks the earlier index when it's a tie-break for equal data. That's what we want anyway for a cleanup that's going to use `vphminposuw`.\n\n\nThis is what it might look like, *without* being clever about indices so it might be taking the last one.\n\n\n\n```\nint loop_vc_nested_noselect(const std::array<uint8_t, H*W> &img, const std::array<Vec32uc, 8> &idx) {\n  int sum = 0;\n\n  for (int i=0; i<H*W; i+=W) {\n    __m256i tmpidx = _mm256_loadu_si256((__m256i*)&idx[0]);\n    __m256i tmp = _mm256_loadu_si256((__m256i*)&img[i]);\n    Vec16us vMaxlo = _mm256_unpacklo_epi8(tmpidx, tmp);\n    Vec16us vMaxhi = _mm256_unpackhi_epi8(tmpidx, tmp);\n\n    for (int j=1; j<8; j++) {\n      Vec32uc vCurr, iCurr;\n      iCurr.load(&idx[j]);  \/\/ these get hoisted out of the outer loop and reused across img iters\n      vCurr.load(&img[i+j*32]);\n      Vec16us lo = _mm256_unpacklo_epi8(iCurr, vCurr);\n      Vec16us hi = _mm256_unpackhi_epi8(iCurr, vCurr);\n      vMaxlo = max(vMaxlo, lo);\n      vMaxhi = max(vMaxhi, hi);\n          \/\/ vMax = max(vMax, max(lo,hi));  \/\/ GCC was optimizing to two dep chains anyway, and that's better on big-cores that can do more than 1 load+shuffle+max per clock\n    }\n    Vec16us vMax = max(vMaxlo, vMaxhi);\n\n    \/\/ silly GCC uses vpextrw even though we're already truncating narrower\n    auto maxidx = (uint8_t)horizontal_max(vMax); \/\/ retrieve the payload from the bottom of the max\n    \/\/ TODO: use phminposuw like the last part of maxpos_u8_noscan_unpack\n    \/\/ with indices loaded and inverted once, outside the outer loop.  (Manually unrolled if compilers don't do that for you)\n    sum += maxidx;\n  }\n\n  return sum;\n}\n\n```\n\nInstead of loading indices, you could maybe just compute them with `_mm256_sub_epi8(idx, _mm256_set1_epi8(-1))` (or `add` to go in descending order down from 255), although compilers will probably constant-propagate through that and make 8 vectors of constants, and the RIP-relative addressing mode to load that is larger code-size than `[rsi+disp8]` for the first 5 loads, but that's just the startup code. After the compiler's done unrolling, you definitely want it to have 8 vectors of indices that it generates once ahead of the loop.\n\n\n[**Godbolt**](https:\/\/godbolt.org\/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1DIApACYAQuYukl9ZATwDKjdAGFUtAK4sGe1wAyeAyYAHI%2BAEaYxCAAzACcpAAOqAqETgwe3r56KWmOAkEh4SxRMQm2mPYFDEIETMQEWT5%2BXJXVGXUNBEVhkdEgiQr1jc05bcPdvSVlgwCUtqhexMjsHOaxwcjeWADU5mYAbpgOJNtMCgoAdAgHJhoAghtbO5j7sW4NxEwAniax2HuTzMmwY2y8e3%2BbgYPmieFMAKBQMmjmQu2CtGCb2CBF2LCYqjyAH0vAAOIkMVKiBgkhhJJjIADWEDQDGGuwAaidYmYvGjDnN\/lZHkDWcNMITiLs0gAvTBE3EACXeABFdqSuAA2Ui7ADqqt2ZgArJqhUjHjjdrRUKgkkTDsgKZhxegWQJ2cN0CAQF9flCvDjyQQdYqAFS6\/7YfZmTV4FjAHVi3Ge72%2Bv4fLnIHl8nWkyPR2PoVRzfYAdmFD126ME0p8Bo0ZseVcz2f5AFkCTq8B3VDrDm5lmR0QPiMRG08m7t%2BFKIJa8P8VQ3YhZ0VCwxHl6vLAvdSWTOWgVWq92CVdrUxXeZC6oTEaLA2jSrBcvD0fDj2z6gLxAr3HgLeLHnR9nwrV8q2nXZZxrbQFy4IVdhgj4803GDt0sPcD0nI9h0HT9v1\/IsAJg4D4N2AB6MjdgIBBnTeYBMFxBBUDwF1diWXFUCoKiaLYrwCGiK0bSSXZDHQXZiEwLwlDEhliFSBR0XjdF%2BOIBQwLfEdiDwy8Yz\/AD50sbRQx5W8n3HbCqwo3YvDZPBgBCMT8xYhgwA4XEmEOJg6CYCJ6GswQ6F2B4OQADQAWiNLgzCuZs3ACaylAUsBICYVUFzxAkICYUgIjmQUsKPKyAHE3DcXYQoAeQAJQU6i3jQFh6Qk8TnS8Whg12c4jAS50MtUESGBkhcFxYYIspy\/LKwsk9%2BvSuwTgICB%2B0Hd4o3fTscNHLsexA9TLMohRqQYYJgF2CIGUZAB3Bp0AUqg5JYbi3lcXZLsIBAROG2IVXxVRxtynVLvqpZaDE4Y6FoKdgjEurdkwBpMWdXFfr23Z1tm76%2BqWns%2B003aCv3FVzWBTYuLgrCW15dsCQeWhaH3CwmOIPAZVkMQiV%2B7GCUFUsiZfLCKNleVkYJPAiwNJmWbZ2giRoQbZbwVTFvRtLMfR2naBA8jKKIepIfwBQLhUtiGE6gwjBAXZKQYMKMSxMSLCEAAxV7viSJIBOnVYFOOU4pQUJIIbIiTzzEzinvRcXfaIVSwKFhU%2BrFjG1QVGVkEEEkeQgIgiQiQgFC55P0vVum8vHDYqiUdT45F1Qk4NX7iTJCkqUMWl6SZQv8ZJ1w8Coau62sdKZoA36k9M8yyyJkVJwkghllNhQfHL3niZReFq0xEIgUtRvUhJclKUOtubI75kk05bkqbRyaGfUym%2BWrfBvYNDmWGNTUiSUAhiCJTBA9JBAXYAA\/DQOpgFtBAWYcBsRwHSBAUacB2oQGllIKjCyGDMFYKwcA0k4DEgQLARAtoEDoEQNgRA6QEDEHoOwXQ%2BhVYIHaggagiBeCIEELMEQswJCzBkJBGggqDDhF0OAWYKhxpSBiOYWYVhZh2FmAIbEIhsQuDd2wkSDmH88CCVfiwd%2BJp24XWtH\/ABUFn7Oj7FrbWux6SKzRmILw7BoYSmrCJayWpXrvQcd4N4LBUjJjssdGg1ICDqU0QY2MuwEA6PSm\/D%2BRimQxNMXgQB0N4SWJvqRKydVTZyxhjxX6bEuLUVSG8LwniqiYDYIIBSok4aqB\/gyXEsN6Q\/FDvfE4WopJowqZqPRkSOYElMX0iA1odQxPUUeTMpIemHDJA3TK8ytRXHogQIk1pLoQAWL0lZayiQxOAAgbZ1irLMFHKgS6VFUC7CiqSMKecwmPCsgIdxjhMCAzePURkXzCkEhAS4\/qJA8TBCfq49KNAlZsWQMgZYjBVjFL6sTbCVlgjHEaJgMS6SX5iE2eVaqXjqLSgYlwCAYU1E6lSsgGiTAPZSiTIYFpNz6IhG%2BPxbihhiUEC4CkrUEANCqCoFQDQGhJrYSYHxG5hw0UGmAfM5CK59q3IkBld2J0OW4ioJiJICkilEBBQwHUhs2AiQUsaI0YVDiQQghEH4\/EFBiqsqES5r1MCuWataVAjJ1UQVhqNU2fTAZMSUIJK5pIzr5xEs1WGgLEWwxIHZYIYhrJ5gKhEu5Oj\/V5AGRzYIxJ\/6jOlQwAU2TKL%2Bp8U4txsMIioAIEQR6fS408WjiQdSNk0j2UxQanNyBDgEDSHcr%2BeAs5ZtSFMqsc8F6QQDIIIMcx\/WkRERgqy\/jDiOUsNKUQBgpSrplP1FinUaV0o5abWxEof4RF2A2ScVlUoKHxBi%2BlqBGp0AEpdEGYlDirqxYIG5qV8BCuiIwXEElTqYh%2BbsbAKoQqUrppc9Vq6wpVDjEmmoU5gV%2BtQIcPdSJV4zwnA8S0nq7QOidC6Fu80HBEjMNSrybI3RsmTAQL0PpRx%2Bg%2BDOggQYQzhnzL%2BeMiZ3RMZY2mKED9kC5j47pIsGEKzHhrEvR66UlygUnBBKCuJ5zfRU6uD4654IGSsN9XcZY5NHgidoqiL7xZxP0Qk0OXgh0fygBZk0eBQyCmkzeO8D4zL8ymlWVzUSCCNQGfZr86BHNpGcxAIL7nPOxnjPpUy47L7IG6T7Hs1owuGNPsY1AKTAEhcDkWHUxXUuZgy2jHsMScufzy0kvAhXs7Wd7FZpI3d1LqctIhFUcEUJQgVQhIe6FTOo3E2jTSXZNKT2wngTS2kfxeaIil0tT0Q1rOicxVi7Em1vHYgJYjA0xISSkl22S8lFKnUINENSQjJu4VDktxL\/47yGaMiZEi\/mMGVc1D07LtnBkNcZCY\/%2BqTZxTYe6OVLzYul\/YUrVwHCTgfJLB2kyHy1oezbfFlm56VObo3GYJGH1WCSI8xgTmreAJl4BJyiyiKt8dLJxljcZkzTmURKmVa6ClbSOFGjKdV%2BqCDvt2FgYStHgh1IYD8a6PxKWDQ1a5BSUQ60CVeXnYAYU0ASVqggJguJqRi5uf46N%2BvTZcEEheawCgEBeCFfQawRTj3bFQEydShNOnpfh6T5OWNCeoD7FTzrBUrJpDpj8XYXOeo%2Bw9o04gVzMDHFNqUrwRzXXureGICSF5I8\/xsqIRw3VzlySBmOAqEr9Vjxs5jCAnG52S1ZoIdmlPubwSsnPZmSfflvDaaHKcD0I41rrc%2BvbSLQ%2BUQACoVRVBVS2Z3bEIFHUvK54Ge9WiNrY7oiK94KAPhR6kiTGTqSsm9Il2Leqh3O4rtF0R%2BJhzBB83i\/axbr4O1KYjMVIIdhhLiyPNk5I6Yu0%2B5Opn1A56BVJjcXI3JjcNV0MpQfglgxUjxFNowjM1Rq8bxvsp5kUWp55iBF5l5\/NPdHgOAFhaBOAjReA\/AOAtBSBUBOA3ARsVwFAlgVg3gNgeBSACBNAyCFhGQ4hYgrhYgRDRCxCxDtQKCOBJBqDeD6DOBeAFAQAwEeDaCyDSA4BYAkAGpwDohyBKAdDX0YhgAuAeQ%2BA6AVIlCIAIg5C85zkfhOAuC7CGgfgKoIhtAFpHDeAGoakCAKoGBaAHC1DSAsAIg09Pg6YlDuBeAsB8QjBxBgj8AJIHA8Bjgoi6CJQTg%2BI1guCcQqg5DMQIhvhiAfgPAsA5Cf44wvCFgtUmBgAFAOQ8BMBLoKoPYaCuD%2BBBARAxB2ApAZBBBFAVB1BgjdA2hzZjBmD9A8AIglDYBmA2AQAHRaBSB0VOAuEeA8p6CkgagoiwoKpYhdgwoH1qUFwxAsBiADAfleAsNohmYsBZjtl2gFoMgXBBoxhWhSBAgsQZgBg2g8h0gBB3jchUgASGBph%2BgYgJgqkUiBAuhRhPAWg9BKMag4SehviISkSRgmgETxhbAsTwTShfiFhWDlhVgJByDKDZDgiGCOBdhVBSRNQwpNRJBdhgAYVbkhCzBIJcBCBgVOC5heBVCtBNiBCRDhDxCJSRDJDOAZDSAaC6CaTFDlDuDeCFhNDEAUAwCjD9C3QX0ICUBxiFRiAbJGRzD2pbtKAbDgjnCSivCcok0Si3CPCHA7SfDgN\/DAi5DQjwjcUoiuDYjDBgAEi6Ckjni0i5DMjYV%2BI7S8ipC6DCjijSiMA1g6DKiWBqi%2BADB6jGjmjWjGA7TOjhAt1ejpBCzBi1A5DdBoFxiUBJjCiHiIB5j2AliViBgOB1i%2BCtidjOA9iDijiGgTjvpgBvhVh\/EawwpdRcVDjdRKQwobIzt0AwoqAC8ahrj0U7jnR4BiToSahXj3AcSPivjigMS\/iQSaggTTz8gMgCTZgoSOhYSsSLyniYTah8T0TCTIS8TugnzJhGgbyiTFhSTeiKSOAqC5S5CaS6SGSmSWSupTps5jSGBGQSwIAeSY5oxVEBSVS1CRTBDxTJTxD9AZSqSFSFDbBlShTOypCzASLriyLKLNj0U0hnBJAgA). GCC `-O3 -march=alderlake` fully unrolls, loading all 8 `index` vectors before the outer loop and using them from registers. (Same in the original version.)\n\n\nThe inner loop looks like this; notice that it uses the same memory source operand twice to save front-bandwidth at the cost of more back-end uops. This is actually ok on Gracemont as well as Alder Lake; `vpunpckl\/hbw` is 2 front-end uops with or without a memory source operand. With 1.0 vs. 0.66 cycle throughput, but with separate loads I think the front-end would be a worse bottleneck depending how fast it can decode 2-uop instructions. And the `vpmaxuw` per unpack is extra vector ALU work to keep ports busy so it doesn't bottleneck on loads.\n\n\nClang `-mtune=gracemont` chooses differently, but it doesn't load twice even tuning for Alder Lake \/ Ice Lake.\n\n\n\n```\n.L7:\n        vpunpcklbw      ymm11, ymm7, YMMWORD PTR [rax+32]\n        vpunpckhbw      ymm10, ymm7, YMMWORD PTR [rax+32]\n        add     rax, 256\n        vpunpcklbw      ymm0, ymm8, YMMWORD PTR [rax-256]\n        vpunpckhbw      ymm9, ymm8, YMMWORD PTR [rax-256]\n        vpmaxuw ymm0, ymm0, ymm11\n        vpmaxuw ymm9, ymm9, ymm10\n        vpunpcklbw      ymm11, ymm6, YMMWORD PTR [rax-192]\n        vpunpckhbw      ymm10, ymm6, YMMWORD PTR [rax-192]\n        vpmaxuw ymm0, ymm0, ymm11\n        vpunpcklbw      ymm11, ymm5, YMMWORD PTR [rax-160]\n        vpmaxuw ymm9, ymm9, ymm10\n        vpunpckhbw      ymm10, ymm5, YMMWORD PTR [rax-160]\n        vpmaxuw ymm0, ymm0, ymm11\n...\n\n```\n\n<https:\/\/uica.uops.info\/> predicts Ice Lake could run it at 14 cycles per iteration, vs. 17 for the `vpblendvb` version. And that's nearly bottlenecked on vector ALU ports, so Alder Lake would be even worse with the `vpblendvb` version.\n\n\nI haven't analyzed by hand for Gracemont, or tried LLVM-MCA which might have a Gracemont model.\n\n\nI also haven't looked at optimizing it to use `vphminposuw` as part of the cleanup, which would save even more, helping pay for the extra shuffle work we're doing per vector.\n\n\n\n\n---\n\n\n**Or consider a branchy strategy**, like finding the max and then searching the array for for the first match. (compare\/movemask aka `to_bits(curr == bcast_max)`, and if non-zero, return `tzcnt(mask)`). You never need to load vectors of index data, and an early match reduces the amount of work. (But it can mispredict which might be much worse; still worth a try. But usefully microbenchmarking things that depend on correct branch prediction is hard - a microbenchmark can learn a pattern. Or if you make it totally random, it predicts worse than real data distributions.)\n\n\nWith only 8 vectors of data, that second pass loop can be fully unrolled with no loads. The first pass can leave the data in registers. (But it would have to be fully unrolled, too, perhaps checking a pair of ymm regs at a time for a match, with shift\/or and a 64-bit tzcnt. `vpmovmskb r32, ymm` is single-uop on Gracemont.) And it would mean separate load + max instructions in the first pass, not memory-source. Gracemont doesn't have a uop-cache but apparently its decoders manage ok for throughput. Perhaps not wonderfully with back-to-back 2-uop instructions.\n\n\n(This is basically the same strategy your current cleanup is using, find the max then search for its position, but across the whole 8-vector array. Allowing reduction to 128-bit for most of the horizontal max work between the first and second pass is nice.)\n\n\n\n\n---\n\n\n##### Commented version of your original, looking at how it compiled to asm:\n\n\n\n```\nint loop_vc_nested(const std::array<uint8_t, H*W> &img, const std::array<Vec32uc, 8> &idx) {\n  int sum = 0;\n  Vec32uc vMax, iMax, vCurr, iCurr;\n\n  for (int i=0; i<H*W; i+=W) {\n    iMax.load(&idx[0]);\n    vMax.load(&img[i]);\n\n    for (int j=1; j<8; j++) {\n      iCurr.load(&idx[j]);  \/\/ these get hoisted out of the outer loop and reused across img iters\n      vCurr.load(&img[i+j*32]);\n      \/\/ unsigned > isn't available until AVX-512.  VCL uses !(a == max(a,b))\n      \/\/ GCC XORs the compare result, clang uses max and a==min(a,b)\n      iMax = select(vCurr > vMax, iCurr, iMax);\n      \/\/ scanning backwards from the end with a==max(a,b), we could still find the earliest max\n      vMax = max(vMax, vCurr);\n    }\n\n#if 1\n   Vec32uc vMaxAll{horizontal_max(vMax)};\n   \/\/size_t maxidx = horizontal_find_first(vMax == vMaxAll); \/\/ total disaster on clang: non-inlined BSF wrapper forces vector spill\/reload of the idx vectors\n   size_t maxidx = _tzcnt_u32(to_bits(vMax == vMaxAll));\n#else\n    size_t maxidx = maxpos_u8_noscan_unpack(vMax);\n#endif\n    sum += iMax[maxidx];\n  }\n\n  return sum;\n}\n\n```\n\nwhich compiles to code that loads the first 4 vectors early, the some processing, then loading more as it goes. ymm1 = set1(-1), XOR with it does a NOT of the compare result.\n\n\n\n```\n# GCC13.2 -O3 -march=alderlake for the version of your source above\nloop_vc_nested(std::array<unsigned char, 208896ul> const&, std::array<Vec32uc, 8ul> const&):\n        push    rbp\n        mov     rax, rdi\n        xor     ecx, ecx\n        vpcmpeqd        ymm1, ymm1, ymm1   # set1(-1)\n        mov     rbp, rsp\n        and     rsp, -32                   # align the stack for the store that we index with movzx\n\n        vmovdqu ymm9, YMMWORD PTR [rsi+32] # idx[32..63]\n        vmovdqu ymm8, YMMWORD PTR [rsi]    # idx[0..31]\n        ...        # and all 8 vectors of idx\n        lea     rsi, [rdi+208896]         # img.end()\n.L2:\n        vmovdqu ymm0, YMMWORD PTR [rax+32]\n        vpmaxub ymm11, ymm0, YMMWORD PTR [rax]\n        add     rax, 256\n        vpmaxub ymm10, ymm11, YMMWORD PTR [rax-192]\n        vpcmpeqb        ymm0, ymm11, YMMWORD PTR [rax-256]\n        vpcmpeqb        ymm11, ymm11, ymm10\n        vpxor   ymm0, ymm0, ymm1\n        vpxor   ymm11, ymm11, ymm1\n        vpblendvb       ymm0, ymm8, ymm9, ymm0\n        vpblendvb       ymm0, ymm0, ymm7, ymm11\n        vpmaxub ymm11, ymm10, YMMWORD PTR [rax-160]\n        vpcmpeqb        ymm10, ymm10, ymm11\n        vpxor   ymm10, ymm10, ymm1\n        vpblendvb       ymm0, ymm0, ymm6, ymm10\n        vpmaxub ymm10, ymm11, YMMWORD PTR [rax-128]\n        vpcmpeqb        ymm11, ymm11, ymm10\n        vpxor   ymm11, ymm11, ymm1\n        vpblendvb       ymm0, ymm0, ymm5, ymm11\n        vpmaxub ymm11, ymm10, YMMWORD PTR [rax-96]\n        vpcmpeqb        ymm10, ymm10, ymm11\n        vpxor   ymm10, ymm10, ymm1\n        vpblendvb       ymm0, ymm0, ymm4, ymm10\n        vpmaxub ymm10, ymm11, YMMWORD PTR [rax-64]\n        vpcmpeqb        ymm11, ymm11, ymm10\n        vpxor   ymm11, ymm11, ymm1\n        vpblendvb       ymm0, ymm0, ymm3, ymm11\n        vpmaxub ymm11, ymm10, YMMWORD PTR [rax-32]\n        vpcmpeqb        ymm10, ymm10, ymm11\n        vpxor   ymm10, ymm10, ymm1\n        vpblendvb       ymm0, ymm0, ymm2, ymm10\n ## end of unrolled inner loop\n        vextracti128    xmm10, ymm11, 0x1   # start of horizontal_max\n        vpmaxub xmm12, xmm11, xmm10\n        vmovdqa YMMWORD PTR [rsp-32], ymm0   # store iMax\n        vpunpckhqdq     xmm10, xmm12, xmm12\n    ...\n        vpmaxub xmm10, xmm10, xmm12       # end of horizontal_max\n        vpbroadcastb    ymm10, xmm10\n        vpcmpeqb        ymm10, ymm10, ymm11\n        vpmovmskb       edx, ymm10\n        tzcnt   edx, edx        # your actual original used BSF, much worse on AMD\n        and     edx, 31         # this isn't in the source anywhere!\n        movzx   edx, BYTE PTR [rsp-32+rdx]\n        add     ecx, edx        # sum += \n        cmp     rsi, rax\n        jne     .L2         }while(ptr != endptr);\n\n        mov     eax, ecx\n        vzeroupper\n        ret\n\n```\n\nAs mentioned in the comments I added, saving an instruction around the blend (to get the opposite condition) could be done with `curr == max(vmax, curr)`, but that's true on a tie when your condition isn't. Looping backward could fix that, but might be harder for the prefetchers.\n\n\n(In asm at least, you could *load* all 8 vectors in forward order, or one from each cache line, but process them backwards. That makes out-of-order exec work even harder to hide load latency, assuming prefetch keeps streaming in order.)"}
{"questionId":"fa08c55a290649f6b4d74c10b1ee38bc","question":"How to resolve the Gradle Build Error: java.lang.NoSuchMethodError\nI am trying to build my project but every time I get the following errors:\n\n\n##### Error 1\n\n\n\n```\njava.lang.NoSuchMethodError: 'kotlin.sequences.Sequence com.google.devtools.ksp.processing.Resolver.getPackagesWithAnnotation(java.lang.String)'\n    at androidx.room.compiler.processing.ksp.KspRoundEnv.getElementsAnnotatedWith(KspRoundEnv.kt:107)\n    at androidx.room.compiler.processing.CommonProcessorDelegate.processRound(XBasicAnnotationProcessor.kt:100)\n    at androidx.room.compiler.processing.ksp.KspBasicAnnotationProcessor.process(KspBasicAnnotationProcessor.kt:62)\n    at com.google.devtools.ksp.AbstractKotlinSymbolProcessingExtension$doAnalysis$6$1.invoke(KotlinSymbolProcessingExtension.kt:291)\n    at com.google.devtools.ksp.AbstractKotlinSymbolProcessingExtension$doAnalysis$6$1.invoke(KotlinSymbolProcessingExtension.kt:289)\n    at com.google.devtools.ksp.AbstractKotlinSymbolProcessingExtension.handleException(KotlinSymbolProcessingExtension.kt:394)\n    at com.google.devtools.ksp.AbstractKotlinSymbolProcessingExtension.doAnalysis(KotlinSymbolProcessingExtension.kt:289)\n    at org.jetbrains.kotlin.cli.jvm.compiler.TopDownAnalyzerFacadeForJVM.analyzeFilesWithJavaIntegration(TopDownAnalyzerFacadeForJVM.kt:123)\n    at org.jetbrains.kotlin.cli.jvm.compiler.TopDownAnalyzerFacadeForJVM.analyzeFilesWithJavaIntegration$default(TopDownAnalyzerFacadeForJVM.kt:99)\n    at org.jetbrains.kotlin.cli.jvm.compiler.KotlinToJVMBytecodeCompiler$analyze$1.invoke(KotlinToJVMBytecodeCompiler.kt:257)\n    at org.jetbrains.kotlin.cli.jvm.compiler.KotlinToJVMBytecodeCompiler$analyze$1.invoke(KotlinToJVMBytecodeCompiler.kt:42)\n    at org.jetbrains.kotlin.cli.common.messages.AnalyzerWithCompilerReport.analyzeAndReport(AnalyzerWithCompilerReport.kt:115)\n    at org.jetbrains.kotlin.cli.jvm.compiler.KotlinToJVMBytecodeCompiler.analyze(KotlinToJVMBytecodeCompiler.kt:248)\n    at org.jetbrains.kotlin.cli.jvm.compiler.KotlinToJVMBytecodeCompiler.compileModules$cli(KotlinToJVMBytecodeCompiler.kt:88)\n    at org.jetbrains.kotlin.cli.jvm.compiler.KotlinToJVMBytecodeCompiler.compileModules$cli$default(KotlinToJVMBytecodeCompiler.kt:47)\n    at org.jetbrains.kotlin.cli.jvm.K2JVMCompiler.doExecute(K2JVMCompiler.kt:167)\n    at org.jetbrains.kotlin.cli.jvm.K2JVMCompiler.doExecute(K2JVMCompiler.kt:53)\n    at org.jetbrains.kotlin.cli.common.CLICompiler.execImpl(CLICompiler.kt:101)\n    at org.jetbrains.kotlin.cli.common.CLICompiler.execImpl(CLICompiler.kt:47)\n    at org.jetbrains.kotlin.cli.common.CLITool.exec(CLITool.kt:101)\n    at org.jetbrains.kotlin.daemon.CompileServiceImpl.compile(CompileServiceImpl.kt:1645)\n    at java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n    at java.base\/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.base\/java.lang.reflect.Method.invoke(Method.java:568)\n    at java.rmi\/sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:360)\n    at java.rmi\/sun.rmi.transport.Transport$1.run(Transport.java:200)\n    at java.rmi\/sun.rmi.transport.Transport$1.run(Transport.java:197)\n    at java.base\/java.security.AccessController.doPrivileged(AccessController.java:712)\n    at java.rmi\/sun.rmi.transport.Transport.serviceCall(Transport.java:196)\n    at java.rmi\/sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:587)\n    at java.rmi\/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:828)\n    at java.rmi\/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:705)\n    at java.base\/java.security.AccessController.doPrivileged(AccessController.java:399)\n    at java.rmi\/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:704)\n    at java.base\/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base\/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base\/java.lang.Thread.run(Thread.java:833)\n\n```\n\n##### Error 2\n\n\n\n```\nExecution failed for task ':core:database:kspDebugKotlin'.\n> A failure occurred while executing org.jetbrains.kotlin.compilerRunner.GradleCompilerRunnerWithWorkers$GradleKotlinCompilerWorkAction\n   > Internal compiler error. See log for more details\n\n```\n\n##### File Contents\n\n\nThis is my project level build.gradle.kts file:\n\n\n\n```\n\/\/ Top-level build file where you can add configuration options common to all sub-projects\/modules.\nplugins {\n    id(\"com.android.application\") version \"8.1.2\" apply false\n    id(\"org.jetbrains.kotlin.android\") version \"1.8.10\" apply false\n    id(\"com.android.library\") version \"8.1.2\" apply false\n    id(\"com.google.dagger.hilt.android\") version \"2.48\" apply false\n    id(\"com.google.devtools.ksp\") version \"1.8.10-1.0.9\" apply false\n}\n\n```\n\nThis is my module level build.gradle.kts file:\n\n\n\n```\nplugins {\n    id(\"com.android.library\")\n    id(\"org.jetbrains.kotlin.android\")\n    id(\"com.google.devtools.ksp\")\n    id(\"com.google.dagger.hilt.android\")\n}\n\nandroid {\n    namespace = \"com.yangian.superclock.core.database\"\n    compileSdk = 34\n\n    defaultConfig {\n        minSdk = 28\n\n        testInstrumentationRunner = \"androidx.test.runner.AndroidJUnitRunner\"\n        consumerProguardFiles(\"consumer-rules.pro\")\n    }\n\n    buildTypes {\n        release {\n            isMinifyEnabled = false\n            proguardFiles(\n                getDefaultProguardFile(\"proguard-android-optimize.txt\"),\n                \"proguard-rules.pro\"\n            )\n        }\n    }\n    compileOptions {\n        sourceCompatibility = JavaVersion.VERSION_17\n        targetCompatibility = JavaVersion.VERSION_17\n    }\n    kotlinOptions {\n        jvmTarget = \"17\"\n    }\n}\n\ndependencies {\n\n    implementation(\"androidx.core:core-ktx:1.12.0\")\n    implementation(\"androidx.appcompat:appcompat:1.6.1\")\n    implementation(\"com.google.android.material:material:1.10.0\")\n    implementation(\"org.jetbrains.kotlinx:kotlinx-coroutines-core:1.7.3\")\n    implementation(\"org.jetbrains.kotlinx:kotlinx-coroutines-android:1.7.3\")\n\/\/    implementation(\"org.jetbrains.kotlinx:kotlinx-coroutines-guava:1.7.3\")\n    implementation(\"org.jetbrains.kotlin:kotlin-test:1.9.10\")\n\n    testImplementation(\"junit:junit:4.13.2\")\n    androidTestImplementation(\"org.jetbrains.kotlinx:kotlinx-coroutines-test:1.7.3\")\n    androidTestImplementation(\"androidx.test.ext:junit:1.1.5\")\n    androidTestImplementation(\"androidx.test.espresso:espresso-core:3.5.1\")\n    androidTestImplementation(\"androidx.test:core:1.5.0\")\n\n    \/\/ Room\n    val roomVersion = \"2.6.0\"\n\n    implementation(\"androidx.room:room-runtime:$roomVersion\")\n    implementation(\"androidx.room:room-ktx:$roomVersion\")\n    ksp(\"androidx.room:room-compiler:$roomVersion\")\n\n    \/\/ Dagger Hilt\n    val daggerVersion = \"2.48\"\n\n    implementation(\"com.google.dagger:hilt-android:$daggerVersion\")\n    ksp(\"com.google.dagger:hilt-android-compiler:$daggerVersion\")\n}\n\n```\n\nI am new to Android Development and the gradle errors always scares me. I tried to get help from Bard and Bing but nothing worked. So, you guys are my last resort. Please help me to resolve these errors. Feel free to ask for more information about the project in order to resolve the error.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"kotlin"},"answer":"I have a feeling (haven't dug too deep into it yet) that Dagger\/Hilt version `2.48` are not compatible with somewhat outdated versions of Kotlin language (which is what you seem to be having, based on the content of your Gradle files).\n\n\nYou might need to bump\n\n\n- `org.jetbrains.kotlin.android` to `1.9.10`;\n- `com.google.devtools.ksp` to `1.9.10-1.0.13`;\n\n\nYou can keep Dagger\/Hilt set to `2.48.1` and Room set to `2.6.0`.\n\n\nI just upgraded Room to `2.6.0` in the project I work on and these are the versions that I ended up having. Works like a charm, the exception that you mentioned is gone.\n\n\n**Upd.** Yep, there is actually a [discussion](https:\/\/issuetracker.google.com\/issues\/303703303#comment3) of that same issue on Google's issue tracker; the latest versions of Kotlin and KSP are required to use Room 2.6.0."}
{"questionId":"dbaa2e97e2b24a6f8f43f1886580c00e","question":"Chrome Browser Error: Attestation check for Topics on https:\/\/pagead2.googlesyndication.com\/ failed\nIn the latest Chrome version (Version 118.0.5993.71 (Official Build) (64-bit)), for a software product I'm selling, I'm getting:\n\n\nAttestation check for Topics on <https:\/\/pagead2.googlesyndication.com\/> failed.\n\n\non line 1 (the `<DOCTYPE html>` tag).\n\n\nThis appears in Dev Console. Google is providing no documentation that I can find on this on what to do. No links explain anything. Am I supposed to add some `META` tags in the HTML to explain my software product?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"From what I understand, if you set the header\n\n\n\n```\nPermissions-Policy: browsing-topics=()\n\n```\n\nin your responses it should disable the attestation check whilst disabling the topic api.\n\n\nIn an .htaccess file, one can add:\n\n\n\n```\n<IfModule mod_headers.c>\n    Header set Permissions-Policy \"browsing-topics=()\"\n<\/IfModule>"}
{"questionId":"389b50e2551c406f8b7365f980ba0c3b","question":"OpenAI Python Package Error: 'ChatCompletion' object is not subscriptable\nAfter updating my OpenAI package to version 1.1.1, I got this error when trying to read the ChatGPT API response:\n\n\n\n> \n> 'ChatCompletion' object is not subscriptable\n> \n> \n> \n\n\nHere is my code:\n\n\n\n```\nmessages = [\n        {\"role\": \"system\", \"content\": '''You answer question about some service'''\n        },\n        {\"role\": \"user\", \"content\": 'The user question is ...'},\n    ]\nresponse = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=0\n    )\nresponse_message = response[\"choices\"][0][\"message\"][\"content\"]\n\n```\n\nHow can I resolve this error?","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"python"},"answer":"In the latest OpenAI package the `response.choices` object type is changed and in this way you must read the response:\n\n\n\n```\nprint(response.choices[0].message.content)\n\n```\n\nThe complete working code:\n\n\n\n```\nfrom openai import OpenAI\n\nclient = OpenAI(api_key='YourKey')\nGPT_MODEL = \"gpt-4-1106-preview\" #\"gpt-3.5-turbo-1106\"\nmessages = [\n        {\"role\": \"system\", \"content\": 'You answer question about Web  services.'\n        },\n        {\"role\": \"user\", \"content\": 'the user message'},\n    ]\nresponse = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=0\n    )\nresponse_message = response.choices[0].message.content\nprint(response_message )\n\n```\n\nSee [this example](https:\/\/github.com\/openai\/openai-python\/blob\/448ac7d046d12c26064b2050f3ce82bde3a24943\/README.md?plain=1#L143) in the project README."}
{"questionId":"53c28ae00ea443a5bd7d5b86911e7d53","question":"Why does GCC generate code that conditionally executes a SIMD implementation?\nThe following code produces assembly that conditionally executes SIMD in GCC 12.3 when compiled with `-O3`. For completeness, the code always executes SIMD in GCC 13.2 and never executes SIMD in clang 17.0.1.\n\n\n\n```\n#include <array>\n\n__attribute__((noinline)) void fn(std::array<int, 4>& lhs, const std::array<int, 4>& rhs)\n{\n    for (std::size_t idx = 0; idx != 4; ++idx) {\n        lhs[idx] = lhs[idx] + rhs[idx];\n    }\n}\n\n```\n\nHere is the [link](https:\/\/www.godbolt.org\/z\/7jhcx981h) in godbolt.\n\n\nHere is the actual assembly from GCC 12.3 (with -O3):\n\n\n\n```\nfn(std::array<int, 4ul>&, std::array<int, 4ul> const&):\n        lea     rdx, [rsi+4]\n        mov     rax, rdi\n        sub     rax, rdx\n        cmp     rax, 8\n        jbe     .L2\n        movdqu  xmm0, XMMWORD PTR [rsi]\n        movdqu  xmm1, XMMWORD PTR [rdi]\n        paddd   xmm0, xmm1\n        movups  XMMWORD PTR [rdi], xmm0\n        ret\n.L2:\n        mov     eax, DWORD PTR [rsi]\n        add     DWORD PTR [rdi], eax\n        mov     eax, DWORD PTR [rsi+4]\n        add     DWORD PTR [rdi+4], eax\n        mov     eax, DWORD PTR [rsi+8]\n        add     DWORD PTR [rdi+8], eax\n        mov     eax, DWORD PTR [rsi+12]\n        add     DWORD PTR [rdi+12], eax\n        ret\n\n```\n\nI am very interested to know a) the purpose of the first 5 assembly instructions and b) if there is anything that can be done to cause GCC 12.3 to emit the code of GCC 13.2 (ideally, without manually writing SSE).","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"It seems GCC12 is treating the `class` reference like it would a simple `int *`, in terms of whether `lhs` and `rhs` could *partially* overlap.\n\n\nExact overlap would be fine, if `lhs[idx]` is the same int as `rhs[idx]`, we read it twice before writing it. But with partial overlap, `rhs[3]` for example could have been updated by one of the `lhs[0..2]` additions, which wouldn't happen with SIMD if we did all the loads first before any of the stores.\n\n\nGCC13 knows that class objects aren't allowed to partially overlap (except for common initial sequence stuff for different struct\/class types, which I think doesn't apply here). That would be UB so it can assume it doesn't happen. GCC12's code-gen is a missed optimization.\n\n\n\n\n---\n\n\nSo how do we help GCC12? The usual go-to is `__restrict` for removing overlap checks or enabling auto-vectorization at all when the compiler doesn't want to invent checks + a fallback. In C, `restrict` is part of the language, but in C++ it's only an extension. (Supported by the major mainstream compilers, and you can use the preprocessor to `#define` it to the empty string on others.) You can use `__restrict` with references as well as pointers. (At least GCC and Clang accept it with no warnings at `-Wall`; I didn't double-check the docs to be sure this is standard.)\n\n\n\n```\n\/\/ downside: fn_restrict(same, same) would be UB\nvoid fn_restrict(std::array<int, 4>&__restrict lhs, const std::array<int, 4>& rhs)\n{\n    for (std::size_t idx = 0; idx != 4; ++idx) {\n        lhs[idx] = lhs[idx] + rhs[idx];\n    }\n}\n\n```\n\n\n\n---\n\n\n### Or manually read all of `lhs` before writing any of it\n\n\nSince your `array` is small enough to fit in one SIMD register, there's no inefficiency in copying. This would be bad for `array<int, 1000>` or something!\n\n\n\n```\n\/\/ downside: only efficient for small arrays that fit in a few vector regs at most\nvoid fn_temporary(std::array<int, 4>& lhs, const std::array<int, 4>& rhs)\n{\n    auto sum = lhs;    \/\/ read the possibly-aliasing data into a temporary\n    for (std::size_t idx = 0; idx != 4; ++idx) {\n        sum[idx] += rhs[idx];  \/\/ update the temporary\n    }\n    lhs = sum;   \/\/ store back, after all loads\n}\n\n```\n\nBoth of these compile to the same auto-vectorized asm as GCC13, with no wasted instructions ([**Godbolt**](https:\/\/www.godbolt.org\/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXAGx8BBAKoBnTAAUAHpwAMvAFYTStJg1DIApACYAQuYukl9ZATwDKjdAGFUtAK4sGe1wAyeAyYAHI%2BAEaYxCAAnKQADqgKhE4MHt6%2BekkpjgJBIeEsUTEAzLaY9nkMQgRMxAQZPn5cFVVptfUEBWGR0XG2dQ1NWa0KQ93BvcX9pQCUtqhexMjsHOalwcjeWADUJqVu9cRMAJ4H2CYaAIJX1wBuqHjou1QMAPrEmOPEeA4Q43QIBAxzOBzcwQIpF2ABYLuZJO9Pt8CL8HLtaAgFNC0AxxrtAcDQedDpDoXDSpczJJdsQsXM7iYAOxWG67dmvEi7AEEIEgFIAL0w7wIu2eqn2pQAIrsNAcLGL0BKwGADjKKQrrNZxXN9iy7hzDRisSYAKwWcVmmVq40KM0WpVW\/aWWkm82W01S%2BUGjnMr03P2MgM3R7PV4fAiYFhJE7EU48vnE8Fk2Hw6m2nECfGEkHEE4kiGCclpml0hQMgP6tkcpheIgEnySmWYu2lVnXQ38YjcnOC4Wi8VN2XyxXK1XS1Nt51WSw6vXto0chQ%2Be0e62WG1l1eOz3e6vswP721D5csPe3Jn%2B27Bh5PF5vd4kPDABNEvNg0lFydUmktzN40UcyTT8oW\/BFXXLRkqw7Dkux7XlgT7EVRyHOUp0HFUbQ1adtSVXVmQXRdbW3VQnRtFsSLIl0t3dHcvTbH0D0vKCrw4BZaE4U1eD8DgtFIVBODcLUXQUJYVkwZ1Sh4UgCE0NiFgAaxAGEmQAOhhMxYgADg0U1JC4U1DI0JlDP0TgYW4uT%2BM4XgFBADQZLkhY4FgGAoDciAkDQaM6GichKG8hJfJiYAuDMcoaFoSNiDsiAIisiJgnqU5OGkxLmDjAB5CJtEwBxUt4by2EETKGFoFLeN4LAIi8YAjloWg7O4Kqo0MYBxEq0h8C%2BBw8Hub4rMwVQ8rrNY%2BMhSorNoPAIljU4PCwKzUTwFgCoWKgDGABQADU8EwAB3TKEkYAqZEEEQxHYAyzvkJQ1Cs3RWgMIwUGEmxpoiOzYGYNgQAiVBkihfqYm4LSeGcn72AICIIlIYHODMDREdKeT%2BISaomoAWky0pdkxgB1MRaF4VBgd%2BLAvogBY7Dy6oXAYdxPGafwGZ6IoSmyZJUgEEYWkSLnqjZvoYjGSpaY6CZeb0GneoEToGiFmYRcGLopbGCZFY5rhqbE1YJHYzjLM6gSOF2VQtMkTHJBhXZgGQZBdjC1TcYgXBCC5DZtd4WTKrmBYEEwJgsBiKnSCU00HI4jgLNIVbTS4VSzCZGFYmM\/TJBMrTYlNUgeL4k3bPsxzfdIFzEBQVAfPoMgKAgQLgpAULSjMPg6Gi2L4s69LktO7uspy2nTqKxgCFK8qrOq2r6sa06sBYNqOr47rxf6pq%2BKGkbI1Oiao74j65oWsbvd%2BVbmvWzadr2w7jp46T%2BHO0RxC4GEbsUFR1E63QW%2Be4w3v0GbKYLFQOjNIWMcZ40Jg1EmZNngDUgNTMWss\/AQFcGrUggQpjs36K0HI3N0hM1GPzXIaRNbYLaOLOWksCF8xltUeWkxCjC2llQzINCNaYKYdrRYyw9ZcKjlxXOVkTZmwtlbG2dsHZcFKInbkbsiDdk9nMb2Tl\/aB2DpQRSIAI5mWjkbfONlbBFx9loP2OizC8DjgnJOKc05SEztnQRxsDHGLYqXDyXlK5BWrv5OuniG7bDalwJkGhWiRXbpQTufE%2B4VTSklfuuV8rNVIMPEqZUKpL0wDVOqRMmrSTngvI%2BXU8A9UcKvQaw1kCjSsjvKaM0D4YEKctU%2B4M%2BAX12gdI6J0kn32EI\/K6Ocelv3up\/EA5Qf6vQ3JYf%2Bn14AQEhn9AG4w4b9FBi0uZrAoYw2WSDRGyNUbAIxpwbGuMCZE2gdEcmcDQ60LSPTRmrCWboFIcrXB1Q0GvJIRwpW0tEF0JYczUW7RKFdGecw1W1CwUKy%2BVrHWPCroGw4AIvOJNOAiMttbXYASjCOzUhoVSXBZH4HkZJL2xcTEIpjqtGEWlVJMjMBHWI4UtLNxhFwYyjj9EcELg5Fxpjw5mFUqaYJXBYgwlKFpGEkrGUwhzlHFGHKUVcrJajKO5iFXWSVbyhYwMUjOBhEAA%3D))\n\n\n\n```\n# GCC12 -O3\nfn_temporary(std::array<int, 4ul>&, std::array<int, 4ul> const&):\n        movdqu  xmm0, XMMWORD PTR [rsi]\n        movdqu  xmm1, XMMWORD PTR [rdi]\n        paddd   xmm0, xmm1\n        movups  XMMWORD PTR [rdi], xmm0\n        ret\n\n```\n\nPromising alignment (like `alignas(16)` one one of the types?) could let it use `paddd xmm1, [rdi]`, a memory source operand, without AVX."}
{"questionId":"50b510c0cc6a4cb3ac1886c7a43dea3e","question":"Why is there no parallel `std::accumulate` in the C++ standard?\nI think it is confusing that there is no parallel version of [`std::accumulate`](https:\/\/en.cppreference.com\/w\/cpp\/algorithm\/accumulate) in the C++ standard. It appears to me that it would be trivial to implement it in parallel, for instance, based on OpenMP or SIMD instructions. Does anybody have a good explanation for why the standard committee chose to introduce a parallel version of `std::reduce` but not `std::accumulate`? Is it because of the different iterator types?","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"cpp"},"answer":"They introduced `reduce` because `accumulate` *cannot* be parallelized as it was written.\n\n\nThe C++ standard defines the expected behavior of the various functions. `accumulate` is defined as follows:\n\n\n\n> \n> Computes its result by initializing the accumulator `acc` with the initial value `init` and then modifies it with `acc = std::move(acc) + *i` or `acc = binary_op(std::move(acc), *i)` for every\n> iterator `i` in the range `[first, last)` **in order**.\n> \n> \n> \n\n\nEmphasis added. Those two words make the algorithm unparallelizable. And because those words are part of the standard, users of `accumulate` are allowed to *rely* on the \"in order\" guarantee. They can make `operator+` or `binary_op` non-associative operations, ones that rely on the ordering of the operation. So the committee couldn't just take those words away without potentially breaking a bunch of code.\n\n\nAnd they didn't want to have a parallel version of `accumulate` that had fundamentally different behavior from the non-parallel version. Therefore, `reduce` was introduced without the \"in order\" guarantee (and with other language that helped make it parallelizable)."}
{"questionId":"d6f595ad12904216abb62e3e66277ebd","question":"FileVersionInfo.ProductVersion suddenly contains git commit hash\nI have a .NET 6 WPF project and cannot figure out what changed as the `ProductVersion` of my assembly suddenly contains a git commit hash.\n\n\nMy project file always looked like this, where I also updated the version information:\n\n\n\n```\n  <PropertyGroup>\n    <OutputType>WinExe<\/OutputType>\n    <TargetFramework>net6.0-windows<\/TargetFramework>\n    <UseWPF>true<\/UseWPF>\n    ...\n    <Version>0.9.2<\/Version>\n  <\/PropertyGroup>\n\n```\n\nThen, in my code I use the following to get the version as string:\n\n\n\n```\n string? productVersion = FileVersionInfo.GetVersionInfo(Assembly.GetEntryAssembly()!.Location).ProductVersion;\n\n```\n\nThis always returned the version as expected, e.g. `0.9.2`, exactly as it was set in the project file. But something changed and now it looks like this: `0.9.2+abf5b643b64475a8b1d7d89284e3478b1ba4a431`. This is also what shows up in Explorer on the Details tabs of the .dll and .exe files.\n\n\nI do regularly update Visual Studio in order to receive updates on .NET 8 which I use in other projects, so it may be related to some update. But I've no idea what exactly changed or how I can define the `ProductVersion` to not include the git commit hash (and yes, it's the commit hash). Haven't worked on the WPF project for a while as well, so don't know when this behavior changed.\n\n\nIronically, I needed the git commit in the past so had to put in some [extra effort](https:\/\/stackoverflow.com\/a\/71747677\/1649108) to have this available, though abbreviated to 8 characters.\n\n\nAny ideas?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"csharp"},"answer":"The described behavior can be disabled by adding\n\n\n\n```\n<IncludeSourceRevisionInInformationalVersion>false<\/IncludeSourceRevisionInInformationalVersion>\n\n```\n\nto the project file.\n\n\nIt seems to be introduced by SourceLink related changes in the SDK 8 version. I found an existing github issue as well:\n<https:\/\/github.com\/dotnet\/sdk\/issues\/34568>"}
{"questionId":"ab07ba8c2f9643d281e7e828f27a8dd6","question":"Why is builtin sorted() slower for a list containing descending numbers if each number appears twice consecutively?\nI sorted four similar lists. List `d` consistently takes much longer than the others, which all take about the same time:\n\n\n\n```\na:  33.5 ms\nb:  33.4 ms\nc:  36.4 ms\nd: 110.9 ms\n\n```\n\nWhy is that?\n\n\nTest script ([Attempt This Online!](https:\/\/ato.pxeger.com\/run?1=fZHBasMwDEDZ1V8h2CHxSNM4YbAY-iUhFLtxWkPsBMc9jNIv2aWX7Sf2J_uayUkaGIwadBDSexbSx9fw7k-9vd0-z77dvP08fbeuN-C1UdqDNkPvPDg1KOEJsbCDfJ9lWQgiMKs0bLfAoO0daNAWnLBHFVtaAzxDlSXAEsgTKBJI0xSzfVmWIWoiVzx_gM-GWZIvklVxQIWoON8w7P_noWL9cGGLSYO-rCYN4vIx_hfO7zCbBkMFCZNbYVQYPhLy0EScBLYbPdqPXS9FN8a0Cj31VAmrxZLRNp73GnfCyEZwGHHXqokRpQnYs5HK7RilEzU4bX3cRpcgunK4BM0LUwXw15S1VzBjROcbLqe8n_QX)):\n\n\n\n```\nfrom timeit import repeat\n\nn = 2_000_000\na = [i \/\/ 1 for i in range(n)]  # [0, 1, 2, 3, ..., 1_999_999]\nb = [i \/\/ 2 for i in range(n)]  # [0, 0, 1, 1, 2, 2, ..., 999_999]\nc = a[::-1]                     # [1_999_999, ..., 3, 2, 1, 0]\nd = b[::-1]                     # [999_999, ..., 2, 2, 1, 1, 0, 0]\n\nfor name in 'abcd':\n    lst = globals()[name]\n    time = min(repeat(lambda: sorted(lst), number=1))\n    print(f'{name}: {time*1e3 :5.1f} ms')","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"python"},"answer":"As alluded to in the comments by btilly and Amadan, this is due to how the [Timsort](https:\/\/en.wikipedia.org\/wiki\/Timsort) sorting algorithm works. Detailed description of the algorithm is [here](https:\/\/bugs.python.org\/file4451\/timsort.txt).\n\n\nTimsort speeds up operation on partially sorted arrays by identifying runs of sorted elements.\n\n\n\n> \n> A run is either\n> \"ascending\", which means non-decreasing:\n> \n> \n> a0 <= a1 <= a2 <= ...\n> \n> \n> or \"descending\", which means strictly decreasing:\n> \n> \n> a0 > a1 > a2 > ...\n> \n> \n> Note that a run is always at least 2 long, unless we start at the array's\n> last element.\n> \n> \n> \n\n\nYour arrays **a**, **b** and **c** each consist of just one run.\nThe array **d** has 1 million runs.\n\n\nThe reason why the descending run cannot be `>=` is to make the sort stable, i.e. keep the order of equal elements:\n\n\n\n> \n> The definition of descending is strict, because the main routine reverses\n> a descending run in-place, transforming a descending run into an ascending\n> run. Reversal is done via the obvious fast \"swap elements starting at each\n> end, and converge at the middle\" method, and that can violate stability if\n> the slice contains any equal elements. Using a strict definition of\n> descending ensures that a descending run contains distinct elements.\n> \n> \n> \n\n\nPython 3.11 has slightly improved version of timsort, sometimes called [powersort](https:\/\/www.i-programmer.info\/news\/216-python\/15954-python-now-uses-powersort.html), but it uses the same run detection and thus has the same performance."}
{"questionId":"d110ab8f2b154cb38fdef74c22c20a3b","question":"x = x\\*0.90; gives lossy conversion error. x\\*=0.90; does not. Why?\nI have written the code:\n\n\n\n```\nint x = 18;\nx *= 0.90; \nSystem.out.println(x);\n\n```\n\nThis code printed `16`\n\n\nHowever, when I wrote\n\n\n\n```\nint x = 18;\nx = x * 0.90; \nSystem.out.println(x);\n\n```\n\nit gave me the following error: `incompatible types: possible lossy conversion from double to int` \n\n\nI expected both of these code examples to result in the exact same error as `x *= y;` is the same as `x = x * y;`, but `x *= 0.90;` somehow works and `x = x * 0.90;` does not. Why is this the case?","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"java"},"answer":"Because the Java Language Specifcation (JLS) says so. It's a bit odd, but, when using the compound assignment operators (`*=`, `+=`, etcetera), the cast is implied.\n\n\nSee [JLS \u00a715.26.2](https:\/\/docs.oracle.com\/javase\/specs\/jls\/se8\/html\/jls-15.html#jls-15.26.2) which clearly shows the cast in the example right at the top of that section.\n\n\nWhy does it do that? Well, I don't think SO is the right place to ask 'what were the designers thinking 30 years ago when this part of the JLS spec was written up'.\n\n\nEDIT: This answer used to mention 'probably because of C' but as comments show, no, in C neither form requires an explicit cast."}
{"questionId":"997c9d020d5c489ca91e39b74d3a99d0","question":"When is it ok to be !ok() with C++20 chrono dates?\nThe `<chrono>` library allows dates to silently fall into a state of !ok(). For example:\n\n\n\n```\n#include <chrono>\n#include <iostream>\n\nint\nmain()\n{\n    using namespace std;\n    using namespace chrono;\n\n    auto date = 2023y\/October\/31;\n    cout << date.ok() << '\\n';\n    date += months{1};\n    cout << date.ok() << '\\n';\n}\n\n```\n\nOutput:\n\n\n\n```\n1\n0\n\n```\n\n[Demo.](https:\/\/wandbox.org\/permlink\/YJSKryKXQ503eLZG)\n\n\nI get that Oct 31 is a valid date and that Nov 31 is not. But why isn't Nov 31 an error (assert or throw)? Or why does it not snap back to Nov 30, or roll over into Dec 1 as other date libraries do?\n\n\nIs it not error prone to just let Nov 31 silently exist?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"cpp"},"answer":"The question, in part, almost answers itself:\n\n\n\n> \n> Or why does it not snap back to Nov 30, or roll over into Dec 1 as other date libraries do?\n> \n> \n> \n\n\nBecause there isn't any consistent practice on what should happen, `<chrono>` leaves it up to the client for what should happen. Therefore literally *anything* the programmer can dream up and implement *can* happen.\n\n\nFor example, here is how to snap back to Nov 30:\n\n\n\n```\nauto date = 2023y\/October\/31;\ndate += months{1};\nif (!date.ok())\n    date = date.year()\/date.month()\/last;\n\n```\n\nAnd here is how to roll into December:\n\n\n\n```\nauto date = 2023y\/October\/31;\ndate += months{1};\nif (!date.ok())\n    date = sys_days{date};\n\n```\n\nThe former is quite obvious what it is doing in the code. But the latter deserves a little more explanation: Conversion to `sys_days` is just converting a `{year, month, day}` data structure into a `{count_of_days}` data structure. And that conversion is allowed to happen even if the day field is overflowed. This is just like the C timing API between `tm` and `time_t`.\n\n\nAnd just like there is no invalid `time_t`, there is no invalid `sys_days`. It is just a count of days since (or before) 1970-01-01. And when you convert that count back to `{year, month, day}`, it results in a valid (`.ok() == true`) `year_month_day`.\n\n\nObviously the programmer could also declare an error:\n\n\n\n```\nauto date = 2023y\/October\/31;\ndate += months{1};\nif (!date.ok())\n    throw \"oops!\";\n\n```\n\nOne cool aspect of this behavior (prior to corrections added by the programmer) is that the date arithmetic follows normal arithmetic rules:\n\n\n\n```\n z = x + y;\n assert(x == z - y);\n\n```\n\nI.e., if you add a month, and then subtract a month, you are guaranteed to get back the same date:\n\n\n\n```\nauto date = 2023y\/October\/31;\nassert( date.ok());\ndate += months{1};\nassert(!date.ok());\ndate -= months{1};\nassert( date.ok());\nassert(date == 2023y\/October\/31);\n\n```\n\n[Demo.](https:\/\/wandbox.org\/permlink\/Md9tAz7JQ1C5ZyrC)\n\n\nAnd sometimes the correct action is none of flag-error, roll-over or snap-back. Sometimes the correct action is to *ignore* the invalid date!\n\n\nConsider this problem:\n\n\nI want to find all dates for some year `y` which are the 5th Friday of the month (because that is party day or whatever). Here is a very efficient function which collects all of the 5th Fridays of a year:\n\n\n\n```\n#include <array>\n#include <chrono>\n#include <utility>\n#include <iostream>\n\nstd::pair<std::array<std::chrono::year_month_day, 5>, unsigned>\nfifth_friday(std::chrono::year y)\n{\n    using namespace std::chrono;\n\n    constexpr auto nan = 0y\/0\/0;\n    std::array<year_month_day, 5> dates{nan, nan, nan, nan, nan};\n    unsigned n = 0;\n    for (auto d = Friday[5]\/January\/y; d.year() == y; d += months{1})\n    {\n        if (d.ok())\n        {\n            dates[n] = year_month_day{d};\n            ++n;\n        }\n    }\n    return {dates, n};\n}\n\nint\nmain()\n{\n    using namespace std::chrono;\n\n    auto current_year = year_month_day{floor<days>(system_clock::now())}.year();\n    auto dates = fifth_friday(current_year);\n    std::cout << \"Fifth Friday dates for \" << current_year << \" are:\\n\";\n    for (auto i = 0u; i < dates.second; ++i)\n        std::cout << dates.first[i] << '\\n';\n}\n\n```\n\nExample output:\n\n\n\n```\nFifth Friday dates for 2023 are:\n2023-03-31\n2023-06-30\n2023-09-29\n2023-12-29\n\n```\n\n[Demo.](https:\/\/wandbox.org\/permlink\/nAeQhjsmGn2pgoc1)\n\n\nIt turns out that it is an invariant that every year will have either 4 or 5 months which will have 5 Fridays. So we can efficiently return the results as a pair<array<year\\_month\\_day, 5>, unsigned>, where the second member of the pair will always be either 4 or 5.\n\n\nThe first job is just to initialize the array with a bunch of `year_month_day`s. I've arbitrarily chosen `0y\/0\/0` as a good initialization value. What do I like about this value? One of the things I like is that it is `!ok()`! If I accidentally access `.first[4]` when `.second == 4`, an extra bit of safety is that the resultant `year_month_day` is `!ok()`. So being able to construct these `!ok()` values without an assert or exception is important just for that reason (like a nan). The cost? Nothing. These are compile-time constants.\n\n\nNext I iterate over each month for the year `y`. The first thing to do is construct the 5th Friday for January. And then increment the loop by 1 month until the year changes.\n\n\nNow since not every month has a 5th Friday, this may not result in a valid date. But in this function the proper response to constructing an invalid date is not an assert nor an exception, nor a snap-back or roll-over. The proper response is to ignore the date and iterate on to the next month. If it is a valid date, then it pushed on to the result.\n\n\nMany invalid dates were computed during the execution of this program. And none of them represented errors. And invalid dates were even used within computations (adding a month). But because the arithmetic is regular, everything just works.\n\n\nSo in summary, it is really best to leave the behavior of invalid dates up to the clever programmer. Because clever programmers can create all kinds of ingenious solutions to their problems, given the flexibility to do so."}
{"questionId":"7c7b3ca845e44d5c8410c646ef973798","question":"ld: warning: ignoring duplicate libraries: '-lgcc' after the recent update of Xcodebuild tools\nRecently, I upgraded to Xcode 15, and with that the update of the xcodebuild tools. However, suddenly, my standalone C++ applications are not able to use the g++-13 compiler for some reason? Anyone knows what might work, I don't want to downgrade to Xcode 14.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"cpp"},"answer":"From Apple's internal employees, this is the fault of Homebrew, you can either wait for Homebrew to fix it or try other ways to install GNU tools."}
{"questionId":"f2336d898d1d4fc2bdf62debde26bcb7","question":"Why is optimization forbidden if a C compiler cannot prove lack of UB?\nIf a C program has undefined behavior, anything can happen. Therefore compilers may assume that any given program does not contain UB. So, suppose our program contains the following:\n\n\n\n```\nx += 5;\n\/* Do something else without x in the meantime. *\/ \nx += 7;\n\n```\n\nOf course, this can be optimized to\n\n\n\n```\n\/* Do something without x. *\/\nx += 12;\n\n```\n\nor similarly the other way.\n\n\nIf x has type `unsigned int` then there is no possibility of UB in the above program. On the other hand, if x has type `signed int` then there is a chance of overflow and hence UB. Since the compiler may assume that our program contains no UB, we can do the same optimization above. In fact, in this case the compiler can even assume that `x - 12 <= MAX_INT`.\n\n\nHowever, this seems to contradict Jens Gustedt's famous [\"Modern C\"](https:\/\/www.manning.com\/books\/modern-c) (pg 42):\n\n\n\n> \n> But such an optimization can also be forbidden because the compiler can\u2019t prove that a certain operation will not force program termination. In our example, much depends on the type of x. If the current value of x could be close to the upper limit of the type, the innocent-looking operation x += 7 may produce an overflow. Such overflows are handled differently according to the type. As we have seen, overflow of an unsigned type is not a problem, and the result of the condensed operation will always be consistent with the two separate ones. For other types, such as signed integer types (signed) and floating-point types (double), an overflow may raise an exception and terminate the program. **In that case, the optimization cannot be performed.**\n> \n> \n> \n\n\n(Emphasis mine). If the compiler can (and does) assume our program has no UB, why can this optimization not be performed?\n\n\n[1]: [Jens Gustedt. Modern C. Manning, 2019, 9781617295812. hal-02383654](https:\/\/inria.hal.science\/hal-02383654)","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"c"},"answer":"TL:DR: You're right, such optimization is not forbidden for `signed int`, only for `float`\/`double`, and not just because of exceptions in that case.\n\n\nOne reason for things to be UB is that some obscure machine could raise an exception. But hitting UB is *not guaranteed* to raise an exception on all machines (unless you're compiling with `gcc -fsanitize=undefined`, for types of UB it or clang can reliably detect, or [`gcc -ftrapv`](https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/Code-Gen-Options.html) to define the behaviour of signed int overflow as trapping). When a compiler treats UB as an optimization opportunity by assuming something won't happen, things are very different: UB is not a synonym for \"fault\" or \"trap\".\n\n\n**There are operations that might trap on normal CPUs, such as deref of unknown pointers, and integer division on some ISAs** (such as x86 but not ARM). These would work as examples if you're looking for an operation that compilers potentially need to be careful of to avoid introducing an exception before side-effects that need to happen, or before a branch that might cause the abstract machine not to reach an undefined operation at all.\n\n\n\n\n---\n\n\nSigned integer overflow is UB so anything can happen at any point in the execution of a program where that happens (in C++, and according to some interpretations of the C standard), even when compiling for a machine with non-trapping `add` instructions (like all modern ISAs).\n\n\nSome implementations might define the behaviour as raising an exception. If they define *where* that exception gets raised, then it would prevent the optimization; each addition needs to happen as written so it can trap there if that operation in the abstract machine overflows. But that would be defining the behaviour, the exact opposite of UB meaning [absolutely zero guarantees](http:\/\/blog.llvm.org\/2011\/05\/what-every-c-programmer-should-know.html) about what your program actually does.\n\n\nIn C, if n3128 is accepted1, any visible side-effects sequenced before the abstract machine encounters UB must happen. But after UB is encountered, literally anything is allowed, including doing I\/O. UB doesn't have to fault and stop execution. If a compiler was compiling the `+=` operations with MIPS signed-overflow-trapping `add` instructions instead of the usual `addu`, it would be legal to optimize to `x+=12` *after* the intervening code even if it contained I\/O operations or other visible side-effects (like a `volatile` read or write). Even if the `x+=5` caused signed overflow UB in the abstract machine, it's fine if the actual behaviour is to trap later (for example when the abstract machine would have done the `x+=7` part). As long as it's at or after the abstract machine hit UB, literally anything is allowed. (In C++, it would also be legal to do the possibly-trapping `addi $s0, $s0, 12` even *before* a `printf` or something, due to the explicit lack of requirements on behaviour even before the first undefined operation, for an execution that does encounter UB. But only if the compiler can prove that printf definitely returns, so in practice this optimization can usually only happen for `volatile` accesses if at all. But even without retroactive effects, we can either do `x+=5` before and `x+=7` after, or `x+=12` after. Not faulting is valid behaviour for signed overflow, but the abstract machine has done an undefined operation so anything that happens later, like printing and then trapping, or just having the addition wrap, is allowed.)\n\n\n**The compiler just has to avoid introducing exceptions on paths of execution that shouldn't have any**. (Which isn't a problem for integer addition on mainstream ISAs; most don't even have a trapping signed-add instruction, and compilers targeting MIPS use `addu` even for signed math so they can optimize freely, and because historically programmers didn't want trapping on `int` math.)\n\n\n#### Footnote 1: C vs. C++, and whether C UB should be \"concrete\" or \"abstract\"\n\n\nSee *[Does undefined behaviour retroactively mean that earlier visible side-effects aren't guaranteed?](https:\/\/stackoverflow.com\/questions\/77146088\/does-undefined-behaviour-retroactively-mean-that-earlier-visible-side-effects-ar)* and *[n3128: Taming the Demons -- Undefined Behavior and Partial Program Correctness](https:\/\/www.open-std.org\/jtc1\/sc22\/wg14\/www\/docs\/n3128.pdf)*, a proposal to have ISO C clearly specify that visible side-effects (like I\/O) before the abstract machine reaches an undefined operation must still happen. (Common interpretations of the current ISO C standard treat UB like in C++, where the [C++ standard explicitly allows](https:\/\/eel.is\/c++draft\/intro.abstract#5) \"breaking\" stuff along an inevitable path to UB.)\n\n\n\n\n---\n\n\n### Practical example of compilers doing this\n\n\nBoth `int` and `unsigned` can do this optimization, it's only FP types that can't, but that's (also) because of rounding even if you compile with `gcc -fno-trapping-math` (an FP math option). See it in action on [Godbolt](https:\/\/godbolt.org\/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1AB9U8lJL6yAngGVG6AMKpaAVxYM9DgDJ4GmADl3ACNMYj0AB1QFQlsGZzcPSOjYgV9\/IJZQ8K4LTCsbASECJmICBPdPXMtMazji0oJ0wJCwvQUSsoqk6s6mvxastq4ASgtUV2Jkdg4\/AgBqGIYAawBSAGYAIVWNAEE5%2Bf5UYxjgf3QIA9UR%2BdWAdm29%2Bef51VuAJm31gBF5gFYNo9di9Fn5lrcfvMuICdsCXm9Vp8Nr87jCni9iJgCJMGK80bt7t9YbDXAxTudDqhjqTyZgLjS8Gc6a8bvcgSCEUjIQCtrCQUtwciofiOR8viiRRisTi8by9oTiXsqLRUEwFkdjMrVQRqCq1Szbg8%2BfCxUKeeyXgKIb9oXK4c9OeL5qi7SDMdjiLjVPiFXsOGNaJw\/rxPBwtKRUJxHIsJlNMB91jxSARNP6xssQOt3gA6O4ANkkfw0AA4NFxi3m\/gBOSTrfScSQh1MRzi8BQgDTJ1NjOCwJBoFgROhhciUAdD%2BjhZDALhZ0hYABueGmADU8JgAO4AeQijE4SZotAIYXbEGCzeCflKAE997xL8xiNet8FtLUU9xeAO2IItwxaLeYa8FgwSuMAjhiLQ7afvOmAsIYwDiEB854JidQLpg0Hhpgqi1K4x53uQgj5M2tB4MExA3s4WDNgQxB4Cwd5jMqTDAAoa6bjue4wfwggiGI7BSDIgiKCo6jIbouQGEYICmMY5hkcE7aQGMqARIUZKcAAtFu6zzFpVAMKgWl0UwERDkYWnwQQCC8KgGHEPRWDKRAYw1HUdgQA43RVKQPgDJk2TJDEGk%2BcFqQMM0gXDHkBT1H0YXVPk77xY0UWtDkFgJS4lTtH06VDDkbmxtMEgBkGTbIZGHCvBWWkFvMwDIMgULrNm7zzBAuCECQCajLwH5aCM6aZpI2Z\/JIebFvmXAaHmU3FlWdaBhwjakKG4bVW2HZdkBPYwIgKCoIOw5kBQEDjqdKDINJM55honaLsumAcduu6hgedDHsQp7nshD43oRANPi%2Bb7WIR36MAQf4Ac2IFgRBtBQYRWDwUYSHhvgaE2BhWG8DheEETBcwkchimUU%2B1EzOGdEMUxfAGGxr1cR9vC8cIojiEJ7OiWoza6NIt2yWY%2BjkS5qnqXE0E6esdkOU5mHwG5yUeZ4XkME4OU9H5GsFUFuRRCFcSJaQhsRXrMXuRpDRdFrvlW6lZQW5lHSNCbrtOwFGVleMkylaM9YcMG63NtVtV5vVkjzDdCFQnm2YaAnnXdUQxB9SMA3dmMCCYEwWDhK5pAZlm2YLXmXB\/NNk1\/FwlaB2tjGzZ2G12a2Fg7YNaaB%2B8lWbW3nfDaQDkxHYkhAA) with GCC13 and Clang 16\n\n\n\n```\nint sink;    \/\/ volatile int sink doesn't make a difference\nint foo_signed(int x) {\n    x += 5;\n    sink = 1;\n    x += 7;\n    return x;\n}\n\/\/ also unsigned and float versions\n\n```\n\n\n```\n# GCC -O3 -fno-trapping-math\nfoo_signed:                               # input in EDI, retval in EAX\n        mov     DWORD PTR sink[rip], 1\n        lea     eax, [rdi+12]             # x86 can use LEA as a copy-and-add\n        ret\nfoo_unsigned:\n        mov     DWORD PTR sink[rip], 1\n        lea     eax, [rdi+12]\n        ret\nfoo_float:                    # first arg and retval in XMM0\n        addss   xmm0, DWORD PTR .LC0[rip]     # add Scalar Single-precision\n        mov     DWORD PTR sink[rip], 1\n        addss   xmm0, DWORD PTR .LC1[rip]     # two separate 5.0f and 7.0f adds\n        ret\n\n```\n\n\n\n---\n\n\n#### Earlier version of an answer, making some different points for the same conclusion\n\n\nYou're correct; assuming `x` is a local variable so literally nothing can possibly use the `x += 5` result, it's safe to optimize `x+=5; ... ; x+=7` to `x+=12` for both `signed` and `unsigned` integer types.\n\n\nUnsigned integer math is of course fine.\n\n\nSigned integer math has to produce the right result in any case where the abstract machine *doesn't* encounter UB. `x+=12` does that. There's no guarantee that signed overflow raises an exception at any specific point in your program, that's the whole point of optimization in modern C based on the assumption that undefined behaviour won't happen. For an execution that will encounter UB, literally anything can happen anywhere before or after that point (but see footnote 1 above re: \"breaking\" stuff along an inevitable path to UB).\n\n\nThis optimization would be safe even for turning `x-=5; x+=7` into `x+=2`, where the abstract machine could wrap twice (encountering UB) but the asm doesn't wrap, since \"happens to work\" is an allowed behaviour, and common in practice. (Even using MIPS trapping `add` instructions, for example.)\n\n\nIf you use compiler options like `gcc -fwrapv`, that defines the behaviour of signed integer math to be 2's complement wrapping, removing UB and making the situation identical to unsigned.\n\n\nGCC does sometimes miss optimizations with signed integer math because of some reluctance for GCC internals to create signed overflow in a temporary in the asm where none would have existed in the abstract machine. This is a missed optimization when compiling for a machine that allows non-trapping integer math (i.e. all modern ISAs.) For example, GCC will optimize `a+b+c+d+e+f` into `(a+b)+(c+d)+(e+f)` for `unsigned int` but not for `signed int` without `-fwrapv`. Clang does for both for AArch64 and RISC-V, although chooses not to for x86. ([**Godbolt**](https:\/\/godbolt.org\/#z:OYLghAFBqd5TKALEBjA9gEwKYFFMCWALugE4A0BIEAZgQDbYB2AhgLbYgDkAjF%2BTXRMiAZVQtGIHgBYBQogFUAztgAKAD24AGfgCsp5eiyagA%2BudTkVjVEQJDqzTAGF09AK5smBpwBkCTNgAcp4ARtikIABM5AAO6ErE9kyuHl4G8Yl2Qv6BIWzhkTHW2LbJIkQspESpnt48Vtg22UwVVUS5wWER0VaV1bXpDUr9HQFdBT1RAJRW6O6kqJxcAPQrANQA4s7O6%2BixdmwEAF7YmOuJwIHnAUTrhDQ0EcxE9ACe64Kk60zo66TYFhKITkdbuJiMJRKdZveZglTrAC0NAA7qQWLEAG7rCAopAEVBIdZIIE\/P7YR6lO5CdYYUgA2yBKF7GjrIhIbDrIFsaYAUi0AEE1rSjCZaUh0AlsNChJz6EJgOKWAEWVzMJhoV91qF0OzaUDpaC8QSiSpsGxobFSOhQixQu8Lu5Qoj9ocJPyBR7eVEAMw4OiBdYAFXWtw9IcE6AgIZYoJDoTjtMTmET2ETNETwETSGm615AHYAEIe9al\/7YIgLJhc72F0K11C1zC17C1mi14C1pC8n3FwUFgAiXv7vvB\/uDXt9\/vGwbBTEu13D2qq0a5iYTs8ss5Ts7Ts4zs6zs5zeaLJbLAMrpGrLFr9aihcbD%2BbD9bD\/bD87D%2B7va9%2BaHgpcLM9DcAArPw3hcDo5DoNwuzAgsSx5r6fDkEQ2hAbMADWIDSFoAB0oH5gAbDIAAcZE%2BqBWgUVEsggVw0j8GwIDUeQkHQbBXD8EoIBaGhGGzHAsCIBAKAYGwsQMBElDUBJUmMJEqCoKKwA8MRWj8TgmIEtgABqBDYCiADysTMNwqF0PQRARLxEChBh5ChAEVRvBZ\/DOawpBvMZoS6FS7nkBJHDCMZEJuVB\/A4KE7jAM4Ej0LxvBReaxjAJIkXkIQDJ2Ji0qOdg6ilO4NmBbcTSOfQBChOi3muDgjlEKQBAsclsw0EYwBKAZRmmeZyVyMIYgSJwMiDYoKgaI5%2Bj0WlIDmKYlhVaEvGQLMrrJEliLGT6\/DoHl9IEDgq0QLMJRlA4EBOIM3igeQfjjPkhQgMRcQJEkQg3axb1ZMknRPT0r3nS0bQDG4dTfcD5SjP93SREDoxfXdIztLDkzw2d8yLKNwFgRBjlces6hkcRiLEdIIppes6n4QRWg4vgxBkMhPo8NM\/DoZF0yzByLA4JEp3kDhbEMUx5AsWxHF7dwPF8QJXPkMJyBoOgknSRQVAQPJ6sgFUbCYmRiLKap6maVl2A6UsPUmWZkGWQwNmkHZDmZZ5rmBW73m%2Bf5tiBcFLxhe8jnRbF8X0IlgU4GwaUZdB2VUgQeVJdBhXFaVA3lQx0HLbVbz1cs0FNS17ntZ13WGTb\/WWfIw2SGNgjCMoaiaJl%2Bg%2BoYc0LUt1UnetBybdw227TBB3Ncd8BnU0CeXdd4PpFE%2Bb3UwmBo89MSZB9KRz94C8\/Zvq9TI0zTQ%2B0X271DQig2MeRw70KNg2kO%2BL\/f18TGvmOITjhh4%2BxBPcFypB9aGxYJTMUNM6YM0ICQb43pWbs3ljobm5Beb82oNhVi\/FRb40ylxWW\/FOaINxlwKIzEMG\/xwTLBBmEha4QIkRUi0gKJURomROi38uDDyljBShBCgKK1EuJVWCkZKa21opNAakfQxG0rpa2fU7b8Cso7Z2jlPYRVQmo72AUBr%2B1CuFYO2AYpxQSklVCUcY4FyigQHKid8qZVTqgEqyxUKZ0qtVXO%2BdGrNVanwUuLAupyNtoFBuohxB11kCEpuU1W4GFUvNCwhge4Txgv3IQW0drrD2qPI6%2BU1pH2nt4K6y8voNAejfdGGR3otBKXvFoB9IjDCnhdVoiNt4GAvi01Gj1b7DFaY\/dpMNukVLZnMT%2BUgiHgXIZxf%2BxNSbk3WMAZS1MfT4SiJApmMCULwN4UgnCUQyL4QotIaQREZBaGkKzMiWhXpYKmdLbiVg5Y7P4fAMSKs1aKVklrIROtUCSOkRbWRFd5HBIdrZagLtoJqI9i5L2fltGoV0UQQOEU46GNDiYyOqUTCxysTYpOBUiqOPTi44QFVMo51cp4zKRcfF8I6v48uvUgkDRCbXUaET5BRJbtBfQMQ4ld0SStZJG00mDwyciNEGJMRZIiGPXJgsOmOGKW00py96mVN%2Bp9VVtS\/pDOeo04%2Bl8%2BkQ0NQUq%2BGremnx1S\/S1H9sbjPYZMrhhNZlkwpos1AyzVnrOgSzNmHNBLoJ9HQn0ABOcNeE6LSHzGGsNd1bkusobxfBQb2EkLudwh5OzZgHUSA4aQQA%3D%3D)). Again, this is a missed optimization due to GCC being over-cautious for some unknown reason; it's perfectly valid. 2's complement signed math is identical to unsigned binary math, so is associative; the final result will be correct in cases where the optimized computation wrapped back and forth but the abstract machine didn't, for example.\n\n\nSigned overflow UB is only a thing in the abstract machine, not asm; most mainstream ISAs don't even have integer addition instructions that trap on overflow. (MIPS does, but compilers don't use them for `int` math, so they can do optimizations that produce values that didn't exist in the abstract machine.)\n\n\nSemi related: [Why doesn't GCC optimize a\\*a\\*a\\*a\\*a\\*a to (a\\*a\\*a)\\*(a\\*a\\*a)?](https:\/\/stackoverflow.com\/questions\/6430448\/why-doesnt-gcc-optimize-aaaaaa-to-aaaaaa) (answers show that compilers do optimize to three multiplies for integer math, even for signed `int`.)\n\n\n\n\n---\n\n\n## FP exceptions aren't the only issue for float\/double\n\n\n**Floating-point math can't do this optimization because it could give a different result in non-overflowing cases, due to different rounding.** Two smaller numbers could both round down, vs. one larger number overcoming the threshold.\n\n\ne.g. for a number [large enough that the nearest representable `double` values are `16` apart from each other](https:\/\/en.wikipedia.org\/wiki\/Double-precision_floating-point_format#Precision_limitations_on_integer_values), `8` would get half-way and round to nearest-even (assuming the default rounding mode). But any less, like `7` or `5`, will always round back down; `x + 7 == x`, so both the `5` and the `7` would be lost, but `x+12` all in one go would get over the hump to the next representable float or double, producing `x+16`.\n\n\n(The magnitude of 1 unit-in-the-last-place (of the mantissa) depends on the exponent of a float\/double. For large enough FP values, it's 1.0. For even larger values, e.g. `double` from 253 to 254 only even numbers are representable, and so on with larger exponents.)\n\n\nIf you compile with GCC's buggy default of `-ftrapping-math`, it will try to respect FP exception semantics. It doesn't reliably generate 2 FP exceptions if overflow happens twice, so it might not care about that.\n\n\nBut yes, with `#pragma STDC FENV_ACCESS ON`, every separate FP operation is supposed to have an observable effect. (<https:\/\/en.cppreference.com\/w\/c\/numeric\/fenv>). But if you don't call `fegetexcept` to actually observe FP exception flags between two operations, they could in theory still be optimized *if* we can prove that rounding would be the same, since I don't think even ISO C's `FENV_ACCESS ON` is supposed to support actually running exception \/ signal handlers for each trapping operation.\n\n\nFor example two identity operations like `x *= 1.0;` could be collapsed to one, which will raise exceptions on NaN. Or `x *= 2; x *= 2;` could be optimized to `x *= 4;` because multiplying by exact powers of 2 doesn't change the mantissa and thus doesn't cause rounding. It doesn't matter if the first or second multiply overflowed to `+-Inf`, that will still be the final result. (Unless `Inf * 2` raises exception flags that an overflowing multiply wouldn't have already raised? I don't think so.)\n\n\nAnd they're both changing the exponent in the same direction, unlike `x *= 4; x *= 0.5;` which could overflow to +Inf for large numbers, so isn't equivalent to `x *= 2`. Also, if `x *= 0.5; x *= 0.5;` produces subnormal results, it actually *could* round twice when right-shifting the mantissa; IEEE FP has gradual underflow (subnormals with a special encoding for the exponent) but non-gradual overflow to +Inf.\n\n\nFiguring out whether it's safe to optimize `x * 0.5 * 0.5` to `x *= 0.25` is beyond the scope of this answer. GCC and clang don't optimize `x *= 2.0f; x *= 2.0f;` into `x *= 4.0f;` even with `-fno-trapping-math`, but I think that's a missed optimization."}
{"questionId":"b226ff20041345b3aa6a20de2c842966","question":"Transparent search for a std::map with a std::pair as a key\nIf there is a `std::map<std::pair<std::string, std::string>, some_type>` what is the best way to find its values?\n\n\nI guess the most obvious one is to do something like this:\n`map.find(std::make_pair(str1, str2));`\nbut this will lead to a copy-construction of the `str1` and `str2` strings during the pair construction.\n\n\nI hoped that maybe `map.find(std::make_pair(std::ref(str1), std::ref(str2)));` could help, but unfortunately no, this still produces string copies.\n\n\n`map.find(std::make_pair(std::move(str1), std::move(str2))` should work but let's assume that these strings (`str1`, `str2`) are const or should not be moved.\n\n\nSo I'm asking whether there is any other way to do a the map search without making redundant string copies?\n\n\n(Note that using `std::string_view` for the `std::map key` is NOT an option since the map should own its strings.)","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"cpp"},"answer":"C++14 added the following overload for [`std::map::find`](https:\/\/en.cppreference.com\/w\/cpp\/container\/map\/find) which allows transparent search:\n\n\n\n```\ntemplate< class K >\nconst_iterator find( const K& x ) const;\n\n```\n\nTo make use of this, you still need the `std::map` to have a suitable comparator:\n\n\n\n```\nstruct pair_less {\n    \/\/ important for transparent comparison, see std::map:find documentation\n    using is_transparent = std::true_type;\n\n    template <typename T, typename U>\n      requires pair_less_than_comparable<T, U> \/\/ C++20, see below\n    bool operator()(const T& a, const U& b) const {\n        if (a.first < b.first) return true;\n        if (b.first < a.first) return false;\n        return a.second < b.second;\n    }\n};\n\nint main() {\n    std::map<std::pair<std::string, std::string>, int, pair_less> m { \/* ... *\/ };\n    \/\/ ...\n    std::string a = \"a\", b = \"b\";\n    \/\/ now works and creates no copies\n    auto pos = m.find(std::make_pair(std::ref(a), std::ref(b)));\n}\n\n```\n\nThis `pair_less` is totally transparent.\nIt can directly compare `std::pair<X, Y>` to `std::pair<const X&, const Y&>`.\n\n\n### Note on `std::pair::operator<`\n\n\nThe C++ standard requires [`std::pair::operator<`](https:\/\/en.cppreference.com\/w\/cpp\/utility\/pair\/operator_cmp) to be transparent since [LWG Defect 3865](https:\/\/cplusplus.github.io\/LWG\/lwg-defects.html#3865).\nTheoretically, this would make `std::less<void>` or `std::ranges::less` sufficient as a transparent comparator.\n\n\n[However, libstdc++ does not yet implement transparent `std::pair` comparisons](https:\/\/github.com\/gcc-mirror\/gcc\/blob\/ed5bf2080c538b63459891dbd8f217823f7c6127\/libstdc%2B%2B-v3\/include\/bits\/stl_pair.h#L840-L844) at the time of writing, so `std::less<void>` would only work for compilers other than GCC.\n\n\n### Proper C++20 constraints\n\n\nProperly constraining the call operator of `pair_less` is not so easy.\nIt's not strictly necessary anyway, but moves the error to the call site of the operator and may be better for diagnostics.\n\n\nThe usual approach would be something along the lines of\n\n\n\n```\ntemplate <typename T, typename U>\nconcept pair_less_than_comparable\n  = requires (const T& t, const U& u) {\n      { t.first < u.first } -> \/* boolean-testable *\/;\n      { t.second < u.second } -> \/* boolean-testable *\/;\n  }\n\n```\n\nSee also [`boolean-testable`](https:\/\/en.cppreference.com\/w\/cpp\/concepts\/boolean-testable)."}
{"questionId":"d99e2f6fcc69465ba9f0ec20834b3e8a","question":"Will Box::new() make a copy from stack to heap?\nFrom the doc it says:\n\n\n\n> \n> \n> ```\n> pub fn new(x: T) -> Box<T>\n> \n> ```\n> \n> Allocates memory on the heap and then places `x` into it.\n> \n> \n> \n\n\nBut \"place\" is a tricky word. If we write\n\n\n\n```\nlet arr_boxed = Box::new([0;1000]);\n\n```\n\nWill the `[0;1000]` be initialized on the heap in-place?\n\n\nIf we write\n\n\n\n```\nlet arr = [0;1000];\nlet arr_boxed = Box::new(arr);\n\n```\n\nWill the compiler be smart enough to initialize the `[0;1000]` on the heap in the first place?","questionMetadata":{"Type":"Optimization","Level":"Advanced","Tag":"rust"},"answer":"> \n> Will Box::new() make a copy from stack to heap?\n> \n> \n> \n\n\nSometimes. The Rust language does not guarantee this optimization to happen, and seems to leave it up to LLVM to figure this out.\nBecause of this, it doesn't matter at all if you initialize the array\nfirst and then pass it, as that is essentially the same thing for the backend.\n\n\nIn practice, performance will depend on the case.\nThe example you gave is actually special, because the data is all zeroes:\n\n\n\n```\npub fn foo() -> Box<[i32; 1000]> {\n    return Box::new([0; 1000]);\n}\n\n```\n\nIn my [testing](https:\/\/godbolt.org\/z\/adfTnGdYM), the compiler was able to turn that into an allocation + a call to `memset` on the heap data.\n\n\n*Note*: only with optimizations turned on though. In debug mode it will copy.\n\n\n\n\n---\n\n\nOn the other hand, you might want to initialize your data with a known value:\n\n\n\n```\npub fn bar(v: i32) -> Box<[i32; 1000]> {\n    return Box::new([v; 1000]);\n}\n\n```\n\n[Much to my horror](https:\/\/godbolt.org\/z\/cEdq5x9vq), the compiler decides\nto initialize the entire data on the stack, and then call `memcpy`. (At least it unrolled the fill loop) :).\nThis happens even for really large data like `[v; 100000]`, which will crash your program with a stack overflow. Using a compile time known (non zero) literal like `[64; 100000]` behaves the same way.\n\n\n\n\n---\n\n\nIf you *really* want to make sure, you could do something like this:\n\n\n\n```\npub fn baz(v: i32) -> Box<[i32; 1000]>{\n    unsafe {\n        let b = std::alloc::alloc(\n            std::alloc::Layout::array::<i32>(1000).unwrap_unchecked()\n        ) as *mut i32;\n        for i in 0..1000 {\n            *b.add(i) = v;\n        }\n        Box::from_raw(b as *mut [i32; 1000])\n    }\n}\n\n```\n\nwhich does [the right thing](https:\/\/godbolt.org\/z\/ehb6fqvef).\n\n\n\n\n---\n\n\nA safe version of `baz` would be:\n\n\n\n```\nuse std::convert::TryInto;\n\npub fn quux(v: i32) -> Box<[i32; 1000]> {\n    let mut b = Vec::with_capacity(1000);\n    b.extend(std::iter::repeat(v).take(1000));\n    b.into_boxed_slice().try_into().unwrap()\n}\n\n```\n\nWhich the compiler optimizes [quite nicely](https:\/\/godbolt.org\/z\/Pa4x6Y1vY), to essentially the identical assembly as `baz`.\n\n\n\n\n---\n\n\nEven shorter would be\n\n\n`vec![v; 1000].into_boxed_slice().try_into::<Box<[i32; 1000]>>().unwrap()`\n\n\nwhich is probably the best version."}
{"questionId":"d2a403ea04f94769a5a54b85ee62198c","question":"Why did I got an error ModuleNotFoundError: No module named 'distutils'?\nI've installed `scikit-fuzzy` but when I `import skfuzzy as fuzz` I get an error\n\n\n\n```\nModuleNotFoundError: No module named 'distutils'\"\n\n```\n\nI already tried to `pip uninstall distutils` and got this output\n\n\n\n```\nNote: you may need to restart the kernel to use updated packages.\nWARNING: Skipping distutils as it is not installed.\n\n```\n\nThen I tried to install it again `pip install distutils`\n\n\n\n```\nNote: you may need to restart the kernel to use updated packages.\nERROR: Could not find a version that satisfies the requirement distutils (from versions: none)\nERROR: No matching distribution found for distutils\n\n```\n\nSince I use **Python 3.12** I also tried to use `pip3` to install but it say invalid syntax.\n\n\nWhere did I go wrong?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"Python 3.12 does not come with a stdlib distutils module ([changelog](https:\/\/docs.python.org\/3\/whatsnew\/3.12.html)), because `distutils` was deprecated in 3.10 and removed in 3.12. See [*PEP 632 \u2013 Deprecate distutils module*](https:\/\/peps.python.org\/pep-0632\/).\n\n\nYou can still use `distutils` on Python 3.12+ by installing `setuptools`.\n\n\nWhen that doesn't work, you may need stay on Python < 3.12 until the 3rd-party package (`skfuzzy` in this case) publishes an updated release for Python 3.12 support."}
{"questionId":"cc2cc3553cf94968911850bd1695ed2d","question":"Why does std::println(std::vector) fail to compile?\nI have the following code:\n\n\n\n```\n#include <print>\n#include <vector>\n\nint main() {\n    std::vector<int> v{1, 2, 3};\n    std::println(\"{}\", v);\n}\n\n```\n\nAmong the numerous errors this produces, there is (`clang++ -std=c++23 -stdlib=libc++`, <https:\/\/godbolt.org\/z\/3z9Tseh37>):\n\n\n\n```\n[...]\/format_arg_store.h:167:17: error: static assertion failed due to [...]\n  167 |   static_assert(__arg != __arg_t::__none, \"the supplied type is not formattable\");\n      |                 ^~~~~~~~~~~~~~~~~~~~~~~~\n\n```\n\nThis confuses me because according to the [cppreference C++23 compiler support page](https:\/\/en.cppreference.com\/w\/cpp\/23), libc++ support `std::println` and implements [P2286: Formatting Ranges](https:\/\/wg21.link\/P2286R8).\n\n\nAm I doing something wrong or is this a standard library bug?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"cpp"},"answer":"> \n> Am I doing something wrong or is this a standard library bug?\n> \n> \n> \n\n\nFormatting ranges are specified to be supported in `<format>`, but the standard does not specify that `<print>` must include `<format>`. (So just include `<format>` to make the code well-formed).\n\n\nHowever, from a user-friendly perspective, including `<print>` should support formatting ranges.\n\n\nThere is already an open issue about this, see [71925](https:\/\/github.com\/llvm\/llvm-project\/issues\/71925)."}
{"questionId":"c7fd291ae3124dd3b248d181f5af1530","question":"What happens when an assumption, i.e. [[assume]] contains UB?\nIn C++23, the `[[assume(expression)]]` attribute makes it so that if *expression* is `false`, the behavior is undefined.\nFor example:\n\n\n\n```\nint div(int x, int y) {\n    [[assume(y == 1)]];\n    return x \/ y;\n}\n\n```\n\nThis compiles to the same code as if `y` was always `1`.\n\n\n\n```\ndiv(int, int):\n        mov     eax, edi\n        ret\n\n```\n\nHowever, what happens if there is *another level* of undefined behavior?\n\n\n\n```\nint div(int x, int y) {\n    [[assume(x \/ 0 == 1)]];\n    return x \/ y;\n}\n\n```\n\nNow there is UB inside the assumption, but the assumption is not evaluated.\nWhat does this mean? Is it just nonsense or can the compiler do anything with this assumption?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"From [[dcl.attr.assume]](https:\/\/eel.is\/c++draft\/dcl.attr.assume):\n\n\n\n> \n> The expression is not evaluated.\n> \n> \n> \n\n\nSo the undefined behavior of the expression does not immediately imply undefined behavior of the program with the given inputs.\n\n\nHowever it continues:\n\n\n\n> \n> If the converted expression would evaluate to true at the point where the assumption appears, the assumption has no effect.\n> Otherwise, the behavior is undefined.\n> \n> \n> \n\n\nEither the evaluation of an expression that would have undefined behavior is not an evaluation of the expression that evaluates to `true` and the behavior of the program would be undefined per the \"*Otherwise*\" sentence as well, or alternatively, if you consider it unspecified whether or not \"*If the converted expression would evaluate to true at the point where the assumption appears*\" is satisfied if the evaluation would have undefined behavior, then it would still not be specified whether or not the \"*Otherwise*\" branch is taken and so there will again be undefined behavior overall, since undefined behavior for one choice of unspecified behavior implies undefined behavior overall.\n\n\nThis is specifically addressed in the proposal [P1774](https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/papers\/2022\/p1774r6.pdf) in chapter 4.3. as a subtle difference between the decision to make behavior undefined if the assumption does not evaluate to `true` vs. making the behavior undefined if the assumption evaluates to `false`.\n\n\nThis way it is for example possible to write assumptions like `x + y == z` without the compiler having to consider the special case of signed overflow, which might make the assumption unusable for optimization."}
{"questionId":"acbefb0912a34103b4f0d251368bc2f0","question":"Why are \"1...2\" and \"1....2\" not syntax errors?\nConsider:\n\n\n\n```\nuse 5.016;\nuse warnings \"all\";\nuse Data::Dumper;\n\nmy @abc = (1, 2, 3);\n\nmy @bbb = @abc[1..2];\nmy @ccc = @abc[1...2];\nmy @ddd = @abc[1....2];\n\nsay Dumper \"@bbb\"; # Output: '2 3'\nsay Dumper \"@ccc\"; # Output: '2 3'\nsay Dumper \"@ddd\"; # Output: ''\n\n```\n\nWhy aren't there any syntax errors in the code above?\n\n\nWhat do `1...2` (three dots) and `1....2` (four dots) mean here?","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"perl"},"answer":"1..2` is tokenized as `1` `..` `2`, and parsed as a [range operator](https:\/\/perldoc.perl.org\/perlop#Range-Operators) with two integer constant operands. When evaluated, it is evaluated in list context, and it produces the integers from 1 to 2 inclusive.\n\n\n`1...2` is tokenized as `1` `...` `2`, and parsed as a range operator with two integer constant operands. `...` is different than `..` in scalar context, but they're the same in list context.\n\n\n`1....2` is tokenized as `1` `...` `.2`, and parsed as a range operator with two constant operands. When evaluated, it is evaluated in list context, and it produces the integers from 1 to 0.2 inclusive, which is to say it produces nothing.\n\n\n\n\n---\n\n\nA side note:\n\n\nWhen tokenizing, Perl normally gobbles up as many characters as it can with no regard as to what follows. One exception is that a `.` will never be included in a numeric literal if followed by another `.`.\n\n\nThis means that `1..2` is always tokenized as `1`, `..`, `2` and never `1.` `.2` or `1.` `.` `2`.\n\n\nThis means that `1...2` is always tokenized as `1`, `...`, `2` and never `1.`, `..`, `2` or `1`, `..`, `.2`.\n\n\nThis also means that `1.....2` will never be tokenized as `1.` `...` `.2` even though that would prevent the parser from throwing a syntax error.\n\n\n(Thanks @Dada for helping me correct this.)\n\n\n\n\n---\n\n\nA side note:\n\n\nAt least some constant ranges are flattened into an array constant.\n\n\n`1..2` in list context and `1...2` in list context both compile to a two-element array constant, just like `1, 2`.\n\n\n`1....2` in list context compiles to an empty array constant."}
{"questionId":"dbfa600add7e400cb710a7d73e0f0d57","question":"Starting from which version does Gradle support Java 21?\nWhen trying to configure the project using Java 21 I'm getting the error:\n\n\n\n> \n> Unsupported Java. Your build is currently configured to use Java 21 and Gradle 8.3.\n> \n> \n> \n\n\nUnfortunately, there is no information in official documentation for now: <https:\/\/docs.gradle.org\/current\/userguide\/compatibility.html>\n\n\n\n> \n> A Java version between 8 and 20 is required to execute Gradle. Java 21\n> and later versions are not yet supported.\n> \n> \n> \n\n\nStarting from which version does Gradle support Java 21?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"java"},"answer":"# Current\n\n\nFor full Java 21 support, you can use [Gradle 8.5](https:\/\/github.com\/gradle\/gradle\/releases). The [release notes](https:\/\/docs.gradle.org\/8.5\/release-notes.html#full-java-21-support) say:\n\n\n\n> \n> With this release, Gradle now fully supports compiling, testing and running on Java 21.\n> \n> \n>"}
{"questionId":"e3283148d0414eefb7ea8ab77117dbef","question":"Are records syntactic sugar for classes?\nI recently discovered Java [records](https:\/\/openjdk.org\/jeps\/395), and it was amazing to learn about their existence and purpose.\n\n\nHowever, I began to wonder if they are essentially just classes behind the scenes.\n\n\nDoes this mean that during compilation, they get converted into an explicit class with final private fields, a public constructor, and the necessary getters?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"java"},"answer":"Under the hood it will still create a class (that's derived from [`java.lang.Record`](https:\/\/docs.oracle.com\/en\/java\/javase\/17\/docs\/api\/java.base\/java\/lang\/Record.html)).\n\n\nYou can try it out yourself:\n\n\n\n```\n\/\/ test.java\nrecord Test(int foo, String bar) {}\n\n```\n\nWhen you compile this with `javac test.java` and disassemble it again with `javap Test` you'll get this code:\n\n\n\n```\nfinal class Test extends java.lang.Record {\n  Test(int, java.lang.String);\n  public final java.lang.String toString();\n  public final int hashCode();\n  public final boolean equals(java.lang.Object);\n  public int foo();\n  public java.lang.String bar();\n}"}
{"questionId":"87eb7da3a51a4a5d94947b4a7f9e9e65","question":"Why is changing a property from \"init\" to \"set\" a binary breaking change?\nI am coming back to C# after a long time and was trying to catch up using the book *[C# 10 in a Nutshell](https:\/\/www.oreilly.com\/library\/view\/c-10-in\/9781098121945\/)*.\n\n\nThe author there mentions that changing a property's accessor from `init` to `set` or vice versa is a breaking change. I can understand how changing it from `set` to `init` can be a breaking change, but I just can\u2019t understand why changing it the other way around would be a breaking change.\n\n\nFor example:\n\n\n\n```\n\/\/ Assembly 1\nTest obj = new(){A = 20};\n\n\/\/ Assembly 2\nclass Test\n{\n   public int A {get; init;} = 10;\n}\n\n```\n\nThis code in Assembly 1 should not be affected even if I change the `init` property accessor to `set`. Why then is this a breaking change?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"csharp"},"answer":"This is because `init` accessors are compiled into a setter with a `modreq` declaration. The IL code for an int property `P` might look something like this (See on [SharpLab](https:\/\/sharplab.io\/#v2:EYLgZgpghgLgrgJwgZwLQAUEEsC2UECeAwgPYB2yMCcAxjCQsgDQwhYA2TAJiANQA+AAQBMARgCwAKEEBmAAQi5ROQG8pcjQvlYyMOelVyA5hBgBuOTqzm5AXym2gA==)):\n\n\n\n```\n.method public hidebysig specialname    \n    instance void modreq([System.Runtime]System.Runtime.CompilerServices.IsExternalInit) set_P (    \n        int32 'value'   \n    ) cil managed   \n{   \n    ...\n}\n\n```\n\nNotice the token `modreq([System.Runtime]System.Runtime.CompilerServices.IsExternalInit)`.\n\n\nA normal setter does not generate this `modreq`.\n\n\nOn the caller's side, the call instruction must supply the `modreq` declaration as part of the signature of the thing to call, if and only if a `modreq` exists on that method. Therefore, the call to an `init` accessor would look like this:\n\n\n\n```\ncallvirt instance void modreq([System.Runtime]System.Runtime.CompilerServices.IsExternalInit) SomeClass::set_P(int32)\n\n```\n\nand not just\n\n\n\n```\ncallvirt instance void SomeClass::set_P(int32)\n\n```\n\nIf you changed to a setter, then all the calls to the `init` accessor must be changed to remove the `modreq`, or else it would not resolve the method correctly. Hence, this is a breaking change.\n\n\nAs for why `modreq` is used instead of a regular attribute to mark the property, see [this section](https:\/\/learn.microsoft.com\/en-us\/dotnet\/csharp\/language-reference\/proposals\/csharp-9.0\/init#breaking-changes) in the draft spec. To summarise, this is a trade-off between binary compatibility and \"what would a compiler not aware of `init` accessors do\". In the end they decided to sacrifice binary compatibility, so that a compiler that doesn't know about `init` *doesn't* allow code that sets the property."}
{"questionId":"75294870c23a49c69ce7d8e2b1aedea5","question":"why use template in year\\_month class?\nIn MSVC chrono implementation I see the following code\n\n\n\n```\n    _EXPORT_STD template <int = 0>\n_NODISCARD constexpr year_month operator+(const year_month& _Left, const months& _Right) noexcept {\n    const auto _Mo  = static_cast<long long>(static_cast<unsigned int>(_Left.month())) + (_Right.count() - 1);\n    const auto _Div = (_Mo >= 0 ? _Mo : _Mo - 11) \/ 12;\n    return year_month{_Left.year() + years{_Div}, month{static_cast<unsigned int>(_Mo - _Div * 12 + 1)}};\n}\n\n```\n\nCan someone explain me why it uses template with unnamed parameter ?","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"cpp"},"answer":"You have two very similar overloads for `operator+`, originally without the template\n\n\n\n```\ntemplate <int = 0>\nconstexpr year_month operator+(const year_month& _Left, const months& _Right) noexcept\n\nconstexpr year_month operator+(const year_month& _Left, const years& _Right) noexcept\n\n```\n\nIt was discovered that if you have a value that is convertible to both `months` and `years`, there was an ambiguity here. Which conversion should be chosen?\n\n\nBy making one of the operators (a dummy) template, the non-template is chosen (if possible) because templates have lower priority in overload resolution.\n\n\nThe standard specifies this requirement a bit convoluted (backwards in my opinion):\n\n\n\n> \n> \"If the argument supplied by the caller for the months parameter is convertible to years, its implicit conversion sequence to years is worse than its implicit conversion sequence to months\"\n> \n> \n> \n\n\nSo if the conversions are equally good, the non-template is to be chosen. Only if conversion to months is better, the template gets a chance.\n\n\n(And the standard doesn't explicitly say that is has to be a template, but that is a way of implementing this requirement)."}
{"questionId":"59e09c0e420a4c92b6343093da60a677","question":"Are enum values allowed in a std::integer\\_sequence?\nThis code compiles and executes fine using GCC 13 and Clang 17, but fails to compile on MSVC. I am wondering if the code is required to work according to the standard or if this is a problem with MSVC.\n[Demo](https:\/\/godbolt.org\/z\/hv1P73TfT)\n\n\n\n```\n#include <utility>\n#include <iostream>\n\nenum e : int { A=5, B, C, D };\n\nauto x = std::integer_sequence<e, A, B, C, D>{};\nauto y = std::integer_sequence<unsigned, 9, 4, 3, 8>{};\nauto z = std::integer_sequence<int, 0, 1, 2, 3>{};\n\ntemplate<typename T, T... ints>\nvoid print_sequence(std::integer_sequence<T, ints...> int_seq)\n{\n    std::cout << \"The sequence of size \" << int_seq.size() << \": \";\n    ((std::cout << ints << ' '), ...);\n    std::cout << '\\n';\n}\n\nint main(int, char**)\n{\n    print_sequence(x);\n    print_sequence(y);\n    print_sequence(z);\n    return 0;\n}\n\n```\n\nMSVC gives this error:\n\n\n\n> \n> error C2338: static\\_assert failed: 'integer\\_sequence<T, I...> requires T to be an integral type.'\n> \n> \n>","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"cpp"},"answer":"## Update:\n\n\nI submitted [PR 112473](https:\/\/gcc.gnu.org\/bugzilla\/show_bug.cgi?id=112473) and this has been fixed in GCC-trunk.\n\n\n\n\n---\n\n\nMSVC-STL and libc++\u2020 are right, as `e` is not an integral type (i.e. `std::is_integral_v<e>` is `false`).\n\n\nFrom [[intseq.intseq]](https:\/\/eel.is\/c++draft\/intseq.intseq):\n\n\n\n> \n> \n> ```\n> namespace std {\n>   template<class T, T... I> struct integer_sequence {\n>     using value_type = T;\n>     static constexpr size_t size() noexcept { return sizeof...(I); }\n>   };\n> }\n> \n> ```\n> \n> *Mandates*: `T` is an integer type.\n> \n> \n> \n\n\nIt is worth noting that, in contrast, `integral_constant` has no requirement for `T`, so things like `integral_constant<pair<int, int>, pair{0, 0}>` are perfectly fine in C++20.\n\n\n\n\n---\n\n\n\u2020 Clang's libc++ needs to be enabled through the `-stdlib=libc++` flag, and it also has a [corresponding `static_assert`](https:\/\/godbolt.org\/z\/WKcjaMqKv) in the `integer_sequence` implementation."}
{"questionId":"f7292f1010da42e68a9369d6fe7186ce","question":"Is int &ref = ref; well formed\nI have learned that evaluating an uninitialized variable is undefined behavior. In particular, `int i = i;` is undefined behavior. I have read [What's the behavior of an uninitialized variable used as its own initializer?](https:\/\/stackoverflow.com\/questions\/54200465\/)\n\n\nBut, is using a reference variable to initialize itself also undefined behavior? In particular, is `int &ref = ref;` also UB, as per the C++ standard?\n\n\n\n```\nint &ref = ref; \/\/ Is this well-formed or ill-formed or UB\n\n```\n\nAll compilers [compile](https:\/\/godbolt.org\/z\/T7jdTdEah) the above program (with clang giving a warning). Is this because it is undefined behavior so anything is allowed to happen, or is the program well-formed?\n\n\n\n\n---\n\n\nAdditionally, if I were to assign some value to `ref`, will the behavior of the program change from the previous case?\n\n\n\n```\nint &ref = ref;\n\nint main()\n{\n    ref = 1; \/\/does this change the behavior of the program from previous case\n}\n\n```\n\nI noticed that for this 2nd snippet, we get a segfault.\n\n\nSome references that I've read are:\n\n\n[What's the behavior of an uninitialized variable used as its own initializer?](https:\/\/stackoverflow.com\/questions\/54200465\/)\n\n\n[Why is initialization of a new variable by itself valid?](https:\/\/stackoverflow.com\/questions\/28152298\/)","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"cpp"},"answer":"It is not defined behaviour.\n\n\n[[dcl.ref]p5](https:\/\/wg21.link\/dcl.ref#5.sentence-3):\n\n\n\n> \n> [...] A reference shall be initialized to refer to a valid object or function.\n> \n> \n> \n\n\nAnd there is no `int` object to refer to.\n\n\n\n\n---\n\n\nYou could also say that the reference is being used outside of its lifetime.\n\n\n[[basic.life]](https:\/\/eel.is\/c++draft\/basic.life#2):\n\n\n\n> \n> 3. The lifetime of a reference begins when its initialization is complete. The lifetime of a reference ends as if it were a scalar object requiring storage.\n> 4. The properties ascribed to objects and references throughout this document apply for a given object or reference only during its lifetime.\n> \n> \n> \n\n\nSo, the reference cannot be \"used\" to initialize itself before it is itself initialized, which is how Clang complains about this (<https:\/\/godbolt.org\/z\/Ea4qPoWbs>):\n\n\n\n> \n> \n> ```\n> note: use of reference outside its lifetime is not allowed in a constant expression\n>     2 |     int& ref = ref;\n>                        ^\n> \n> ```\n> \n>"}
{"questionId":"0ddde388700142109be2681710892c3a","question":"Emulating signed integers using unsigned integers in C\nIn Jens Gustedt's book Modern C, on page 59, he explains how signed integers can be emulated using unsigned integers.\nHis example code shows how one can implement a comparison of two unsigned integers reinterpreted as signed integers:\n\n\n\n```\nbool is_negative(unsigned a) { \n   unsigned const int_max = UINT_MAX \/2;\n   return a > int_max;\n}\n\nbool is_signed_less(unsigned a, unsigned b) {\n   if (is_negative(b) && !is_negative(a)) return false;\n   else return a < b; \n} \n\n```\n\nDo I misunderstand something here or does he miss the second special case where `is_negative(a) = true` and `is_negative(b) = false`?\n\n\nFor example if we want to have `a = -1` and `b = 1`, then, using two's complement, we would represent them as\n\n\n\n```\nunsigned int a = UINT_MAX; \nunsigned int b = 1;    \n\n```\n\n(e.g. for a 4 bit integer we would have a = 1111 and b = 0001).\nNow we have `is_negative(a)` returns `true`, and `is_negative(b)` returns `false`. When calling `is_signed_less(a, b)` we end up in the `else` clause and `a < b` (now interpreted as unsigned integers) will return false. However, it is clearly true that -1 < 1, so the function returns the wrong result.\n\n\nIs this a typo in the code of the book or is there something that I do not understand?","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"c"},"answer":"This is what happens when people try to be \"smart\" instead of following \"keep it simple, stupid\" best practices. Good engineering involves writing the code as *simple* as you possibly can, for example:\n\n\n\n```\nbool is_signed_less_correct (unsigned a, unsigned b) {\n  bool is_neg_a = is_negative(a);\n  bool is_neg_b = is_negative(b);\n\n  if(is_neg_a != is_neg_b) \/\/ one is negative\n  {\n    return is_neg_a; \/\/ if one is negative and it is a, return true otherwise false\n  } \n\n  \/\/ both are negative or both are positive\n  return a < b;\n}\n\n```\n\nEven this code a bit too \"smart\" still, since it implicitly uses the fact that -1 == 0xFFFF... is the largest 2's complement signed number and therefore `a < b` holds true no matter if these are negative or not, as long as they are both of the same signedness.\n\n\nThen of course you would always write a little unit test to sanity check it: <https:\/\/godbolt.org\/z\/h4nKsffqr>\n\n\nOutput:\n\n\n\n```\n-2 < -1 ? true  (is_signed_less_gustedt)\n-1 < -1 ? false (is_signed_less_gustedt)\n-1 <  0 ? false (is_signed_less_gustedt)\n 0 < -1 ? false (is_signed_less_gustedt)\n 0 <  1 ? true  (is_signed_less_gustedt)\n 1 <  0 ? false (is_signed_less_gustedt)\n 1 <  1 ? false (is_signed_less_gustedt)\n\n-2 < -1 ? true  (is_signed_less_correct)\n-1 < -1 ? false (is_signed_less_correct)\n-1 <  0 ? true  (is_signed_less_correct)\n 0 < -1 ? false (is_signed_less_correct)\n 0 <  1 ? true  (is_signed_less_correct)\n 1 <  0 ? false (is_signed_less_correct)\n 1 <  1 ? false (is_signed_less_correct)"}
{"questionId":"ce39941ef28c4de5a6218179b7bc51e9","question":"Why does std::format() throw at runtime for incorrect format specifiers?\nRecently I discovered that [the following code compiles on a few major compilers](https:\/\/godbolt.org\/z\/91WKTr1Yn), and then throws at runtime:\n\n\n\n```\nstd::cout << std::format(\"{:*<{}}\", 10, \"Hello\") << std::endl;\n\n```\n\n\n```\nterminate called after throwing an instance of 'std::format_error'\n  what():  format error: argument used for width or precision must be a non-negative integer\n\n```\n\nIt throws because \"10\" should come after \"Hello\", not before.\n\n\nBut the obvious question is: Why isn't it failing at compile-time? My understanding was that these arguments would be type-checked at compile-time, and obviously a `const char*` can't be used as a width specifier. Why is this not a compile error?\n\n\n**If you don't understand why this is confusing**, please know that the first argument of `std::format()` is of type [`std::format_string<...>`](https:\/\/en.cppreference.com\/w\/cpp\/utility\/format\/basic_format_string). This type takes a string literal\/string view **at compile time** (due to its `consteval` constructor), and at compile-time it reads the contents of the given string to see if the format arguments match the format string. Therefore, a call of `std::format(\"{}\");` is **guaranteed** not to compile, since the string \"{}\" is read at compile-time as a format specifier, but the type lists show that no arguments were passed, so what would be put in that space?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"TL;DR: Placeholders inside format specifiers are a known shortcoming of compile time validation in C++20. It's being [fixed with C++26](https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/papers\/2023\/p2757r3.html).\n\n\n**Original answer:**\n\n\nDisclaimer: this is an idea I came up with after looking into it just now, I'm in no way sure about this.\n\n\nI think there is an issue here with how the compile time validation works for your case.\n\n\nNormally, compile time checking of, say, `std::format(\"{:d}\", \"hi!\")` works, because it calls `std::formatter<const char*>::parse(\"{:d}\")`, which is constexpr, and will throw because it sees that `\":d\"` doesn't fit the type of the formatter.\n\n\nYou pass this format string: `\"{:*<{}}\"` with arg types `int` then `const char*`. So we get the following parse calls:\n\n\n- `std::formatter<int>::parse(\"{:*<{}}\")` - which is the place I'm a bit fuzzy on. From the interface of [`parse_context`](https:\/\/en.cppreference.com\/w\/cpp\/utility\/format\/basic_format_parse_context) I'm going to guess they simply store the ID of the next argument, so they can use it to fetch the desired width when it's time to actually format things. But I don't see the `parse_context` providing a way to actually check what the type of the next argument is. So the parsing just succeeds, because for all it knows the next argument could be an `int`.\n- `std::formatter<const char*>::parse(\"{}\")` - parsing the format string of the \"inner\" placeholder. Nothing wrong here. (**Edit 3:** I'm not sure this parse call actually happens. It's probably the responsibility of the outermost parse call to process its whole format specifier including nested placeholders.)\n\n\nAnd with that, the parsing has succeeded and the error will be found only at runtime, when actually reading the argument for the width from the stored arg ID.\n\n\n**Edit:** It looks like they actually addressed this exact issue in C++26 by adding `check_dynamic_spec` to the `parse_context`. With that, it should be possible to check placeholders within format specs as well, I'd imagine.\n\n\n**Edit 2:** Here is the [paper](https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/papers\/2023\/p2757r3.html) that introduced these new methods, and provides a very similar motivating example to yours."}
{"questionId":"29e09190f56a4acba86b209cb84173f5","question":"Is it possible to make zero-allocation coroutine runtime in C++?\nIn Rust, asynchronous functions do not require allocating on the heap. Function `async` returns a compiler-generated structure on which you call a compiler generated `poll` method. The design of async looks logical and clear.\n\n\nThe design of coroutines in C++ is strange. It forces you to do allocation on the heap, because you have no other way to return something using `co_return`.\n\n\n(It is possible to make a custom allocator that will make allocations in a buffer on the stack, but this will unnecessarily complicate the code.)\n\n\nWhy was it decided in the design of C++ that an object returned by a coroutine must have a `promise_type`?\n\n\nWhy is `await_ready`, `await_suspend`, `await_resume` not enough?\n\n\n(It looks strange, and this is what enforces you to do allocation; you can't just directly construct a `SomeTask<T>` object (with three `await_*` methods) and return it.)\n\n\nHow can we make a zero-alloc coroutines without a custom allocator?","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"cpp"},"answer":"> \n> Essentially it forces you to do allocation on the heap, because you have no other way to return something using `co_return`\n> \n> \n> \n\n\nThat's not the reason for the allocation.\n\n\nCoroutines require storage to do their job. Not *just* the marshaling of return values, but for tracking the entire thing. The coroutine's stack, the thing that has to be paused and resumed, is part of this.\n\n\nThe whole point of a coroutine is that it can suspend execution and have that execution resumed by somebody. Who that somebody is is not known statically. And that is the reason why dynamic allocation is needed; coroutine lifetimes are not limited by the scope of their initial invoker.\n\n\nIf function X invokes coroutine A, and coroutine A suspends itself pending some async request, function X may just return the coroutine to someone else who will wait on it. This means that function X's call stack is gone. If coroutine A lived on function X's stack, its storage is gone now.\n\n\nThat's bad.\n\n\nC++ stores coroutine A in dynamically-allocated memory to *allow* it to survive its calling invocation. This is the presumed default case of coroutines.\n\n\nPresumably, the circumstances you are describing are those where function X itself is a coroutine that awaits on A, or otherwise will not leave its own scope until A is finished. This would allow the coroutine state of A to live on X's stack without problems. C++ compilers are permitted to optimize these conditions if they are detected; they can elide the dynamic allocation.\n\n\nBut otherwise, that dynamic allocation is *necessary*.\n\n\nI don't know Rust. So I don't know what Rust is doing with its `async` functions. Maybe its async functions are unable to persist beyond the boundaries of their calling scope. Maybe Rust is better able to detect when the async function *needs* to be able to live beyond its caller's scope. Maybe you have to explicitly tell Rust when you want a coroutine to escape its caller's scope. I don't know.\n\n\nBut C++ isn't like that. C++ coroutines are *presumed* to leave the scope of their invoking function."}
{"questionId":"af4a38f8d7cd43eca41ceb5f7454ef20","question":"How can I multiply a INumber with an int?\nHow should I implement the multiplication by int (`z * 2`) on the last line?\n\n\n\n```\npublic static TResult Test<TResult>() where TResult : INumber<TResult>\n{\n    TResult x = TResult.AdditiveIdentity;\n    TResult y = TResult.MultiplicativeIdentity;\n    TResult z = x * y;\n    TResult z2 = z * 2; \/\/ <--- this gives the CS0019 error \"The operator * cannot be applied to operands of type 'TResult' and 'int'\n    return z2;\n}\n\n```\n\n--- the suggested solution is to add an interface, but it breaks this:\n`IMultiplyOperators<TResult, int, TResult>`\n\n\n\n```\npublic static void Tester()\n{\n    Test<decimal>(); \/\/ CS0315 Tye type decimal cannot be used as type parameter TResult..... there is no boxing conversion\n}\n\n```\n\nFor now I will inject the function myself and use:\n\n\n\n```\npublic static TResult Test<TResult>(Func<TResult, int, TResult> mul) where TResult : INumber<TResult>\n{\n    TResult x = TResult.AdditiveIdentity;\n    TResult y = TResult.MultiplicativeIdentity;\n    TResult z = x * y;\n    TResult z2 = mul(z, 2);\n    return z2;\n}","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"csharp"},"answer":"I suggest [converting](https:\/\/learn.microsoft.com\/en-us\/dotnet\/api\/system.numerics.inumberbase-1.createchecked?view=net-7.0) `2` to `TResult` and only then multiply:\n\n\n\n```\n   TResult z2 = z * TResult.CreateChecked(2);\n\n```\n\nHere we create `TResult` instance from integer value `2`, while *checking* `2` for overflow and similar possible errors (an exception will be thrown if `2` can't be converted into `TResult`: either `OverflowException` or `NotSupportedException`).\n\n\n**Code:**\n\n\n\n```\npublic static TResult Test<TResult>() where TResult : INumber<TResult>\n{\n    TResult x = TResult.AdditiveIdentity;\n    TResult y = TResult.MultiplicativeIdentity;\n    TResult z = x * y;\n    TResult z2 = z * TResult.CreateChecked(2);\n    return z2;\n}\n\n```\n\n**Demo:**\n\n\n\n```\nConsole.WriteLine(Test<decimal>());\n\n```\n\n**Output:**\n\n\n\n```\n0\n\n```\n\n[Fiddle](https:\/\/dotnetfiddle.net\/Cs0rto)"}
{"questionId":"4064c5ceff514874b1d6b5ffa7f553ac","question":"Why does ranges::for\\_each return the function?\nThe legacy `std::for_each` returns function as the standard *only* requires `Function` to meet *Cpp17MoveConstructible* according to [[alg.foreach]](https:\/\/eel.is\/c++draft\/algorithms#alg.foreach-1):\n\n\n\n> \n> \n> ```\n> template<class InputIterator, class Function>\n>   constexpr Function for_each(InputIterator first, InputIterator last, Function f);\n> \n> ```\n> \n> *Preconditions*: `Function` meets the *Cpp17MoveConstructible* requirements.\n> \n> \n> [Note:\u2002`Function` need not meet the requirements of *Cpp17CopyConstructible*. end note]\n> \n> \n> \n\n\nThis is reasonable since the user may want to reuse the function after the call.\n\n\nThe parallel version of `for_each` has no return:\n\n\n\n> \n> \n> ```\n> template<class ExecutionPolicy, class ForwardIterator, class Function>\n>   void for_each(ExecutionPolicy&& exec,\n>                 ForwardIterator first, ForwardIterator last,\n>                 Function f);\n> \n> ```\n> \n> *Preconditions*: `Function` meets the *Cpp17CopyConstructible* requirements.\n> \n> \n> \n\n\nThis is because the standard requires `Function` to meet the *Cpp17CopyConstructible*, so returning the function is unnecessary as the user can *freely* create a copy if they want on the call side.\n\n\nI noticed that `ranges::for_each` also returns the function:\n\n\n\n> \n> \n> ```\n> template<input_iterator I, sentinel_for<I> S, class Proj = identity,\n>          indirectly_unary_invocable<projected<I, Proj>> Fun>\n>   constexpr ranges::for_each_result<I, Fun>\n>     ranges::for_each(I first, S last, Fun f, Proj proj = {});\n> \n> ```\n> \n> \n\n\nHowever, the function signature already requires `Fun` to satisfy `indirectly_unary_invocable` which already guarantees that it is copy constructible.\n\n\nThe question is, why does the `ranges::for_each` still return the function? What's the point of doing this?","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"cpp"},"answer":"It returns the functor because that allowed some clever tricks with stateful functors back in the day (in C++98, I assume). You don't see those often today, because lambdas are usually more straightforward.\n\n\nHere's an example:\n\n\n\n```\n#include <algorithm>\n#include <iostream>\n\nstruct EvenCounter\n{\n    int count;\n\n    EvenCounter() : count(0) {}\n\n    void operator()(int x)\n    {\n        if (x % 2 == 0)\n            count++;\n    }\n};\n\nint main()\n{\n    int array[] = {1,2,3,4,5};\n    int num_even = std::for_each(array, array+5, EvenCounter()).count;\n    std::cout << num_even << '\\n';\n}\n\n```\n\n\n> \n> This is reasonable since the user may want to reuse the function after the call.\n> \n> \n> \n\n\nI think the logic is backwards here. The function isn't required to be copyable simply because there's no reason for `for_each` to copy it.\n\n\nIf you have a non-copyable (or even non-movable) function, you can pass it by reference using `std::ref` to avoid copies\/moves, so you don't win anything here from the algorithm returning the function back to you.\n\n\nThere was no `std::ref` in C++98, but there was also no move semantics, so `for_each` couldn't have worked with non-copyable functors in the first place."}
{"questionId":"25cf5e3a59cf4b92bf45ec9325bf1cec","question":"dyld[45923]: Library not loaded: \/usr\/local\/opt\/libavif\/lib\/libavif.15.dylib\n> \n> \n> ```\n> dyld[45923]: Library not loaded: \/usr\/local\/opt\/libavif\/lib\/libavif.15.dylib\n>  Referenced from: <735019D8-AF6E-3CD1-9AD4-3BE47CB22D4F> \/usr\/local\/Cellar\/gd\/2.3.3_5\/lib\/libgd.3.dylib\n>  Reason: tried: '\/usr\/local\/opt\/libavif\/lib\/libavif.15.dylib' (no such file), '\/System\/Volumes\/Preboot\/Cryptexes\/OS\/usr\/local\/opt\/libavif\/lib\/libavif.15.dylib' (no such file), '\/usr\/local\/opt\/libavif\/lib\/libavif.15.dylib' (no such file), '\/usr\/local\/lib\/libavif.15.dylib' (no such file), '\/usr\/lib\/libavif.15.dylib' (no such file, not in dyld cache), '\/usr\/local\/Cellar\/libavif\/1.0.1\/lib\/libavif.15.dylib' (no such file), '\/System\/Volumes\/Preboot\/Cryptexes\/OS\/usr\/local\/Cellar\/libavif\/1.0.1\/lib\/libavif.15.dylib' (no such file), '\/usr\/local\/Cellar\/libavif\/1.0.1\/lib\/libavif.15.dylib' (no such file), '\/usr\/local\/lib\/libavif.15.dylib' (no such file), '\/usr\/lib\/libavif.15.dylib' (no such file, not in dyld cache)\n> \n> ```\n> \n> \n\n\nI updated MacOS to Sonoma and I had problem with XAMPP. So, I reinstalled XAMPP. Now this error is showing. I can't use PHP and Composer.","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"other"},"answer":"I had the same issue after installing macOS Ventura.\n\n\nTry to reinstall gd:\n\n\n\n```\nbrew reinstall gd"}
{"questionId":"a55a1fe6d34041a0a1270ebe4e06887b","question":"Dependent name in local class\n#include <type_traits>\n\ntemplate<typename... Ts>\nvoid boo()\n{\n    struct A\n    {\n        struct B {};\n    };\n    static_assert(std::is_class_v<A>);\n    static_assert(std::is_class_v<A::B>); \/\/ compilation failure; need typename.\n}\n\nint main()\n{\n    boo<int>();\n}\n\n```\n\nWhy is `A::B` a dependent name and `A` is not?\nCan not see anything in standard about that.","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"cpp"},"answer":"This is [CWG 2074](https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/cwg_active.html#2074), which is still unresolved [**emphasis** mine]:\n\n\n\n> \n> ### 2074. Type-dependence of local class of function template\n> \n> \n> According to 13.8.3.2 [temp.dep.type] paragraph 9, a local class in a\n> function template is dependent if and only if it contains a subobject\n> of a dependent type. However, given an example like\n> \n> \n> \n> ```\n>   template<typename T> void f() {\n>     struct X {\n>       typedef int type;\n>   #ifdef DEPENDENT\n>       T x;\n>   #endif\n>     };\n>   X::type y;    \/\/ #1\n>   }\n>   void g() { f<int>(); }\n> \n> ```\n> \n> there is **implementation variance** in the treatment of #1, but **whether\n> or not DEPENDENT is defined appears to make no difference.**\n> \n> \n> [...]\n> \n> \n> Perhaps the right answer is that the types should be dependent but a member of the current instantiation, permitting name lookup without typename.\n> \n> \n> **Additional notes (September, 2022):**\n> \n> \n> At present, **the term \"current instantiation\" is defined for class\n> templates only, and thus does not apply to function templates**.\n> \n> \n> Moreover, the resolution for this issue should also handle local\n> enums, with particular attention to 9.7.2 [enum.udecl] paragraph 1:\n> \n> \n> The elaborated-enum-specifier shall not name a dependent type and...\n> This rule, without amendment, would disallow the following reasonable\n> example if local enums were made dependent types:\n> \n> \n> \n> ```\n> template <class T>\n> void f() {\n>   enum class E { e1, e2 };\n>   using enum E;\n> }\n> \n> ```\n> \n> \n\n\nThe spec appears underspecified, as vendors seems to (on an implementation variance basis) enforce [[temp.dep.type]\/7.3](https:\/\/eel.is\/c++draft\/temp.dep.type#7.3) [**emphasis** mine]:\n\n\n\n> \n> \/7 A type is dependent if it is\n> \n> \n> - [...]\n> - \/7.3 **a nested class** or enumeration that is a direct member of **a class that is the current instantiation**,\n> \n> \n> \n\n\nBut as noted in CWG 2074, *current instantiation* as per [[temp.dep.type]\/1](https:\/\/eel.is\/c++draft\/temp.dep.type#1) does not cover what are actually the rules for the case of function templates."}
{"questionId":"254e99c0f69042d096300989a65d783d","question":"Java 21 built-in HTTP client pins the carrier thread\nI'm using Java *Corretto 21.0.0.35.1 build 21+35-LTS*, and the built-in Java HTTP client to retrieve a response as an `InputStream`. I'm making parallel requests using virtual threads, and for the most part, it's working well. However, occasionally, my testing encounters a \"Pinning\" event, as seen in the stack trace below.\n\n\nI believed that the JDK had been updated to fully support virtual threads, and in my understanding, the HTTP client shouldn't be pinning a carrier thread at all. However, it appears that this pinning event occurs sometimes when reading and (automatically) closing an `InputStream`.\n\n\nIs this behavior expected, or could it still be a bug in the JDK?\n\n\nThe code:\n\n\n\n```\nHttpResponse<InputStream> response = httpClient.send(request, HttpResponse.BodyHandlers.ofInputStream());\ntry (InputStream responseBody = response.body()) {\n  return parser.parse(responseBody); \/\/ LINE 52 in the trace below\n}\n\n```\n\nThe trace\n\n\n\n```\n* Pinning event captured:\n  java.lang.VirtualThread.parkOnCarrierThread(java.lang.VirtualThread.java:687)\n  java.lang.VirtualThread.park(java.lang.VirtualThread.java:603)\n  java.lang.System$2.parkVirtualThread(java.lang.System$2.java:2639)\n  jdk.internal.misc.VirtualThreads.park(jdk.internal.misc.VirtualThreads.java:54)\n  java.util.concurrent.locks.LockSupport.park(java.util.concurrent.locks.LockSupport.java:219)\n  java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(java.util.concurrent.locks.AbstractQueuedSynchronizer.java:754)\n  java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(java.util.concurrent.locks.AbstractQueuedSynchronizer.java:990)\n  java.util.concurrent.locks.ReentrantLock$Sync.lock(java.util.concurrent.locks.ReentrantLock$Sync.java:153)\n  java.util.concurrent.locks.ReentrantLock.lock(java.util.concurrent.locks.ReentrantLock.java:322)\n  sun.nio.ch.SocketChannelImpl.implCloseNonBlockingMode(sun.nio.ch.SocketChannelImpl.java:1091)\n  sun.nio.ch.SocketChannelImpl.implCloseSelectableChannel(sun.nio.ch.SocketChannelImpl.java:1124)\n  java.nio.channels.spi.AbstractSelectableChannel.implCloseChannel(java.nio.channels.spi.AbstractSelectableChannel.java:258)\n  java.nio.channels.spi.AbstractInterruptibleChannel.close(java.nio.channels.spi.AbstractInterruptibleChannel.java:113)\n  jdk.internal.net.http.PlainHttpConnection.close(jdk.internal.net.http.PlainHttpConnection.java:427)\n  jdk.internal.net.http.PlainHttpConnection.close(jdk.internal.net.http.PlainHttpConnection.java:406)\n  jdk.internal.net.http.Http1Response.lambda$readBody$1(jdk.internal.net.http.Http1Response.java:355)\n  jdk.internal.net.http.Http1Response$$Lambda+0x00007f4cb5e6c438.749276779.accept(jdk.internal.net.http.Http1Response$$Lambda+0x00007f4cb5e6c438.749276779.java:-1)\n  jdk.internal.net.http.ResponseContent$ChunkedBodyParser.onError(jdk.internal.net.http.ResponseContent$ChunkedBodyParser.java:185)\n  jdk.internal.net.http.Http1Response$BodyReader.onReadError(jdk.internal.net.http.Http1Response$BodyReader.java:677)\n  jdk.internal.net.http.Http1AsyncReceiver.checkForErrors(jdk.internal.net.http.Http1AsyncReceiver.java:302)\n  jdk.internal.net.http.Http1AsyncReceiver.flush(jdk.internal.net.http.Http1AsyncReceiver.java:268)\n  jdk.internal.net.http.Http1AsyncReceiver$$Lambda+0x00007f4cb5e31228.555093431.run(jdk.internal.net.http.Http1AsyncReceiver$$Lambda+0x00007f4cb5e31228.555093431.java:-1)\n  jdk.internal.net.http.common.SequentialScheduler$LockingRestartableTask.run(jdk.internal.net.http.common.SequentialScheduler$LockingRestartableTask.java:182)\n  jdk.internal.net.http.common.SequentialScheduler$CompleteRestartableTask.run(jdk.internal.net.http.common.SequentialScheduler$CompleteRestartableTask.java:149)\n  jdk.internal.net.http.common.SequentialScheduler$SchedulableTask.run(jdk.internal.net.http.common.SequentialScheduler$SchedulableTask.java:207)\n  jdk.internal.net.http.HttpClientImpl$DelegatingExecutor.execute(jdk.internal.net.http.HttpClientImpl$DelegatingExecutor.java:177)\n  jdk.internal.net.http.common.SequentialScheduler.runOrSchedule(jdk.internal.net.http.common.SequentialScheduler.java:282)\n  jdk.internal.net.http.common.SequentialScheduler.runOrSchedule(jdk.internal.net.http.common.SequentialScheduler.java:251)\n  jdk.internal.net.http.Http1AsyncReceiver.onReadError(jdk.internal.net.http.Http1AsyncReceiver.java:516)\n  jdk.internal.net.http.Http1AsyncReceiver.lambda$handlePendingDelegate$3(jdk.internal.net.http.Http1AsyncReceiver.java:380)\n  jdk.internal.net.http.Http1AsyncReceiver$$Lambda+0x00007f4cb5e33ca0.84679411.run(jdk.internal.net.http.Http1AsyncReceiver$$Lambda+0x00007f4cb5e33ca0.84679411.java:-1)\n  jdk.internal.net.http.Http1AsyncReceiver$Http1AsyncDelegateSubscription.cancel(jdk.internal.net.http.Http1AsyncReceiver$Http1AsyncDelegateSubscription.java:163)\n  jdk.internal.net.http.common.HttpBodySubscriberWrapper$SubscriptionWrapper.cancel(jdk.internal.net.http.common.HttpBodySubscriberWrapper$SubscriptionWrapper.java:92)\n  jdk.internal.net.http.ResponseSubscribers$HttpResponseInputStream.close(jdk.internal.net.http.ResponseSubscribers$HttpResponseInputStream.java:653)\n\n  com.acme.service.server.StatusClient.getResponse(com.acme.service.server.StatusClient.java:52)\n  com.acme.service.server.StatusClient_ClientProxy.getResponse(com.acme.service.server.StatusClient_ClientProxy.java:-1)\n  com.acme.client.Request.execute(com.acme.client.Request.java:96)\n  com.acme.service.server.serviceStatusProvider.getStatusHistorys(com.acme.service.server.serviceStatusProvider.java:237)\n  com.acme.service.api.RemoteStatusCheck.getStatusHistory(com.acme.service.api.RemoteStatusCheck.java:163)\n  com.acme.service.api.RemoteStatusCheck.lambda$doChecks$0(com.acme.service.api.RemoteStatusCheck.java:132)\n  com.acme.service.api.RemoteStatusCheck$$Lambda+0x00007f4cb9f0d8d0.979953307.call(com.acme.service.api.RemoteStatusCheck$$Lambda+0x00007f4cb9f0d8d0.979953307.java:-1)\n  java.util.concurrent.FutureTask.run(java.util.concurrent.FutureTask.java:317)\n  java.lang.VirtualThread.runWith(java.lang.VirtualThread.java:341)\n  java.lang.VirtualThread.run(java.lang.VirtualThread.java:311)\n  java.lang.VirtualThread$VThreadContinuation$1.run(java.lang.VirtualThread$VThreadContinuation$1.java:192)\n  jdk.internal.vm.Continuation.enter0(jdk.internal.vm.Continuation.java:320)\n  jdk.internal.vm.Continuation.enter(jdk.internal.vm.Continuation.java:312)\n  jdk.internal.vm.Continuation.enterSpecial(jdk.internal.vm.Continuation.java:-1)","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"java"},"answer":"The method `java.nio.channels.spi.AbstractInterruptibleChannel.close()` (lines 108 - 115 in Temurin-21+35 (build 21+35-LTS), but probably all OpenJDK derivatives) is implemented as:\n\n\n\n```\npublic final void close() throws IOException {\n    synchronized (closeLock) {\n        if (closed)\n            return;\n        closed = true;\n        implCloseChannel();\n    }\n}\n\n```\n\nLine 113 in your stacktrace corresponds to the `implCloseChannel()` call which also corresponds to the previous line in your stacktrace, and this is in the middle of that synchronized block. Virtual threads will be pinned if they are parked\/blocked in `synchronized` blocks, so that is why it is pinned.\n\n\nIn other words, given the code as it is, pinning is the expected and correct behaviour, and thus not a bug.\n\n\nWhether the use of `synchronized` here is an oversight in getting rid of synchronized blocks in the JDK, or if there is a specific reason this still uses `synchronized`, I don't know. Given it is a private lock object, I guess it should be possible to get rid of it (i.e. it is not part of the \"API\" of a channel) by replacing it with a `ReentrantLock` or similar, but maybe there are other implementation reasons to keep this for now.\n\n\nI have asked about this on the nio-dev list in thread [Should AbstractInterruptibleChannel.close() still use a synchronized block?](https:\/\/mail.openjdk.org\/pipermail\/nio-dev\/2023-October\/015199.html)\n\n\nAlan Bateman [responded there](https:\/\/mail.openjdk.org\/pipermail\/nio-dev\/2023-October\/015200.html) with:\n\n\n\n> \n> We decided it wasn't worth doing because it's rare to set SO\\_LINGER.\n> Temporary pinning due to contention on the readLock or writeLock when\n> closing is okay.\n> \n> \n> In the mean-time, we are working to remove restriction on synchronized\n> blocks. We hope to have something in the loom repo soon.\n> \n> \n>"}
{"questionId":"df1e209acf53446393eec57b8eb1f2c3","question":"Java 17 vs Java 8 double representation\nWhat is the reason of the difference of the same values when doing average between 2 different JVM's (Java 8 and Java 17)?\n\n\nIs that because the floating point? Or has something else changed between 2 versions?\n\n\nJava 17\n\n\n\n```\npublic class Main {\n    public static void main(String[] args) {\n\n        List<Double> amountList = List.of(27.19, 18.97, 6.44, 106.36);\n\n        System.out.println(\"JAVA 17 result: \" + amountList.stream().mapToDouble(x -> x).average().orElseThrow());\n\n    }\n}\n\n```\n\nresult: 39.739999999999995\n\n\nJava 8\n\n\n\n```\npublic class Main {\n\n    public static void main(String[] args) {\n\n        List<Double> amountList = Arrays.asList(27.19, 18.97, 6.44, 106.36);\n\n        System.out.println(\"JAVA 8 result: \" + amountList.stream().mapToDouble(x -> x).average().orElse(0.0));\n    }\n}\n\n```\n\nresult: 39.74000000000001","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"java"},"answer":"The relevant issue is [JDK-8214761: Bug in parallel Kahan summation implementation](https:\/\/bugs.openjdk.org\/browse\/JDK-8214761)\n\n\nSince it is mentioned in this bug report that `DoubleSummaryStatistics` is affected as well, we can construct an example that eliminates all other influences:\n\n\n\n```\npublic class Main {\n    public static void main(String[] args) {\n      DoubleSummaryStatistics s = new DoubleSummaryStatistics();\n      s.accept(27.19);\n      s.accept(18.97);\n      s.accept(6.44);\n      s.accept(106.36);\n      System.out.println(System.getProperty(\"java.version\")+\": \"+s.getAverage());\n    }\n}\n\n```\n\nwhich I used to produce\n\n\n\n```\n1.8.0_162: 39.74000000000001\n\n```\n\n\n```\n17: 39.74000000000001\n\n```\n\n(with the release version of Java\u00a017)\n\n\nand\n\n\n\n```\n17.0.2: 39.739999999999995\n\n```\n\nwhich matches the version of the [backport of the fix](https:\/\/bugs.openjdk.org\/browse\/JDK-8274015).\n\n\nGenerally, [the contract of the method](https:\/\/docs.oracle.com\/en\/java\/javase\/21\/docs\/api\/java.base\/java\/util\/stream\/DoubleStream.html#average()) says that the result does not have to match the result of just adding the values and dividing by the size. There\u2019s the implementation\u2019s freedom to provide an error correction but it\u2019s also important to keep in mind that floating point addition is not strictly associative but we have to treat it as associative to be able to support parallel processing.\n\n\n\n\n---\n\n\nWe may even verify that the change is an improvement:\n\n\n\n```\nDoubleSummaryStatistics s = new DoubleSummaryStatistics();\ns.accept(27.19);\ns.accept(18.97);\ns.accept(6.44);\ns.accept(106.36);\ndouble average = s.getAverage();\nSystem.out.println(System.getProperty(\"java.version\") + \": \" + average);\n\nBigDecimal d = new BigDecimal(\"27.19\");\nd = d.add(new BigDecimal(\"18.97\"));\nd = d.add(new BigDecimal(\"6.44\"));\nd = d.add(new BigDecimal(\"106.36\"));\n\nBigDecimal realAverage = d.divide(BigDecimal.valueOf(4), MathContext.UNLIMITED);\nSystem.out.println(\"actual: \" + realAverage\n        + \", error: \" + realAverage.subtract(BigDecimal.valueOf(average)).abs());\n\n```\n\nwhich prints, e.g.\n\n\n\n```\n1.8.0_162: 39.74000000000001\nactual: 39.74, error: 1E-14\n\n```\n\n\n```\n17.0.2: 39.739999999999995\nactual: 39.74, error: 5E-15\n\n```\n\nNote that this is the error of the decimal representations as printed. If you want to know how close the actual `double` representation is to the correct value, you have to replace `BigDecimal.valueOf(average)` with `new BigDecimal(average)`. Then, the difference between the errors is a bit less, however, the new algorithm is closer to the correct value for both."}
{"questionId":"dbc904833dca4025ad9958ed42a84c47","question":"Can you convert std::vector to std::array at compile-time without making the vector twice?\nI'm calculating some data at compile-time using `std::vector` and want to return the results as an array, so it can be used further at run-time. I'm having trouble setting the array size without doing the calculation twice.\n\n\nHere's a simplified example of what I've done so far. The code compiles and works as expected.\n\n\n\n```\nconstexpr auto make_vector() { \n  \/\/ complex calculation here\n  return std::vector{1, 2, 3}; \n}\n\nconstexpr auto make_array() {\n  const auto vec = make_vector();\n  std::array<int, make_vector().size()> result{};\n\n  std::copy(vec.cbegin(), vec.cend(), result.begin());\n  return result;\n}\n\nint main() {\n  constexpr auto result = make_array(); \/\/ std::array<int, 3>{1, 2, 3}\n}\n\n```\n\nI understand why it's not possible to use `vec.size()` for the array size and why `make_vector().size()` produces a compile-time constant. It just doesn't seem the *right* approach to do it twice.\n\n\nIs there a way to avoid calling `make_vector` twice? Am I missing a basic concept here?","questionMetadata":{"Type":"Optimization","Level":"Intermediate","Tag":"cpp"},"answer":"> \n> Is there a way to avoid calling `make_vector` twice? Am I missing a basic concept here?\n> \n> \n> \n\n\nCurrently, no. The fundamental issue is that `std::vector` is not a literal type, so you cannot simply store it in a `constexpr` variable where its size and contents can be used arbitrarily. A `std::vector` can only live temporarily during a constant expression(1).\n\n\nDuring this first constant expression, the `.size()` is not a constant expression, and there's no way to \"elevate it to that status\".\nTherefore, you don't know the size of the array you're creating, and the vector contents are wasted the first time(2).\n\n\nThis issue is also the motivation in [P0784: More constexpr containers](https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/papers\/2019\/p0784r7.html)(3):\n\n\n\n> \n> Amongst other things, the lack of variable size [constexpr] containers forces them to use primitive fixed-size data structures in the implementation, and to parse the input JSON string twice; once to determine the size of the data structures, and once to parse the JSON into those structures.\n> \n> \n> \n\n\nYou're not the first to run into this problem, and you'll just have to be patient until the C++ committee and compiler developers figure this one out.\n\n\nThe latest paper addressing this problem is [P1974: Non-transient constexpr allocation](https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/papers\/2023\/p2670r1.html). The key issue is that any `T*` allocated at compile time and stored in a `constexpr` variable (directly, or through a container) can point to mutable memory. This constness issue means that a `constexpr std::vector<std::string>` would contain a mixture of mutable\/immutable pointers (akin to `char ** const`), and that breaks the assumption that we can omit `delete` entirely and keep static memory instead. Refer to the paper for more details.\n\n\n\n\n---\n\n\n(1) The reason is that even though `std::vector` can use `new` in constant expressions, all memory must be de-allocated before the end of the constant expression. This is referred to as \"transient allocations\".\n\n\n(2) As commenter @HolyBlackCat has pointed out, it's possible that your compiler memoizes the call to `make_vector()` so that even though you call it twice, the function doesn't need to be evaluated twice.\n\n\n(3) This paper has been accepted into C++20, but does not yet solve the issue with wasted container creation."}
{"questionId":"401501b0a578407db6e449ac2667fdd8","question":"Is it ok to use std::ignore in order to discard a return value of a function to avoid any related compiler warnings?\nI know that you can use `static_cast<void>`, but it just seems too verbose for me, and not reflecting the original intent that I want to discard a return value, not to cast it to anything.\n\n\nRecently I stumbled upon [`std::ignore`](https:\/\/en.cppreference.com\/w\/cpp\/utility\/tuple\/ignore), which can accept a value of any type, the name is clear and readable, and to me it seems fitting.\n\n\nI know that the initial intent was to use `std::ignore` alongside with [`std::tie`](https:\/\/en.cppreference.com\/w\/cpp\/utility\/tuple\/tie) to discard any unwanted values, but I guess the original intent of `static_cast` was to actually cast values for some better reasons than discarding values so the compiler won't complain.\n\n\nSo, is it OK to use `std::ignore` for the purpose I described in the question?\n\n\nFor example:\n\n\n\n```\nstd::ignore = std::transform(...);","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"cpp"},"answer":"Yes, it is okay. In fact, it is considered much better style by some people.\n\n\n\n> \n> Never cast to `(void)` to ignore a `[[nodiscard]]` return value. If you deliberately want to discard such a result, first think hard about whether that is really a good idea (there is usually a good reason the author of the function or of the return type used `[[nodiscard]]` in the first place). If you still think it\u2019s appropriate and your code reviewer agrees, use `std::ignore =` to turn off the warning which is simple, portable, and easy to grep.\n> \n> \n> \n\n\n- [CppCoreGuidelines ES.48: Avoid casts](http:\/\/isocpp.github.io\/CppCoreGuidelines\/CppCoreGuidelines#es48-avoid-casts)\n\n\n*Note: [Herb Sutter committed the guideline](https:\/\/github.com\/isocpp\/CppCoreGuidelines\/pull\/1685). With the `std::ignore` rule, you can consistently teach not to use casts, and don't have to make an exception for casting to `void`.*\n\n\nAn argument against `std::ignore` is that it is only defined in terms of its effect in `std::tie`:\n\n\n\n> \n> \n> ```\n> template<class... TTypes>\n> constexpr tuple<TTypes&...> tie(TTypes&... t) noexcept;\n> \n> ```\n> \n> *Returns*: `tuple<TTypes&...>(t...)`. When an argument in `t` is `ignore`, assigning any value to the corresponding tuple element has no effect.\n> \n> \n> \n\n\n- [[utilities] `std::tie`](https:\/\/eel.is\/c++draft\/utilities#lib:tie)\n\n\nThis is mostly a philosophical issue though. In every major standard library, `std::ignore` is implemented in such a way that you can do `std::ignore = ...` on its own, and this may soon be well-defined. See [P2968: Make `std::ignore` a first-class object](https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/papers\/2023\/p2968r0.html).\n\n\nIn the end, it's stylistic preference. You can use `(void)`, `static_cast<void>`, or `std::ignore`. They are all acceptable, and which one to use is a matter of opinion.\nWhat matters is that you use a consistent style throughout your project, i.e. if you use `std::ignore` to discard results in one place, use it everywhere."}
{"questionId":"fb7ac9979fc848bd8ed45997b260f217","question":"Null checking with primary constructor in C# 12\nI using C# 12.\nIn C# 12 I can use primary constructor:\n\n\n\n```\npublic class UserService(IUnitOfWork uow) : IUserService\n{\n}\n\n```\n\nBefore C# 12 I used null checking for items that I inject in constructor:\n\n\n\n```\npublic class UserService : IUserService\n{\n    private readonly IUnitOfWork _uow;\n\n    public UserService(IUnitOfWork uow)\n    {\n        ArgumentNullException.ThrowIfNull(uow);\n        _uow = uow;\n    }\n}\n\n```\n\nNow how can I do null checking in C# 12 ?  \n\nIs it need to use fail fast with primary constructor ?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"csharp"},"answer":"As far as I know if you want to switch to primary constructors one of the easiest options would be to introduce field\/property:\n\n\n\n```\npublic class UserService(IUnitOfWork uow) : IUserService\n{\n    private readonly IUnitOfWork _uow = uow \n         ?? throw new ArgumentNullException(nameof(uow));\n}\n\n```\n\nNote that you can also name the field the same as your constructor parameter (`_uow` -> `uow`), if you don't want to clutter your class with an extra name (as suggested by [Heinzi](https:\/\/stackoverflow.com\/users\/87698\/heinzi)) which has additional benefit of shadowing the mutable primary ctor parameter by an immutable field.\n\n\nYou can also encapsulate the logic into helper method. Something along these lines:\n\n\n\n```\npublic class UserService(IUnitOfWork uow) : IUserService\n{\n    private readonly IUnitOfWork uow = uow.IsNotNull();\n}\n\npublic static class Check\n{\n    [return:NotNull]\n    public static T IsNotNull<T>(this T t,\n         [CallerArgumentExpression(\"t\")] string? paramName = null) where T : class\n    {\n        ArgumentNullException.ThrowIfNull(t, paramName);\n        return t;\n    }\n}"}
{"questionId":"4268658ea9ac4e9aaa3a871e3133394b","question":"C++23: char now supports Unicode?\nDoes C++23 now provide support for Unicode characters in its basic `char` type, and to what degree?\n\n\n\n\n---\n\n\nSo on [cppreference for character literals](https:\/\/en.cppreference.com\/w\/cpp\/language\/character_literal), a character literal:\n\n\n\n```\n'c-char'\n\n```\n\nis defined as either:\n\n\n\n> \n> - a `basic-c-char`\n> - an escape sequence, as defined in escape sequences\n> - a universal character name, as defined in escape sequences\n> \n> \n> \n\n\nand then for `basic-c-char`, it's defined that:\n\n\n\n> \n> A character from the basic source character set (until C++23) translation character set (since C++23), except the single-quote `'`, backslash `\\`, or new-line character\n> \n> \n> \n\n\nOn the [cppreference's page for character sets](https:\/\/en.cppreference.com\/w\/cpp\/language\/charset), it then defines the \"translation character set\" as consisting of the following:\n\n\n\n> \n> - each abstract character assigned a code point in the Unicode codespace, and (since C++23)\n> - a distinct character for each Unicode scalar value not assigned to an abstract character.\n> \n> \n> \n\n\nand states:\n\n\n\n> \n> The translation character set is a superset of the basic character set and the basic literal character set (see below).\n> \n> \n> \n\n\nIt seems to me that the \"basic character set\" (given on the just-above page) is basically a subset of ASCII. I also always thought of `char` as namely being ASCII (with support for ISO-8859 character sets, such as per [Microsoft's page on the character types](https:\/\/learn.microsoft.com\/en-us\/cpp\/cpp\/char-wchar-t-char16-t-char32-t?view=msvc-170)). But now with the change to the translation character set for `basic-c-char`, it seems it supports Unicode to some fuller extent.\n\n\nI'm aware that the actual encoding is implementation defined (apart from the null character and incrementing decimal digit characters it seems). But my main question is what characters are really supported by this \"translation character set\"? Is it all of Unicode? I feel as though I'm reading more into this than is actually the case.","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"Effectively not much changed (with two important differences):\n\n\nBefore C++23 the first translation phase defined that any character in the source file that isn't an element of the [basic source character set](https:\/\/timsong-cpp.github.io\/cppwp\/n4868\/lex#charset-1) (which is a subset of the ASCII character set) was to be mapped to a *universal-character-name*, i.e. it would be replaced by a sequence of the form `\\UXXXXXXXX` where `XXXXXXXX` is the number of the ISO\/IEC 10646 (equivalently Unicode) code point for the character.\n\n\nThen when writing a character literal `'X'` where `X` is replaced with a character that is not in the basic source character set you would get `'\\UXXXXXXXX'` after the first translation phase and then the [*c-char -> universal-character-name* grammar](https:\/\/timsong-cpp.github.io\/cppwp\/n4868\/lex#nt:c-char) applied.\n\n\nSo you could always write non-ASCII characters in a character literal, assuming the source encoding permitted to write such character. Source file encoding and supported source characters outside the basic source character set were implementation-defined as the *source character set (encoding)*. Regardless of source character set, you could already write any Unicode scalar value directly into a character literal with a universal character name.\n\n\nHow this character literal will then behave is a different question, because the encoding used for to determine the value of the `char` from the universal-character-name (or any character of the basic source character set) is implementation-defined as well (the *execution character set encoding* in C++20 or *ordinary literal encoding* in C++23). Obviously if `char` is 8bit wide it can't represent all Unicode scalar values. If the character was not representable in `char`, then the behavior was implementation-defined.\n\n\nThe changes for C++23 are now that support for UTF-8 source encoding became mandatory, implying support for all Unicode scalar values in the source file, (although other encodings can of course also be supported) and that the first phase was changed, so that instead of rewriting everything to the basic source character set via universal character names, now the source characters are mapped to a *translation character set* sequence which is essentially a Unicode scalar value sequence. Unicode code points that are not Unicode scalar value, i.e. surrogate code points, aren't elements of the translation character set (and can't be produced by decoding any source file).\n\n\nTherefore, in C++23 when getting to the translation phase where the character literal's value is determined, a single Unicode scalar value in the source file matches the *basic-c-char* grammar as you showed in your question.\n\n\nThe value of the character literal is still determined as before by implementation-defined encoding. However, in contrast to C++20, the literal is now ill-formed if the character is not representable in `char` via this encoding.\n\n\nSo the two differences are that UTF-8 source file encoding must be supported and that a single source character (meaning a single Unicode scalar value) in the character literal that is not representable in the implementation-defined ordinary literal encoding will now cause the literal to be ill-formed instead of having an implementation-defined value.\n\n\n\n\n---\n\n\nAnalogously to the above, string literals (rather than character literals) haven't really changed either. The encoding is still implementation-defined using the same *ordinary literal encoding* and primarily only the internal representation in the translation phases changed. And in the same way as for character literals, with C++23 the literal becomes ill-formed if a character (i.e. translation character set element or Unicode scalar value) is not representable in the ordinary literal character encoding. However that encoding may be e.g. UTF-8, so that a single Unicode scalar value in the source file may map to multiple `char` in the encoded string, as has always been the case."}
{"questionId":"5035c708e20d42e0b83c1c68fc08850b","question":"How do I split a string by a character without ignoring trailing split-characters?\nI have a string similar to the following\n\n\n\n```\nmy_string <- \"apple,banana,orange,\"\n\n```\n\nAnd I want to split by `,` to produce the output:\n\n\n\n```\nlist(c('apple', 'banana', 'orange', \"\"))\n\n```\n\nI thought strsplit would accomplish this but it treats the trailing ',' like it doesn't exist\n\n\n\n```\nmy_string <- \"apple,banana,orange,\"\n\nstrsplit(my_string, split = ',')\n#> [[1]]\n#> [1] \"apple\"  \"banana\" \"orange\"\n\n```\n\nCreated on 2023-11-15 by the [reprex package](https:\/\/reprex.tidyverse.org) (v2.0.1)\n\n\nWhat is the simplest approach to achieve the desired output?\n\n\nSome more test cases with example strings and desired outputs\n\n\n\n```\nstring1 = \"apple,banana,orange,\"\noutput1 = list(c('apple', 'banana', 'orange', ''))\n\nstring2 =  \"apple,banana,orange,pear\"\noutput2 = list(c('apple', 'banana', 'orange', 'pear'))\n\nstring3 =  \",apple,banana,orange\"\noutput3 = list(c('', 'apple', 'banana', 'orange'))\n\n## Examples of non-comma separated strings\n# '|' separator\nstring4 =  \"|apple|banana|orange|\"\noutput4 = list(c('', 'apple', 'banana', 'orange', ''))\n\n# 'x' separator\nstring5 =  \"xapplexbananaxorangex\"\noutput5 = list(c('', 'apple', 'banana', 'orange', ''))\n\n\n```\n\nEDIT:\n\n\nIdeally solution should generalize to any splitting character\n\n\nWould also prefer a base-R solution (although do still link any packages which supply this functionality since their source code might be useful to look through!)","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"r"},"answer":"Pasting another separator at the end should allow `strsplit` to function as intended.  \n\nOtherwise, you could fall back to using the `scan` function, which underpins the `read.csv\/table` functions:\n\n\n\n```\nstrsplit(paste0(string1, \",\"), \",\")\n##[[1]]\n##[1] \"apple\"  \"banana\" \"orange\" \"\"\n\n```\n\nGeneralisably taking into account regex replacement:\n\n\n\n```\nL <- list(string1, string2, string3, string4, string5)\nmapply(\n    function(x,s) strsplit(paste0(x, gsub(\"\\\\\\\\\", \"\", s)), split=s),\n    L,\n    c(\",\", \",\", \",\", \"\\\\|\", \"x\")\n)\n\n##[[1]]\n##[1] \"apple\"  \"banana\" \"orange\" \"\"      \n##\n##[[2]]\n##[1] \"apple\"  \"banana\" \"orange\" \"pear\"  \n##\n##[[3]]\n##[1] \"\"       \"apple\"  \"banana\" \"orange\"\n##\n##[[4]]\n##[1] \"\"       \"apple\"  \"banana\" \"orange\" \"\"      \n##\n##[[5]]\n##[1] \"\"       \"apple\"  \"banana\" \"orange\" \"\" \n\n```\n\n`scan` option:\n\n\n\n```\nscan(text=string1, sep=\",\", what=\"\")\n##Read 4 items\n##[1] \"apple\"  \"banana\" \"orange\" \"\"\n\n```\n\nGeneralising:\n\n\n\n```\nmapply(\n    function(x,s) scan(text=x, sep=s, what=\"\"),\n    L,\n    c(\",\", \",\", \",\", \"|\", \"x\")\n)"}
{"questionId":"9886ab720cb6415c88a84dcf04ffbe36","question":"Which version of the C Standard Library does the C++23 Standard incorporate?\n(My original question was going to be about \"What happened to `_BitInt`?\" but that was based on a misreading of some cppreference pages).\n\n\nThe Library Introduction section 16.2 of the C++23 Draft Standard says that the C Standard library is supported in C++. The only reference to a specific C standard, however, is in a footnote (#141) in 16.3. This is to the 2018 C Standard, which doesn't mention `_BitInt`. But I found a description of `_BitInt` in the draft C 23 Standard.\n\n\nDoes the C++23 Standard incorporate a specific version of the C Standard Library?\n\n\n- If so, which one?\n- If not, is there a mechanism to incorporate the libraries of future C Standards?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"cpp"},"answer":"Quoting [N4950 2.2](https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/papers\/2023\/n4950.pdf) (the final working draft of the C++23 standard):\n\n\n\n> \n> 2. The library described in ISO\/IEC 9899:2018, Clause 7, is hereinafter called the C standard library3.\n> \n> \n> 3 With the qualifications noted in Clause 17 through Clause 33 and in C.7, the C standard library is a subset of the C++\n> standard library\n> \n> \n> \n\n\nwhere [ISO\/IEC 9899:2018 is the C17 standard](https:\/\/en.wikipedia.org\/wiki\/C17_(C_standard_revision)).\n\n\nNote while the C++23 standard names a specific C standard library version, it is still possible for an implementation to provide *any* additional features as language extensions. That could include support for later versions of the C standard library or support for C core language features (like `_BitInt(N)`). For example, both GCC and Clang provide [C99 VLAs](https:\/\/stackoverflow.com\/q\/1887097\/11082165) as a language extension when compiling C++ code, and Clang [supports C23's `_BitInt(N)` as an extension](https:\/\/clang.llvm.org\/docs\/LanguageExtensions.html#extended-integer-types) when compiling C++ code or C code for earlier editions."}
{"questionId":"b0e08ea332834fdc884652ee8da82dba","question":"In Python is there a way to get the code object of top level code?\nIs it possible to get the code object of top level code within a module? For example, if you have a python file like this:\n\n\n\n```\nmyvar = 1\nprint('hello from top level')\n\ndef myfunction():\n    print('hello from function')\n\n```\n\nand you want to access the code object for `myfunction`, then you can use `myfunction.__code__`. For example, `myfunction.__code__.co_consts` will contain the string `'hello from function'` etc...\n\n\nIs there a way to get the code object for the top level code? That is, for the code:\n\n\n\n```\nmyvar = 1\n\nprint('hello from top level')\n\n```\n\nI would like something like `__main__.__code__.co_consts` that will contain `'hello from top level'`, but I cannot find any way to get this. Does such a thing exist?","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"python"},"answer":"The code that is executed at the top level of a module is not directly accessible as a code object in the same way that functions' code objects are, because the top-level code is executed immediately when the module is imported or run, and it doesn't exist as a separate entity like a function does.\n\n\nBut when Python runs a script, it compiles it first to bytecode and stores it in a code object. The top-level code (`__main__` module), have a code object, but it is not directly exposed, so you need to use `inspect` module to dig deeper:\n\n\n\n```\nimport inspect\n\ndef get_top_level_code_object():\n    frame = inspect.currentframe()\n\n    # Go back to the top-level frame\n    while frame.f_back:\n        frame = frame.f_back\n\n    # The code object is stored in f_code\n    return frame.f_code\n\nif __name__ == \"__main__\":\n    top_level_code_obj = get_top_level_code_object()\n    print(top_level_code_obj.co_consts) \n\n```\n\nwould yield\n\n\n\n```\n(0, None, <code object get_top_level_code_object at 0x7f970ad658f0, file \"\/tmp\/test.py\", line 3>, '__main__')"}
{"questionId":"c53cfb3ed5434e56a1c278abfc0ccc85","question":"How can I constrain template parameter pack arguments to a \"chain\" sequence?\nSuppose I have two classes:\n\n\n\n```\ntemplate <typename X, typename Y>\nclass Functor {};\n\ntemplate <typename Start, typename End, typename ...Functors>\nclass Template {};\n\n```\n\n`Template` has the constraints:\n\n\n- All `Functors` must be type `Functor`\n- All `Functor` must be in a *chain* sequence, such that\n\n\n\t- the **first** `Functor` must have `Start` as its **first** argument\n\t- the **last** `Functor` must have `End` as its **second** argument\n\t- each `Functor`'s **first** argument is the **second** argument of the `Functor` preceding itE.g. `Functor<A,B>, Functor<B, C>, Functor<C, D>, ...` etc.\n\n\n### Example:\n\n\nStarting with: `char`\n\n\nEnding with: `long`\n\n\n`Template<char, long, Functor<char, A>, Functor<A, B>, Functor<B, C>, Functor<C, long>> t;`\n\n\n\n```\n                1         2         3         4\n           \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\nargument: char       A         B         C        long\nFunctor #\n  = 1      Functor<char, A>,\n    2                Functor<A, B>,\n    3                           Functor<B, C>,\n    4                                    Functor<C, long>\n\n```\n\n### Code\n\n\n\n```\nnamespace ns\n{\n    template <typename X, typename Y = X>\n    class Functor\n    {\n    public:\n        using first  = X;\n        using second = Y;\n        Functor(X lVal) : x(lVal) {}\n    private:\n        X x;\n    };\n\n    template <typename Start, typename End, typename ...Functors>\n        requires(std::is_convertible_v<Functors, Functor> && ...)    \/\/error\n    class Template\n    {\n        \/\/ How does one use `std::is_convertible_v` on\n        \/\/ an un-specialized template class?\n    };\n\n    template <typename Start, typename End>\n    class Template<Start, End, Functor<Start, End>>\n    {};\n}\n\n```\n\nQuestions:\n\n\n1. What is the best approach?\n\t- Can this be done with fold expression(s)?\n\t- Or concepts?\n2. How does one use `std::is_convertible` (or any of the [other metaprogramming traits](https:\/\/en.cppreference.com\/w\/cpp\/header\/type_traits#Type_relationships)) on an un-specialized template class?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"As a single requires-expression:\n\n\n\n```\n    requires requires(Functors... f) {\n        []<typename... X, typename... Y>(Functor<X, Y>&...)\n            requires std::same_as<void(Start, Y...), void(X..., End)> {\n        }(f...);\n    }\n\n```\n\nFirst, we deduce the `X` and `Y` of each `Functor`; this step will fail if any of the `Functors` are not an instantiation of `Functor`. (Strictly speaking, it will also allow types derived from `Functor<X, Y>`; to prevent this, you could use `std::type_identity`.) Then, we check that the chain of types `Start, Y...` is the same as the chain `X..., End` (note: *not* `Start, X...`!); this will hold precisely if the `<X, Y>...` form a chain from `Start` to `End`.\n\n\nNote that it will also hold for `<Start, End, Functor<Start, End>>`, which you've listed as a separate case, and for `<Start, Start>`, i.e. if `Start` and `End` are the same type, it will allow the chain to be empty; if you want to disallow this you can add `sizeof...(Functors) != 0u` as an extra constraint.\n\n\nI'm using function types for a typelist, which is concise but does have the potential drawback of decaying the types; you could equally use e.g. `std::tuple`, and that would allow relaxing the constraint to e.g. `std::convertible_to` (`std::tuple<T...>` is convertible to `std::tuple<U...>` iff each `T` is convertible to its respective `U`).\n\n\nIf an invalid chain is passed, gcc will output an error ending with something like:\n\n\n\n```\nnote: the expression 'is_same_v<_Tp, _Up> [with _Tp = void(char, A, C, C, int); _Up = void(char, A, B, C, long int)]' evaluated to 'false'\n\n```\n\n[Demo](https:\/\/gcc.godbolt.org\/z\/fz4TKfbbj).\n\n\nA slightly more verbose but perhaps clearer way to write the constraint is:\n\n\n\n```\n    requires requires(std::tuple<Functors...> f) {\n        []<typename... X, typename... Y>(std::tuple<Functor<X, Y>...>)\n            requires requires(std::tuple<Start, Y...> c, std::tuple<X..., End> d) {\n                []<typename... C, typename... D>(std::tuple<C...>, std::tuple<D...>)\n                    requires (std::same_as<C, D> and...) {}(c, d);\n            } {}(f); }\n\n```\n\nHere we're using type deduction to form the typelists `C := {Start, Y...}` and `D := {X..., End}`, then performing a conjunction fold over the elementwise type comparison, giving an error on invalid chain along the lines of:\n\n\n\n```\nnote: the expression '(same_as<C, D> && ...) [with C = {char, A, C, C, int}; D = {char, A, B, C, long int}]' evaluated to 'false'\n\n```\n\n[Demo](https:\/\/gcc.godbolt.org\/z\/6EjGejM4P)."}
{"questionId":"4ceae08c827047a6b95d00be01588553","question":"What is \\_Nullable pointer in C?\nWhat does \\_Nullable mean in following declaration?\n\n\n\n```\nvoid foo(int *_Nullable ptr)\n\n```\n\ngcc of version 11.4 doesn't compile that code, because it treats \\_Nullable keyword as an argument name. Yet I see this keyword in man 7 pages:\n<https:\/\/man7.org\/linux\/man-pages\/man2\/epoll_ctl.2.html>\n<https:\/\/man7.org\/linux\/man-pages\/man2\/timer_gettime.2.html>\n\n\n[Godbolt](https:\/\/godbolt.org\/#g:!((g:!((g:!((h:codeEditor,i:(filename:%271%27,fontScale:14,fontUsePx:%270%27,j:1,lang:c%2B%2B,selection:(endColumn:1,endLineNumber:5,positionColumn:1,positionLineNumber:5,selectionStartColumn:1,selectionStartLineNumber:5,startColumn:1,startLineNumber:5),source:%27%23include+%3Ciostream%3E%0A%0Avoid+foo(int+*_Nullable+n)+%7B%7D%0A%0Aint+main()+%7B%0A++++return+0%3B%0A%7D%0A%27),l:%275%27,n:%270%27,o:%27C%2B%2B+source+%231%27,t:%270%27)),k:50,l:%274%27,n:%270%27,o:%27%27,s:0,t:%270%27),(g:!((h:compiler,i:(compiler:g114,deviceViewOpen:%271%27,filters:(b:%270%27,binary:%271%27,binaryObject:%271%27,commentOnly:%270%27,debugCalls:%271%27,demangle:%270%27,directives:%270%27,execute:%271%27,intel:%270%27,libraryCode:%270%27,trim:%271%27),flagsViewOpen:%271%27,fontScale:14,fontUsePx:%270%27,j:1,lang:c%2B%2B,libs:!(),options:%27-O0%27,overrides:!(),selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1),l:%275%27,n:%270%27,o:%27+x86-64+gcc+11.4+(Editor+%231)%27,t:%270%27)),k:50,l:%274%27,n:%270%27,o:%27%27,s:0,t:%270%27)),l:%272%27,n:%270%27,o:%27%27,t:%270%27)),version:4)","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"c"},"answer":"_Nullable` simply means that the pointer passed to the function is allowed to be `NULL`. It is however not a keyword in C (and doesn't occur even once in the draft for the C23 standard) but an extension supported only by some compilers, for example [clang](\/questions\/tagged\/clang \"show questions tagged 'clang'\") and [icx](\/questions\/tagged\/icx \"show questions tagged 'icx'\").\n\n\nThe Linux `man` pages uses `_Nullable` to indicate to the readers that they are allowed to pass in a `NULL` pointer, but the actual declarations in the header files are free from `_Nullable`.\n\n\n\n\n---\n\n\nLibraries who wishes to give the compiler the static analysis and optimizing hints `_Nullable` and `_Nonnull` (indicating that the pointer may *not* be `NULL`) will optionally declare pointers as `_Nullable` and `_Nonnull` depending on which platform\/compiler that is used.\n\n\nExample: In `zipconf.h` (a platform specific include file for `libzip`) on Linux, you would see\n\n\n\n```\n#define _Nullable\n#define _Nonnull\n\n```\n\nand all the `zip` functions *will* have both `_Nullable` and `_Nonnull` in their declarations. On Linux these will however be replaced with nothing thanks to the above `#define`s. On a platform dominated by [clang](\/questions\/tagged\/clang \"show questions tagged 'clang'\") or [icx](\/questions\/tagged\/icx \"show questions tagged 'icx'\") these definitions will not exist and the implementation specific keywords will then be passed on to the compiler as-is."}
{"questionId":"f2e9fc6060764adc93bc6dc079943b6d","question":"What happened to Versioning in Xcode 15?\nI have an iOS app with a widget.\nNormal housekeeping with updates includes bumping the CFBundleVersion of the app and the Extension to stay in synch.\n\n\nI just downloaded Xcode15 (release) and while the App Target still have the version and build in the same place in General\/Identity, the Extension\u2019s General\/ is now missing the Version and Build. The version is in the pList, but not the build number, besides changing it there is a hack.\n\n\nAnyone know where I can cleanly update the Extension Version\/Build?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"swift"},"answer":"Apple deleted the simple, convenient way to change the build and version for extensions. Now, for the extension you have to go to the Target and change it under Build Settings \/ Versioning\n\n\nSince the App and the Extension are always supposed to have the same Version\/Build why can not that just be the easy default, with a wonky path to changing from default where required?"}
{"questionId":"f44a6306a6e34b5c9a95ca35a2ae9dd6","question":"Laziness inside a data type\nI thought I understood laziness well until I came up with the following code, which yields a `<<loop>>` error.\n\n\n\n```\nweird = ([1],[2]) <> weird\nmain = print (head $ fst weird)\n\n```\n\nIntuitively, here is what I thought Haskell would do :\n\"I need first element of `weird`. And I need the head of this first element. So I need compute `fst weird`. Now I know that `fst weird = [1] ++ fst weird` (or do I ??) from the semigroup instance for pairs. So that's great, I should return `1`\"\n\n\nWhere did I get this wrong ?","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"haskell"},"answer":"It is pattern matching that is at fault here. Indeed, if we look at the [`instance` of `Semigroup` for the 2-tuple instance\u00a0[src]](https:\/\/hackage.haskell.org\/package\/base-4.19.0.0\/docs\/src\/GHC.Base.html#line-428), we see:\n\n\n\n> \n> \n> ```\n> instance (Semigroup a, Semigroup b) => Semigroup (a, b) where\n>     (a,b) <> (a',b') = (a<>a',b<>b')\n>     stimes n (a,b) = (stimes n a, stimes n b)\n> \n> ```\n> \n> \n\n\nso here it takes two 2-tuples and then it will combine the two. But this means it *does* pattern matching on the first *and* second operand. For the second operand, there is problem, since that is the result of a computation, so that triggers the system in evaluating these.\n\n\nThe matching might look unnecessary, but it is possible to pass `undefined` or some other mechanism that causes a loop, like it did here, and the code thus basically asks to check if the second operand is a 2-tuple.\n\n\nWhat we can do is work with an *irrefutable* pattern, such that we will assume that the data constructor holds and only unpack it if necessary. So we can implement some sort of sum ourselves with:\n\n\n\n```\n(<^>) :: (Semigroup a, Semigroup b) => (a, b) -> (a, b) -> (a, b)\n**~**(a,b) <^> **~**(a',b') = (a<>a',b<>b')\n```\n\nand then our own implementation works with:\n\n\n\n```\nweird = ([1],[1]) <^> weird\nmain = print (head $ fst weird)\n\n```\n\nso we made the implementation more lazy to combine the two 2-tuples.\n\n\nPersonally I would think the combinations for `Semigroup`s, etc. on 2-tuples (and any *n*-tuple) can be done with irrefutable patterns. I don't know if there are good reasons why that is not the case in the base package."}
{"questionId":"ec5f607020a44485ba7ab5375fc4e493","question":"Can a class be indestructible (have no destructor)?\nConsider:\n\n\n\n```\n#include <type_traits>\n\ntemplate <typename T>\nstruct A {\n  ~A() requires(std::is_void_v<T>);\n  \/\/~A() requires(!std::is_void_v<T>) = default;\n};\n\ntemplate struct A<int>;\n\n```\n\nI'd think that's impossible for a class not to have a destructor and gcc 13.2 seems to agree with me but clang 17.0 and msvc 19.38 don't and complain:\n\n\n\n```\n[clang] error: no viable destructor found for class 'A<int>'\n[msvc ] error C7653: 'A<int>': failed to select a destructor for the class\n\n```\n\n(Adding the commented line to the code works around this issue and make all compilers happy.)\n\n\nWhich compilers are correct? What are the relevant quotes from the Standard?","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"cpp"},"answer":"A (completed) class must always have a destructor. However, that destructor might be implicitly-declared, deleted or left undefined.\n\n\nAt the end of the class definition a destructor is chosen from the prospective destructors via overload resolution. If the user didn't declare any prospective destructor, then one is implicitly-declared. So there is always at least one candidate for this overload resolution. The program is ill-formed if the overload resolution fails.\n\n\nSo GCC is wrong and MSVC and Clang are correct. In your case there is no implicitly-declared prospective destructor, because you declared a prospective destructor manually. That single prospective destructor is not viable in overload resolution because its constraints are not satisfied. So overload resolution fails.\n\n\nThis is specified in (draft N4868) [[class.dtor]\/4](https:\/\/timsong-cpp.github.io\/cppwp\/n4868\/class.dtor#4.sentence-2).\n\n\nIf the selected destructor is defined as deleted, then the class still has a destructor, but it will be impossible to destroy objects of the class type, i.e. the class is also \"indestructible\". Any program trying to destroy such objects would be ill-formed.\n\n\nSimilarly, if the selected destructor is left undefined, then any attempt to destroy an object of the class type would render the program IFNDR (ill-formed, no diagnostic required) for violating the one-definition-rule."}
{"questionId":"86eba5efd4b8411fa6478da03e363364","question":"Alternative to .concat() of empty dataframe, now that it is being deprecated?\nI have two dataframes that can both be empty, and I want to concat them.\n\n\nBefore I could just do :\n\n\n\n```\noutput_df= pd.concat([df1, df2])\n\n```\n\nBut now I run into\n\n\n\n> \n> FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n> \n> \n> \n\n\nAn easy fix would be:\n\n\n\n```\nif not df1.empty and not df2.empty:\n    result_df = pd.concat([df1, df2], axis=0)\nelif not df1.empty:\n    result_df = df1.copy()\nelif not df2.empty:\n    result_df = df2.copy()\nelse:\n    result_df = pd.DataFrame()\n\n```\n\nBut that seems pretty ugly. Does anyone have a better solution ?\n\n\nFYI: this appeared after pandas released [v2.1.0](https:\/\/pandas.pydata.org\/docs\/whatsnew\/v2.1.0.html#deprecations)","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"To be precise, [`concat`](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.concat.html) is not deprecated (and won't be IMHO) but I can trigger [*this*](https:\/\/github.com\/pandas-dev\/pandas\/blob\/a0babcb2c63dd721ea47e75f6229c5fe727b2395\/pandas\/core\/internals\/concat.py#L492) `FutureWarning` in [`2.1.1`](https:\/\/github.com\/pandas-dev\/pandas\/releases\/tag\/v2.1.1) with the following example, while `df2` being an empty DataFrame with a different [`dtypes`](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.dtypes.html) than `df1` :\n\n\n\n```\ndf1 = pd.DataFrame({\"A\": [.1, .2, .3]})\ndf2 = pd.DataFrame(columns=[\"A\"], dtype=\"object\")\n\nout = pd.concat([df1, df2]) ; print(out)\n\n     A\n0  0.1\n1  0.2\n2  0.3\n\n```\n\nAs a solution in your case, you can try something like you did :\n\n\n\n```\nout = (df1.copy() if df2.empty else df2.copy() if df1.empty\n       else pd.concat([df1, df2]) # if both DataFrames non empty\n      )\n\n```\n\nOr maybe even this one? :\n\n\n\n```\nout = pd.concat([df1.astype(df2.dtypes), df2.astype(df1.dtypes)])"}
{"questionId":"a604580e4d274aada64e6a1af87d0b4e","question":"What is wrong with the inheritance hierarchy in my example?\nHere is the code:\n\n\n\n```\n#include <iostream>\n\nstruct Parent\n{\n    virtual ~Parent() = default;\n};\n\nclass Buddy\n{\npublic:\n    virtual ~Buddy() = default;\n    Buddy(Parent& p) : parent_ { p } {}\n    bool IsHovered() const { return is_hovered_; }\n\nprivate:\n    bool    is_hovered_ = false;\n    Parent& parent_;\n};\n\nclass Child : public Parent, public Buddy\n{\npublic:\n    Child() : Buddy { *this } {}\n\n    bool BuddyIsHovered() const { return IsHovered(); }\n};\n\nint main()\n{\n    Child c;\n    \/\/ Expected 'false', but 'true' is always printed.\n    std::cout << std::boolalpha << c.BuddyIsHovered() << '\\n';\n}\n\n\n```\n\nI can assume that the problem (may be it's an undefined behaviour) occurred because of multiple inheritance and mutual usage of `Child` and `Buddy`. From the other side, I also have assumptions that:\n\n\n- Compiler firstly allocates memory for `Child`;\n- Then compiler runs `Child` constructor; at this point it seems to me, that compiler should know correct addresses for all 3 classes;\n- The order of initializations of classes would be: `Parent`, `Buddy`, `Child`. So, when the `Child` constructor\\_ is running, `Buddy`'s memory has been already initialized, and thus `Buddy` can be used. I mean, `Child`'s code can use `Buddy`'s code, which not touches uninitialized data members of `Buddy`.\n\n\nWhich of my assumptions are correct, and which are wrong?\n\n\nI use [MSVS 2022](https:\/\/en.wikipedia.org\/wiki\/Microsoft_Visual_Studio#2022) and [C++14](https:\/\/en.wikipedia.org\/wiki\/C%2B%2B14).","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"cpp"},"answer":"[This is a MSVC bug](https:\/\/developercommunity.visualstudio.com\/t\/Ambiguity-in-class-construction-is-not-d\/10537718).\n\n\nWhat seems to happen is that MSVC interprets\n\n\n\n```\nChild() : Buddy { *this } {}\n\n```\n\nas\n\n\n\n```\nChild() : Buddy { static_cast<Buddy &>(*this) } {}\n\n```\n\nWhich calls the copy constructor of `Buddy`. That initializes `parent_` with itself, which causes undefined behavior (due to reading it before its lifetime starts, which starts when the initialization is finished).\n\n\nThe fix is to explicitly cast to `Parent &`:\n\n\n\n```\nChild() : Buddy { static_cast<Parent &>(*this) } {}"}
{"questionId":"8c77f10fc9514cb7ab77a34c39065260","question":"In a lambda, what does the second list of attributes do?\nC++23 allows `[[...]]` attributes in lambda expressions:\n\n\n\n```\nauto lambda = [] [[nodiscard]]\n{\n    return 42;\n};\n\n```\n\nBut the grammar has place for **two** lists of attributes, roughly before and after the parameter list:\n\n\n\n```\nauto lambda = [] [[nodiscard]] () [[deprecated]]\n\/\/               (1)~~~~~~~~~~    (2)~~~~~~~~~~~  \/\/ (2) seems to have no effect here\n{\n    return 42;\n};\n\n```\n\nThe two lists apply to different things, but I can't figure out what the second list is for, and what attributes I can put here. For any attribute I tried at (2), Clang warns: `'...' attribute cannot be applied to types`.\n\n\nThe standard says this:\n\n\n[`[expr.prim.lambda.closure]\/6`](http:\/\/eel.is\/c++draft\/expr.prim.lambda#closure-6)\n\n\n\n> \n> ... An *attribute-specifier-seq* [at location (2)] appertains to the type of the corresponding function call operator or operator template.\n> \n> \n> An attribute-specifier-seq [at location (1)] appertains to the corresponding function call operator or operator template.\n> \n> \n> \n\n\n*\"appertains to the type of the corresponding function call operator\"* leaves me confused. What attributes can I put here? Is this solely for compiler-specific attributes, and if so, what non-standard attributes can be used here?\n\n\nAlso apparently the list (2) existed well before C++23 ([this proposal](https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/papers\/2021\/p2173r1.pdf) added the list (1)).","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"Attributes at location (2) apply to the type of `operator()` (which is a function type), whereas at location (1) they apply to the function declaration itself.\n\n\nNone of the standard attributes apply to types, so no standard attribute applies when put in location (2).\n\n\nLooking at vendor-specific type attributes from <https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/Common-Type-Attributes.html> and <https:\/\/clang.llvm.org\/docs\/AttributeReference.html#type-attributes>, you can see that most type attributes only apply to object types of some kind (e.g., `[[gnu::aligned]]`, `[[gnu::vector_size]]`).\n\n\nOne attribute that seems to apply to any type (including function types) is [`[[clang::annotate_type(...)]]`](https:\/\/clang.llvm.org\/docs\/AttributeReference.html#annotate-type):\n\n\n\n```\n\/\/ [] [[clang::annotate_type(\"xyz\") () {}  \/\/ Error\n[] () [[clang::annotate_type(\"xyz\")]] {}\n\/\/ (For completeness, `clang::annotate` is used for declarations)\n[] [[clang::annotate(\"annotates operator()\")]] () [[clang::annotate_type(\"annotates type of operator(), `void()`\")]] {}\n\n```\n\nThere may be other attributes that apply to function types, but I haven't found them.\n\n\nIt also perfectly mirrors attribute placement in regular function declarations:\n\n\n\n```\nstruct X {\n    auto operator() [[clang::annotate(\"annotates operator()\")]] () [[clang::annotate_type(\"annotates void()\")]] {}\n};\n\n```\n\nBut yes, you usually want the attributes to apply to the function declaration and not the function type, so you should never have to use an attributes at location (2)."}
{"questionId":"d8dfb6d21d5e4b05ac759e01adaf1f5f","question":"Why do I need to specify the type of a default constructed object in this situation?\nI don't understand why in `foobar` below I need to specify `std::vector<int>{}` whereas in `foobar2` I do not:\n\n\n\n```\n#include <iostream>\n#include <memory>\n#include <vector>\n#include <tuple>\n\nstd::tuple<std::unique_ptr<int>, std::vector<int>> foobar() {\n    std::unique_ptr<int> test = std::make_unique<int>(42);\n    return { std::move(test), {} };    \/\/ <= this is a syntax error\n    \/\/ return { std::move(test), std::vector<int>{} }  \/\/ <= this compiles...\n}\n\nstd::tuple<int, std::vector<int>> foobar2() {\n    return { {},  {} };\n}\n\nint main() {\n    std::cout << *std::get<0>(foobar()) << \"\\n\";\n    std::cout << std::get<0>(foobar2()) << \"\\n\";\n    return 0;\n}\n\n```\n\nThe error message from GCC is\n\n\n\n```\n<source>: In function 'std::tuple<std::unique_ptr<int, std::default_delete<int> >, std::vector<int, std::allocator<int> > > foobar()':\n<source>:8:34: error: could not convert '{std::move<unique_ptr<int>&>(test), <brace-enclosed initializer list>()}' from '<brace-enclosed initializer list>' to 'std::tuple<std::unique_ptr<int, std::default_delete<int> >, std::vector<int, std::allocator<int> > >'\n    8 |     return { std::move(test), {} };    \/\/ <= this is a syntax error\n      |                                  ^\n      |                                  |\n      |                                  <brace-enclosed initializer list>\nCompiler returned: 1","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"cpp"},"answer":"Given a\n\n\n\n```\ntemplate< class... Types >\nclass tuple;\n\n```\n\nThere are [two possible constructors that can be used here](https:\/\/en.cppreference.com\/w\/cpp\/utility\/tuple\/tuple). The first one is:\n\n\n\n```\ntuple( const Types&... args );\n\n```\n\nHowever it can only be used if all tuple members are copy-constructible. The `unique_ptr` is, of course, not copy-constructible.\n\n\nThis leaves only one other possible constructor:\n\n\n\n```\ntemplate< class... UTypes >\ntuple( UTypes&&... args );\n\n```\n\nThat is, a forwarding constructor, a \"Hail Mary\" that forwards all its parameters to the constructor of each underlying tuple member.\n\n\n\n```\n{}\n\n```\n\nAn empty braced-init list is typeless, and cannot be bound to a forwarding reference.\n\n\nThis could possibly work if only there was one more constructor:\n\n\n\n```\ntuple(Types && ... Args);\n\n```\n\nthat participates in overload resolution if all member types are movable. Alas, there isn't.\n\n\n\n> \n> whereas in foobar2 I do not:\n> \n> \n> \n\n\n`foobar2`'s tuple members are copy-constructible. The first constructor overload gets used."}
{"questionId":"aadc0422b8c34df9bba4199a3705e1ad","question":"Is it undefined behaviour to use pointer after allocated memory?\nI have the following code:\n\n\n\n```\nuint8_t buffer[16];\nuint8_t data[16];\nuint8_t buffer_length = 16;\nuint8_t data_length = 0;\n\nmemcpy(buffer + buffer_length, data, data_length);\n\n```\n\n`memcpy` should be a no-op, because `data_length` is zero. However `buffer + buffer_length` points just outside of the allocated memory. I wonder if it could trigger some kind of undefined behaviour? Should I wrap this `memcpy` with an additional `if`?\n\n\nI understand that any reasonable implementation of `memcpy` would work fine, however this question is more from the code correctness perspective and avoiding undefined behaviours.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"c"},"answer":"As the answer of Stephen C points out, the C17 specification is a bit vague about whether or not this is well-defined.\n\n\nHowever, the C23 specification clarifies this in a footnote to the part of 7.1.4 stating\n\n\n\n> \n> If a function argument is described as being an array, the pointer passed to the function shall\n> have a value such that all address computations and accesses to objects (that would be valid if\n> the pointer did point to the first element of such an array) are valid.\n> \n> \n> \n\n\nThe footnote (235) reads:\n\n\n\n> \n> This includes, for example, passing a valid pointer that points one-past-the-end of an array along with a size of 0, or using any valid pointer with a size of 0.\n> \n> \n> \n\n\nThe first part of the sentence explicitly defines the OP case as well-defined.\n\n\nAdding this statement can be seen as admitting that the C17 specification is not sufficiently clear on this point and thus, it cannot be ruled out that an implementer of a C17 compiler may in good faith interpret the standard such that this case is not defined behavior.\n\n\nHowever, C23 should remove that uncertainty."}
{"questionId":"4bf2480f83c04dc4a54645683a2515a0","question":"If arrays are passed by reference, why should I use int(&)[]?\nConsider:\n\n\n\n```\n#include <iostream>\nusing namespace std;\n\nvoid Change(int arr[3]) {\n    for (int i = 0; i < 3; i++) {\n        arr[i] = 1;\n    }\n}\n\nint Test() {\n    int arr[3] = { 0, 0, 0 };\n\n    for (int i = 0; i < 3; i++) {\n        cout << arr[i] << endl;\n    }\n\n    Change(arr);\n\n    for (int i = 0; i < 3; i++) {\n        cout << arr[i] << endl;\n    }\n\n    return 0;\n}\n\n```\n\nSince arrays are passed by default as a pointer to their first element and not copied, changing the value of an element of the array in a function effectively results in changing the value of that element of the array in the function caller, which is why the above code outputs\n\n\n\n```\n0\n0\n0\n1\n1\n1\n\n```\n\nIf this is the case, then why would anyone need to pass an array like the following?\n\n\n\n```\nvoid Change(int (&arr)[3])\n\n```\n\nI am aware that the parentheses are needed to make the argument a reference to an array instead of an array of references, but what do I gain?","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"cpp"},"answer":"This function declaration:\n\n\n\n```\nvoid Change(int arr[3])\n\n```\n\nis adjusted by the compiler to:\n\n\n\n```\nvoid Change(int *arr)\n\n```\n\nSo, the function knows nothing about the size of the passed array.\n\n\nIf you declare the function like:\n\n\n\n```\nvoid Change(int ( &arr )[3])\n\n```\n\nthen within the function, you can get the size of the array using, for example, the standard function `std::size()`, or get its first and last iterators like `std::begin(arr)` and `std::end(arr)`. And moreover, you can pass the array by reference to a template function that accepts its argument by reference, similarly to passing any container.\n\n\nAnother advantage is that the compiler will check that an array of the required size is passed, and the user of the function can not pass a null pointer to the functon."}
{"questionId":"6db9b5c768bb43f58a98192357e0714b","question":"Why does overload resolution prefer std::nullptr\\_t over a class when passing {}?\nThe following code prints `nullptr` instead of `empty` ([godbolt link](https:\/\/godbolt.org\/z\/6b7aPb8a1)):\n\n\n\n```\n#include <iostream>\n\nclass empty { };\n\n#if 1\nvoid f(std::nullptr_t) {\n    std::cout << \"nullptr\\n\";\n}\n#endif\n\nvoid f(empty) {\n    std::cout << \"empty\\n\";\n}\n\nint main() {\n    f({});\n}\n\n```\n\nDisabling the `f(nullptr_t)` variant causes `empty` to be printed. What are the rules C++ is using to select the `nullptr_t` variant over the `empty` variant when both are available?","questionMetadata":{"Type":"Debugging","Level":"Advanced","Tag":"cpp"},"answer":"Initializing `std::nullptr_t` (or any other fundamental type) with `{}` is better because it result in an identity conversion, whereas initializing class types results in a user-defined conversion sequence:\n\n\n\n> \n> Otherwise, if the parameter has an aggregate type which can be initialized from the initializer list according to the rules for aggregate initialization ([dcl.init.aggr]), **the implicit conversion sequence is a user-defined conversion sequence** whose second standard conversion sequence is an identity conversion.\n> \n> \n> \n\n\n- [[over.ics.list] p8](https:\/\/eel.is\/c++draft\/over.ics.list#8)\n\n\n`empty` is an aggregate type, so this paragraph applies. `std::nullptr_t` is not a class, so the following paragraph applies:\n\n\n\n> \n> Otherwise, if the parameter type is not a class:\n> \n> \n> - (10.1) [...]\n> - (10.2) if the initializer list has no elements, **the implicit conversion sequence is the identity conversion.**\n> - [...]\n> \n> \n> \n\n\n- [[over.ics.list] p10](https:\/\/eel.is\/c++draft\/over.ics.list#10)\n\n\n[[over.best.ics]](https:\/\/eel.is\/c++draft\/over.best.ics) explains which implicit conversion sequence is better, but it should be obvious that an identity conversion beats everything else."}
{"questionId":"c3b863de466441858125b05866387682","question":"Ruby method with a (\\*) signature\nOn this [interesting blog post](https:\/\/fly.io\/ruby-dispatch\/pattern-matching-on-ruby-objects\/) about pattern matching, there is some code with a method signature of `(*)`\n\n\n\n```\nclass Request < Data.define(:path, :scheme, :format)\n  def deconstruct_keys(*)\n    { path: @path, scheme: @scheme, format: @format }\n  end\n\n  def deconstruct(*)\n    path.split(\"\/\").compact\n  end\nend\n\n```\n\nThis is different than\n\n\n\n```\ndef a_method(*args)\n\n```\n\nI could not find any information in the [Ruby docs](https:\/\/rubyapi.org\/3.2\/o\/method).\n\n\nWhat does `def deconstruct_keys(*)` mean?\n\n\n**Note** This question was mentioned on [Ruby Weekly](https:\/\/rubyweekly.com\/issues\/674)","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"def a_method(*args)\n  ...\nend\n\n```\n\nNormally, when we write this, we get a list of all of the method's arguments stored in the `args` variable.\n\n\n\n```\ndef a_method(*)\n  ...\nend\n\n```\n\nThis is an anonymous form of the same. We accept any number of arguments but don't wish to give a name to that list variable. Now, we haven't given the list of arguments a name, but we can still splat it into another argument list. So while this *isn't* legal\n\n\n\n```\ndef a_method(*)\n  # Should've just named it in the first place :(\n  args = *\n  ...\nend\n\n```\n\nthis, on the other hand, is\n\n\n\n```\ndef a_method(*)\n  another_method(*)\nend\n\n```\n\nand will pass the arguments onto `another_method`. It's equivalent to\n\n\n\n```\ndef a_method(*args)\n  another_method(*args)\nend\n\n```\n\nThe same can be done with Ruby 3 keyword arguments\n\n\n\n```\ndef a_method(**)\n  another_method(**)\nend\n\n```\n\nNote that, if your intention is to forward *all* arguments, you should use the ellipsis syntax.\n\n\n\n```\ndef a_method(...)\n  another_method(...)\nend\n\n```\n\nA lone `*` will act funny when delegating keyword arguments. For instance,\n\n\n\n```\ndef foo(*args, **kwargs)\n  p args\n  p kwargs\nend\n\ndef bar(*)\n  foo(*)\nend\n\nfoo(1, a: 1) # Prints [1] then {:a=>1}\nbar(1, a: 1) # Prints [1, {:a=>1}], then {}\n\n```\n\nWhen calling `foo` directly, named argument syntax gets passed to `**kwargs`, but when delegating through `bar`, it gets converted into a hash and then passed into `*args`. On top of that, `*` won't forward block arguments either, whereas `...` is your general-purpose \"pass all positional, named, and block arguments onward\" catch-all."}
{"questionId":"3cd3d474401a4fd89595489469aa721c","question":"Combined with C++23 Deducing this and conversion operator with auto return type?\nI recently noticed a strange issue regarding C++23 *Deducing this* feature.\n\n\nSuppose we have a struct `S` with a simple conversion operator:\n\n\n\n```\nstruct S {\n  operator int() { return 42; }\n};\n\nint i = S{};\n\n```\n\nSince `42` is of type `int`, we can specify the return type of the conversion operator as `auto`, for example:\n\n\n\n```\nstruct S {\n  operator auto() { return 42; }\n};\n\nint i = S{};\n\n```\n\nThis is [totally fine](https:\/\/godbolt.org\/z\/4EY5hzWzT). If we apply C++23 *Deducing this* feature on it, it would be:\n\n\n\n```\nstruct S {\n  operator auto(this S) { return 42; }\n};\n\nint i = S{};\n\n```\n\nThis is [totally fine](https:\/\/godbolt.org\/z\/Tjv1e6YMo) too. However, when I replace `this S` with `this auto`:\n\n\n\n```\nstruct S {\n  operator auto(this auto) { return 42; }\n};\n\nint i = S{};\n\n```\n\n[**All three compilers reject the above code**](https:\/\/godbolt.org\/z\/xE5K8jdEx). GCC gives:\n\n\n\n```\n<source>:5:9: error: cannot convert 'S' to 'int' in initialization\n    5 | int i = S{};\n\n```\n\nHowever, when I change the return type of the conversion operator back to `int`, then all [three compilers compile fine](https:\/\/godbolt.org\/z\/baf176qbr):\n\n\n\n```\nstruct S {\n  operator int(this auto) { return 42; }\n};\n\nint i = S{};\n\n```\n\nThis confuses me.\n\n\nWhy does the compiler reject `operator auto(this auto) { return 42; }`? I see no reason to reject it as it seems intuitive to me. So I'm wondering what does the standard say about this? Or is this a compiler bug?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"This is [CWG1878](https:\/\/wg21.link\/CWG1878).\n\n\n[[class.conv.fct]p9](https:\/\/wg21.link\/class.conv.fct#9):\n\n\n\n> \n> A conversion function template shall not have a deduced return type.\n> \n> \n> \n\n\n`operator auto(this auto)` is an abbreviated function template, so it is not valid.\n\n\nThe reason it was disallowed is that it was of limited use at the time and I think GCC had a very difficult time with naming this function (See also: [CWG1670](https:\/\/wg21.link\/CWG1670))\n\n\nYou see the same behaviour for something like this:\n\n\n\n```\ntemplate<int = 0>\noperator auto() { return 42; }"}
{"questionId":"78be1a11903a4a7d9cc2b4e11c2bb1fa","question":"A class both derives from and its first member has type deriving from the same base class. Is the class standard-layout?\nAs far as I know, a property of the standard-layout class is that the address of a standard-layout object is equal to its initial member's. I tested the following code with g++ and clang++, but found that `Derived3` **is** a standard-layout class and `&d` **is not** equal to `&d.c`.\n\n\n\n```\n#include <iostream>\nusing namespace std;\n\nstruct Base {};\n\nstruct Derived1 : Base\n{\n  int i;\n};\n\nstruct Derived3 : Base\n{\n  Derived1 c;\n  int i;\n};\n\nint main()\n{\n  cout << is_standard_layout_v<Derived3> << endl;\n\n  Derived3 d;\n  cout << &d << endl;\n  cout << &d.c << endl;\n\n  return 0;\n}","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"cpp"},"answer":"Following the word of [the standard](https:\/\/timsong-cpp.github.io\/cppwp\/class.prop#3), they are indeed standard-layout types. Going through the points one by one:\n\n\n\n> \n> A class S is a standard-layout class if it:\n> \n> \n> - has no non-static data members of type non-standard-layout class (or array of such types) or reference, [...]\n> \n> \n> \n\n\n`int` is standard-layout. `Derived1` is standard layout, as we'll see.\n\n\n\n> \n> - has no non-standard-layout base classes,\n> \n> \n> \n\n\n`Base` is empty, so standard-layout.\n\n\n\n> \n> - has at most one base class subobject of any given type,\n> \n> \n> \n\n\nBoth `Derived1` and `Derived3` has only a single base `Base`.\n\n\n\n> \n> - has all non-static data members and bit-fields in the class and its base classes first declared in the same class, and\n> \n> \n> \n\n\nMeaning, within an inheritance hierarchy, all data members are declared in the same class. This is clearly true for `Derived1`. This is also true for `Derived3` because `Derived1` is not in the inheritance hierarchy.\n\n\nTo make this point clearer, consider a simpler example\n\n\n\n```\nstruct B {};\nstruct D1 : B {};\nstruct D3 : B { D1 c; };\n\n```\n\nWhich also runs into the same address problems as in the question, but clearly fulfills this bullet point.\n\n\n\n> \n> - has no element of the set M(S) of types as a base class, where for any type X, M(X) is defined as follows.\n> [Note 2:\u2002M(X) is the set of the types of all non-base-class subobjects that can be at a zero offset in X. \u2014 end note]\n> \t- If X is a non-union class type with no non-static data members, the set M(X) is empty.\n> \t- If X is a non-union class type with a non-static data member of type X0 that is either of zero size or is the first non-static data member of X (where said member may be an anonymous union), the set M(X) consists of X0 and the elements of M(X0). [...]\n> \n> \n> \n\n\nMeaning, `M(Derived3)` is the set {`Derived1`, `int`}, none of which is a base class of `Derived3`.\n\n\nLikewise, `M(Derived1)` is the set {`int`}, which is not a base class of `Derived1`.\n\n\n\n\n---\n\n\nBeing standard-layout means the class and its first data member is [pointer-interconvertible](https:\/\/timsong-cpp.github.io\/cppwp\/basic.compound#4). To be pedantic, the representation of pointers being different doesn't prove there's a problem, but comparing the results of `reinterpret_cast` does:\n\n\n\n```\nstd::cout << (&d.c == reinterpret_cast<Derived1*>(&d));  \/\/ 0 for clang and gcc\n\n```\n\nThus the compilers are not technically compliant. However, this is an impossible situation: the `Base` subobject in `Derived1` [cannot have the same address](https:\/\/timsong-cpp.github.io\/cppwp\/intro.object#10.2) as the `Base` subobject in `Derived3`, which is why the compilers placed `Derived1` at a four byte offset from the start.\n\n\nStandard-layout classes have a [history](https:\/\/cplusplus.github.io\/CWG\/issues\/1813.html) of [defect](https:\/\/cplusplus.github.io\/CWG\/issues\/1672.html) reports, and this looks like it should be another one."}
{"questionId":"f61103bf9578461397ca641d723c0636","question":"VS Code 1.86 line of code started following me in split at top of screen as I scroll. Why?\nI just upgraded to VS Code 1.86, and it has started making the top \/ first \/ starting part of the code block I am looking at always stay shown at the top of the editor area while I scroll, in a split view\/section. How can I get rid of this and make it go back to the old appearance without this feature?\n\n\nAKA: Why did this change, and how can I disable sticky scroll in the editor area of VS Code?","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"other"},"answer":"VS Code started changing the default value of the `editor.stickyScroll.enabled` setting to `true` in VS Code 1.86 (see [the iteration plan](https:\/\/github.com\/microsoft\/vscode\/issues\/201255) and issue ticket [#202655](https:\/\/github.com\/microsoft\/vscode\/issues\/202655)).\n\n\nThere are multiple ways to turn it back off:\n\n\n- Right click the sticky scroll panel to be prompted to toggle it off\n- OR Run `View: Toggle Sticky Scroll` in the [command palette](https:\/\/code.visualstudio.com\/docs\/getstarted\/userinterface#_command-palette)\n- OR Open your user [settings.json file](https:\/\/code.visualstudio.com\/docs\/getstarted\/settings#_settingsjson) by running `Preferences: Open User Settings (JSON)` in the command palette, and write `\"editor.stickyScroll.enabled\": false` and then save the change\n\n\nSticky Scroll was [released in version 1.70](https:\/\/code.visualstudio.com\/updates\/v1_70#_editor-sticky-scroll), but up until version 1.86, it had been disabled by default.\n\n\nThe default has now been fully changed to on [in version 1.87](https:\/\/code.visualstudio.com\/updates\/v1_87#_editor-sticky-scroll). You can also change the maximum number of lines that the sticky scroll panel takes up with the `editor.stickyScroll.maxLineCount` setting.\n\n\nNote that sticky scroll is also supported in several other areas of the VS Code workbench, such as the terminal and tree views, which each have their corresponding settings and commands."}
{"questionId":"d1bc9918929e47219ad0f0eb6c136633","question":"WillPopScope is deprecated in Flutter\n'WillPopScope' is deprecated and shouldn't be used. Use PopScope instead. This feature was deprecated after v3.12.0-1.0.pre\n\n\n\n```\nWillPopScope(\n  onWillPop: () async {\n    \/\/ your logic        \n    return false;\n  },\n)","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"dart"},"answer":"**Solved**\n\n\nreplace old widget from `WillPopScope` to `PopScope` new widget check below code\n\n\n\n```\n\/\/\/ NEW CODE\nPopScope(\n  canPop: true,\n  onPopInvoked : (didPop){\n   \/\/ logic\n  },\n)"}
{"questionId":"ebfb84f797f44f129788035156e2da81","question":"Checking whether sets of columns are the same, row wise in R, in any order\nI am working in R, and would prefer a dplyr solution if possible.\n\n\n**sample data:**\n\n\n\n```\ndata.frame(\n  col1 = c(\"a\", \"b\", \"c\", \"d\"),\n  col2 = c(\"a\", \"b\", \"d\", \"a\"),\n  col3 = rep(\"a\", 4L),\n  col4 = c(\"a\", \"b\", \"d\", \"a\"),\n  col5 = c(\"a\", \"a\", \"c\", \"d\"),\n  col6 = rep(c(\"b\", \"a\"), each = 2L)\n)\n\n```\n\n\n\n\n| col1 | col2 | col3 | col4 | col5 | col6 |\n| --- | --- | --- | --- | --- | --- |\n| a | a | a | a | a | b |\n| b | b | a | b | a | b |\n| c | d | a | d | c | a |\n| d | a | a | a | d | a |\n\n\n\n**Question**\n\n\nI would like to know for each row, whether **col1,** **col2** and **col3** are the same as **col4, col5** and **col6,** but the order of col1 - col3 and col4 - col6 should be ignored.\n\n\nSo for row 1, if col1 - col3 contained a,a,b respectively, and col4 - col6 contained b,a,a respectively, then that would be considered a match.\n\n\n**Desired result**\n\n\nHave put a note on \"assessment\" column to aid understanding\n\n\n\n\n\n| col1 | col2 | col3 | col4 | col5 | col6 | assessment |\n| --- | --- | --- | --- | --- | --- | --- |\n| a | a | a | a | a | b | FALSE (because 1-3 are not same as 4-6) |\n| b | b | a | b | a | b | TRUE (because 1-3 are the same as 4-6, if ignore order) |\n| c | d | a | d | c | a | TRUE (because 1-3 are the same as 4-6, if ignore order) |\n| d | a | a | a | d | a | TRUE (because 1-3 are the same as 4-6, if ignore order) |","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"r"},"answer":"Using dplyr you can do the following:\n\n\n\n```\ndf %>%\n  rowwise() %>%\n  mutate(result = all(sort(c_across(col1:col3)) == sort(c_across(col4:col6))))\n\n# A tibble: 4 \u00d7 7\n# Rowwise: \n  col1  col2  col3  col4  col5  col6  result\n  <chr> <chr> <chr> <chr> <chr> <chr> <lgl> \n1 a     a     a     a     a     b     FALSE \n2 b     b     a     b     a     b     TRUE  \n3 c     d     a     d     c     a     TRUE  \n4 d     a     a     a     d     a     TRUE  "}
{"questionId":"8943522fdd1f4ac9aad6ac4d94558e19","question":"Angular 17 Http client injection\nI am building a new app for testing with angular 17 trying to connect to a simple API rest. I am desperate to get this to work. I have asked chat GPT, reviewed stack's older posts and even download an old application, but with no success.\n\n\nHere is my app config and the error:\n\n\n**app.module.ts**\n\n\n\n```\nimport {FormsModule, ReactiveFormsModule} from '@angular\/forms';\nimport { BrowserModule } from '@angular\/platform-browser';\nimport { NgModule } from '@angular\/core';\nimport {AppComponent} from \".\/app.component\";\nimport {HttpClientModule} from \"@angular\/common\/http\";\nimport {ClienteService} from \".\/cliente.service\";\nimport {ClienteComponent} from \".\/cliente\/cliente.component\";\nimport {AppRoutingModule} from \".\/app-routing.module\";\n\n\n\n@NgModule({\n  declarations: [\n    AppComponent,\n    ClienteComponent\n  ],\n  imports: [\n    BrowserModule,\n    FormsModule,\n    ReactiveFormsModule,\n    HttpClientModule,\n    AppRoutingModule\n\n  ],\n  providers: [ClienteService],\n  bootstrap: [AppComponent]\n})\nexport class AppModule { }\n\n```\n\n**app-routing.module.ts**\n\n\n\n```\n\nimport { NgModule } from '@angular\/core';\nimport { Routes, RouterModule } from '@angular\/router';\n\nimport {ClienteComponent} from \".\/cliente\/cliente.component\";\n\nconst appRoutes: Routes = [\n  { path: '', redirectTo: '\/clientes', pathMatch: 'full' },\n  { path: 'clientes', component: ClienteComponent}\n];\n\n@NgModule({\n  imports: [RouterModule.forRoot(appRoutes)],\n  exports: [RouterModule]\n})\nexport class AppRoutingModule {\n\n}\n\n```\n\n**cliente.service.ts**\n\n\n\n```\nimport { Injectable } from '@angular\/core';\nimport { HttpClient } from '@angular\/common\/http';\n\n@Injectable({\n  providedIn: 'root'\n})\nexport class ClienteService {\n\n  private clienteUrl: string;\n\n  constructor(private http: HttpClient) {\n    this.clienteUrl = 'http:\/\/localhost:8081\/clientes';\n  }\n  getClientes(nombre?: string, telefono?: string, direccion?: string) {\n    const params: any = { nombre, telefono, direccion };\n    return this.http.get(this.clienteUrl, { params });\n  }\n\n```\n\n**cliente.component.ts**\n\n\n\n```\nimport {Component, OnInit} from '@angular\/core';\nimport { CommonModule } from '@angular\/common';\nimport {ClienteService} from \"..\/cliente.service\";\n\n@Component({\n  selector: 'app-cliente',\n  standalone: true,\n  imports: [CommonModule],\n  templateUrl: '.\/cliente.component.html',\n  styleUrl: '.\/cliente.component.css'\n})\nexport class ClienteComponent implements OnInit{\n\n  clientes: any[] = [];\n\n  constructor(private clienteService: ClienteService) {\n\n  }\n\n  ngOnInit(): void {\n    this.clienteService.getClientes().subscribe((clientesResponse:any) => {\n      console.log('Respuesta del servicio getAllClientes',clientesResponse);\n      this.clientes=clientesResponse._embedded.clientes;\n\n\n    });\n  }\n\n}\n\n```\n\nThe problem is this:\n\n\n\n```\nERROR NullInjectorError: R3InjectorError(Standalone[_AppComponent])[_ClienteService -> _ClienteService -> _HttpClient -> _HttpClient]: \n  NullInjectorError: No provider for _HttpClient!\n    at NullInjector.get (core.mjs:5601:27)\n    at R3Injector.get (core.mjs:6044:33)\n    at R3Injector.get (core.mjs:6044:33)\n    at injectInjectorOnly (core.mjs:911:40)\n    at Module.\u0275\u0275inject (core.mjs:917:42)\n    at Object.ClienteService_Factory [as factory] (cliente.service.ts:7:28)\n    at core.mjs:6164:43\n    at runInInjectorProfilerContext (core.mjs:867:9)\n    at R3Injector.hydrate (core.mjs:6163:17)\n    at R3Injector.get (core.mjs:6033:33)\nhandleError @ core.mjs:11747","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"typescript"},"answer":"You are mixing Modules and Standalone Components. As of Angular 17 everything is standalone by default, unless you specify otherwise.\n\n\nYou have 2 options.\n\n\n1. Don't use stand alone components, make your components part of modules, then the import arrays in the modules should work.\n2. Work without modules and make the app completely standalone. As intended from Angular 17\n\n\nIf you started a project in angular 17, I suggest against option number one.\n\n\nFor start, go to `main.js` and add `provideHttpClient(withFetch())` to the providers array. Insted of importing HttpClientModule in your AppModule.\n\n\n\n```\nbootstrapApplication(AppComponent, {\n  providers: [\n    provideHttpClient(withFetch()),\n  ],\n});"}
{"questionId":"80534a8b0e5140c9973034b7eaff04bc","question":"Is it really well defined to check pointer alignment using the pointer's integer value?\nIs there a guaranteed (not implementation-defined!) way to check for pointer alignment?\n\n\nThe most common way to query pointer alignment seems to be:\n\n\n1. convert to integer\n2. check whether the integer is a multiple of the alignment:\n\n\n\n```\nbool is_aligned(void const *ptr, size_t alignment) {\n  return reinterpret_cast<intptr_t>(ptr) % alignment == 0;\n}\n\n```\n\nFor example, this is how [Boost.Align checks alignment](https:\/\/github.com\/boostorg\/align\/blob\/8caa2c0\/include\/boost\/align\/detail\/is_aligned.hpp#L21).\n\n\nHowever, in C++17 at least, [basic.compound#3.4](https:\/\/timsong-cpp.github.io\/cppwp\/n4659\/basic.compound#3.4) says:\n\n\n\n> \n> The value representation of pointer types is implementation-defined.\n> \n> \n> \n\n\nFurthermore, [expr.reinterpret.cast#4](https:\/\/timsong-cpp.github.io\/cppwp\/n4659\/expr.reinterpret.cast#4) says:\n\n\n\n> \n> A pointer can be explicitly converted to any integral type large enough to hold it. The mapping function is implementation-defined.\n> \n> \n> \n\n\nIt seems it'd be legal to (for example) represent pointers as integers with reversed bit order, in which case the simple arithmetic above would not work.\n\n\nAFAICT, the only guaranteed way that we can check alignment is using [`std::align`](https:\/\/timsong-cpp.github.io\/cppwp\/n4659\/ptr.align), which if read liberally (I'm sure this is an abuse of `std::align`) could be used like so:\n\n\n\n```\nbool is_aligned(void const *ptr, size_t alignment) {\n  void *mut_ptr = const_cast<void *>(ptr);\n  size_t space = alignment;\n  return std::align(alignment, alignment, mut_ptr, space) == ptr;\n}\n\n```\n\nHowever, on the overwhelming majority of platforms, pointers are just fancy integers, or I'd expect Boost to have a code path for it. Are there any platforms (other than ds9k :P) where pointers *aren't* just fancy integers?\n\n\nWhat would we lose by standardizing \"`reinterpret_cast<intptr_t>(ptr)` shall have the same object representation as `ptr`\" or \"`reinterpret_cast<intptr_t>(ptr) % alignment == 0` if `ptr` is aligned to `alignment`\"? It seems neither of those would exclude segmented memory, or pointers with a trap representation, which are two atypical cases I can think of.\n\n\nIf nothing else, is there a reason not to standardize `std::is_aligned()`?\n\n\n**EDIT:** my specific non-toy use case is I have `std::byte const *ptr;`. The only think I know about `ptr` is that it contains the object representations of 1024 `int`s. How can I check whether it's safe to `reinterpret_cast` `ptr` or use `assume_aligned` on it?","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"cpp"},"answer":"> \n> It seems it'd be legal to (for example) represent pointers as integers with reversed bit order, in which case the simple arithmetic above would not work.\n> \n> \n> \n\n\nYes, nothing forbids that.\n\n\nEven worse, the mapping can be type-dependent. The only requirement that is implementation-independent is that a round-trip `reinterpret_cast` back to the *original* pointer type results in the original pointer value. And that's maybe not far fetched. For types with high alignment you don't really need to store the zero bits at the end and so the pointer type might be smaller. Then a simple `reinterpret_cast` mapping might not provide any possible information to check alignment for these pointer types. See e.g. [Do all pointers have the same size in C++?](https:\/\/stackoverflow.com\/questions\/71870205\/do-all-pointers-have-the-same-size-in-c) for discussion on pointer sizes and [Exotic architectures the standards committees care about](https:\/\/stackoverflow.com\/questions\/6971886\/exotic-architectures-the-standards-committees-care-about\/6986260#6986260) for exotic architectures, at least with regards to C, which also includes examples for this (Cray T90).\n\n\nOf course, in your use you are using `void*` specifically, which avoids this issue.\n\n\nIt is also possible that no integer type large enough to represent all pointer values exists, in which case `intptr_t` shouldn't be available and `reinterpret_cast` to any integer type will be ill-formed. That's specifically considered in the C standard, so there must have existed such C implementations or it was considered that such implementations may not be unlikely, when `(u)intptr_t` where added to C.\n\n\n\n> \n> However, in C++17 at least, basic.compound#3.4 says:\n> \n> \n> \n\n\nNote that the value representation of the pointer is irrelevant. It isn't required that `reinterpret_cast` leaves the bytes in memory unchanged. So the implementation-defined mapping by `reinterpret_cast` could be completely distinct from how integer and pointer object representations relate.\n\n\nFor example, on 64bit systems often the actual available address space is smaller (e.g. only 48bit). Then there are additional bits in the object representation. I could imagine the compiler\/CPU using these extra bits for various purposes. Depending on that purpose the bits may or may not be part of the value representation and they may or may not be stripped or modified by `reinterpret_cast`. Similarly, if a different pointer type instead of `void*` was used, the compiler\/CPU could use the bits that are guaranteed to be zero due to alignment requirements of the type for these purposes.\n\n\n\n> \n> AFAICT the only guaranteed way that we can check alignment is using std::align, which if read liberally (I'm sure this is abuse of std::align) could be used like so:\n> \n> \n> \n\n\nThe function has undefined behavior if the storage to which the input pointer points isn't contiguously at least `alignment` long the way you wrote it. So the size\/space parameters should probably be `1` (or `0`, see [LWG 2421](https:\/\/cplusplus.github.io\/LWG\/lwg-active.html#2421)) instead. I see nothing in the specification that would require the size to be at least as large as the alignment.\n\n\nThe specification of `std::align` does however seem broken to me anyway, so not sure whether that is intended to work. For example it fails to specify to which (or one-past which) object the resulting pointer points, pretending that it is possible to describe a pointer value just by the address it represents, but that's not possible since C++17.\n\n\nI think it is also not in line with the definition of *alignment* in [basic] which doesn't assume any integer representation of addresses and only gives restrictions on when an address *satisfies* a *supported* alignment requirement. An implementation that only supports alignment `1` would e.g. be valid and then there is no definition for when an address is aligned for some other power-of-two.\n\n\n\n> \n> If nothing else, is there a reason not to standardize std::is\\_aligned()?\n> \n> \n> \n\n\nNot that I am aware of. `std::align` seems to have that functionality already as discussed above. A user-friendly wrapper around it would be simple to do, assuming the specification of `std::align` is really as intended and supposed to be guaranteed to work on every implementation.\n\n\n\n> \n> EDIT: my specific non-toy use case is I have std::byte const \\*ptr;. The only think I know about ptr is that it contains the object representations of 1024 ints. How can I check whether it's safe to reinterpret\\_cast ptr or use assume\\_aligned on it?\n> \n> \n> \n\n\nEven if you know that `ptr` is correctly aligned and that the `std::byte` array contains valid object representations for `int`, then it is still not guaranteed that `reinterpret_cast` (which must be followed by `std::launder` either way) will be valid. You also need to make sure that, prior to copying the `int` object representation into the storage, `int` objects have been created. That can happen implicitly, e.g. if you `memcpy` the object representations into the array or use `std::bit_cast` to obtain the filled array into which `ptr` points, but you can't assume it generally. Otherwise you might have an aliasing violation (and\/or precondition violation of `std::launder`). In particular if the object representations were copied into the `std::byte` array by a simple per-byte assignment in a loop, then this will result in an aliasing violation."}
{"questionId":"e2b0820328a44575b68389b7f3a5a46e","question":"Why do I get IDX20803 error after upgrading to IdentityModel v7 from v6?\nAfter upgrading `Microsoft.IdentityModel.Tokens` and `System.IdentityModel.Tokens.Jwt` to `7.0.0`, I get this error:\n\n\n\n> \n> IDX20803: Unable to obtain configuration from: 'https:\/\/example.com\/realms\/Development\/.well-known\/openid-configuration'.\n> \n> \n> \n\n\n\n> \n> Could not load type 'Microsoft.IdentityModel.Json.JsonConvert' from assembly 'Microsoft.IdentityModel.Tokens, Version=7.0.0.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35'. Could not load type 'Microsoft.IdentityModel.Json.JsonConvert' from assembly 'Microsoft.IdentityModel.Tokens, Version=7.0.0.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35'. => Microsoft.IdentityModel.Json.JsonConvert\n> \n> \n> \n\n\nBefore the update, my package references were:\n\n\n\n```\n<PackageReference Include=\"Microsoft.AspNetCore.Authentication.JwtBearer\" Version=\"7.0.10\" \/>\n<PackageReference Include=\"Microsoft.IdentityModel.Tokens\" Version=\"6.32.3\" \/>\n<PackageReference Include=\"System.IdentityModel.Tokens.Jwt\" Version=\"6.32.3\" \/>\n<PackageReference Include=\"System.Text.Json\" Version=\"7.0.3\" \/>\n\n```\n\nAfter the update, my package references are now:\n\n\n\n```\n<PackageReference Include=\"Microsoft.AspNetCore.Authentication.JwtBearer\" Version=\"7.0.11\" \/>\n<PackageReference Include=\"Microsoft.IdentityModel.Tokens\" Version=\"7.0.0\" \/>\n<PackageReference Include=\"System.IdentityModel.Tokens.Jwt\" Version=\"7.0.0\" \/>\n<PackageReference Include=\"System.Text.Json\" Version=\"7.0.3\" \/>\n\n```\n\nWhat's the issue?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"TLDR: add `<PackageReference Include=\"Microsoft.IdentityModel.Protocols.OpenIdConnect\" Version=\"7.0.0\" \/>`\n\n\n\n\n---\n\n\nBased on the [release notes](https:\/\/github.com\/AzureAD\/azure-activedirectory-identitymodel-extensions-for-dotnet\/wiki\/IdentityModel-7x#serialization-issues), between v6 and v7 of `System.IdentityModel.Tokens.Jwt`, the JSON serialiser\/deserialiser has been changed from Newtonsoft Json.NET to `System.Text.Json`.\n\n\nIt has 2 implicit dependencies:\n\n\n- `Microsoft.IdentityModel.Tokens` (that you've made explicit in this case)\n- `Microsoft.IdentityModel.JsonWebTokens`\n\n\n[As defined](https:\/\/www.nuget.org\/packages\/System.IdentityModel.Tokens.Jwt\/7.0.0#dependencies-body-tab), IdentityModel v7.0.0 also upgrades these implicit dependencies to their corresponding v7.0.0 - as expected & good so far.\n\n\nThe issue isn't with the upgraded packages but instead, highlights a problem with the `Microsoft.AspNetCore.Authentication.JwtBearer` package, which would be used alongside.\n\n\nThis package has an implicit dependency on `Microsoft.IdentityModel.Protocols.OpenIdConnect`.\n\n\nHowever, the latest `Microsoft.AspNetCore.Authentication.JwtBearer` v7.0.11 package incorrectly still [states](https:\/\/www.nuget.org\/packages\/Microsoft.AspNetCore.Authentication.JwtBearer\/7.0.11#dependencies-body-tab) that `Microsoft.IdentityModel.Protocols.OpenIdConnect` v6.15.1 is valid.\n\n\nThis is wrong in this case, as v6.15.1 isn't compatible with Identity Model 7 & its implicit dependencies.\n\n\n\n\n---\n\n\nThe solution is making the `Microsoft.IdentityModel.Protocols.OpenIdConnect` dependency explicit and specifying [v7.0.0](https:\/\/www.nuget.org\/packages\/Microsoft.IdentityModel.Protocols.OpenIdConnect\/7.0.0#readme-body-tab) in your project, to override the implicit v6 package - fixing the dependency version mismatch.\n\n\nThis should be a temporary fix until Microsoft hopefully fix this in their upcoming package updates.\n\n\n\n```\n<PackageReference Include=\"Microsoft.IdentityModel.Protocols.OpenIdConnect\" Version=\"7.0.0\" \/>\n\n```\n\n\n\n---\n\n\nThis is the most minimal set of packages that fix this issue:\n\n\n\n```\n<ItemGroup>\n    <PackageReference Include=\"Microsoft.AspNetCore.Authentication.JwtBearer\" Version=\"7.0.11\" \/>\n    <PackageReference Include=\"Microsoft.IdentityModel.Protocols.OpenIdConnect\" Version=\"7.0.0\" \/>\n    <PackageReference Include=\"System.IdentityModel.Tokens.Jwt\" Version=\"7.0.0\" \/>\n<\/ItemGroup>"}
{"questionId":"d7c7acdb5a9c49c98788e3a91f518b44","question":"java.lang.NoSuchMethodError: 'org.apache.commons.io.output.UnsynchronizedByteArrayOutputStream$Builder org.apache.poi-poi-ooxml-5.2.4\nI have upgraded from `org.apache.poi-poi-ooxml-5.2.3` to `org.apache.poi-poi-ooxml-5.2.4` due to **Security Violation Threat** in `5.2.3`\n\n\nNow, I am facing run time exception as `java.lang.NoSuchMethodError`\n\n\n**Exception:**\n\n\n\n```\n[ERROR] ErrorPageFilter - Forwarding to error page from request [\/reports\/myapp\/myreport] due to exception ['org.apache.commons.io.output.UnsynchronizedByteArrayOutputStream$Builder org.apache.commons.io.output.UnsynchronizedByteArrayOutputStream.builder()']\njava.lang.NoSuchMethodError: 'org.apache.commons.io.output.UnsynchronizedByteArrayOutputStream$Builder org.apache.commons.io.output.UnsynchronizedByteArrayOutputStream.builder()'\n    at org.apache.poi.xssf.usermodel.XSSFWorkbook.newPackage(XSSFWorkbook.java:521) ~[poi-ooxml-5.2.4.jar:5.2.4]\n    at org.apache.poi.xssf.usermodel.XSSFWorkbook.<init>(XSSFWorkbook.java:231) ~[poi-ooxml-5.2.4.jar:5.2.4]\n    at org.apache.poi.xssf.usermodel.XSSFWorkbook.<init>(XSSFWorkbook.java:227) ~[poi-ooxml-5.2.4.jar:5.2.4]\n    at org.apache.poi.xssf.usermodel.XSSFWorkbook.<init>(XSSFWorkbook.java:215) ~[poi-ooxml-5.2.4.jar:5.2.4]\n    at myapp.reports.service.impl.MyReportsExcelExporter.<init>(MyReportsExcelExporter.java:37) ~[classes\/:0.0.1-SNAPSHOT]\n\n```\n\n**Code:**\n\n\n\n```\nimport org.apache.poi.xssf.usermodel.XSSFWorkbook;\n\npublic class MyReportsExcelExporter {\n    protected XSSFWorkbook workbook;\n    ...\n    public MyReportsExcelExporter() {\n        this.workbook = new XSSFWorkbook(); \/\/Facing issue here, while initializing the workbook.\n    }\n    ...\n}\n\n```\n\nLooking at the version change, it seems like a minor upgrade but now existing code has stopped working.\n\n\nWhat's probably wrong?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"java"},"answer":"You will need to add\/upgrade **Apache Commons IO** dependency version **>= 2.12.x**.\n\n\n**Note:** The `builder()` method present in `UnsynchronizedByteArrayOutputStream` got introduced from **2.12.x** version of **commons-io** onwards.\n\n\nI took the latest dependency of **commons-io** which is **2.14.0** at the time of writing the answer.\n\n\n**pom.xml** (Maven):\n\n\n\n```\n<dependencies>\n    <dependency>\n        <groupId>commons-io<\/groupId>\n        <artifactId>commons-io<\/artifactId>\n        <version>2.14.0<\/version>\n    <\/dependency>\n<\/dependencies>\n\n```\n\nOr\n\n\n**build.gradle** (Gradle):\n\n\n\n```\ndependencies {\n   implementation 'commons-io:commons-io:2.14.0'\n}\n\n```\n\nIt will work."}
{"questionId":"876feebdb9334dbc99d2abce14324c50","question":"type argument \u2018nw\\_proxy\\_config\\_t\u2018 is neither an Objective-C object nor a block type\nCannot build on latest Xcode 15 Stable, iOS 17 stable\n\n\nRelated to\n<https:\/\/github.com\/pichillilorenzo\/flutter_inappwebview\/issues\/1735>\n\n\n\n```\nflutter run \n\n```\n\n\n```\nCould not build the precompiled application for the device.\nError (Xcode): type argument 'nw_proxy_config_t' (aka 'struct nw_proxy_config *') is neither an Objective-C object nor a block type\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/iPhoneOS.platform\/Developer\/SDKs\/iPhoneOS17.0.sdk\/System\/Library\/Frameworks\/WebKit.framework\/Headers\/WKWebsiteDataStore.h:119:46\n\nParse Issue (Xcode): Could not build module 'WebKit'\n\/build\/ios\/Debug-iphoneos\/flutter_inappwebview\/flutter_inappwebview.framework\/Headers\/flutter_inappwebview-Swift.h:285:8\n\n\nError launching application on iPhone ","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"Fix: Upgrade flutter from master branch, delete podfile.lock and rerun pod install\n\n\nTemporal fix:\n\n\n**Workaround** 1 from GitHub:\n\n\nUse sudo and yor favorite text editor:\n\n\n\n```\nopen \/Applications\/Xcode-beta.app\/Contents\/Developer\/Platforms\/iPhoneOS.platform\/Developer\/SDKs\/iPhoneOS17.0.sdk\/System\/Library\/Frameworks\/WebKit.framework\/Headers\/WKWebsiteDataStore.h,\n\n```\n\nchange the `__IPHONE_OS_VERSION_MAX_ALLOWED` from 170000 to 180000\n\n\n**Workaround 2**:\n\n\n\n```\npost_integrate do |installer|\n  compiler_flags_key = 'COMPILER_FLAGS'\n  project_path = 'Pods\/Pods.xcodeproj'\n\n  project = Xcodeproj::Project.open(project_path)\n  project.targets.each do |target|\n    target.build_phases.each do |build_phase|\n      if build_phase.is_a?(Xcodeproj::Project::Object::PBXSourcesBuildPhase)\n        build_phase.files.each do |file|\n          if !file.settings.nil? && file.settings.key?(compiler_flags_key)\n            compiler_flags = file.settings[compiler_flags_key]\n            file.settings[compiler_flags_key] = compiler_flags.gsub(\/-DOS_OBJECT_USE_OBJC=0\\s*\/, '')\n          end\n        end\n      end\n    end\n  end\n  project.save()\nend\n\n\n```\n\n**Workaround 3** update\n\n\nCocoaPods\n\n\n\n```\ngem install cocoapods \n\n```\n\nand run :\n\n\n\n```\nflutter clean \n\nflutter upgrade\n\ncd ios && pod repo update\n\nflutter run \n\n```\n\n**Extra :**\n\n\n\n```\nUpdate flutter_inappwebview to latest version ^5.8.0\n\n```\n\nAnd the project must build without problems"}
{"questionId":"2053ff7ac5394933910e1846cd5bb6b5","question":"NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported\nI try to load a dataset using the `datasets` python module in my local Python Notebook. I am running a Python 3.10.13 kernel as I do for my virtual environment.\n\n\nI cannot load the datasets I am following from a tutorial. Here's the error:\n\n\n\n```\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\n\/Users\/ari\/Downloads\/00-fine-tuning.ipynb Celda 2 line 3\n      1 from datasets import load_dataset\n----> 3 data = load_dataset(\n      4     \"jamescalam\/agent-conversations-retrieval-tool\",\n      5     split=\"train\"\n      6 )\n      7 data\n\nFile ~\/Documents\/fastapi_language_tutor\/env\/lib\/python3.10\/site-packages\/datasets\/load.py:2149, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\n   2145 # Build dataset for splits\n   2146 keep_in_memory = (\n   2147     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\n   2148 )\n-> 2149 ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)\n   2150 # Rename and cast features to match task schema\n   2151 if task is not None:\n   2152     # To avoid issuing the same warning twice\n\nFile ~\/Documents\/fastapi_language_tutor\/env\/lib\/python3.10\/site-packages\/datasets\/builder.py:1173, in DatasetBuilder.as_dataset(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\n   1171 is_local = not is_remote_filesystem(self._fs)\n   1172 if not is_local:\n-> 1173     raise NotImplementedError(f\"Loading a dataset cached in a {type(self._fs).__name__} is not supported.\")\n   1174 if not os.path.exists(self._output_dir):\n   1175     raise FileNotFoundError(\n   1176         f\"Dataset {self.dataset_name}: could not find data in {self._output_dir}. Please make sure to call \"\n   1177         \"builder.download_and_prepare(), or use \"\n   1178         \"datasets.load_dataset() before trying to access the Dataset object.\"\n   1179     )\n\nNotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported.\n\n```\n\nHow do I resolve this? I don't understand how this error is applicable, given that the dataset is something I am fetching and thus *cannot* be cached in my LocalFileSystem in the first place.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"Try doing:\n\n\n\n```\npip install -U datasets\n\n```\n\n\n> \n> This error stems from a breaking change in fsspec. It has been fixed in the latest datasets release (2.14.6). Updating the installation with pip install -U datasets should fix the issue.\n> \n> \n> \n\n\ngit link : <https:\/\/github.com\/huggingface\/datasets\/issues\/6352>\n\n\n\n\n---\n\n\nif you are using `fsspec` then do:\n\n\n\n```\npip install fsspec==2023.9.2\n\n```\n\nThere is a problem with `fsspec==2023.10.0`\n\n\ngit link : <https:\/\/github.com\/huggingface\/datasets\/issues\/6330>"}
{"questionId":"1696853e64db4adab3addd6c69df71e9","question":"Using Box to optimise memory allocation of optional, known length arrays\nI have a question about memory allocation strategies in a specific situation.\n\n\nI have a large number of structs that are coming from a JSON API via serde. There is a specific field in the JSON response which will, for a small minority of cases, exist and contain an array of exactly 11 elements. In the majority of cases this field will not exist at all in the response. All responses need to be stored.\n\n\nThe majority of responses will be of the format:\n\n\n\n```\n{\n  id: 1,\n  event: \"None\"\n}\n\n```\n\nA minority of responses will be of the format:\n\n\n\n```\n{\n  id: 2,\n  event: \"something\",\n  event_details : [11 json objects here]\n}\n\n```\n\nIf I define my struct for parsing the JSON as:\n\n\n\n```\n#[derive(Deserialize, Debug)]\nstruct Event {\n  id: u32,\n  event: String,\n  \/\/ EventDetail is a struct with an u32 and string field\n  events: Option<[EventDetail; 11]>\n }\n\n```\n\nI can see using [deepsizeof](https:\/\/docs.rs\/deepsize\/latest\/deepsize\/trait.DeepSizeOf.html) crate that my `Vec<Event>` that gets created takes up ~500MB of memory. If I change the `events` field to be `events: Option<Box<[EventDetail; 11]>>` the memory usage drops to ~150MB.\n\n\nMy understanding here is that since Box is allocated at runtime instead of compile time, when a JSON response comes in that is lacking the `events` field, then the 11 item array does not need to be allocated. Am I correct here? And is this a valid use case for Box or is there a better way to do this?\n\n\nI also tried `events: Option<Vec<EventDetail>>`, which resulted in slightly higher memory assignment than Box. I assume this is down to padding.","questionMetadata":{"Type":"Optimization","Level":"Intermediate","Tag":"rust"},"answer":"Yes, this is all correct. `Option<[T; N]>` always allocates space for the array, even if it's not used, whereas `Option<Box<...>>` allocates enough space for the box pointer, but *not* the thing it points to unless needed. Hiding the large data behind a `Box` indirection is 100% the correct call here.\n\n\n`Vec` is a pointer (basically a `Box`) plus the size of the vector and its capacity, so `Option<Vec<...>>` will usually be three times the overhead, as it will allocate space for *all of* the pointer, the size and the capacity, of the (potentially nonexistent) vector. But at that point, we're arguing in units of bytes, so it's silly to debate size differences at that scale.\n\n\n(`Vec` also stores the memory allocator, but the default allocator `Global` is a zero-sized type)\n\n\nSince you know the size in advance, I'd go with `Option<Box<[EventDetail; 11]>>` rather than the vector, since the vector is just pointlessly pushing data to runtime that could be known at compile-time."}
{"questionId":"22a5d5a812c641258ebb335143d722fd","question":"Why doesn't std::ranges::find compile while std::find works fine?\nConsider this code:\n\n\n\n```\n#include <vector>\n#include <iostream>\n#include <cstdint>\n#include <ranges>\n\nint main()\n{\n    struct S {\n        int  a;\n        int  b;\n        bool operator==(int other) const { return a == other; }\n    };\n    std::vector<S> iv{\n        {1, 2},\n        {3, 4}\n    };\n\n    \/\/ this works\n    if (auto const it{std::find(iv.begin(), iv.end(), 1)}; it != iv.end()) {\n        std::cout << \"Found!\\n\" << \"\\n\";\n    }\n\n    \/\/std::ranges::find(iv, 2); \/\/ <-- why does this not compile\n    \n}\n\n```\n\nMy impression was that the call convention of ranges would be a 1-to-1 mapping of the corresponding original algorithm (i.e., just skip the `.begin()` and `.end()`, and it should work as before).\n\n\nClearly this is not the case here. What am I missing?\n\n\nA link to the code:\n<https:\/\/godbolt.org\/z\/3a6z9c5of>","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"cpp"},"answer":"ranges::find` uses [`ranges::equal_to`](https:\/\/en.cppreference.com\/w\/cpp\/utility\/functional\/ranges\/equal_to) by default to compare the elements of the range with the value, and the call operator of `ranges::equal_to` constrains the two types to satisfy [`equality_comparable_with`](https:\/\/en.cppreference.com\/w\/cpp\/concepts\/equality_comparable), which is `S` and `int` in your example.\n\n\n`equal_comparable_with` requires `S` to be comparable with itself and needs to have a common reference with `const int&`, which is not your case.\n\n\nThe simple workaround is to use the projection function to project `S` to `S::a`\n\n\n\n```\nif (auto const it{std::ranges::find(iv, 1, &S::a)}; it != iv.end()) {\n    std::cout << \"Found!\\n\" << \"\\n\";\n}"}
{"questionId":"559333a0d112480ab22599e6dfcaca00","question":"C# notnull constraint with nullable types have unexpected behavior\nI have read about `notnull` constraint in C# and it was written that \"This allows either value types or non-nullable reference types but not nullable reference types.\"\n(the quote is from \"Programming C# - 10.0 By Ian Griffiths)\n\n\nI tried checking this constraint in the code below:\n\n\n\n```\nMyTestClass<int?> instance1 = new MyTestClass<int?>();\nMyTestClass<string?> instance2 = new MyTestClass<string?>();\n\npublic class MyTestClass<T> where T : notnull\n{\n    T Value { get; set; }\n\n    public MyTestClass()\n    {\n        Value = default(T);\n        if (Value == null)\n            Console.WriteLine($\"Type of T is {typeof(T)} and its default value is 'Null'\");\n        else\n            Console.WriteLine($\"Type of T is {typeof(T)} and its default value is {Value}\");\n    }\n}\n\n```\n\nas you can see I instantiated my generic class with nullable types `int?` (nullable value type) and `string?` (nullable reference type) and it still works for me.\n\n\nIt also prints the output like this for me:\n\n\n\n```\nType of T is System.Nullable`1[System.Int32] and its default value is 'Null'\nType of T is System.String and its default value is 'Null'\nType of T is System.Int32 and its default value is 0\nType of T is System.String and its default value is 'Null'\"\n\n```\n\nIt behaves 'string?' as 'string' and detects both as non-nullable.\nwhat can be the reason for these to happen?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"csharp"},"answer":"The `notnull` constraint, as defined, \"limits the type parameter to non-nullable types. The type may be a value type or a non-nullable reference type.\" [[1](https:\/\/learn.microsoft.com\/en-us\/dotnet\/csharp\/language-reference\/keywords\/where-generic-type-constraint)]\n\n\nThe constraint is available for code in a nullable enable context, and on compilation checks for type parameters that do not match the constraint (i.e. `string?`, `int[]?`), and creates a **warning** and not an error, specifically `CS8714: Nullability of type argument doesn't match 'notnull' constraint.`\n\n\nSo, programs will compile when breaking the `notnull` constraint, but will raise a compile-time warning when it can. It's important to mention that it only raises a warning when it can because \"Generic declarations that include the notnull constraint can be used in a nullable oblivious context, but compiler does not enforce the constraint.\" [[1](https:\/\/learn.microsoft.com\/en-us\/dotnet\/csharp\/language-reference\/keywords\/where-generic-type-constraint)]\n\n\nSo, nullable reference types *can* be passed as valid type parameters to type parameters specified with the `notnull` constraint, but should not be. A warning should be being raised in your demo unless it is not running in a `#nullable enable` context.\n\n\nSources:\n\n\n1. <https:\/\/learn.microsoft.com\/en-us\/dotnet\/csharp\/language-reference\/keywords\/where-generic-type-constraint>"}
{"questionId":"368a7720660f46208e61d74c36f77610","question":"Postman removed offline mode (Scratch Pad) in new versions, Is there a way to enable it?\nPostman removed offline mode (Scratch Pad)\n\n\n<https:\/\/learning.postman.com\/docs\/getting-started\/basics\/using-scratch-pad\/>\n\n\n\n> \n> **The Scratch Pad is deprecated and no longer supported.** The Scratch Pad is being discontinued and won\u2019t receive any updates, bug fixes, or security updates. You can use the lightweight API Client when not signed in to Postman to send API requests, including HTTP, WebSocket, gRPC, and GraphQL requests. Learn more about the [lightweight Postman API Client](https:\/\/learning.postman.com\/docs\/getting-started\/basics\/using-api-client\/).\n> \n> \n> \n\n\nToday after I opened my offline Postman it automatically updated and alerted me to sign in and didn't allow me to use offline mode. The new lightweight mode does not have a lot of features.\n\n\nIs there a way to enable it?","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"other"},"answer":"I found it's possible to downgrade Postman to an older version.\n\n\nVersion 10.17 is the last version of Scratch Pad mode.\n\n\nBut it does not exist on the official Postman site, I downloaded it from [filehorse](https:\/\/www.filehorse.com\/download-postman\/82250\/), and It works for me.\n\n\nI will be happy to know if you know another way to enable Scratch Pad mode in new versions."}
{"questionId":"a0acecec86de41ec9d325a196480e4ee","question":"React Native: \"error Cannot start server in new window because no terminal app was specified\"\nAfter updating to React Native v73.1, I'm getting the below error when trying to run the app in Windows:\n\n\n\n> \n> error Cannot start server in new window because no terminal app was specified.\n> \n> \n> \n\n\nHow can I fix it?\n\n\nOn a Mac machine, there aren't any issues. Only Windows is facing this issue.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"javascript"},"answer":"I had the same issue using React Native 0.73.1, when I had to run release mode.\n\n\nIn the case of debug mode, you can simply use `npm start` and then `a`.\n\n\nHowever, if you want to run release mode, use `npm start` to run Metro, open a new terminal and execute `npx react-native run-android --mode=release`.\n\n\nThis worked in my case."}
{"questionId":"30b763d721bb475aa46106eb5531772c","question":"Can you use a braced-init-list as a (default) template argument?\nI need to define a C++ template that accepts several 3D coordinates as their parameters. When all dimensions of these coordinates are defined as separate integer variables, the parameter list would become exceedingly long - 3 coordinates need 9 parameters, which makes the template hard to use.\n\n\nThus, it's highly desirable to declare the templates in a way to use compile-time arrays. Their default arguments should also be declared directly at the location of the template declaration as values, rather than as variable names.\n\n\nAfter some experimentation, to my surprise, I found GCC 13 will accept the following C++ program with `std=c++20`:\n\n\n\n```\n#include <cstdio>\n#include <array>\n\ntemplate <\n    std::array<int, 3> offset = {0, 0, 0}\n>\nstruct Array\n{\n    void operator() (int i, int j, int k) \n    {\n        printf(\"(%d, %d, %d)\\n\", i + offset[0], j + offset[1], k + offset[2]);\n    }\n};\n\nint main(void)\n{\n    Array arr_default;\n    arr_default(0, 0, 0);\n\n    Array<{1, 1, 1}> arr;\n    arr(0, 0, 0);\n\n    return 0;\n}\n\n```\n\nHowever, clang 18 rejects the [braced-init-list](https:\/\/timsong-cpp.github.io\/cppwp\/n4868\/dcl.init.general#nt:braced-init-list) as invalid:\n\n\n\n```\ntest2.cpp:5:30: error: expected expression\n    5 |         std::array<int, 3> offset = {0, 0, 0}\n      |                                     ^\ntest2.cpp:17:8: error: no viable constructor or deduction guide for deduction of template arguments of 'Array'\n   17 |         Array arr_default;\n      |               ^\ntest2.cpp:7:8: note: candidate template ignored: couldn't infer template argument 'offset'\n    7 | struct Array\n      |        ^\ntest2.cpp:7:8: note: candidate function template not viable: requires 1 argument, but 0 were provided\n    7 | struct Array\n      |        ^~~~~\ntest2.cpp:20:8: error: expected expression\n   20 |         Array<{1, 1, 1}> arr;\n      |               ^\n3 errors generated.\n\n```\n\n## Question\n\n\nIs it really a legal C++ program? If it is, what syntax should I use to convince clang to accept it? If it's not, how can I fix the code (and should I report a GCC bug for accepting it unquestionably)?","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"cpp"},"answer":"[#### Problem](https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/cwg_active.html#2450)\n\n\nThis is [CWG 2450](https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/cwg_active.html#2450) and\/or [CWG 2049](https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/cwg_active.html#2049) and *although the current grammar does not allow this, it is proposed to be allowed\/valid* for the reason mentioned below. That means, **Gcc is just preemptively allowing the syntax**. From CWG 2450:\n\n\n\n> \n> Since non-type template parameters can now have class types, it would seem to make sense to allow a braced-init-list as a template-argument, **but the grammar does not permit it.**\n> \n> \n> \n\n\n(emphasis mine)\n\n\nIn case you're wondering how the current grammar makes does not allow this, from [temp.name#1](https:\/\/timsong-cpp.github.io\/cppwp\/n4868\/temp.names#1):\n\n\n\n> \n> \n> ```\n> template-argument:\n>     constant-expression\n>     type-id\n>     id-expression \n> \n> ```\n> \n> \n\n\nAnd since `{1, 1, 1}` is not any of the above three listed constructs, it cannot be used as a template argument **as per the current grammar** rules.\n\n\n\n\n---\n\n\n\n> \n> Is there an alternative and more compatible way to achieve my goals?\n> \n> \n> \n\n\n[#### Solution](https:\/\/godbolt.org\/z\/6T5fh8W3Y)\n\n\nYou can explicitly write `std::array` before the braced init list as shown below:\n\n\n\n```\n#include <cstdio>\n#include <array>\n\ntemplate <\n\/\/------------------------------vvvvvvvvvv----------->added this\n    std::array<int, 3> offset = std::array{0, 0, 0}\n>\nstruct Array\n{\n    void operator() (int i, int j, int k) \n    {\n        printf(\"(%d, %d, %d)\\n\", i + offset[0], j + offset[1], k + offset[2]);\n    }\n};\n\nint main(void)\n{\n    Array arr_default;\n    arr_default(0, 0, 0);\n\/\/--------vvvvvvvvvv------------------->added this\n    Array<std::array{1, 1, 1}> arr;\n    arr(0, 0, 0);\n\n    return 0;\n}\n\n```\n\n\n\n---\n\n\n#### Note\n\n\nAlso note that [clang trunk](https:\/\/godbolt.org\/z\/4b4b3Whdc) also starts accepting the program while gcc and msvc already accepted it from earlier versions."}
{"questionId":"9c0e9ebb64ff421db3e561a0554171cb","question":"Failed to download metadata for repository 'pgdg-common' on CentOS 7\nWhen I try to install or update packages on my [CentOS Stream](https:\/\/en.wikipedia.org\/wiki\/CentOS_Stream) 8, then this error occurs:\n\n\n\n```\ncd ~\nsudo yum update\n\n```\n\nOutput:\n\n\n\n```\nPostgreSQL common RPMs for RHEL \/ Rocky 8 - x86_64                                                           613  B\/s | 659  B     00:01\nPostgreSQL common RPMs for RHEL \/ Rocky 8 - x86_64                                                           1.6 MB\/s | 1.7 kB     00:00\nPostgreSQL common RPMs for RHEL \/ Rocky 8 - x86_64                                                           503  B\/s | 659  B     00:01\n\nError: Failed to download metadata for repo 'pgdg-common': repomd.xml GPG signature verification error: Bad GPG signature\n\n```\n\nI tried to install [Docker](https:\/\/en.wikipedia.org\/wiki\/Docker_%28software%29) or just update the packages or libraries on my CentOS 7 server.\n\n\n### Server details\n\n\n\n```\ncd ~\nhostnamectl\n\n```\n\nOutput:\n\n\n\n```\n   Static hostname: e2e-101-138\n         Icon name: computer-vm\n           Chassis: vm\n        Machine ID: 7bxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n           Boot ID: c0xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n    Virtualization: kvm\n  Operating System: CentOS Stream 8\n       CPE OS Name: cpe:\/o:centos:centos:8\n            Kernel: Linux 4.18.0-529.el8.x86_64\n      Architecture: x86-64","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"You can try:\n\n\n\n```\nrpm -Uvh https:\/\/download.postgresql.org\/pub\/repos\/yum\/reporpms\/EL-8-x86_64\/pgdg-redhat-repo-latest.noarch.rpm\n\n```\n\nBut for me, this didn't fix the issue.\nUntil a proper solution is found, for me, renaming the following file worked:\n\n\n\n```\ncd \/etc\/yum.repos.d\/\nmv pgdg-redhat-all.repo pgdg-redhat-all.repo.old\nyum update -y\n\n```\n\nAt the very least, it should let you install and update packages now."}
{"questionId":"962a01bfa4cd4a498378f4e97f3448cc","question":"Why does std::sleep\\_for(std::chrono::hours::max()) return immediately on linux?\nI was running a C++ program that provides a service, and noticed that it was taking 100% of a CPU even when serving no requests. I narrowed the problem down to a while loop which calls `std::sleep_for` in order to prevent the service from exiting.\n\n\nTo test, I compiled and ran this simple test program:\n\n\n\n```\n#include <chrono>\n#include <thread>\n\nint main(int argc, char * argv[])\n{\n    std::this_thread::sleep_for(std::chrono::hours::max());\n}\n\n```\n\nMy expectation was that this would sleep for a very long time, and indeed when I tried it on my M1 Mac I saw the expected behavior. However when I ran it on a Redhat Linux 8 machine it returned immediately. I also tried it on a Rocky Linux 8 docker container, running on the Mac, and this also returned immediately. This confirms that this occurs on RHEL 8 systems in general - or at least with gcc 8.5.0, since that compiler version is the same on both linux systems (the compiler on the Mac is the Apple-provided clang).\n\n\nThis explains why my service was taking 100% of CPU, since it was calling this in a while loop. But I've never heard of this behavior. Has anyone else?\n\n\nOf course, I can easily solve the problem by sleeping for `std::chrono::seconds(1)`. I'm only asking this question out of intellectual curiosity.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"cpp"},"answer":"This is a bug in libstdc++ <https:\/\/godbolt.org\/z\/vce44vjx5>, looks like an overflow.\n\n\nIt inlines `nanossleep()` call with\n\n\n\n```\ntimespec req{ -3600, 0 };  \/\/ -1 hour.\n\n```\n\n\n```\nmain:                                   # @main\n        push    rbx\n        sub     rsp, 16\n        mov     qword ptr [rsp], -3600\n        mov     qword ptr [rsp + 8], 0\n        mov     rbx, rsp\n.LBB0_1:                                # =>This Inner Loop Header: Depth=1\n        mov     rdi, rbx\n        mov     rsi, rbx\n        call    nanosleep@PLT"}
{"questionId":"3cfb8fe907fe4876b39d726a324da318","question":"Critical security vulnerability in reCAPTCHA Enterprise\nI am building a Flutter app, and I am using Firebase Auth for authentication. I recently deployed my Flutter application via the play store, but got the following message:\n\n\n\n> \n> Your latest production release (106 (1.9.4)) contains SDK issues:\n> com.google.android.recaptcha:recaptcha:18.1.2\n> This SDK version has a note from the SDK developer. Here's what the SDK developer told us:\n> \n> \n> A critical security vulnerability was discovered in reCAPTCHA Enterprise for Mobile. The vulnerability has been patched in the latest SDK release. Customers will need to update their Android application with the reCAPTCHA Enterprise for Mobile SDK, version 18.4.0 or above. We strongly recommend you update to the latest version as soon as possible.\n> \n> \n> \n\n\n**How can I fix this problem?** I am already using the latest version of flutter and firebase auth. My flutter doctor is coming back all healthy. My build.gradle files don't contain any references to reCAPTCHA. From what I can tell, the entire reCAPTCHA setup is handled by firebase?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"dart"},"answer":"As answered by Martin Reindl, you can override reCaptcha version by adding [recaptcha\\_enterprise\\_flutter: ^18.4.0](https:\/\/pub.dev\/packages\/recaptcha_enterprise_flutter) or `implementation 'com.google.android.recaptcha:recaptcha:18.4.0'` in dependencies section of your app-level build.gradle file.\n\n\nThis happens because the last version firebase auth, uses recaptcha:18.1.2.\n\n\nIn [this github issue](https:\/\/github.com\/firebase\/firebase-android-sdk\/issues\/5638#issuecomment-1887702418) it is confirmed that the fix is scheduled for the next release."}
{"questionId":"5f3c6157a1704c9db62be9117864adfe","question":"Why does GCC copy object for each comparison in `std::ranges::max`?\nConsider the following example ([Godbolt](https:\/\/godbolt.org\/z\/hcz3TzYK7)):\n\n\n\n```\n#include <vector>\n#include <iostream>\n#include <ranges>\n#include <algorithm>\n\nstruct A\n{\n    A() {}\n    A( const A& ) { std::cout << \"Copy\\n\"; }\n    A( A&& ) noexcept { std::cout << \"Move\\n\"; }\n\n    A& operator=(const A&) { std::cout << \"Copy assigned\\n\"; return *this; }\n    A& operator=( A&& ) noexcept { std::cout << \"Move assigned\\n\"; return *this; }\n\n    int x = 10;\n};\n\nint main()\n{\n    std::vector<A> vec( 10 );\n    std::cout << \"Init\\n\";\n    std::cout << std::ranges::max( vec, [] ( const auto& a, const auto& b ) { std::cout << \"cmp\" << std::endl; return a.x < b.x; } ).x;\n}\n\n```\n\nThis program compiled with GCC 13.2 (even with [`-O3`](https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/Optimize-Options.html#index-O3) optimization turned on) produces the following output:\n\n\n\n```\nInit\nCopy\nCopy\ncmp\nCopy\ncmp\nCopy\ncmp\nCopy\ncmp\nCopy\ncmp\nCopy\ncmp\nCopy\ncmp\nCopy\ncmp\nCopy\ncmp\n10\n\n```\n\nBut compiled with Clang 17 (with `-stdlib=libc++` and any optimization level), it performs no copying at all (except for the returned value, as I understand it):\n\n\n\n```\nInit\ncmp\ncmp\ncmp\ncmp\ncmp\ncmp\ncmp\ncmp\ncmp\nCopy\n10\n\n```\n\nIf `A` has a costly copy-constructor, this difference will drastically decrease performance.\n\n\nIs there a reason why GCC has this implementation of `std::ranges::max` or is it a bug?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"It's, what I presume, a \"bug\" in the gcc implementation and I wrote a [bugreport](https:\/\/gcc.gnu.org\/bugzilla\/show_bug.cgi?id=112349).\n\n\nLLVM has two versions inside the `operator()` overload being used:\n\n\n\n```\nauto __first = std::ranges::begin(__r);\nauto __last = std::ranges::end(__r);\n\n_LIBCPP_ASSERT(__first != __last, \"range must contain at least one element\");\n\nif constexpr (std::ranges::forward_range<_Rp>) {\n    \/\/ MY COMMENT: This is what's actually being used:\n\n    auto __comp_lhs_rhs_swapped = [&](auto&& __lhs, auto&& __rhs) {\n        return std::invoke(__comp, __rhs, __lhs);\n    };\n    return *std::ranges::min_element(std::move(__first),\n                                        std::move(__last),\n                                        __comp_lhs_rhs_swapped, __proj);\n} else {\n    std::ranges::range_value_t<_Rp> __result = *__first;\n    while (++__first != __last) {\n        if (std::invoke(__comp, std::invoke(__proj, __result),\n                                std::invoke(__proj, *__first)))\n            __result = *__first;\n    }\n    return __result;\n}\n\n```\n\n.. but it doesn't matter if I disable the version currently being used and instead use the `while` loop. It still doesn't copy.\n\n\nNow for the `operator()` overload in GCC's case:\n\n\n\n```\nauto __first = std::ranges::begin(__r);\nauto __last = std::ranges::end(__r);\n\n__glibcxx_assert(__first != __last);\n\nauto __result = *__first;        \nwhile (++__first != __last) {\n    auto __tmp = *__first;\n    if (std::__invoke(__comp, std::__invoke(__proj, __result),\n                              std::__invoke(__proj, __tmp)))\n        __result = std::move(__tmp);\n}\nreturn __result;\n\n```\n\nThe copy is here:\n\n\n\n```\nauto __tmp = *__first;\n\n```\n\nI assume it *should* be:\n\n\n\n```\nauto& __tmp = *__first;\n\n```\n\nbecause with that change, it doesn't copy anymore.\n\n\nNote: I've added `std::` and `std::ranges::` in a couple of places to be able to use the algorithms outside their natural habitat which is inside the standard library implementations.\n\n\n\n\n---\n\n\n**Update**\n\n\nThe bug is now confirmed. [Jonathan Wakely](https:\/\/stackoverflow.com\/users\/981959\/jonathan-wakely) also replied:\n\n\n\n> \n> \n> > \n> > *[me]* `auto& __tmp = *__first;`\n> > \n> > \n> > \n> \n> \n> *[JW]* That won't compile for a `move_iterator` or a proxy reference. I think `auto&&` would be OK.\n> \n> \n> \n> > \n> > *[me]* ...or just supplying `*__first` to `std::__invoke`\n> > \n> > \n> > \n> \n> \n> *[JW]* I think that would be OK too.\n> \n> \n> \n\n\nSo, if his initial assessment is correct, it should be a low hanging fruit for someone to pick and we can hope for a fix in a near release."}
{"questionId":"703a13e617aa499390491a92ffd8cf37","question":"Where is the order in which ELF relocations are applied specified?\nConsider the following two files on a Linux system:\n\n\n**use\\_message.cpp**\n\n\n\n```\n#include <iostream>\n\nextern const char* message;\nvoid print_message();\n\nint main() {\n    std::cout << message << '\\n';\n    print_message();\n}\n\n```\n\n**libmessage.cpp**\n\n\n\n```\n#include <iostream>\nconst char* message = \"Meow!\";   \/\/ 1. absolute address of string literal\n                                 \/\/    needs runtime relocation in a .so\nvoid print_message() {\n    std::cout << message << '\\n';\n}\n\n```\n\nWe can compile *use\\_message.cpp* into an object file, compile *libmessage.cpp* into a shared library, and link them together, like so:\n\n\n\n```\n$ g++ use_message.cpp -c -pie -o use_message.o\n$ g++ libmessage.cpp -fPIC -shared -o libmessage.so\n$ g++ use_message.o libmessage.so -o use_message\n\n```\n\nThe definition for `message` originally lives in *libmessage.so*. When `use_message` is executed, the dynamic linker performs relocations that:\n\n\n1. Update the `message` definition inside *libmessage.so* with the load address of the string data\n2. Copy the definition of `message` from *libmessage.so* into *use\\_message*'s `.bss` section\n3. Update the global offset table in *libmessage.so* to point to the new version of `message` inside *use\\_message*\n\n\nThe relevant relocations, as dumped by `readelf`, are:\n\n\n**use\\_message**\n\n\n\n```\n  Offset          Info           Type           Sym. Value    Sym. Name + Addend\n000000004150  000c00000005 R_X86_64_COPY     0000000000004150 message + 0\n\n```\n\nThis is relocation number 2 in list I wrote before.\n\n\n**libmessage.so**\n\n\n\n```\n  Offset          Info           Type           Sym. Value    Sym. Name + Addend\n000000004040  000000000008 R_X86_64_RELATIVE                    2000\n000000003fd8  000b00000006 R_X86_64_GLOB_DAT 0000000000004040 message + 0\n\n```\n\nThese are relocation numbers 1 and 3, respectively.\n\n\nThere's a dependency between relocation numbers 1 and 2: the update to *libmessage.so*'s `message` definition must happen before this value is copied into *use\\_message*, otherwise *use\\_message* will not point to the correct location.\n\n\nMy question is: how is the order for applying relocations specified? Is there something encoded in the ELF files that specifies this? Or in the ABI? Or is the dynamic linker just expected to work out the dependencies between relocations itself and ensure that any relocations that write to a given memory address are run before any relocations that read from the same location? Does the static linker only output relocations such that the ones in the executable can always be processed after the shared library ones?","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"cpp"},"answer":"> \n> My question is: how is the order for applying relocations specified? Is there something encoded in the ELF files that specifies this? Or in the ABI? Or is the dynamic linker just expected to work out the dependencies between relocations itself and ensure that any relocations that write to a given memory address are run before any relocations that read from the same location? Does the static linker only output relocations such that the ones in the executable can always be processed after the shared library ones?\n> \n> \n> \n\n\nI think the relocation resolving order is not specified by a standard.\nDynamic loaders define an order. To support copy relocations, the main executable is relocated the last.\nLinkers only produce copy relocations for executable links (-no-pie\/-pie) and are aware of the dynamic loader semantics.\n\n\n\n\n---\n\n\nQuoting\n<https:\/\/maskray.me\/blog\/2021-01-18-gnu-indirect-function#relocation-resolving-order>:\n\n\nThere are two parts: the order within a module and the order between two modules.\n\n\nglibc rtld processes relocations in the reverse search order (reversed l\\_initfini) with a special case for the rtld itself. The main executable needs to be processed the last to process R\\_\\*\\_COPY. If A has an ifunc referencing B, generally B needs to be relocated before A. Without ifunc, the resolving order of shared objects can be arbitrary.\n\n\nLet's say we have the following dependency tree.\n\n\n\n```\nmain\n  dep1.so\n    dep2.so\n      dep3.so\n        libc.so.6\n      dep4.so\n        dep3.so\n        libc.so.6\n    libc.so.6\n  libc.so.6\n\n```\n\nl\\_initfini contains main, dep1.so, dep2.so, dep4.so, dep3.so, libc.so.6, ld.so. The relocation resolving order is ld.so (bootstrap), libc.so.6, dep3.so, dep4.so, dep2.so, dep1.so, main, ld.so.\n\n\nWithin a module, glibc rtld resolves relocations in order. Assume that both DT\\_RELA (.rela.dyn) and DT\\_PLTREL (.rela.plt) are present, glibc logic is like the following:\n\n\n\n```\n\/\/ Simplified from elf\/dynamic-link.h\nranges[0] = {DT_RELA, DT_RELASZ, 0};\nranges[1] = {DT_JMPREL, DT_PLTRELSZ, do_lazy};\nif (!do_lazy && ranges[0].start + ranges[0].size == ranges[1].start) { \/\/ the equality operator is always satisfied in practice\n  ranges[0].size += size;\n  ranges[1] = {};\n}\nfor (int ranges_index = 0; ranges_index < 2; ++ranges_index)\n  elf_dynamic_do_Rela (... ranges[ranges_index]);\n\n```\n\n\n\n---\n\n\nmusl `ldso\/dynlink.c` has:\n\n\n\n```\n\/* The main program must be relocated LAST since it may contain\n * copy relocations which depend on libraries' relocations. *\/\nreloc_all(app.next);\nreloc_all(&app);\n\n```\n\nFreeBSD rtld uses a more sophisticated order, which make certain ifunc code more robust.\n\n\n\n```\n$ g++ use_message.cpp -c -pie -o use_message.o\n$ g++ libmessage.cpp -fPIC -shared -o libmessage.so\n$ g++ use_message.o libmessage.so -o use_message\n\n```\n\n\n\n---\n\n\nBTW, `use_message` (with -fPIE relocatable files) needs copy relocations because of GCC [`HAVE_LD_PIE_COPYRELOC`](https:\/\/maskray.me\/blog\/2021-01-09-copy-relocations-canonical-plt-entries-and-protected).\nFor Clang and GCC's other architectures, the PIE modes will not lead to copy relocations."}
{"questionId":"2d85663ffce944b18a275c7b3586da54","question":"NETSDK1083: The specified RuntimeIdentifier \"win10-x64\" is not recognized\nWhen building a .NET MAUI app on Windows using .NET v8.0.100-rc.1.23455.8, I get the following error:\n\n\n\n> \n> NETSDK1083: The specified RuntimeIdentifier \"win10-x64\" is not recognized\n> \n> \n> \n\n\nOn the [.NET SDK error list](https:\/\/learn.microsoft.com\/en-us\/dotnet\/core\/tools\/sdk-errors\/), there isn't much information about this error or how to solve it:\n\n\n\n> \n> NETSDK1083 The specified RuntimeIdentifier '{0}' is not recognized.\n> \n> \n>","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"## Explanation\n\n\nLong story short, it's a signing problem in this release of .NET 8 RC that is preventing .NET MAUI v8.0.0-rc.1.9171 from installing automatically. You can find more information about the error here: <https:\/\/github.com\/dotnet\/maui\/issues\/17330>.\n\n\n## Work Around\n\n\nTo work around the signing problem, execute the following command in the Terminal:\n\n\n#### Windows\n\n\n\n```\ndotnet workload install maui --skip-sign-check --source https:\/\/api.nuget.org\/v3\/index.json\n\n```\n\n#### macOS\n\n\n\n```\nsudo dotnet workload install maui --skip-sign-check --source https:\/\/api.nuget.org\/v3\/index.json"}
{"questionId":"0a4814d8ef064111bd405d9ad763e543","question":"Sorting by element address\nI stumbled across a situation where a library sorts a container (e.g. `std::vector<T>`) with a user-provided comparison object. For one particular case, the user actually doesn't want to sort the container, but the sorting happens unconditionally.\n\n\nSo, to try to avoid this situation, I thought to try using a comparison object that sorts based on element address. Equivalently, we have:\n\n\n\n```\nstd::vector nums{1, 5, 4};\nauto cmp = [](auto& a, auto& b) { return &a < &b; };\nstd::sort(nums.begin(), nums.end(), cmp);\n\n```\n\nThis \"works\" because `std::vector<T>` elements are stored in (contiguous) memory locations in the same order as the elements in the vector. The end result is that the `nums` vector appears to have been left untouched even after sorting.\n\n\nHowever, once I replace `std::vector<T>` with `std::array<T, N>`, I get a segmentation violation (see <https:\/\/gcc.godbolt.org\/z\/9srehdbhG>).\n\n\nMy first thought is that I'm violating the type requirements as listed at <https:\/\/en.cppreference.com\/w\/cpp\/algorithm\/sort>:\n\n\n\n> \n> - `RandomIt` must meet the requirements of *ValueSwappable* and *LegacyRandomAccessIterator*.\n> - The type of dereferenced `RandomIt` must meet the requirements of *MoveAssignable* and *MoveConstructible*.\n> - `Compare` must meet the requirements of *Compare*.\n> \n> \n> \n\n\nMy assumption was that the addresses of the elements would remain stable throughout the sort - this is almost certainly wrong.\n\n\nSo, which requirements\/preconditions of `std::sort()` am I violating?","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"cpp"},"answer":"The problem is that `std::sort` isn't restricted to calling the comparator with references to elements of the specified range, it is also calling a comparison between a helper variable and an element of the range.\n\n\nIt's legal to take the address of that helper variable, but it's not legal to do a pointer comparison.\n\n\nIf you switch to `std::less()(&a, &b)` the segmentation fault goes away but it still is going to act really weird -- your comparator isn't invariant under copies.\n\n\nThis is a violation of [the concept required for sort compare functions](https:\/\/eel.is\/c++draft\/alg.sorting#general-3):\n\n\n\n> \n> For algorithms other than those described in [alg.binary.search], comp shall induce a strict weak ordering on the values.\n> \n> \n> \n\n\nNote \"ordering on the *values*\", not \"ordering on the *objects*\". That means you cannot rely on any properties of the objects being sorted (such as memory address) other than the value alone."}
{"questionId":"2282e02cc10b4fdead1750c35bda4da4","question":"Print lines that have no duplicates in a file and preserve sort order linux\nI have the following file:\n\n\n\n```\n2\n1\n4\n3\n2\n1\n\n```\n\n**I want the output like this (unique lines that don't have any duplicates and preserve order):**\n\n\n\n```\n4\n3\n\n```\n\nI tried `sort file.txt | uniq -u` it works, but output is sorted:\n\n\n\n```\n3\n4\n\n```\n\nI tried `awk '!x[$0]++' file.txt` it keeps order, but it prints all values once:\n\n\n\n```\n2\n1\n4\n3","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"other"},"answer":"A couple ideas to choose from:\n\n\n*a)* read the input file twice:\n\n\n\n```\nawk '\nFNR==NR         { counts[$0]++; next }  # 1st pass: keep count\ncounts[$0] == 1                         # 2nd pass: print rows with count == 1\n' file.txt file.txt\n\n```\n\n*b)* read the input file once:\n\n\n\n```\nawk '\n    { lines[NR] = $0                    # maintain ordering of rows\n      counts[$0]++\n    }\nEND { for ( i=1;i<=NR;i++ )             # run thru the indices of the lines[] array and ...\n          if ( counts[lines[i]] == 1 )  # if the associated count == 1 then ...\n             print lines[i]             # print the array entry to stdout\n    }\n' file.txt\n\n```\n\nBoth of these generate:\n\n\n\n```\n4\n3"}
{"questionId":"441eaf2ff2f0489a9c0fad16cd5d164b","question":"The \"images.domains\" configuration is deprecated. Please use \"images.remotePatterns\" configuration instead\nI get the following warning in Next 14:\n\n\n\n> \n> The \"images.domains\" configuration is deprecated. Please use\n> \"images.remotePatterns\" configuration instead.\n> \n> \n> \n\n\nMy next.config.js has this\n\n\n\n```\nimages: {\n  domains: ['res.cloudinary.com'],\n  remotePatterns: [\n    {\n      protocol: \"https\",\n      hostname: \"**\",\n    },\n  ],\n}\n\n```\n\nIf I remove domains from there, I get error:\n\n\n\n> \n> `next\/image`, hostname \"res.cloudinary.com\" is not configured under\n> images in your `next.config.js`\n> \n> \n>","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"javascript"},"answer":"In Next.js Version 14, domains configuration has been deprecated in favour of remotePatterns. This can be defined in the next.config.js file.\n\n\nHere is a reference as per the Next.js docs: [remotePatterns](https:\/\/nextjs.org\/docs\/app\/api-reference\/components\/image#remotepatterns)\n\n\nIn your snippet, after you remove the domains configuration as you have described, it seems that you have mixed up your values in remotePatterns configuration, because you have set the hostname value to the wildcard `'**'` instead of the actual hostname `'res.cloudinary.com'`, thus the error **hostname \"res.cloudinary.com\" is not configured.**\n\n\nTry to do this instead:\n\n\n\n\n\n```\n images: {\n    remotePatterns: [\n      {\n        protocol: 'https',\n        hostname: 'res.cloudinary.com',\n        pathname: '**',\n      },\n    ],\n  },"}
{"questionId":"692b0539aa0141fb8b973f078f5a63d8","question":"Hardcoding large amounts of data without inducing memory spike\nSo I am exploring the concept of storing WebAssembly inside the JavaScript file so it can all be bundled up in one shippable file. I did manage to make a working example of this where it stores the wasm file in a big literal string in base64 and at runtime is converted to a Uint8Array before being processed into a Module and Instance.\n\n\n\n```\nawait Deno.writeTextFile(\n    '.\/static\/wasm\/bundle.js',\n    `import { initSync } from '.\/app.js'\\ninitSync(new WebAssembly.Module(Uint8Array.from(atob('${btoa(\n        [ ...await Deno.readFile('.\/static\/wasm\/app_bg.wasm') ]\n            .map(byte => String.fromCharCode(byte))\n            .join('')\n    )}').split('').map(char => char.charCodeAt(0)))))`\n)\n\n```\n\n*[Source of above snippet](https:\/\/github.com\/BlackAsLight\/BundleWasm)*\n\n\nBut I have been wondering if JavaScript might have problems with processing this literal string in instances that the wasm file was very large. In this snippet the base64 literal string is only needed once at the very start, and I imagine is disposed off by the garbage collector as it's no longer accessible.\n\n\nI am wondering if people have any ideas on how one could store this same type of data, hardcoded in the javascript, where it is only run once, but won't causes any huge memory spikes at the start of the runtime. Increased processing time for reduced peak memory usage is an acceptable trade-off here, but fetching any external resources would defeat the point of the question.","questionMetadata":{"Type":"Optimization","Level":"Advanced","Tag":"javascript"},"answer":"The following will reduce memory usage and load faster (CPU wise), but will result in a bigger file.\n\n\n\n```\nfunction toUint8ArrayString(u8) {\n    return `new Uint8Array([${u8.join(',')}])`;\n}\n\nconst wasmData = toUint8ArrayString(await Deno.readFile('.\/app_bg.wasm'));\n\nawait Deno.writeTextFile('.\/static\/wasm\/bundle.js', `import { initSync } from 'app.js'\\ninitSync(new WebAssembly.Module(${wasmData}))`)\n\n\n```\n\nInstead of having the data in `base64` you generate the array data to be used directly in the `Uint8Array` constructor.\n\n\nFrom:\n\n\n\n```\nconst init = atob('YQ==').split('').map(char => char.charCodeAt(0)); \/\/ [97]\nUint8Array.from(init);\n\n```\n\nTo\n\n\n\n```\nnew Uint8Array([97]); \/\/ or Uint8Array.from([97]);\n\n```\n\nBy doing this, you're avoiding `atob`, `.split`, `.map`, `.join` with all the copies being done by these methods under the hood.\n\n\n\n\n---\n\n\nFor large amounts of data, the best would be to use `fetch` and `base64` encoded data:\n\n\n\n```\nconst url = \"data:application\/wasm;base64,\" + b64wasm;\n\/\/ Your bundle should produce the final url string\n\/\/ const url = \"data:application\/wasm;base64,YQo=\";\nconst res = await fetch(url);\nconst u8wasm = new Uint8Array(await res.arrayBuffer());\nconst module = new WebAssembly.Module(u8wasm);\n\n```\n\nAnd if supported, you can even use [`WebAssembly.compileStreaming`](https:\/\/developer.mozilla.org\/en-US\/docs\/WebAssembly\/JavaScript_interface\/compileStreaming) which should result in the lowest amount of memory usage.\n\n\n\n```\nconst url = \"data:application\/wasm;base64,\" + b64wasm;\n\/\/ Your bundle should produce the final url string\n\/\/ const url = \"data:application\/wasm;base64,YQo=\";\n\nconst module = await WebAssembly.compileStreaming(fetch(url));"}
{"questionId":"f8be805965cc4f56a457a14ce1324d7e","question":"ImportError: cannot import name 'url\\_decode' from 'werkzeug.urls'\nI am building a webapp using Flask. I imported the `flask-login` library to handle user login. But it shows an ImportError.\n\n\nBelow is my folder structure:\n\n\n\n```\n>flask_blog1\n    >flaskblog\n        >static\n        >templates\n        >__init__.py\n        >forms.py\n        >models.py\n        >routes.py\n    >instance\n        >site.db\n    >venv\n    >requirements.txt\n    >run.py\n\n```\n\nMy `run.py`:\n\n\n\n```\nfrom flaskblog import app\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\n```\n\nMy `__init__.py`:\n\n\n\n```\nfrom flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_bcrypt import Bcrypt\nfrom flask_login import LoginManager\n\napp = Flask(__name__)\napp.config[\"SECRET_KEY\"] = \"5791628bb0b13ce0c676dfde280ba245\"\napp.config[\"SQLALCHEMY_DATABASE_URI\"] = \"sqlite:\/\/\/site.db\"\ndb = SQLAlchemy(app)\nbcrypt = Bcrypt(app)\nlogin_manager = LoginManager(app)\n\nfrom flaskblog import routes\n\n```\n\nMy `models.py`:\n\n\n\n```\nfrom datetime import datetime\n\n# from .extensions import db\nfrom flaskblog import db, login_manager\nfrom flask_login import UserMixin\n\n\n@login_manager.user_loader\ndef load_user(user_id):\n    return User.query.get(int(user_id))\n\n\nclass User(db.Model, UserMixin):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(20), unique=True, nullable=False)\n    email = db.Column(db.String(120), unique=True, nullable=False)\n    image_file = db.Column(db.String(20), nullable=False, default=\"default.jpg\")\n    password = db.Column(db.String(60), nullable=False)\n    posts = db.relationship(\"Post\", backref=\"author\", lazy=True)\n\n    def __repr__(self):\n        return f\"User('{self.username}', '{self.email}', '{self.image_file}')\"\n\n\nclass Post(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    title = db.Column(db.String(100), nullable=False)\n    date_posted = db.Column(db.DateTime, nullable=False, default=datetime.utcnow)\n    content = db.Column(db.Text, nullable=False)\n    user_id = db.Column(db.Integer, db.ForeignKey(\"user.id\"), nullable=False)\n\n    def __repr__(self):\n        return f\"Post('{self.title}', '{self.date_posted}')\"\n\n```\n\nMy `routes.py`:\n\n\n\n```\nfrom flask import render_template, flash, redirect, url_for\nfrom flaskblog import app, db, bcrypt\nfrom flaskblog.forms import RegistrationForm, LoginForm\nfrom flaskblog.models import User, Post\nfrom flask_login import login_user\n\nposts = [\n    {\n        \"author\": \"Ashutosh Chapagain\",\n        \"title\": \"Blog Post 1\",\n        \"content\": \"First Post Content\",\n        \"date_posted\": \"October 1, 2023\",\n    },\n    {\n        \"author\": \"Ash Dhakal\",\n        \"title\": \"Blog Post 2\",\n        \"content\": \"Second Post Content\",\n        \"date_posted\": \"October 2, 2023\",\n    },\n]\n\n\n@app.route(\"\/\")\n@app.route(\"\/home\")\ndef home():\n    return render_template(\"home.html\", posts=posts)\n\n\n@app.route(\"\/about\")\ndef about():\n    return render_template(\"about.html\", title=\"About\")\n\n\n@app.route(\"\/register\", methods=[\"GET\", \"POST\"])\ndef register():\n    form = RegistrationForm()\n    if form.validate_on_submit():\n        hashed_password = bcrypt.generate_password_hash(form.password.data).decode(\n            \"utf-8\"\n        )\n        user = User(\n            username=form.username.data, email=form.email.data, password=hashed_password\n        )\n        db.session.add(user)\n        db.session.commit()\n        flash(f\"Your account has been created! You are now able to log in!\", \"success\")\n        return redirect(url_for(\"login\"))\n    return render_template(\"register.html\", title=\"Register\", form=form)\n\n\n@app.route(\"\/login\", methods=[\"GET\", \"POST\"])\ndef login():\n    form = LoginForm()\n    if form.validate_on_submit():\n        user = User.query.filter_by(email=form.email.data).first()\n        if user and bcrypt.check_password_hash(user.password, form.password.data):\n            login_user(user, remember=form.remember.data)\n            return redirect(url_for(\"home\"))\n        else:\n            flash(\"Login Unsuccessful. Please check email and password\", \"danger\")\n    return render_template(\"login.html\", title=\"Login\", form=form)\n\n```\n\nMy `forms.py`:\n\n\n\n```\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField, BooleanField\nfrom wtforms.validators import DataRequired, Length, Email, EqualTo, ValidationError\nfrom flaskblog.models import User\n\n\nclass RegistrationForm(FlaskForm):\n    username = StringField(\n        \"Username\", validators=[DataRequired(), Length(min=2, max=20)]\n    )\n    email = StringField(\"Email\", validators=[DataRequired(), Email()])\n    password = PasswordField(\"Password\", validators=[DataRequired()])\n    confirm_password = PasswordField(\n        \"Confirm Password\", validators=[DataRequired(), EqualTo(\"password\")]\n    )\n    submit = SubmitField(\"Sign Up\")\n\n    def validate_username(self, username):\n        user = User.query.filter_by(username=username.data).first()\n        if user:\n            raise ValidationError(\n                \"That username is taken. Please choose a different one.\"\n            )\n\n    def validate_email(self, email):\n        user = User.query.filter_by(email=email.data).first()\n        if user:\n            raise ValidationError(\"That email is taken. Please choose a different one.\")\n\n\nclass LoginForm(FlaskForm):\n    email = StringField(\"Email\", validators=[DataRequired(), Email()])\n    password = PasswordField(\"Password\", validators=[DataRequired()])\n    remember = BooleanField(\"Remember Me\")\n    submit = SubmitField(\"Login\")\n\n```\n\nThe exact error is:\n\n\n\n```\n(venv) asu@asu-Lenovo-Legion-5-15ARH05:\/media\/asu\/Data\/Projects\/flask_blog1$ python3 run.py\nTraceback (most recent call last):\n  File \"\/media\/asu\/Data\/Projects\/flask_blog1\/run.py\", line 1, in <module>\n    from flaskblog import app\n  File \"\/media\/asu\/Data\/Projects\/flask_blog1\/flaskblog\/__init__.py\", line 4, in <module>\n    from flask_login import LoginManager\n  File \"\/media\/asu\/Data\/Projects\/flask_blog1\/venv\/lib\/python3.10\/site-packages\/flask_login\/__init__.py\", line 12, in <module>\n    from .login_manager import LoginManager\n  File \"\/media\/asu\/Data\/Projects\/flask_blog1\/venv\/lib\/python3.10\/site-packages\/flask_login\/login_manager.py\", line 33, in <module>\n    from .utils import _create_identifier\n  File \"\/media\/asu\/Data\/Projects\/flask_blog1\/venv\/lib\/python3.10\/site-packages\/flask_login\/utils.py\", line 14, in <module>\n    from werkzeug.urls import url_decode\nImportError: cannot import name 'url_decode' from 'werkzeug.urls' (\/media\/asu\/Data\/Projects\/flask_blog1\/venv\/lib\/python3.10\/site-packages\/werkzeug\/urls.py)","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"I can only assume you got the Werkzeug 3.0 update (as flask-login didn't up-bound their werkzeug dependency).\n\n\nIn their ongoing quest to remove all the non-core public APIs of werkzeug, the developers [deprecated most of `werkzeug.urls` in Werkzeug 2.3](https:\/\/werkzeug.palletsprojects.com\/en\/2.3.x\/urls\/) (released April 25th 2023), and [removed it in Werkzeug 3.0](https:\/\/werkzeug.palletsprojects.com\/en\/3.0.x\/urls\/) (released September 30th 2023).\n\n\nYour options are:\n\n\n- force werkzeug to a pre-3.0 version\n- wait for flask-login to release a version compatible with werkzeug 3, [a fix of that and a bunch of other stuff was merged a few minutes ago](https:\/\/github.com\/maxcountryman\/flask-login)\n\n\nedit: flask-login 0.6.3 with the compatibility fix was released October 30th: <https:\/\/github.com\/maxcountryman\/flask-login\/releases\/tag\/0.6.3>"}
{"questionId":"4fac4ff5ac034b669d9d785255821e80","question":"Why std::allocator::allocate calls ::operator new?\nThe documentation for the `std::allocator<T>::allocate` member function says in ([[allocator.members]](https:\/\/eel.is\/c++draft\/allocator.members)) that:\n\n\n\n> \n> *Remarks*: **The storage for the array is obtained by calling \u200b`::\u200boperator new`** ([new.delete]), but it is unspecified when or how often this function is called. This function starts the lifetime of the array object, but not that of any of the array elements.\n> \n> \n> \n\n\nI wonder why it says `::operator new` and not just `operator new`? Does the double colon make any difference? Which other `operator new` could be called here, if that double colon was omitted?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"Prior to [LWG2818](https:\/\/wg21.link\/LWG2818), [[contents]p3](https:\/\/timsong-cpp.github.io\/cppwp\/n4868\/contents#3) read:\n\n\n\n> \n> Whenever a name `x` defined in the standard library is mentioned, the name `x` is assumed to be fully qualified as `::std::x`, unless explicitly described otherwise. For example, if the *Effects*: element for library function `F` is described as calling library function `G`, the function `::std::G` is meant.\n> \n> \n> \n\n\nSo writing `operator new` in the specification would mean `::std::operator new`, which wouldn't make sense. `::operator new` correctly refers to `operator new` in the global namespace.\n\n\nIn an implementation of the standard library, there would be no difference between writing `operator new` and `::operator new` since `std::allocator<T>` wouldn't define a member `operator new` and ADL has no effect since there is only one namespace a free `operator new` could be defined in."}
{"questionId":"a3018a5ae5764565b96fa660f3db2263","question":"pandas or Polars: find index of previous element larger than current one\nSuppose my data looks like this:\n\n\n\n```\ndata = {\n    'value': [1,9,6,7,3, 2,4,5,1,9]\n}\n\n```\n\nFor each row, I would like to find the row number of the latest previous element larger than the current one.\n\n\nSo, my expected output is:\n\n\n\n```\n[None, 0, 1, 2, 1, 1, 3, 4, 1, 0]\n\n```\n\n- the first element `1` has no previous element, so I want `None` in the result\n- the next element `9` is at least as large than all its previous elements, so I want `0` in the result\n- the next element `6`, has its previous element `9` which is larger than it. The distance between them is `1`. So, I want `1` in the result here.\n\n\nI'm aware that I can do this in a loop in Python (or in C \/ Rust if I write an extension).\n\n\nMy question: is it possible to solve this **using entirely dataframe operations**? pandas or Polars, either is fine. But only dataframe operations.\n\n\nSo, none of the following please:\n\n\n- `apply`\n- `map_elements`\n- `map_rows`\n- `iter_rows`\n- Python for loops which loop over the rows and extract elements one-by-one from the dataframes","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"python"},"answer":"This iterates only on the range of rows that this should look. It doesn't loop over the rows themselves in python. If your initial `bound_range` covers all the cases then it won't ever actually do a loop.\n\n\n\n```\nlb=0\nbound_range=3\ndf=df.with_columns(z=pl.lit(None, dtype=pl.UInt64))\nwhile True:\n    df=df.with_columns(\n        z=pl.when(pl.col('value')>=pl.col('value').shift(1).cum_max())\n            .then(pl.lit(0, dtype=pl.UInt64))\n            .when(pl.col('z').is_null())\n            .then(\n                pl.coalesce(\n                    pl.when(pl.col('value')<pl.col('value').shift(x))\n                        .then(pl.lit(x, dtype=pl.UInt64))\n                        for x in range(lb, lb+bound_range)\n                )\n            )\n            .otherwise(pl.col('z'))\n            )\n    if df[1:]['z'].drop_nulls().shape[0]==df.shape[0]-1:\n        break\n    lb+=bound_range\n\n\n```\n\nFor this example I set `bound_range` to 3 to make sure it loops at least once. I ran this with 1M random integers between 0 and 9(inclusive) and I set the bound\\_range to 50 and it took under 2 sec. You could make this smarter in between loops by checking things more explicitly but the best approach there would be data dependent."}
{"questionId":"5c9b2fe767b84e5eab14c16e2460e9dc","question":"org.springframework.boot.loader.JarLauncher cannot be found, but org.springframework.boot.loader.launch.JarLauncher can\nTrying to deploy a **Spring Boot 3.2** app in a layered docker I basically followed [this blogpost](https:\/\/spring.io\/blog\/2020\/01\/27\/creating-docker-images-with-spring-boot-2-3-0-m1) and specifically, my docker entrypoint was:\n\n\n\n```\nENTRYPOINT [\"java\", \"org.springframework.boot.loader.JarLauncher\"]\n\n```\n\nYet the docker doesn't start and display the usual\n\n\n\n```\nError: Could not find or load main class org.springframework.boot.loader.JarLauncher\nCaused by: java.lang.ClassNotFoundException: org.springframework.boot.loader.JarLauncher\n\n```\n\nAfter some digging in the initial jar file, I found out that the JarLauncher class is actually under `org.springframework.boot.loader.launch.JarLauncher`.\n\n\nand if I replace the entrypoint by this one:\n\n\n\n```\nENTRYPOINT [\"java\", \"org.springframework.boot.loader.launch.JarLauncher\"]\n\n```\n\nThe app actually starts.\n\n\nJumping back in the [Spring Doc](https:\/\/docs.spring.io\/spring-boot\/docs\/current\/reference\/html\/executable-jar.html#appendix.executable-jar.launching.manifest) the package should still be `org.springframework.boot.loader.JarLauncher` (according to the doc, I mean)\n\n\nSo... has the class JarLauncher been moved, and the documentation is outdated, or is it some mistake in my configuration?\nSince I can't find any documentation about this change (witch seems to be fairly important)\n\n\nOf course the usual `java -jar` command runs correctly since the manifest file is alright.\n\n\nI double checked by starting a new project with a Spring Boot 2.7, and the JarLauncher class is indeed in the `org.springframework.boot.loader` package","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"java"},"answer":"You are looking at a blog for Spring 2.3 and are using a non final Spring Boot 3.2.\n\n\nThe classes have moved, see [this ticket](https:\/\/github.com\/spring-projects\/spring-boot\/issues\/37667) and [the documentation](https:\/\/docs.spring.io\/spring-boot\/docs\/3.2.0-SNAPSHOT\/reference\/html\/executable-jar.html#appendix.executable-jar.launching.manifest) is perfectly fine if you look at the correct one. You looked at the current production version which is 3.1 and not 3.2 as that is still under development.\n\n\nSo in short that blog post is (sort of) outdated, the documentation is perfectly in order as long as you read the proper one."}
{"questionId":"a282dee4007a45ea946e06e4ece15ead","question":"Failed to convert from bundle ID\nAfter updating to iOS 17 and Xcode 15, the following warning appears in Xcode:\n\n\n\n> \n> -warning-\n> nw\\_parameters\\_set\\_source\\_application\\_by\\_bundle\\_id\\_internal Failed to convert from bundle ID (my bundel ID) to UUID. This could lead to wrong data usage accounting.\n> \n> \n> \n\n\nHow do I solve this problem?\n\n\nIt worked great before and I didn't change anything.","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"other"},"answer":"Seems like we can ignore that.\n\n\nApples answer in Apples forum:  [\"I\u2019d classify that as log noise.\"](https:\/\/developer.apple.com\/forums\/thread\/739086?answerId=767505022#767505022)"}
{"questionId":"a02015c6580a4cd39985ee7056638671","question":"prevent a trait function from being implemented by other struct\nI just build a trait `Bar` with 2 functions (`alpha()` with implementation and `beta()` with only interfaces), and I hope the struct who implement `Bar` implements only `beta()`, and never implement their own `alpha()`.\n\n\nIs there any way to prevent another struct from implementing their own `alpha()`?\n\n\n\n```\ntrait Bar {\n    fn alpha(&self) {\n        println!(\"you should never implement this function on your own.\");\n    }\n\n    fn beta(&self);\n}\n\nstruct Foo {}\n\n\nimpl Bar for Foo {\n    fn alpha(&self) {\n        println!(\"how do I trigger an error here when a struct implement it's own alpha()?\");\n    }\n\n    fn beta(&self) {\n        println!(\"implement beta() for Foo\");\n    }\n}","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"rust"},"answer":"You can do this, by splitting your trait into two, and providing a blanket implementation for the trait with default method.\n\n\n\n```\npub trait Beta {\n    fn beta(&self);\n}\n\npub trait Alpha: Beta {\n    fn alpha(&self) {\n        \/\/ Default impl\n    }\n}\n\n\/\/ Because of this blanket implementation,\n\/\/ no type can implement `Alpha` directly,\n\/\/ since it would conflict with this impl.\n\/\/ And you cannot implement `Alpha` without `Beta`,\n\/\/ since `Beta` is its _supertrait_.\nimpl<T: Beta> Alpha for T {}\n\n\nstruct Foo;\n\nimpl Beta for Foo {\n    fn beta(&self) {\n        \/\/ Impl\n    }\n}\n\n\/\/ If you uncomment this you will have a compile error,\n\/\/ because of the conflicting implementations of Alpha for Foo\n\/\/ (conflicts with the blanket implementation above)\n\/\/\n\/\/ impl Alpha for Foo {\n\/\/    fn alpha(&self) {\n\/\/        \/\/ override impl\n\/\/    } \n\/\/ }\n\npub fn bar<T: Alpha>(_: T) {}\n\npub fn baz() {\n    bar(Foo);\n}"}
{"questionId":"f7e6c79c898949a39837eb3c724d9b35","question":".NET8 supports Vector512, but why doesn't Vector reach 512 bits?\nMy CPU is AMD Ryzen 7 7840H which supports AVX-512 instruction set. When I run the .NET8 program, the value of `Vector512.IsHardwareAccelerated` is true. But `System.Numerics.Vector<T>` is still 256-bit, and does not reach 512-bit.\nWhy doesn't the `Vector<T>` type reach 512 bits in length? Is it currently unsupported, or do I need to tweak the configuration?\n\n\nExample code:\n\n\n\n```\nTextWriter writer = Console.Out;\nwriter.WriteLine(string.Format(\"Vector512.IsHardwareAccelerated:\\t{0}\", Vector512.IsHardwareAccelerated));\nwriter.WriteLine(string.Format(\"Vector.IsHardwareAccelerated:\\t{0}\", Vector.IsHardwareAccelerated));\nwriter.WriteLine(string.Format(\"Vector<byte>.Count:\\t{0}\\t# {1}bit\", Vector<byte>.Count, Vector<byte>.Count * 8));\n\n```\n\nTest results:\n\n\n\n```\nVector512.IsHardwareAccelerated:        True\nVector.IsHardwareAccelerated:   True\nVector<byte>.Count:     32      # 256bit","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"See <https:\/\/github.com\/dotnet\/runtime\/issues\/92189> - For the same hardware reason that C compilers default to `-mprefer-vector-width=256` when auto-vectorizing large loops, C# doesn't automatically make all vectorized code use 512-bit even if it's available.\n\n\nAlso, for small problems, e.g. 9 floats, it could mean no vectorized iterations happen, just scalar fallback code.\n\n\nAlso, apparently some code-bases (hopefully accidentally) depend on Vector not being wider than 32-byte, so it would be a breaking change for those.\n\n\n\n> \n> @stephentoub wrote: **In .NET 8, the variable-width `Vector<T>` will not automatically support widths greater than 256 bits.** It's likely in .NET 9 you'll be able to opt-in to that, but at present it's not clear whether it'll be enabled by default,\n> \n> \n> \n\n\n\n\n---\n\n\nI commented on the dotnet github issue with some details about the CPU-hardware reasons; I'll reproduce some of that here:\n\n\n- See [SIMD instructions lowering CPU frequency](https:\/\/stackoverflow.com\/questions\/56852812\/simd-instructions-lowering-cpu-frequency)\n- Also <https:\/\/reviews.llvm.org\/D111029> including some Intel testing results that found clang auto-vectorization of SPEC2017 actually got a 1% slowdown with `-mprefer-vector-width=512` vs. `256` on Ice Lake Xeon. But again, that's LLVM auto-vectorization of scalar code, not like C# where this would only affect manually-vectorized loops, so the tuning considerations are somewhat different from `-mprefer-vector-width=256`.\n\n\nIn a program that frequently wakes up for short bursts of computation, its AVX-512 usage will still lower turbo frequency for the core, affecting other programs.\n\n\nThings are different on Zen 4; they handle 512-bit vectors by taking extra cycles in the execution units, so as long as 512-bit vectors don't require more shuffling work or some other effect that would add overhead, 512-bit vectors are a good win for front-end throughput and how far ahead out-of-order exec can see in terms of elements or scalar iterations. (Since a 512-bit uop is still only 1 uop for the front-end.) GCC and Clang default to `-mprefer-vector-width=512` for `-march=znver4`.\n\n\nThere's no turbo penalty or other inherent downsides to 512-bit vectors on Zen 4 (AFAIK; I don't know how misaligned loads perform). It's just a matter of whether software can use them efficiently (without needing more bloated code for loop prologues \/ epilogues, e.g. scalar cleanup if a masked final iteration doesn't Just Work.) AVX-512 masked stores are efficient on Zen 4, despite the fact that AVX1\/2 `vmaskmovps` \/ `vpmaskmovd` aren't. (<https:\/\/uops.info\/>)\n\n\nFor code where you have exactly 32 bytes of something, if the 32-byte vectors are no longer an option then that's a loss. C#'s scalable vector-length model isn't ideal for those cases. ARM SVE or RISC-V Vector extensions where the hardware ISA are designed around a variable vector-length with masking to handle vectors shorter than the HW's native length, but doing the same thing for C# `Vector<>` probably wouldn't work well because lots of hardware (x86 with AVX2, or AArch64 without SVE) can't efficiently support masking for arbitrary-length stuff.\n\n\n\n\n---\n\n\nI wrote more about Intel on the github issue, which I'm not going to copy\/paste all of here.\n\n\nThere can be significant overall throughput gains from 512-bit vectors for some workloads on Intel CPUs. But it comes with downsides, like more expensive misaligned memory access."}
{"questionId":"c9066c27c01a49d3b785e4688eca7806","question":"SwiftData @Query with #Predicate on Relationship Model\nXCode 15 beta 6.\n\n\nJust want to do a very simple Query Predicate where the relationship model matches:\n\n\n\n```\n@Query(\n    filter: #Predicate<Piece> {\n        $0.artist == selectedArtist\n    },\n    sort: [\n        SortDescriptor(\\Piece.age, order: .reverse)\n    ]\n) var pieces: [Piece]\n\n```\n\nand I'm receiving error:\n\n\n\n> \n> Cannot convert value of type 'PredicateExpressions.Equal<PredicateExpressions.KeyPath<PredicateExpressions.Variable, Artist>, PredicateExpressions.Value>' to closure result type 'any StandardPredicateExpression'\n> \n> \n> \n\n\nI also tried .id and .persistentModelId on the artist but no luck.\n\n\nThis seems like the most basic predicate use case so I'd be shocked if it's not supported. Any ideas what i'm missing?\n\n\nMy models are basic:\n\n\n\n```\n@Model\nfinal class Piece {\n    var age: Int\n    var artist: Artist\n}\n\n```\n\nAnd\n\n\n\n```\n@Model\nfinal class Artist {\n    var name: String\n    \n    @Relationship(\n        deleteRule: .cascade, \n        inverse: \\Piece.artist\n    )\n    var pieces: [Piece]?\n}","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"swift"},"answer":"Per [answer here](https:\/\/stackoverflow.com\/a\/76632341\/1807644), it seems like `#Predicate` will not let you use another `@Model` in the code block. Based on this logic, I don't think there's a way to do this as part of `@Query` without jumping through a bunch of hoops.\n\n\nIf your dataset is relatively small, the best suggestion is to filter it yourself:\n\n\n\n```\n@Query(\n    sort: [\n        SortDescriptor(\\Piece.age, order: .reverse)\n    ]\n) var pieces: [Piece]\n\nprivate var filteredPieces: [Piece] {\n    return pieces.compactMap { piece in\n        guard let artist = piece.artist else {\n            return nil\n        }\n        return artist == selectedArtist ? piece : nil\n    }\n}\n\n```\n\nand you can use your filtered data below:\n\n\n\n```\nvar body: some View {\n    List {\n        ForEach(filteredItems) { item in\n            \/\/show filtered stuff\n        }\n    }\n}"}
{"questionId":"98fee7c97da24616b56a3e001ad9a3d3","question":"Error: ! Failed to collect lazy table. Caused by error in `db\\_collect()` - using biomaRt package in R\nI'm currently working on a bioinformatics project using R, and I'm encountering an error when trying to use the `biomaRt` package. After installing the package and loading it into R, I tried to select a `biomaRt` database to use in my analysis.\n\n\nHere's the code I ran when I received an error:\n\n\n\n```\nlibrary(biomaRt)\nensembl <- useEnsembl(biomart = \"ensembl\", dataset = \"hsapiens_gene_ensembl\")\n\n```\n\nThe error message:\n\n\n\n```\nError in `collect()`:\n! Failed to collect lazy table.\nCaused by error in `db_collect()`:\n! Arguments in `...` must be used.\n\u2716 Problematic argument:\n\u2022 ..1 = Inf\n\u2139 Did you misspell an argument name?\n\nBacktrace:\n     \u2586\n  1. \u251c\u2500biomaRt::useEnsembl(biomart = \"genes\", dataset = \"hsapiens_gene_ensembl\")\n  2. \u2502 \u2514\u2500biomaRt:::.getEnsemblSSL()\n  3. \u2502   \u2514\u2500BiocFileCache::BiocFileCache(cache, ask = FALSE)\n  4. \u2502     \u2514\u2500BiocFileCache:::.sql_create_db(bfc)\n  5. \u2502       \u2514\u2500BiocFileCache:::.sql_validate_version(bfc)\n  6. \u2502         \u2514\u2500BiocFileCache:::.sql_schema_version(bfc)\n  7. \u2502           \u251c\u2500base::tryCatch(...)\n  8. \u2502           \u2502 \u2514\u2500base (local) tryCatchList(expr, classes, parentenv, handlers)\n  9. \u2502           \u2514\u2500tbl(src, \"metadata\") %>% collect(Inf)\n 10. \u251c\u2500dplyr::collect(., Inf)\n 11. \u2514\u2500dbplyr:::collect.tbl_sql(., Inf)\n 12.   \u251c\u2500base::tryCatch(...)\n 13.   \u2502 \u2514\u2500base (local) tryCatchList(expr, classes, parentenv, handlers)\n 14.   \u2502   \u2514\u2500base (local) tryCatchOne(expr, names, parentenv, handlers[[1L]])\n 15.   \u2502     \u2514\u2500base (local) doTryCatch(return(expr), name, parentenv, handler)\n 16.   \u2514\u2500dbplyr::db_collect(x$src$con, sql, n = n, warn_incomplete = warn_incomplete, ...)`\n\nR Version: 4.3.1 (2023-06-16 ucrt)\nBiomaRt Version: 2.58.0\nOperating System: Windows\n\n```\n\nI tried updating all the packages (`biomaRt` and `dbplyr`) and restarting R, but nothing helped.\n\n\nI would greatly appreciate any guidance or insights on how to resolve this error. Thank you in advance for your help!","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"r"},"answer":"This is probably due to the `dbplyr` upgrade, there is already a [merged PR in `BiocFileCache` to solve this](https:\/\/github.com\/Bioconductor\/BiocFileCache\/pull\/50).\n\n\nSomehow the `Inf` argument in `tbl(src, \"metadata\") %>% collect(Inf)` got into the `...` of `dbplyr::db_collect <- function(con, sql, n = -1, warn_incomplete = TRUE, ...)` instead of the `n` argument.\n\n\nWhile `BiocFileCache` 2.10.1 is waiting to be built on the Bioconductor servers, **downgrading `dbplyr` solves this issue for me (`devtools::install_version(\"dbplyr\", version = \"2.3.4\")`)**.\n\n\nI suppose installing the latest `BiocFileCache` from their Github repo would work as well."}
{"questionId":"97bbcd4998d14cf5be7f3dce4a48bff4","question":"Is it possible to use the index of an array in a map block?\nI would like to use the index of the array in a map routine. For example, this Raku code:\n\n\n`raku -e 'my @a = \"First\", \"Second\", \"Third\", \"First\"; say @a.map({ \"Index of $_ is: ;\" })'`\n\n\nprints:\n\n\n`(Index of First is: ; Index of Second is: ; Index of Third is: ; Index of First is: ;)`\n\n\nWould it be possible to get the index of the array, like:\n\n\n`(Index of First is: 0; Index of Second is: 1; Index of Third is: 2; Index of First is: 3;)`\n\n\nThanks!","questionMetadata":{"Type":"Implementation","Level":"Beginner","Tag":"perl"},"answer":"There is [`.kv`](https:\/\/docs.raku.org\/type\/List#routine_kv) to produce \"keys\" and values. For an array, the keys are the indexes 0, 1, 2...\n\n\n\n```\n >>> my @a = \"First\", \"Second\", \"Third\", \"First\";\n[First Second Third First]\n\n>>> @a.kv\n(0 First 1 Second 2 Third 3 First)\n\n```\n\nThis is a flat sequence of length 2N; we can tell `map` to take 2 things at a time, either via an explicit signature:\n\n\n\n```\n>>> @a.kv.map(-> $index, $value { \"Index of $value is $index;\" })\n(Index of First is 0; Index of Second is 1; Index of Third is 2; Index of First is 3;)\n\n```\n\nor via [placeholder](https:\/\/docs.raku.org\/language\/variables#The_%5E_twigil) variables:\n\n\n\n```\n>>> @a.kv.map({ \"Index of $^value is $^index;\" })\n(Index of First is 0; Index of Second is 1; Index of Third is 2; Index of First is 3;)\n\n```\n\nSince *index* comes lexicographically before *value*, it works out as in the previous explicit signature case, i.e., the same signature is produced for us behind the scenes. We can see this:\n\n\n\n```\n>>> &{ $^value, $^index }\n-> $index, $value { #`(Block|140736965530488) ... }\n\n```\n\nNote that what matters is the Unicode order of variable names, not the order of appearance.\n\n\n\n\n---\n\n\nIn the spirit of TIMTOWDI, some alternatives (IMO not better):\n\n\n- Rely on the [anonymous scalar state variable $](https:\/\/docs.raku.org\/syntax\/state#The_$_variable) to produce a stream of 0, 1, 2...\n\n\n\n```\n# Limitation: generated anew per closure and per mention,\n# so cannot use, e.g., in string interpolations with closure and more than once\n>>> @a.map({ \"Index of $_ is \" ~ $++ ~ \";\" })\n(Index of First is 0; Index of Second is 1; Index of Third is 2; Index of First is 3;)\n\n```\n\n- Using [`.pairs`](https:\/\/docs.raku.org\/type\/List#routine_pairs), cousine of `.kv` to produce \"key => value\" pairs instead of a flat list\n\n\n\n```\n>>> @a.pairs\n(0 => First 1 => Second 2 => Third 3 => First)\n\n# Reach over .key & .value\n>>> @a.pairs.map({ \"Index of $_.value() is $_.key();\" })\n(Index of First is 0; Index of Second is 1; Index of Third is 2; Index of First is 3;)\n\n# Unpack with signature\n>>> @a.pairs.map(-> (:key($index), :$value) { \"Index of $value is $index;\" })\n(Index of First is 0; Index of Second is 1; Index of Third is 2; Index of First is 3;)\n\n```\n\n- Instead of mapping the array's values themselves, we can map the keys of it:\n\n\n\n```\n# Array.keys produces 0, 1, 2...\n>>> @a.keys.map({ \"Index of @a[$_] is $_;\" })\n(Index of First is 0; Index of Second is 1; Index of Third is 2; Index of First is 3;)\n\n# ^@a is equivalent to ^@a.elems === 0 ..^ @a.elems, i.e., 0, 1, 2... again\n>>> ^@a .map({ \"Index of @a[$_] is $_;\" })\n(Index of First is 0; Index of Second is 1; Index of Third is 2; Index of First is 3;)\n\n```\n\nThis last one is the least idiomatic I think."}
{"questionId":"8826dd611e514f78a49864904b2e618e","question":"WillPopScope is deprecated after flutter 3.12\nWhen we use below code after flutter version 3.12.0 we are getting deprecated message, what to use insted of this now?\n\n\n\n```\nWillPopScope(\n          onWillPop: () {\n            setStatusBarColor(statusBarColorPrimary,statusBarIconBrightness: Brightness.light);\n            finish(context);\n            return Future.value(true);\n          },)","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"dart"},"answer":"WillPopScope` is deprecated after flutter `v3.12.0-1.0.pre`.\n\n\nNow you can use `PopScope` as below:\n\n\n\n```\nreturn PopScope(\n    canPop: true, \/\/When false, blocks the current route from being popped.\n    onPopInvoked: (didPop) {\n        \/\/do your logic here:\n        setStatusBarColor(\n            statusBarColorPrimary,\n            statusBarIconBrightness: Brightness.light,\n        );\n        finish(context);\n    },\n    child: Scaffold(\/*other child and ui etc*\/),\n);"}
{"questionId":"2a661f7b35a04691b7a724f06d1c253f","question":"How do I use std::formatter directly?\nLet's say I want to format a single object directly using `std::formatter`, bypassing `std::format`. How do I do that?\n\n\nAccording to [*Formatter*](https:\/\/en.cppreference.com\/w\/cpp\/named_req\/Formatter), I need to call `.format(value, format_ctx)`, where `format_ctx` is a `std::format_context` or `std::basic_format_context<...>`. But how do I construct this context?\n\n\nThe standard doesn't seem to provide a way to construct one. And looking at [libstdc++ sources](https:\/\/github.com\/gcc-mirror\/gcc\/blob\/6d16e460240fe547cb7d9648f91494126213c835\/libstdc%2B%2B-v3\/include\/std\/format#L3557), the member variables of `basic_format_context` are all private, there's no non-default constructor, and no way to set them without being a `friend`.\n\n\nDoes this mean that `std::formatter` is impossible to use manually by design?\n\n\n\n\n---\n\n\n**Why am I doing this?**\n\n\nI want to format a value using the \"debug format\" (`\"{?:}\"`) if it's supported, falling back to the regular `\"{}\"`.\n\n\nThe way to check for support seems to be `requires(std::formatter<T> f){f.set_debug_format();}`, and I figured that if I'm already interacting with the formatter directly, I might as well use only the formatter itself.","questionMetadata":{"Type":"Debugging","Level":"Advanced","Tag":"cpp"},"answer":"The use case you're describing, where you never actually call `std::(v)format(_to)` and just use the formatter directly, isn't supported. You should just call `std::format` with the format string that you want to use, i.e., `{}` or `{:?}`.\n\n\nAnd even if you could bypass `std::format`, you would just be creating work for yourself. You'd have to call `std::formatter<T>::parse` *and* `std::formatter<T>:format` manually (since the former sets up state that is used by the latter). And you'd have to manually set up the contents of the `std::basic_format_parse_context` and `std::basic_format_context` objects, assuming that they supported that in the first place.\n\n\nOn the other hand, a formatter can invoke another formatter, by passing down the `std::basic_format_parse_context` and later the `std::basic_format_context` that were passed to it by the library. That's basically how you'd implement a range formatter (if it weren't for the fact that the standard already provides one)."}
{"questionId":"9cfaf46a13aa41459ba1924d1405ebbf","question":"Idiomatic Option to Option when T -> U is defined with From\nGiven two arbitrary Sized types `T` and `U`, where `U: From<T>`, is there a reason why standard library does not provide `From<Option<T>> for Option<U>` with a `where U: From<T>`? I tried to do it, but get a conflicting implementation error, so clearly there is a limitation, just not sure where. And yes, I understand I can do it with a `Option::map()`, but it seems like std should give this out of the box.\n\n\n\n```\nenum Opt<T> {\n    Some(T),\n    None,\n}\n\nimpl<T: From<U>, U> From<Opt<T>> for Opt<U> {\n    fn from(opt: Opt<T>) -> Self {\n        match opt {\n            Opt::Some(t) => Opt::Some(t.into()),\n            Opt::None => Opt::None,\n        }\n    }\n}\n\nstruct A;\nstruct B;\n\nimpl From<A> for B {\n    fn from(_: A) -> Self {\n        B\n    }\n}\n\nfn main() {\n    let a = Opt::Some(A);\n    let _b: Opt<B> = a.into();\n}\n\n```\n\nError\n\n\n\n```\nerror[E0119]: conflicting implementations of trait `From<Opt<_>>` for type `Opt<_>`\n --> src\/bin\/main.rs:6:1\n  |\n6 | impl<T: From<U>, U> From<Opt<T>> for Opt<U> {\n  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  |\n  = note: conflicting implementation in crate `core`:\n          - impl<T> From<T> for T;","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"rust"},"answer":"Yes, there is a reason, because it conflicts with the `impl<T> From<T> for T` as you've noticed in case `T == U`. Just like you can't write this impl, std can't either (even the current specialization doesn't support this, and specialization isn't used for public interfaces). There is a desire for such impl (and similar impls for other types), it's just not clear how to do it."}
{"questionId":"670d4413905e477d97e07fe3e367a05f","question":"std::enable\\_if\\_t works with gcc and clang but does not compile with msvc\nI have a friend function template `operator<<` that [works](https:\/\/godbolt.org\/z\/WGW7e48vf) with both gcc and clang but not with msvc.\n\n\n\n```\n#include <iostream>\n#include <type_traits>\n\ntemplate< typename T, std::enable_if_t< T{1}, int> =0 >\nclass Foo\n{\n    template< typename Ar, typename R> \n    friend Ar& operator<<(Ar& os, const Foo<R>& foo)\n    {\n        return os;\n    }          \n};\n\nint main()\n{\n    Foo<int> i;\n    std::cout << i;  \/\/works with gcc and clang but does not compile with msvc\n}\n\n```\n\nI want to know which compiler has the correct behavior according to the c++ standard. The msvc error says:\n\n\n\n```\n<source>(4): error C2972: 'Foo': template parameter 'unnamed-parameter': the type of non-type argument is invalid\n<source>(6): note: see declaration of 'Foo'\n<source>(4): note: the template instantiation context (the oldest one first) is\n<source>(11): note: see reference to class template instantiation 'Foo<T,__formal>' being compiled\n<source>(15): error C2679: binary '<<': no operator found which takes a right-hand operand of type 'Foo<int,0>' (or there is no acceptable conversion)","questionMetadata":{"Type":"Debugging","Level":"Advanced","Tag":"cpp"},"answer":"This is a MSVC bug and has nothing to do with the `operator<<` at all.\n\n\nJust mentioning `Foo<U>` anywhere with `U` being a template parameter causes the error because MSVC tries to check validity of `std::enable_if_t< T{1}, int>` immediately, even though no concrete type for `U`\/`T` is known yet. This then fails.\n\n\nMSVC doesn't correctly recognize that `std::enable_if_t< T{1}, int>` is a dependent type and therefore shouldn't be checked until after a concrete `U`\/`T` is substituted.\n\n\nSee <https:\/\/godbolt.org\/z\/ncb3va3ro> for a reduced example:\n\n\n\n```\ntemplate< typename T, std::enable_if_t< T{1}, int> =0 >\nclass Foo\n{      \n};\n\ntemplate<typename R>\nusing U = Foo<R>;  \/\/ <- same error here"}
{"questionId":"0dd3b04f2e914add9e66563654f72ac8","question":"Android Compose preview \"Render problem\" with API 34 and Compose UI 1.5.0\nI recently started a new app using Compose. I have AS Hedgehog 2023.1.1 installed. I am using API 34 and get an error in the compose preview. \"Render problem\" accompanied by a\n\n\n\n```\nLayout fidelity warning\n\nThe graphics preview in the layout editor may not be accurate: \n    - The current rendering only supports APIs up to 33. You may encounter crashes if using with higher APIs. To avoid, you can set a lower API for your preview.\n\n```\n\nHow do I set a lower API for previews?\n\n\nGoogling I only find results telling me to drop the API level of the entire app, OR answers from 2 years ago pointing to using a very old version of compose.","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"kotlin"},"answer":"Use @Preview(apiLevel = 33), like below\n\n\n\n```\n@Preview(apiLevel = 33)\n@Composable\nfun YourComposablePreview() {\n    ...\n}"}
{"questionId":"365abc8a893949429a80bdac269e0567","question":"build\\_runner showing error The getter 'macroKeyword' isn't defined for the class 'ClassDeclaration'\nWhen I am running build\\_runner command it is showing me following error:\n\n\n\n```\nFailed to build build_runner:build_runner:\n..\/..\/.pub-cache\/hosted\/pub.dev\/dart_style-2.3.5\/lib\/src\/front_end\/ast_node_visitor.dart:251:16: Error: The getter 'macroKeyword' isn't defined for the class 'ClassDeclaration'.\n  - 'ClassDeclaration' is from 'package:analyzer\/src\/dart\/ast\/ast.dart' ('..\/..\/.pub-cache\/hosted\/pub.dev\/analyzer-6.2.0\/lib\/src\/dart\/ast\/ast.dart').\nTry correcting the name to the name of an existing getter, or defining a getter or field named 'macroKeyword'.\n          node.macroKeyword,\n                ^^^^^^^^^^^^\n..\/..\/.pub-cache\/hosted\/pub.dev\/dart_style-2.3.5\/lib\/src\/source_visitor.dart:595:19: Error: The getter 'macroKeyword' isn't defined for the class 'ClassDeclaration'.\n  - 'ClassDeclaration' is from 'package:analyzer\/src\/dart\/ast\/ast.dart' ('..\/..\/.pub-cache\/hosted\/pub.dev\/analyzer-6.2.0\/lib\/src\/dart\/ast\/ast.dart').\nTry correcting the name to the name of an existing getter, or defining a getter or field named 'macroKeyword'.\n    modifier(node.macroKeyword);\n                  ^^^^^^^^^^^^","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"dart"},"answer":"Add `analyzer: ^5.11.1` to your `dev_dependencies` and run the following:\n\n\n\n```\nflutter clean\n\nflutter pub get\n\ndart run build_runner build\n\n```\n\nNote: If the analyzer version 5.11.1 doesn't work for you, try changing the version and it will work. You can check for the version here <https:\/\/pub.dev\/packages\/analyzer\/changelog>"}
{"questionId":"44ef2d481ddf479b8aec05228d28a533","question":"ModuleNotFoundError: No module named 'imp'\nI need to install the [eb](https:\/\/docs.aws.amazon.com\/elasticbeanstalk\/latest\/dg\/eb-cli3-install-advanced.html) command on windows.\n\n\nI would like to try to deploy an application on AWS using the elasticbeanstalk service, and through this command you can configure and deploy an environment directly with a configuration file.\n\n\nTo do this I followed the [guide](https:\/\/github.com\/aws\/aws-elastic-beanstalk-cli-setup). I first installed python via the site (Python version 3.12.0), and then all the steps described in the guide link.\n\n\nNow if I run the eb command from cmd I always get this error.\n\n\n\n```\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"C:\\Users\\Utente\\.ebcli-virtual-env\\Scripts\\eb.exe\\__main__.py\", line 4, in <module>\n  File \"C:\\Users\\Utente\\.ebcli-virtual-env\\Lib\\site-packages\\ebcli\\core\\ebcore.py\", line 16, in <module>\n    from cement.core import foundation, handler, hook\n  File \"C:\\Users\\Utente\\.ebcli-virtual-env\\Lib\\site-packages\\cement\\core\\foundation.py\", line 11, in <module>\n    from ..core import output, extension, arg, controller, meta, cache, mail\n  File \"C:\\Users\\Utente\\.ebcli-virtual-env\\Lib\\site-packages\\cement\\core\\extension.py\", line 8, in <module>\n    from imp import reload  # pragma: no cover\n    ^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'imp'\n\n```\n\nI've tried several things but can't come to a conclusion.\nDoes anyone know how to help me?\n\n\nI also tried installing previous versions of python, even though I didn't like it as a solution, but still I still have the problem.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"I encountered this as well. As far as I understand its a depreciation issue.\n\n\n**awsebcli** will install with **Python 3.12** but imp will not.\n\n\nIf you type **import imp** into **Python 3.11** you will get the following response\n\n\n\n```\nDeprecationWarning: the imp module is deprecated in favour of importlib and \nslated for removal in Python 3.12; see the module's documentation for \nalternative uses\n\n```\n\nAt the time of writing this Electric Beanstalk is only supporting 3.8, 3.9 & 3.11\n\n\n<https:\/\/docs.aws.amazon.com\/elasticbeanstalk\/latest\/platforms\/platforms-supported.html#platforms-supported.python>"}
{"questionId":"05ce1177c8b948de9a7649498cc56a23","question":"Error generating JWT token during ASP.NET Core Web API authorization\nI have a Web API project through which I register and log in. During authorization, where I should get a token, an error appears:\n`An unhandled exception has occurred while executing the request. System.TypeInitializationException: The type initializer for System.IdentityModel.Tokens.Jwt.JsonExtensions' threw an exception. System.TypeLoadException: Could not load type 'Microsoft.IdentityModel.Json.JsonConvert' from assembly 'Microsoft.IdentityModel.Tokens, Version=7.0.0.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35'`\n\n\nToken generation code:\n\n\n\n```\nusing ASFT.Auth.Interfaces;\nusing Microsoft.IdentityModel.Tokens;\nusing System.IdentityModel.Tokens.Jwt;\nusing System.Security.Claims;\nusing System.Security.Cryptography;\n\nnamespace ASFT.Auth.Services\n{\n    public class TokenService : ITokenService\n    {\n        public string GenerateAccessToken(IEnumerable<Claim> claims)\n        {\n            var signinCredentials = new SigningCredentials(AuthOptions.Key, SecurityAlgorithms.HmacSha256);\n\n            var tokenOptions = new JwtSecurityToken(\n                issuer: AuthOptions.Issuer,\n                audience: AuthOptions.Audience,\n                claims: claims,\n                expires: DateTime.Now.AddHours(48),\n                signingCredentials: signinCredentials);\n\n            return new JwtSecurityTokenHandler().WriteToken(tokenOptions);\n        }\n    }\n}\n\n```\n\n[The error](https:\/\/i.stack.imgur.com\/HR5CR.jpg) is on this line - `return new JwtSecurityTokenHandler().WriteToken(tokenOptions);`\n\n\nConnected packages:\n\n\n\n```\n<PackageReference Include=\"Microsoft.AspNetCore.Authentication.JwtBearer\" Version=\"7.0.11\" \/>\n<PackageReference Include=\"Microsoft.AspNetCore.OpenApi\" Version=\"7.0.11\" \/>\n<PackageReference Include=\"Microsoft.IdentityModel.Tokens\" Version=\"7.0.0\" \/>\n<PackageReference Include=\"Swashbuckle.AspNetCore\" Version=\"6.5.0\" \/>\n<PackageReference Include=\"Microsoft.EntityFrameworkCore\" Version=\"7.0.11\" \/>\n<PackageReference Include=\"Microsoft.EntityFrameworkCore.SqlServer\" Version=\"7.0.11\" \/>\n<PackageReference Include=\"Microsoft.EntityFrameworkCore.Tools\" Version=\"7.0.11\">\n   <PrivateAssets>all<\/PrivateAssets>\n   <IncludeAssets>runtime; build; native; contentfiles; analyzers; buildtransitive<\/IncludeAssets><\/PackageReference>\n\n```\n\nExpected status 200 with token and refresh token in the response body.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"csharp"},"answer":"First of all, you should probably include:\n\n\n\n```\n<PackageReference Include=\"System.IdentityModel.Tokens.Jwt\" Version=\"7.0.0\" \/>\n\n```\n\nin your `.csproj`. Make sure that the `version` value is correct, you're going to want it to match the requirements from your other NuGet packages.\n\n\nIf this doesn't work, this may be an issue caused by the runtime attempting to remove assemblies that it believes that the runtime already has and are being duplicated in your project.\n\n\nTry adding this to your `.csproj` file\n\n\n\n```\n<PropertyGroup>\n    <_FunctionsSkipCleanOutput>true<\/_FunctionsSkipCleanOutput>\n<\/PropertyGroup>\n\n```\n\nYou can find a more detailed description and explanation of the issue [here.](https:\/\/bryanknox.github.io\/2022\/07\/15\/functionsskipcleanoutput-and-functionspreserveddependencies.html)"}
{"questionId":"13b470648fe0481487fc02cd370ee160","question":"Why can T not be deduced for std::span when passing a std::vector?\nIn the following C++20 code, passing a `std::vector` to a templated function with a `std::span<T>` parameter fails, because obviously the compiler cannot deduce the template parameter. I have tried this with GCC, Clang and MSVC; all fail.\n\n\nInvoking like this works: `f3(std::span(vi))` or `f3(std::span(vp))`.\n\n\nI would like to know *why* this fails, because in my understanding, `std::vector` is a range, and `std::span` has a [deduction guide](https:\/\/en.cppreference.com\/w\/cpp\/container\/span\/deduction_guides) for ranges.\n\n\n\n```\n#include <memory>\n#include <vector>\n#include <span>\n\nvoid f1(std::span<int> s)\n{\n}\n\nvoid f2(std::span<std::shared_ptr<int>> s)\n{\n}\n\ntemplate<typename T>\nvoid f3(std::span<T> s)\n{\n}\n\nint main(int argc, char* argv[])\n{\n    std::vector<int> vi;\n    std::vector<std::shared_ptr<int>> vp;\n\n    f1(vi);\n    f2(vp);\n    f3(vi); \/\/ ERROR: no matching function for call to 'f3'\n    f3(vp); \/\/ ERROR: no matching function for call to 'f3'\n\n    return 0;\n}","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"cpp"},"answer":"If a function argument participates in template argument deduction, no implicit conversions are allowed for that function argument."}
{"questionId":"84e753a3d79f418b873532bbba9c3981","question":"Integer parameter calls float overload and float parameter calls integer overload\nToday I ran into roughly the following code:\n\n\n\n```\n#include <iostream>\n\nvoid f(float&& f) { std::cout << f << \"f \"; }\nvoid f(int&& i) { std::cout << i << \"i \"; }\n\nint main()\n{\n    int iv = 2; float fv = 1.0f;\n    f(2);  f(1.0f);\n    f(iv); f(fv);\n}\n\n```\n\n[Godbolt link](https:\/\/godbolt.org\/z\/roW7qd6fY)\n\n\nThe first two f-calls print `2i 1f`, as expected.\n\n\nNow for the second line, I would have expected that it either doesn\u2019t compile at all, since iv and fv are not temporaries (and thus can't bind to an r value reference), or that it creates a copy of the variable to pass to the function, and thus print `2i 1f` a second time.\n\n\nHowever, somehow it prints `2f 1i`, which is just about the last thing I would have expected.\n\n\nIf you copy the code into cppinsights, it transforms the calls into\n\n\n\n```\nf(static_cast<float>(iv));\nf(static_cast<int>(fv));\n\n```\n\nSo it seemingly very intentionally casts the integer to a float, and the float to an integer, but I don't have any idea why it does that, and don't really know how to google that either. Why does this happen? What are the rules that lead to this result?","questionMetadata":{"Type":"Debugging","Level":"Advanced","Tag":"cpp"},"answer":"The behavior of the program can be understood from [reference-initialization](https:\/\/timsong-cpp.github.io\/cppwp\/n4868\/dcl.init#ref-5.4).\n\n\nFrom [dcl.init#ref-5.4](https:\/\/timsong-cpp.github.io\/cppwp\/n4868\/dcl.init#ref-5.4):\n\n\n\n> \n> [Example 6:\n> \n> \n> \n> ```\n> double d2 = 1.0;\n> double&& rrd2 = d2;                 \/\/ error: initializer is lvalue of related type\n> int i3 = 2;\n> double&& rrd3 = i3;                 \/\/ rrd3 refers to temporary with value 2.0\n> \n> ```\n> \n> -end example]\n> \n> \n> \n\n\n\n\n---\n\n\n#### Case 1\n\n\nHere we discuss why `void f(int&& i)` isn't viable for the call `f(iv)`.\n\n\nThe lvalue `iv` cannot be bind to the rvalue reference parameter `i` in `void f(int&& i)` for the call `f(iv)` and so the overload `f(int&&)` isn't **viable**. Basically, `int&& i = iv;` isn't allowed because `iv` is an **lvalue of related type.**\n\n\n\n\n---\n\n\n#### Case 2\n\n\nHere we discuss why `void f(float&& i)` is viable for the call `f(iv)`.\n\n\nFor the call `f(iv)` the overload `void f(float&& f)` is viable because here first the initializer expression `iv` is [implicitly converted](https:\/\/timsong-cpp.github.io\/cppwp\/n4659\/dcl.init.ref#5.2.2.2) to a prvalue of the destination type(`float`) and then **temporary materialization** can happen such that the parameter `f` can be bound to that materialized temporary `2.0f`(which is an **xvalue**).\n\n\n\n\n---\n\n\nSimilarly for the call `f(fv)`, the overload `void f(float&& i)` isn't viable because `fv` is an lvalue of related type. And for the call `f(fv)` the overload `void f(int&& i)` can be used because first the initializer is implicitly converted to a prvalue and then temporary materialization happens such that `i` can be bound to the materialized temporary `1`(of type `int`)."}
{"questionId":"3cf27cd7e2d540318c88c6025b6c1edd","question":"Feature flag \\_\\_VUE\\_PROD\\_HYDRATION\\_MISMATCH\\_DETAILS\\_\\_ is not explicitly defined. Where do I define it?\nSince just upgrading vue.js to version 3.4.4 I now get the following warning in the console of my browser:\n\n\nFeature flag **VUE\\_PROD\\_HYDRATION\\_MISMATCH\\_DETAILS** is not explicitly defined. You are running the esm-bundler build of Vue, which expects these compile-time feature flags to be globally injected via the bundler config in order to get better tree-shaking in the production bundle.\n\n\nSo I read that it's some new flag they've introduced.\n\n\nEDIT:\n\n\nI'm using a vue.config.js file in my project. Is this where the flag should be added? It currently looks like this:\n\n\n\n```\nconst { defineConfig } = require('@vue\/cli-service')\nmodule.exports = defineConfig({\n  transpileDependencies: true\n})\n\n```\n\n**My question:** \"*Where* do I set this flag\"?\nI find a lot of info about that I should define this flag but no info on \"*where*\" I should define it...","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"javascript"},"answer":"If you are using the no longer maintained @vue\/cli-service with webpack, it is caused by the fact that the tool does not define the default for this feature flag. It does so for the other two bundler feature flags, and while I created a PR to add this default (as I just hit this myself - <https:\/\/github.com\/vuejs\/vue-cli\/pull\/7443>), I do not expect it to get merged and released as the latest release is from July 2022.\n\n\nSo, you should define it yourself in the webpack configuration using the define plugin.\n\n\nAs they question is update to indicate the use of `vue.config.js`, you can set it like this:\n\n\n\n```\nconst { defineConfig } = require('@vue\/cli-service')\nconst webpack = require('webpack');\n\nmodule.exports = defineConfig({\n  configureWebpack: {\n    plugins: [\n      new webpack.DefinePlugin({\n        \/\/ Vue CLI is in maintenance mode, and probably won't merge my PR to fix this in their tooling\n        \/\/ https:\/\/github.com\/vuejs\/vue-cli\/pull\/7443\n        __VUE_PROD_HYDRATION_MISMATCH_DETAILS__: 'false',\n      })\n    ],\n  },\n});\n\n\n```\n\nSee <https:\/\/cli.vuejs.org\/config\/#configurewebpack> for details on the configuration options."}
{"questionId":"4035de6ef38542a0a3aa8c841dd1c8a5","question":"Error: Type 'DecoderCallback' not found. ImageStreamCompleter load(PdfPageImageProvider key, DecoderCallback decode)\nAfter my app failed on iOS 17, I had to do a lot of upgrading, including buying a new Macbook, just so I can do a build for iOS 17. It turned out that the TextFormField in Flutter didn't work with Autocorrect in iOS 17.\n\n\nSo, after upgrading Macbook, XCode and Android Studio, I had a lot of packages to update as well (my app was only 2 years old). I am now using Android Studio Giraffe. Getting my app to work again because of all the updates to the packages took quite a while, but I have it all working now.\n\n\nI normally develop on my Windows computer, so I copied all my changes to Windows, upgraded Android Studio to Giraffe, ran Flutter Pub Upgrade to get latest version of Flutter and Dart and attempted a build, but this is where I have stalled.\n\n\nI am getting error...\n\n\n\n```\n\/C:\/Users\/me\/AppData\/Local\/Pub\/Cache\/hosted\/pub.dev\/pdfx-2.5.0\/lib\/src\/viewer\/pdf_page_image_provider.dart:24:55: Error: Type 'DecoderCallback' not found.   ImageStreamCompleter load(PdfPageImageProvider key, DecoderCallback decode) =>\n                                                      ^^^^^^^^^^^^^^^ \/C:\/Users\/me\/AppData\/Local\/Pub\/Cache\/hosted\/pub.dev\/pdfx-2.5.0\/lib\/src\/viewer\/pdf_page_image_provider.dart:40:7: Error: Type 'DecoderCallback' not found.\n      DecoderCallback decode) async {\n      ^^^^^^^^^^^^^^^ \/C:\/Users\/me\/AppData\/Local\/Pub\/Cache\/hosted\/pub.dev\/pdfx-2.5.0\/lib\/src\/viewer\/pdf_page_image_provider.dart:24:55: Error: 'DecoderCallback' isn't a type.   ImageStreamCompleter load(PdfPageImageProvider key, DecoderCallback decode) =>\n                                                      ^^^^^^^^^^^^^^^ \/C:\/Users\/me\/AppData\/Local\/Pub\/Cache\/hosted\/pub.dev\/pdfx-2.5.0\/lib\/src\/viewer\/pdf_page_image_provider.dart:40:7: Error: 'DecoderCallback' isn't a type.\n      DecoderCallback decode) async {\n      ^^^^^^^^^^^^^^^ Target kernel_snapshot failed: Exception\n\n\nFAILURE: Build failed with an exception.\n\n* Where: Script 'E:\\Dev\\flutter-new\\packages\\flutter_tools\\gradle\\src\\main\\groovy\\flutter.groovy' line: 1350\n\n* What went wrong: Execution failed for task ':app:compileFlutterBuildDebug'.\n> Process 'command 'E:\\Dev\\flutter-new\\bin\\flutter.bat'' finished with non-zero exit value 1\n\n* Try:\n> Run with --stacktrace option to get the stack trace.\n> Run with --info or --debug option to get more log output.\n> Run with --scan to get full insights.\n\n* Get more help at https:\/\/help.gradle.org\n\nBUILD FAILED in 7s Exception: Gradle task assembleDebug failed with exit code 1\n\n```\n\nI am a bit stuck, and there is very little I can find to resolve this.\n\n\nMy flutter doctor is...\n\n\n\n```\n[\u221a] Flutter (Channel stable, 3.16.0, on Microsoft Windows [Version 10.0.22621.2715], locale en-GB)\n[\u221a] Windows Version (Installed version of Windows is version 10 or higher)\n[\u221a] Android toolchain - develop for Android devices (Android SDK version 33.0.2)\n[\u221a] Chrome - develop for the web\n[!] Visual Studio - develop Windows apps (Visual Studio Community 2017 15.9.48)\n    X Visual Studio 2019 or later is required.\n      Download at https:\/\/visualstudio.microsoft.com\/downloads\/.\n      Please install the \"Desktop development with C++\" workload, including all of its\ndefault components\n[!] Android Studio (version 2021.2)\n    X Unable to determine bundled Java version.\n[\u221a] Android Studio (version 2022.3)\n[\u221a] Android Studio (version 4.2)\n[\u221a] Connected device (4 available)\n[\u221a] Network resources\n\n! Doctor found issues in 2 categories.\n\n```\n\nI am not writing for Windows, so the reason for VS2019 is not required. I have Java version 8 installed.\n\n\nWhen I try to upgrade Gradle using the AGP Ugrade Assistant... nothing happens.\n\n\nI don't know where else to turn to find out how to get beyond this.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"dart"},"answer":"Currently, pdfx package has a problem due to flutter upgrading to version 3.16 as mentioned in this issue <https:\/\/github.com\/ScerIO\/packages.flutter\/issues\/448>\n\n\nThere is a temporary way to fix it by changing the pdfx package inside your pubspec.yaml\nfrom\n`pdfx: ^2.4.0`\nto\n\n\n\n```\npdfx: \n  git:\n   url: 'https:\/\/github.com\/ScerIO\/packages.flutter' \n   ref: '4be9de9ffed5398fd7d5f44bbb07dcd3d3f1711b'  \n   path: packages\/pdfx"}
{"questionId":"58af3f7bcafb4247a4476136dfebb588","question":"Parametrised classes\nI have a class as follows:\n\n\n\n```\nclass Token a where\n    symbol :: a -> String\n\n```\n\nI also want all instances of `Token` to have a function `convert` which returns a parametrised type. The conversion alone works fine:\n\n\n\n```\nclass Token a b where\n    convert :: a -> b\n\ndata Egal = One | Two\n\ninstance Token Egal Int where\n    convert One = 111\n    convert Two = 222\n\nmain = print $ show (convert One :: Int)\n\n```\n\nBut when I try to use both `symbol` and `convert` I get errors about ambiguity. This is my code:\n\n\n\n```\nclass Token a b where\n    convert :: a -> b\n    symbol :: a -> String\n\ndata Egal = One | Two\n\ninstance Token Egal Int where\n    convert One = 111\n    convert Two = 222\n    symbol One = \"one\"\n    symbol Two = \"two\"\n\nmain = print $ show (convert One :: Int)\n\n```\n\nWhat am I doing wrong?\n\n\n\n\n---\n\n\nEDIT: Reading my own question I started wondering: Should these be two distinct classes and my `data Egal` show instanciate both?","questionMetadata":{"Type":"Debugging","Level":"Advanced","Tag":"haskell"},"answer":"As you have defined things here, you can have instances with the same `a` but conflicting `b`s. Like this:\n\n\n\n```\ninstance Token Char Char where\n    convert = id\n    symbol c = [c]\n\ninstance Token Char Bool where\n    convert = (>'m')\n    symbol c = [c, c, c, c]\n\n```\n\nNow, should `symbol 'x'` be `\"x\"` or `\"xxxx\"`? Both are possible depending which of the above instances gets chosen; it is *ambiguous* which instance should be used for `symbol`, and therefore which answer you should get. There are various ways to fix this. One is to simply allow the ambiguity, and give yourself the ability to specify which instance to use at call sites. You can turn on the `AllowAmbiguousTypes` and `TypeApplications` extensions; then:\n\n\n\n```\n> symbol @_ @Char 'x' -- or, more explicitly, symbol @Char @Char 'x'\n\"x\"\n> symbol @_ @Bool 'x' -- explicitly, symbol @Char @Bool 'x'\n\"xxxx\"\n\n```\n\nBut in many cases, you really want the compiler to check that you haven't made multiple instances with conflicting `a`s. Then you can use either the `FunctionalDependencies` extension:\n\n\n\n```\nclass Token a b | a -> b where\n    convert :: a -> b\n    symbol :: a -> String\n\ninstance Token Char Char where {- ... -}\n\n```\n\nor the `TypeFamilies` extension:\n\n\n\n```\nclass Token a where\n    type Conversion a\n    convert :: a -> Conversion a\n    symbol :: a -> String\n\ninstance Token Char where\n    type Conversion Char = Char\n    {- ... -}\n\n```\n\nThey have more or less the same effect in most cases: conflicting instances are flagged, and there is no ambiguity left."}
{"questionId":"3170b738c30e41f3a28870b6e7646ec5","question":"Error: async\/await is not yet supported in Client Components in next.js\nI am using next.js on \"next\": \"13.4.19\".\nthe project structure is\n--app\n-- layout.tsx\n-- page.tsx\n-- [id]\n--page.tsx\n\n\nin the [id] page.tsx,\n\n\n\"use client\"\n\n\n\n```\nimport { Editor } from '@\/components\/editor';\nimport { useState, useRef, useEffect, useMemo } from 'react'\n\nexport default async function PipelineDesignerEditorPage(\n  { params }: { params: { pipelineId: string } }\n) {\n  console.log('params.pipelineId',params.pipelineId);\n\n  const [loding, setLoding] = useState(false);\n  const [pipelineData, setPipelineData] = useState({});\n\n  useEffect(() => {\n    setLoding(true);\n    let data = getPipeline(params.pipelineId);\n    setPipelineData(data);\n    setLoding(false);\n  }, []);\n\n  return (\n    <div style={{ width: '100%', height: `calc(100vh - 65px)` }}>\n      <Editor pipeline={pipeline} \/>\n    <\/div>\n  )\n}\n\n```\n\nan error **'Error: async\/await is not yet supported in Client Components, only Server Components. This error is often caused by accidentally adding `'use client'` to a module that was originally written for the server.'** appears.\n\n\nI found this page is being rendered in the server side, so I modified a bit\n\n\n\n```\n'user client'\nimport { Editor } from '@\/components\/editor';\nimport { getPipeline } from '@\/lib\/pipelines\/storage';\nimport { useState, useRef, useEffect, useMemo } from 'react'\n\nexport default async function PipelineDesignerEditorPage(\n  { params }: { params: { pipelineId: string } }\n) {\n  console.log('params.pipelineId',params.pipelineId);\n  const pipeline = await getPipeline(params.pipelineId);\n  \n  const [loding, setLoding] = useState(false);\n  useEffect(() => {\n    console.log('useEffect');\n    setLoding(true);\n  }, []);\n\n  return (\n    <div style={{ width: '100%', height: `calc(100vh - 65px)` }}>\n      <Editor pipeline={pipeline} \/>\n    <\/div>\n  )\n}\n\n```\n\nit still doesn't work unless useEffect and useState is removed away.\n\n\nDoes it mean I can't use useState and useEffect in app->[id]->page.tsx, what about click, loading actions which needs to use useState and useEffect","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"javascript"},"answer":"You are mixing client and server components. As the error says, async\/await is only supported in server component (without `\"use client\"`). But, as you mentioned, `useState` and `useEffect` (or events like click) etc... are only supported in client components.\n\n\nThe solution is to split the 2 in 2 different components. Typically the `page.tsx` would be a server component where you fetch data and pass those to a child client component as parameter(s) where you can have state and events if needed.\n\n\nOn a specific note, you probably should have state and effect in the `Editor` or look at `Suspense`. See example under <https:\/\/nextjs.org\/docs\/app\/building-your-application\/routing\/loading-ui-and-streaming#example>"}
{"questionId":"85e644e81af64f2096d92255f41d15e9","question":"Is it possible, using PHOAS, to evaluate a term to normal form, and then stringify it?\nFrom [this](https:\/\/mail.haskell.org\/pipermail\/haskell-cafe\/2008-November\/050768.html) Haskell Cafe post, and borrowing some code examples from [jyp](https:\/\/github.com\/jyp\/topics\/tree\/master\/PHOAS), we can construct a simple PHOAS evaluator in Haskell as:\n\n\n\n```\n{-# LANGUAGE GADTs #-}\n{-# LANGUAGE RankNTypes #-}\n\nimport Data.Char\n\ndata Term v t where\n   Var :: v t -> Term v t\n   App :: Term v (a -> b) -> Term v a -> Term v b\n   Lam :: (v a -> Term v b) -> Term v (a -> b)\n\ndata Exp t = Exp (forall v. Term v t)\n\n-- An evaluator\neval :: Exp t -> t\neval (Exp e) = evalP e\n\ndata Id a = Id {fromId :: a}\n\nevalP :: Term Id t -> t\nevalP (Var (Id a)) = a\nevalP (App e1 e2)  = evalP e1 $ evalP e2\nevalP (Lam f)      = \\a -> evalP (f (Id a))\n\ndata K t a = K t\n\nshowTermGo :: Int -> Term (K Int) t -> String\nshowTermGo _ (Var (K i)) = \"x\" ++ show i\nshowTermGo d (App f x)   = \"(\" ++ showTermGo d f ++ \" \" ++ showTermGo d x ++ \")\"\nshowTermGo d (Lam a)     = \"@x\" ++ show d ++ \" \" ++ showTermGo (d+1) (a (K d))\n\nshowTerm :: Exp t -> String\nshowTerm (Exp e) = showTermGo 0 e\n\n```\n\nThis implementation allows us to create, normalize and stringify \u03bb-calculus terms. The problem is, `eval` has type `Exp t -> t` rather than `Exp t -> Exp t`. As such, it isn't clear to me how to evaluate a term to normal form, and then stringify it. Is that possible with PHOAS?","questionMetadata":{"Type":"Debugging","Level":"Advanced","Tag":"haskell"},"answer":"Let's start by trying the most naive thing:\n\n\n\n```\nevalP' :: Term v a -> Term v a\nevalP' (Var x) = Var x\nevalP' (App x y) =\n  case (evalP' x, evalP' y) of\n    (Lam f, y') -> f (_ y')\n    (x', y') -> App x' y'\nevalP' (Lam f) = Lam (evalP' . f)\n\n```\n\nWe get stuck at that hole because we need a function `Term v a -> v a`, so now we know that we should chose the `v` such that it contains `Term v`. We can chose `v ~ Term v`, but you can't use recursive types directly like that so you need to create a new data type:\n\n\n\n```\ndata FixTerm a = Fix (Term FixTerm a)\n\n```\n\n(I believe the `FixTerm` type is isomorphic to the non-parametric HOAS type.)\n\n\nNow we can use that to define our evaluation function:\n\n\n\n```\nevalP' :: Term FixTerm a -> Term FixTerm a\nevalP' (Var (Fix x)) = evalP' x\nevalP' (App x y) =\n  case (evalP' x, evalP' y) of\n    (Lam f, y') -> f (Fix y')\n    (x', y') -> App x' y'\nevalP' (Lam f) = Lam (evalP' . f)\n\n```\n\nThis works, but unfortunately we can't recover the original `Term v a` from this. It's easy to see that because it never produces a `Var` constructor. We can again try and see where we get stuck:\n\n\n\n```\nfrom :: Term FixTerm a -> Term v a\nfrom (Var (Fix x)) = from x\nfrom (App x y) = App (from x) (from y)\nfrom (Lam f) = Lam (\\x -> from (f (_ x)))\n\n```\n\nThis time we need a function `v a -> FixTerm a`. To be able to do that we can add a case to the `FixTerm` data type, which is reminiscent of the free monad type:\n\n\n\n```\ndata FreeTerm v a = Pure (v a) | Free (Term (FreeTerm v) a)\n\nevalP' :: Term (FreeTerm v) a -> Term (FreeTerm v) a\nevalP' (Var (Pure x)) = Var (Pure x)\nevalP' (Var (Free x)) = evalP' x\nevalP' (App x y) =\n  case (evalP' x, evalP' y) of\n    (Lam f, y') -> f (Free y')\n    (x', y') -> App x' y'\nevalP' (Lam f) = Lam (evalP' . f)\n\nfrom :: Term (FreeTerm v) a -> Term v a\nfrom (Var (Pure x)) = Var x\nfrom (Var (Free x)) = from x\nfrom (App x y) = App (from x) (from y)\nfrom (Lam f) = Lam (\\x -> from (f (Pure x)))\n\n```\n\nNow we can define the top-level eval:\n\n\n\n```\neval' :: Exp a -> Exp a\neval' (Exp x) = Exp (from (evalP' x))"}
{"questionId":"4b691bb148364cc794311bc78e035d23","question":"Firestore update method throw error: \"Error: 13 INTERNAL: Received RST\\_STREAM with code 1\"\nWhile using Cloud Functions, we've encountered the following error:\n\n\nTimestamp: 2023-10-21 18:50:18.281 EEST\nFunction: v8-specialist\n\n\n\n```\n---updateUserByID finish update---\n\nCaused by: Error \n    at WriteBatch.commit (\/workspace\/node_modules\/firebase-admin\/node_modules\/@google-cloud\/firestore\/build\/src\/write-batch.js:433:23) \n    at DocumentReference.update (\/workspace\/node_modules\/firebase-admin\/node_modules\/@google-cloud\/firestore\/build\/src\/reference.js:433:14) \n    at Object.updateUserByID (\/workspace\/dist\/src\/DB\/db.js:74:14) \n    at createSpecialistOrderListService (\/workspace\/dist\/src\/crud\/specialist\/services\/specialistList\/createSpecialistOrderListService.js:38:29) \n    at runMicrotasks (<anonymous>) \n    at processTicksAndRejections (node:internal\/process\/task_queues:96:5) \n    at async getRecommendedSpecialistsListController (\/workspace\/dist\/src\/crud\/specialist\/controllers\/getRecommendedSpecialistsListController.js:25:44) \n    at async \/workspace\/dist\/src\/framework\/express\/middlewares\/express-handler.js:18:36 \n\nError Details:\n- Code: `13`\n- Description: `Received RST_STREAM with code 1`\n- Metadata: `{ internalRepr: Map(0) {}, options: {} }`\n- Note: `Exception occurred in retry method that was not classified as transient`\n\n```\n\nThis error seems to pop up when we execute the following command in our update function:\n\n\n\n```\nconst writeResult = await admin\n      .firestore()\n      .collection(FirestoreCollections.Users)\n      .doc(userID)\n      .update(fieldsToUpdate);\n\n```\n\nExample of `fieldsToUpdate`:\n\n\n\n```\n[\n  {\n    \"boolean\": true,\n    \"number\": 100,\n    \"id\": \"some_id\"\n  }\n]\n\n```\n\nHowever, what's puzzling is that this method seems to work flawlessly in our other cloud functions. In certain situations, even if an error is thrown during the update, the data in Firestore might still get updated.\n\n\n1. The issue persists even when tested locally.\n2. Upon creating a new cloud function with the same method, everything operates smoothly.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"javascript"},"answer":"I had the same problem last week and it seems to be something inside firebase \/ grpc implementation related to long time delays between firebase calls.\n\n\nAlso, firebase library seems to be moving away from RPC but it still keeps it as a default option if you don't set `preferRest: true` ([see docs](https:\/\/firebase.google.com\/docs\/reference\/admin\/node\/firebase-admin.firestore.firestoresettings))\n\n\nFor me it works when I call init right before performing an operation or changing firebase config to prefer using rest comunication.\n\n\nHere is my code snipet:\n\n\n\n```\nfunction getFbDb() {\n  const mainFirebaseApp = firebaseAdmin.initializeApp({\n      credential: firebaseAdmin.credential.applicationDefault()\n    }, uuid.v4());\n\n  const db = mainFirebaseApp.firestore();\n  const settings = {\n    preferRest: true,\n    timestampsInSnapshots: true\n  };\n  db.settings(settings);\n  return { db, firebaseApp: mainFirebaseApp };\n}\nconst { db, firebaseApp } = getFbDb();\n\n```\n\nUse `const { db, firebaseApp } = getFbDb();` everytime you have to execute an operation.\n\n\nAnother *\"dirty option\"* seems to be using a retry approach after the first fail.\n\n\nIf this doesn't work for you keep an eye on issue [2345](https:\/\/github.com\/firebase\/firebase-admin-node\/issues\/2345) and see what comes up from there."}
{"questionId":"a0427a23d8fa4745987cbe56e3af9d91","question":"Why am I encountering the error 'Starting FGS without a type' when executing the code on Android 14?\nThe Code A works well in Android 13 or low, but I get the Error A when I run it in Android 14?\n\n\nHow can I fix it?\n\n\n**Code A**\n\n\n\n```\n private fun startForegroundService() {\n        val builder = if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.O) {            \n            val  mChannelName = getString(R.string.app_name)\n            val notificationManager = this.getSystemService(Context.NOTIFICATION_SERVICE) as NotificationManager\n\n            val notificationChannel = NotificationChannel(\n                CHANNEL_ID,\n                mChannelName,\n                NotificationManager.IMPORTANCE_LOW \n            )\n            notificationManager.createNotificationChannel(notificationChannel)\n            NotificationCompat.Builder(this, notificationChannel.id)\n        } else {\n            NotificationCompat.Builder(this)\n        }\n\n        val myIntent = Intent(this, ActivityMain::class.java)        \n     \n        myIntent.addFlags( Intent.FLAG_ACTIVITY_NEW_TASK or Intent.FLAG_ACTIVITY_CLEAR_TOP)\n        \n        val pendingIntent = PendingIntent.getActivity(\n            this,\n            0,\n            myIntent,\n            PendingIntent.FLAG_IMMUTABLE or PendingIntent.FLAG_UPDATE_CURRENT\n        )\n\n        builder.setSmallIcon(R.drawable.notify_icon)            \n            .setContentTitle(getString(R.string.notificationTitle))\n            .setTicker(getString(R.string.notificationTicker ))      \/\/It will show text on status bar, even without having user to \"pull down\" the incoming notification.\n            .setContentText(getString(R.string.notificationContent))\n            .setContentIntent(pendingIntent)\n\n        val notification = builder.build()       \n        notification.flags = notification.flags or NotificationCompat.FLAG_ONGOING_EVENT or NotificationCompat.FLAG_NO_CLEAR\n\n        startForeground(125, notification)\n    }\n\n```\n\n**Error A**\n\n\n\n```\n  android.app.MissingForegroundServiceTypeException: Starting FGS without a type  callerApp=ProcessRecord{8eb0601 12195:com.hicalc.soundrecorder\/u0a190} targetSDK=34\n                                                                                                        at android.app.MissingForegroundServiceTypeException$1.createFromParcel(MissingForegroundServiceTypeException.java:53)\n                                                                                                        at android.app.MissingForegroundServiceTypeException$1.createFromParcel(MissingForegroundServiceTypeException.java:49)\n                                                                                                        at android.os.Parcel.readParcelableInternal(Parcel.java:4870)","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"kotlin"},"answer":"From Android 14 (sdk 34) you have to set foreground service type.\n<https:\/\/developer.android.com\/about\/versions\/14\/changes\/fgs-types-required#permission-for-fgs-type>\n\n\nCheck SDK version when attempting to go foreground:\n\n\n\n```\nif (Build.VERSION.SDK_INT < Build.VERSION_CODES.TIRAMISU) {\n        startForeground(SERVICE_ID, notification)\n} else {\n        startForeground(SERVICE_ID, notification, \nFOREGROUND_SERVICE_TYPE_MEDIA_PLAYBACK)\n}\n\n```\n\nand update the manifest:\n\n\n\n```\n<uses-permission\n    android:name=\"android.permission.FOREGROUND_SERVICE_xxx\"\n    android:minSdkVersion=\"34\" \/>\n\n\n<application ...>\n    <service\n        android:name=\".feature.exerciseplayer.data.service.YourService\"\n        android:exported=\"true\"\n        android:foregroundServiceType=\"xxx\" \/>\n\n```\n\nNote: replace `xxx` by the service type that suits you."}
{"questionId":"bdc5773f29064b159832eb432ded07ed","question":"Efficiently finding all matches of vector in lookup table, with repeats\nI want to find indices of all matches of a vector `x` in another lookup vector `table`.\n\n\n\n```\ntable = rep(1:5, each=3)\nx = c(2, 5, 2, 6)\n\n```\n\nStandard base R methods don't quite give me what I want. For example using `which(table %in% x)` we only get the matching indices once, even though `2` appears twice in `x`\n\n\n\n```\nwhich(table %in% x)\n# [1]  4  5  6 13 14 15\n\n```\n\nOn the other hand, `match` returns values for every occurrence of x that has a match, but only returns the first index in the lookup table.\n\n\n\n```\nmatch(x, table)\n# [1]  4 13  4 NA\n\n```\n\nWhat I want is a function that returns the indices for \"all x and all y\". I.e. it should return the following desired result:\n\n\n\n```\nmymatch(x, table)\n# c(4, 5, 6, 13, 14, 15, 4, 5, 6)\n\n```\n\nWe can, of course, do this with a loop in R:\n\n\n\n```\nmymatch = function(x, table) {\n  matches = sapply(x, \\(xx) which(table %in% xx)) \n  unlist(matches)\n}\n\nmymatch(x, table)\n# [1]  4  5  6 13 14 15  4  5  6\n\n```\n\nBut this is horribly slow on larger data (I need to do this operation many times on big data)\n\n\n\n```\ntable = rep(1:1e5, each=10)\nx = sample(1:100, 1000, replace = TRUE)\nsystem.time(mymatch(x, table))\n#  user  system elapsed \n# 3.279   2.881   6.157 \n\n```\n\nThis is very slow if we compare it to e.g. `which %in%`:\n\n\n\n```\nsystem.time(which(table %in% x))\n#  user  system elapsed \n# 0.003   0.004   0.008 \n\n```\n\nHoping there is a fast way to do this in R? Otherwise, maybe RCpp is the way to go.","questionMetadata":{"Type":"Optimization","Level":"Intermediate","Tag":"r"},"answer":"Another way is to use `split`:\n\n\n\n```\nunlist(split(seq(table), table)[as.character(x)],use.names = FALSE)\n[1]  4  5  6 13 14 15  4  5  6\n\n```\n\n\n\n---\n\n\nEdit:\n\n\nNote that if `table` is sorted, then you could use `rle + sequence`:\n\n\n\n```\nfaster <- function(x, table){\n  a <- rle(table)\n  n <- length(a$lengths)\n  idx <- match(x, a$values, 0)\n  sequence(a$lengths[idx], cumsum(c(1,a$lengths[-n]))[idx])\n}\n\nset.seed(42)\ntable = rep(1:1e5, each=10)\nx = sample(1:100, 1000, replace = TRUE)\nbench::mark(\n   faster(x, table),\n   #mymatch(x, table) |> as.vector(),\n   join_match(x, table),\n   #unlist(split(seq(table), table)[as.character(x)],use.names = FALSE),\n   check = TRUE\n )\n\n\n# A tibble: 2 \u00d7 13\n  expression     min median `itr\/sec` mem_alloc `gc\/sec` n_itr  n_gc total_time result memory    \n  <bch:expr> <bch:t> <bch:>     <dbl> <bch:byt>    <dbl> <int> <dbl>   <bch:tm> <list> <list>    \n1 faster(x,\u2026  54.4ms  252ms      3.97    54.9MB     1.99     2     1      503ms <int>  <Rprofmem>\n2 join_matc\u2026 127.7ms  254ms      3.93    88.8MB     5.90     2     3      508ms <int>  <Rprofmem>\n# \u2139 2 more variables: time <list>, gc <list>\n\n```\n\n\n\n---\n\n\nThe function works as long as table is sorted. Not necessarily from 1:n.\n\n\n\n```\ntable = c(rep(1:5, each=3), 7,7,7,7,10,10)\nx = c(10, 2, 5,7, 2, 6)\n\nmicrobenchmark::microbenchmark(\n   faster(x, table),\n   #mymatch(x, table) |> as.vector(),\n   join_match(x, table),\n   #unlist(split(seq(table), table)[as.character(x)],use.names = FALSE),\n   check = 'equal'\n )\nUnit: microseconds\n                 expr      min       lq       mean   median       uq       max neval\n     faster(x, table)   23.001   32.751   56.95703   56.400   66.201   222.901   100\n join_match(x, table) 4216.201 4925.302 6616.51401 5572.951 7842.200 21153.402   100"}
{"questionId":"9b5357dbbe8a4761ae019c021d6961ff","question":"Can one forward-declare a function taking a vector of incomplete type with a default value?\nThe code snippet below demonstrates a real issue I faced recently in my program:\n\n\n\n```\n#include<vector>\n\nclass A;\n\nvoid f( const std::vector<A> & = {} );\n\n```\n\nThere is an incomplete class `A`, and a function declaration taking a `vector` of `A`'s with empty default value. And the function is not even called anywhere.\n\n\nIt works fine in GCC, and in Clang 14, but starting from Clang 15 an error appears:\n\n\n\n```\nIn file included from <source>:1:\n\/opt\/compiler-explorer\/clang-15.0.0\/bin\/..\/include\/c++\/v1\/vector:540:52: error: arithmetic on a pointer to an incomplete type 'A'\n        {return static_cast<size_type>(__end_cap() - this->__begin_);}\n                                       ~~~~~~~~~~~ ^\n\/opt\/compiler-explorer\/clang-15.0.0\/bin\/..\/include\/c++\/v1\/vector:760:56: note: in instantiation of member function 'std::vector<A>::capacity' requested here\n      __annotate_contiguous_container(data(), data() + capacity(),\n                                                       ^\n\/opt\/compiler-explorer\/clang-15.0.0\/bin\/..\/include\/c++\/v1\/vector:431:7: note: in instantiation of member function 'std::vector<A>::__annotate_delete' requested here\n      __annotate_delete();\n      ^\n<source>:5:32: note: in instantiation of member function 'std::vector<A>::~vector' requested here\nvoid f( const std::vector<A> & = {} );\n                               ^\n<source>:3:7: note: forward declaration of 'A'\nclass A;\n      ^\n\n```\n\nOnline demo: <https:\/\/godbolt.org\/z\/a8xzshbzP>\n\n\nAre newer versions of Clang correct in rejecting the program?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"Yes, Clang is correct to reject the program. Per [vector.overview#4](http:\/\/eel.is\/c++draft\/vector.overview#4):\n\n\n\n> \n> An incomplete type `T` may be used when instantiating `vector` if the allocator meets the allocator completeness requirements. **`T` shall be complete before any member of the resulting specialization of `vector` is referenced.**\n> \n> \n> \n\n\nIn the default argument of `f` you're referencing a constructor of `vector<A>` before `A` is complete, so the program is ill-formed.\n\n\nHere's a [bug report](https:\/\/github.com\/llvm\/llvm-project\/issues\/57700) (closed as invalid) showing a similar situation. The comment at the bottom suggests why this may have changed in Clang-15.\n\n\n\n> \n> Probably what changed between libc++14 and libc++15 is that the vector move constructor became constexpr, so it's now getting instantiated earlier.\n> \n> \n>"}
{"questionId":"72e099045d534aa0bb816bf17267d9d9","question":"MultiQC: ModuleNotFoundError: No module named 'imp'\n[enter image description here](https:\/\/i.stack.imgur.com\/XIROo.png)\n\n\nI am running **fastqc** and **multiqc** in ubuntu linux terminal. fastqc runs perfectly without any issues but multiqc fails to run, showing the message. No idea how to fix the missing **'imp'** module. I tried to read and apply every solution found in the internet or google.\n\n\nI used the command **'conda install multiqc'** to install multiqc in the existing conda environment. I tried to install it in a new conda environment. Still, its showing the same message.The **python** version currently running is **3.12.0**\n\n\nCould anyone help to fix the issue?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"Python 3.12 is new and the consequences of the changes it introduces (like dropping the `imp` module) need to propagate to the community.\n\n\nStay on Python 3.11 for now."}
{"questionId":"6bdd36481154469693174548e86abeda","question":"Is `import std;` a realistic goal for a C++ project with transitive standard library `#include`s?\nVisual Studio 2022 17.5 and later supports importing the C++23 standard library named module `std` by replacing all instances of `#include <header>` with `import std;` where `<header>` is a standard library header.\n\n\nAccording to the [tutorial](https:\/\/learn.microsoft.com\/en-us\/cpp\/cpp\/tutorial-import-stl-named-module?view=msvc-170) there are some limitations, such as the following:\n\n\n\n> \n> Don't mix and match importing C++ standard library header files and named modules. For example, don't #include <vector> and import std; in the same file.\n> \n> \n> \n\n\nDoes this mean that if I use `import std;` in a file (by \"file\" I assume that the tutorial is referring to a `.cpp` file or other translation unit), then none of the non-standard library headers that it `#include`s is allowed to transitively `#include` a standard library header?\n\n\nI'm pretty sure the answer is obviously yes, because after preprocessing, it doesn't really matter whether an `#include` is transitive or not - its contents will end up in the `.cpp` file mixed with the `import` statement.\n\n\nBut then what can be done to prevent mixing besides an exhaustive search of all transitive `#include`s to see if there are any standard library headers? Especially if a project depends on something like Boost, it doesn't seem that `import std;` will be a realistic option until Boost is itself available as a module, right?\n\n\nSo in the meantime, am I right that `import std;` is basically only realistic for new or small projects where the standard library header dependencies (both direct and transitive) can be fully tracked and migrated at the same time? Or is there a way to somehow migrate a project to `import std;` even if some of its dependencies have not yet migrated?\n\n\nIf I compile the following code with Microsoft (R) C\/C++ Optimizing Compiler Version 19.37.32824 for x64 then I get C2572, which indeed suggests that transitive `#include`s cannot be mixed with `import std;`.\n\n\nheader.hpp\n\n\n\n```\n#include <iostream>\n\n```\n\nmain.cpp\n\n\n\n```\n#include \"header.hpp\"\nimport std;\nint main() {\n  std::cout << \"Hello, world!\\n\";\n}\n\n```\n\n\n```\nC:\\Code\\import-std-test>cl \/std:c++latest \/EHsc \/nologo \/W4 \/MTd \"%VCToolsInstallDir%\\modules\\std.ixx\" main.cpp\nstd.ixx\nmain.cpp\nC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.37.32822\\include\\xtr1common(42): error C2572: 'std::enable_if': redefinition of default argument: parameter 1\nC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.37.32822\\include\\xtr1common(43): note: see declaration of 'std::enable_if'\nmain.cpp(5): fatal error C1117: unrecoverable error importing module 'std': symbol 'byte' has already been defined\nGenerating Code...","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"cpp"},"answer":"Thanks to @BoP's [comment](https:\/\/stackoverflow.com\/questions\/77220989\/is-import-std-a-realistic-goal-for-a-c-project-with-transitive-standard-lib#comment136147907_77220989) it's now clear that mixing `import std;` and `#include <header>` is supposed to work but simply hasn't been implemented yet in MSVC (a fact that unfortunately is not mentioned in the [tutorial](https:\/\/learn.microsoft.com\/en-us\/cpp\/cpp\/tutorial-import-stl-named-module?view=msvc-170)). The linked talk doesn't mention when it will be available, but it sounds like Microsoft is actively working on it.\n\n\nSo to answer the original question: no, `import std;` is not a realistic goal at the moment for most projects, but yes, it will be when MSVC fully implements the standard, which allows for mixing the two."}
{"questionId":"d1d63da8b0b746a6b54d21430c005f9d","question":"EF Core error Incorrect syntax near '$' but EF Core generated the query?\nUsing .NET 8 RC2 and EF Core\n\n\nI created a dynamic query and executed the query which results in the following error\n\n\n\n```\nMicrosoft.Data.SqlClient.SqlException (0x80131904): Incorrect syntax near '$'.\nIncorrect syntax near '$'.\nIncorrect syntax near '$'.\n\n```\n\nIf I run `.ToQueryString()` to see the query generated I get this.\n\n\n\n```\nDECLARE @__startDate_0 smalldatetime = '2023-10-26T00:00:00';\nDECLARE @__endDate_1 smalldatetime = '2023-10-26T00:00:00';\nDECLARE @__siteNos_2 nvarchar(4000) = N'[15,42,56,74,89,98,102,104,109,113,114,115,116,118,120,121,122,123,124,124,125,127,128,129,130,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,156,157,158,159,161,162,163,164,165,167,169,170,171,172,173,175,176,177,178,179,180,181,182,183,185,186,187,188,189,190,191,192,193,194,195,196,197,199,200,201,202,205,206,207,208,209,210,211,212,213,214,215,216,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,244,245,246,247,248,249,250,251,252,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,295,296,296,297,300,301,303,304,305,306,307,308,309,310,311,312,313,316,317,318,319,320,321,322,323,324,325,326,328,331,333,334,335,338,339,340,341,342,343]';\nDECLARE @__startTime_3 time = '07:00:00';  \nDECLARE @__endTime_4 time = '22:00:00';  \nDECLARE @__upperLimit_5 smallint = CAST(30000 AS smallint);  \nDECLARE @__lowerLimit_6 smallint = CAST(0 AS smallint);\nDECLARE @__stationIds_7 nvarchar(4000) = N'[117,116,98]';\nDECLARE @__courseIds_8 nvarchar(4000) = N'[7]';\n\nSELECT [o].[order_no], [o].[Bumped], [o].[Check_no], [o].[checkback], [o].[Course], [o].[Expoed], [o].[item_count], [o].[manual_hold], [o].[ODate], [o].[Order_DateTime], [o].[Order_Index], [o].[Server_no], [o].[Site_no], [o].[StartTime], [o].[Station_no], [o].[Table_no]\nFROM [Orders] AS [o]\nWHERE [o].[Order_DateTime] IS NOT NULL AND CONVERT(date, [o].[Order_DateTime]) >= @__startDate_0 AND CONVERT(date, [o].[Order_DateTime]) <= @__endDate_1 AND [o].[Site_no] IN (\nSELECT [s].[value]\nFROM OPENJSON(@__siteNos_2) WITH ([value] smallint '$') AS [s]  \n) AND [o].[item_count] > CAST(0 AS tinyint) AND CONVERT(time, [o].[Order_DateTime]) >= @__startTime_3 AND CONVERT(time, [o].[Order_DateTime]) <= @__endTime_4 AND [o].[Expoed] <= @__upperLimit_5 AND [o].[Expoed] >= @__lowerLimit_6 AND CAST([o].[Station_no] AS smallint) NOT IN (\nSELECT [s0].[value]\nFROM OPENJSON(@__stationIds_7) WITH ([value] smallint '$') AS [s0]\n ) AND CAST([o].[Course] AS smallint) IN (\nSELECT [c].[value]\nFROM OPENJSON(@__courseIds_8) WITH ([value] smallint '$') AS [c]\n)\n\n```\n\nI can see the '$' which I am assuming EF Core is complaining about but EF core generated the query so how would I fix this?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"csharp"},"answer":"This is due to breaking change in the query translation for EF Core 8 - see [`Contains` in LINQ queries may stop working on older SQL Server versions](https:\/\/learn.microsoft.com\/en-us\/ef\/core\/what-is-new\/ef-core-8.0\/breaking-changes#contains-in-linq-queries-may-stop-working-on-older-sql-server-versions).\n\n\nCheck the compatibility level of the database:\n\n\n\n```\nSELECT compatibility_level\nFROM sys.databases\nWHERE name = 'mydbname';\n\n```\n\n\n> \n> I see some databases are 110 and newer ones we created are 150\n> \n> \n> \n\n\nPass the values to the context options (when registering in DI with `AddDbContext` for example):\n\n\n\n```\nopts\n   .UseSqlServer(@\"<CONNECTION STRING>\", o => o.UseCompatibilityLevel(110)) \/\/ or 150\n\n```\n\nSee the [Mitigations](https:\/\/learn.microsoft.com\/en-us\/ef\/core\/what-is-new\/ef-core-8.0\/breaking-changes#mitigations) part of the breaking change doc.\n\n\nAlso I would argue that you should consider upgrading the database (and server if needed) to support new features (see [`ALTER DATABASE SET COMPATIBILITY_LEVEL`](https:\/\/learn.microsoft.com\/en-us\/sql\/t-sql\/statements\/alter-database-transact-sql-compatibility-level?view=sql-server-ver16))."}
{"questionId":"d636571cd8cd44f1a044a4bfd27632ea","question":"Are C++ compilers actually compliant with zero-size array SFINAE rule?\nAbout a year or two ago I read about SFINAE rules in C++. They [state](https:\/\/en.cppreference.com\/w\/cpp\/language\/sfinae), in particular,\n\n\n\n> \n> The following type errors are SFINAE errors:\n> \n> \n> ...\n> \n> \n> attempting to create an array of void, array of reference, array of function, array of negative size, array of non-integral size, or array of size zero\n> \n> \n> \n\n\nI decided to use this rule in my homework, but it wouldn't work. Gradually reducing it, I came to this small example of code which I don't understand:\n\n\n\n```\n#include <iostream>\n\ntemplate<int I>\nstruct Char {};\n\ntemplate<int I>\nusing Failer = Char<I>[0];\n\ntemplate<int I>\nvoid y(Failer<I> = 0) {\n    std::cout << \"y<\" << I << \">, Failer version\\n\";\n}\n\ntemplate<int I>\nvoid y(int = 0) {\n    std::cout << \"y<\" << I << \">, int version\\n\";\n}\n\nint main() {\n    y<0>();\n    y<1>();\n    y<2>();\n    y<3>();\n}\n\n```\n\nMoreover, several C++ compilers seem to not understand it either. I created a [Godbolt example](https:\/\/godbolt.org\/z\/shPn6sh3c), where you can find three different compilers resolving the `y` ambiguity differently:\n\n\n- gcc reports a compilation error;\n- clang chooses the `int` version (this is what I would think complies with the SFINAE rule);\n- icc chooses the `Failer` version.\n\n\nWhich among them is correct, and what is actually going on?","questionMetadata":{"Type":"Debugging","Level":"Advanced","Tag":"cpp"},"answer":"### Validity of zero-size arrays\n\n\n[[dcl.array] p1](http:\/\/eel.is\/c++draft\/dcl.array#1) states that:\n\n\n\n> \n> [The *constant-expression*] `N` specifies the array bound, i.e., the number of elements in the array; `N` **shall be greater than zero**.\n> \n> \n> \n\n\nZero-size arrays are thus disallowed in principle.\nNote that your zero-size array appears in a function parameter, and this may be relevant according to [[dcl.fct] p5](http:\/\/eel.is\/c++draft\/dcl.fct#5):\n\n\n\n> \n> After determining the type of each parameter, any parameter of type \u201carray of `T`\u201d or of function type `T` is adjusted to be \u201cpointer to `T`\u201d.\n> \n> \n> \n\n\nHowever, this type adjustment rule only kicks on *after* determining the type of the parameters, and one parameter has type `Char<I>[0]`.\nThis should disqualify the first overload from being a candidate.\n**In fact, your program is IFNDR because no specialization of `y` would be well-formed (see [[temp.res.general] p6](http:\/\/eel.is\/c++draft\/temp.res.general#6)).**\n\n\nIt is not totally clear from the wording, but the first overload would be ill-formed despite the type adjustment, and both GCC and clang agree on this (see `-pedantic-errors` diagnostic triggering for `char[0]` parameters).\n\n\nEven if the compiler supports zero-size arrays as an extension, this isn't allowed to affect valid overload resolution according to [[intro.compliance.general] p8](http:\/\/eel.is\/c++draft\/intro.compliance.general#8):\n\n\n\n> \n> A conforming implementation may have extensions (including additional library functions), **provided they do not alter the behavior of any well-formed program**.\n> Implementations are required to diagnose programs that use such extensions that are ill-formed according to this document.\n> Having done so, however, they can compile and execute such programs.\n> \n> \n> \n\n\n### Conclusion\n\n\nYour program is IFNDR because no specialization of the first overload of `y` is valid.\nAll compilers are correct through their own extensions.\n\n\nHowever, if we assume that the first overload of `y` is valid, then it should not be a viable candidate during the call `y<N>()`, and should be removed from the overload set, even if zero-size arrays are supported as a compiler extension.\nOnly clang implements this correctly.\n\n\n### Interaction of default arguments and overload resolution\n\n\nIn this section, let's assume that zero-size arrays were allowed.\nThis is just for the sake of understanding the observed compiler behavior better.\n\n\nThen hypothetically, a call `y<N>(0)` is unambiguous, and all compilers agree and call the `int` overload.\nThis is because `int` requires no conversions, but a conversion from `0` to a pointer type would require pointer conversion.\n\n\nOverload resolution does not consider default arguments; see [Are default argument conversions considered in overload resolution?](https:\/\/stackoverflow.com\/q\/77142545\/5740428).\nThus hypothetically, both overloads of `y` are viable candidates for `y<N>()` and neither is a better match because neither is more specialized according to the rule of partial ordering of function templates. This is GCC's behavior.\n\n\n\n\n---\n\n\n*Note: both GCC's and Clang's behavior can be explained, where Clang is more correct looking past the IFNDR issue. I am unable to explain ICC's behavior; it makes no sense.*"}
{"questionId":"65ca3df61df74b2fbff54db04af200ae","question":"Every Lens' is a Traversal'... how?\n[Control.Lens.Tutorial](https:\/\/hackage.haskell.org\/package\/lens-tutorial-1.0.4\/docs\/Control-Lens-Tutorial.html#:%7E:text=exactly%20one%20value.-,Like,-Lens%27%2C) says:\n\n\n\n> \n> \n> ```\n> type Traversal' a b = forall f . Applicative f => (b -> f b) -> (a -> f a) \n> type Lens'      a b = forall f . Functor     f => (b -> f b) -> (a -> f a) \n> \n> ```\n> \n> Notice that the only difference between a Lens' and a Traversal' is the type class constraint. A Lens' has a Functor constraint and Traversal' has an Applicative constraint. This means that any Lens' is automatically also a valid Traversal' (since Functor is a superclass of Applicative).\n> \n> \n> \n\n\nI simply don't follow the logical sequence here. We know that every `Applicative` is a `Functor`. From this, should it not follow, if at all, that every `Traversal'` is a `Lens'`? The tutorial however reaches the converse conclusion.","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"haskell"},"answer":"A `Lens'` works for *all* functors, which includes all applicative functors, so if a function `g` is a `Lens'`, then `g` is also a `Traversal'`.\n\n\nA `Traversal'` works for all applicative functors, but not necessarily all functors. So if a function `h` is a `Traversal'`, it is not necessarily a `Lens'`.\n\n\n(I'm not the one to describe this formally, but to me at least, it looks like the types are \"contravariant\" in their constraints. `Lens'` is a subtype of `Traversal'` precisely *because* `Functor` is a superclass of `Applicative`.)"}
{"questionId":"299559fda585499eb76d4b5c62558c8d","question":"C++ Exception not caught on clang 17.0.3\nThis is a simplified version of a bug that I ran into today.\n\n\n\n```\n#include <iostream>\n#include <vector>\n#include <stdexcept>\n\nvoid run() {\n    std::vector<int> v;\n    throw std::runtime_error(\"error\");\n}\n\nint main() {\n    try {\n        run();\n    } catch (const std::exception &e) {\n        std::cerr << e.what() << std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n\n```\n\nI compiled and ran this code with Homebrew clang version 17.0.3 on an M1 Mac:\n\n\n\n```\nclang++ -std=c++20 -o main main.cpp\n\n```\n\nThe exception is not caught and the program exits with:\n\n\n\n```\nProcess finished with exit code 133 (interrupted by signal 5: SIGTRAP)\n\n```\n\nIf I remove `std::vector<int> v;` from the run function or compile it on Linux, then it works.\n\n\nEdit:\nSo, I was originally compiling through CLion using cmake which also added `-L\/opt\/homebrew\/opt\/llvm\/lib`.\n\n\nI tested it compiling manually, results:\n\n\n\n```\n# works\nclang++ -std=c++20 -o main main.cpp\n\n# crashes with c++11, c++17 and c++20\nclang++ -std=c++20 -O0 -o main -L\/opt\/homebrew\/opt\/llvm\/lib main.cpp\n\n# works\nclang++ -std=c++20 -O1 -o main -L\/opt\/homebrew\/opt\/llvm\/lib main.cpp\n\n```\n\nGenerated assembly files: <https:\/\/drive.google.com\/drive\/folders\/13UKnC0mp6_3l_QULO62ts_UQHK1ZR23u?usp=share_link>\n\n\nMy guess is that -O1 simply stripped away creating the vector, because adding more code to make sure it's kept causes a trap.\n\n\nCreating an empty class inside `run()`, that has a destructor that prints something also causes this same problem.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"cpp"},"answer":"It was a misconfiguration error.\n\n\nExcerpt from `brew info llvm`.\n\n\n\n```\n==> Caveats\nTo use the bundled libc++ please add the following LDFLAGS:\n  LDFLAGS=\"-L\/opt\/homebrew\/opt\/llvm\/lib\/c++ -Wl,-rpath,\/opt\/homebrew\/opt\/llvm\/lib\/c++\"\n\nllvm is keg-only, which means it was not symlinked into \/opt\/homebrew,\nbecause macOS already provides this software and installing another version in\nparallel can cause all kinds of trouble.\n\nIf you need to have llvm first in your PATH, run:\n  echo 'export PATH=\"\/opt\/homebrew\/opt\/llvm\/bin:$PATH\"' >> ~\/.zshrc\n\nFor compilers to find llvm you may need to set:\n  export LDFLAGS=\"-L\/opt\/homebrew\/opt\/llvm\/lib\"\n  export CPPFLAGS=\"-I\/opt\/homebrew\/opt\/llvm\/include\"\n\n```\n\nUsing `-L\/opt\/homebrew\/opt\/llvm\/lib\/c++` the code runs as expected."}
{"questionId":"a48482bedcbb4ce385d428de96d82df5","question":"Does &'a T imply T: 'a?\nFrom my own understanding and experimentation this appears to be true, but I have yet to find an authoritative source that documents it. Rust by Example has a [bounds](https:\/\/doc.rust-lang.org\/rust-by-example\/scope\/lifetime\/lifetime_bounds.html) section where it says:\n\n\n\n> \n> `T: 'a`: All references in `T` must outlive lifetime `'a`.\n> \n> \n> \n> ```\n> #[derive(Debug)]\n> struct Ref<'a, T: 'a>(&'a T);\n> \/\/ `Ref` contains a reference to a generic type `T` that has\n> \/\/ an unknown lifetime `'a`. `T` is bounded such that any\n> \/\/ *references* in `T` must outlive `'a`. Additionally, the lifetime\n> \/\/ of `Ref` may not exceed `'a`.\n> \n> ```\n> \n> \n\n\nHowever, this looks like a poor demonstration since the `T: 'a` bound does not seem to affect the behavior of `Ref`. Any attempt I've made to construct a `T` that is shorter than `'a` is thwarted with or without `T: 'a`. Even more, a generic reference defined *without* the lifetime bound can be passed off as one *with* it:\n\n\n\n```\nfn f<'a, T>(r: &'a T) {\n    g(r) \/\/ this compiles\n}\n\nfn g<'a, T: 'a>(r: &'a T) {\n    \/\/ ...\n}\n\n```\n\nThe Rust Reference has a similar construction from some examples in the [generic parameters](https:\/\/doc.rust-lang.org\/reference\/items\/generics.html) section (`struct Ref<'a, T> where T: 'a { r: &'a T }`), however it does not elaborate. I have looked through the documentation there, those on references, and on lifetimes and cannot find a link.\n\n\nSo does `&'a T` imply `T: 'a`? If so, where is this documented? And why do these resources have this unnecessary constraint? If not, what are the rules?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"rust"},"answer":"Yes, `&'a T` does imply `T: 'a`.\n\n\nThere was always the requirement that referent had to outlive the referenced lifetime since it is necessary for safety by-construction. However, before Rust 1.31 the bound was not inferred and would have to be provided explicitly as shown in [these](https:\/\/stackoverflow.com\/questions\/38626644\/why-does-the-rust-compiler-request-i-constrain-a-generic-type-parameters-lifeti) [questions](https:\/\/stackoverflow.com\/questions\/41794800\/why-is-the-bound-t-a-required-in-order-to-store-a-reference-a-t).\n\n\nIt was [RFC #2093: infer outlives](https:\/\/github.com\/rust-lang\/rfcs\/blob\/master\/text\/2093-infer-outlives.md) that let the compiler to infer these bounds and thus allowed the user to elide them. Since then, the Rust by Example and Rust Reference snippets are over-specified and the `T: 'a` is no longer needed.\n\n\nSee elsewhere in the Reference on [trait and lifetime bounds](https:\/\/doc.rust-lang.org\/beta\/reference\/trait-bounds.html#implied-bounds):\n\n\n\n> \n> Lifetime bounds required for types to be well-formed are sometimes inferred.\n> \n> \n> \n> ```\n> fn requires_t_outlives_a<'a, T>(x: &'a T) {}\n> \n> ```\n> \n> The type parameter `T` is required to outlive `'a` for the type `&'a T` to be well-formed. This is inferred because the function signature contains the type `&'a T` which is only valid if `T: 'a` holds.\n> \n> \n> \n\n\n\n\n---\n\n\nThere is at least one place where `&'a T` does not automatically infer `T: 'a` and that is when defining associated types for a trait ([demo on the playground](https:\/\/play.rust-lang.org\/?version=stable&mode=debug&edition=2021&gist=6d4939f399c3b7bdc0eebb4174ca1212)) but the compiler will guide you into adding it explicitly:\n\n\n\n```\ntrait MakeRef<'a> {\n    type Type;\n}\n\nimpl<'a, T> MakeRef<'a> for Vec<T> {\n    type Type = &'a T;\n}\n\n```\n\n\n```\nerror[E0309]: the parameter type `T` may not live long enough\n --> src\/lib.rs:6:17\n  |\n6 |     type Type = &'a T;\n  |                 ^^^^^ ...so that the reference type `&'a T` does not outlive the data it points at\n  |\nhelp: consider adding an explicit lifetime bound...\n  |\n5 | impl<'a, T: 'a> MakeRef<'a> for Vec<T> {\n  |           ++++"}
{"questionId":"fb280c9c95de4134b11fc0e07c9a2e71","question":"Use of earlier member in designated initializer\nConsider the following code:\n\n\n\n```\nstruct Foo{\n    std::string s1;\n    std::string s2;\n};\n\nint main(){\n    Foo f{.s1 = \"s1\", .s2 = f.s1 + \"s2\"};\n    std::cout << \"s1='\" << f.s1 << \"', s2='\" << f.s2 << \"'\" << std::endl;\n}\n\n```\n\nEspecially note that the initialization of `s2` accesses the first member of `f`: `.s2 = f.s1 + \"s2\"`.\nThe latest clang, gcc and MSVC are happy with the code and give the naively expected result (they print `\"s1='s1', s2='s1s2'\"`). See live on [godbolt](https:\/\/godbolt.org\/z\/v9djWenoT).\n\n\nQuestion: Is this legal?\nIn other words, does the standard guarantee that `f.s1` gets initialized before the designated initializer `.s2` is evaluated?\n\n\nRelated: There is a [similar question](https:\/\/stackoverflow.com\/q\/64810143\/3740047) asking about whether `.s2 = .s1 + \"s2\"` is legal, which clearly isn't, because it does not compile. Also, [P0328](http:\/\/wg21.link\/p0329) (as per [this](https:\/\/stackoverflow.com\/a\/33829487\/3740047) answer) might be relevant, but I can't see my question being answered there.","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"cpp"},"answer":"In C++, the order is important. Member variables are initialized in order of declaration.\n\n\nInitializer lists, designated or not, follow this rule.\n\n\nIn your case, `s1` is declared before `s2` in `Foo`. Consequently, it will be initialized first.\n\n\nIf you had done the opposite (i.e. define `s1` based on `s2`) it would have been ~~ill-formed~~ *Undefined Behaviour* since you would have used a yet uninitialized member.\n\n\n\n\n---\n\n\nFrom [9.4.5 List-initialization (\u00a73.1)](https:\/\/eel.is\/c++draft\/dcl.init#list-3.1):\n\n\n\n> \n> *If the braced-init-list contains a designated-initializer-list and T is not a reference type, T shall be an aggregate class. The ordered identifiers in the designators of the designated-initializer-list shall form a subsequence of the ordered identifiers in the direct non-static data members of T. Aggregate initialization is performed ([dcl.init.aggr]).*\n> \n> \n> \n\n\nThen from [9.4.2 Aggregates (\u00a77)](https:\/\/eel.is\/c++draft\/dcl.init#aggr-7):\n\n\n\n> \n> *The initializations of the elements of the aggregate are evaluated in the element order. That is, all value computations and side effects associated with a given element are sequenced before those of any element that follows it in order.*\n> \n> \n>"}
{"questionId":"8c71ee7f3d6b434996c07c56eb548edd","question":"How to get the `< ... >` syntax on an object?\nIn Raku: when I create an object with a CALL-ME method, I would like to use the `< ... >` syntax for the signature instead of `( '...' )` when `...` is a Str.\nIn the documentation, there is an example of creating an operator, which is an ordinary Raku sub with a special syntax. This sort of sub automatically converts `< ... >` into a Str.\nFor example, if I have\n\n\n\n```\nuse v6.d;\nclass A {\n   has %.x;\n   method CALL-ME( Str:D $key ) { say %!x{ $key } }\n}\n\nmy A $q .= new( :x( <one two three> Z=> 1..* ) );\n$q('two');\n$q<two>;\n\n\n```\n\nBut this produces\n\n\n\n```\n2\nType A does not support associative indexing.\n  in block <unit> at test.raku line 10","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"perl"},"answer":"An answer based on @raiph's comment:\n\n\nYou need to implement the [`AT-KEY`](https:\/\/docs.raku.org\/language\/subscripts#method_AT-KEY) method on your type.\n\n\nYou can do so directly like this:\n\n\n\n```\nclass A {\n   has %.x;\n   method CALL-ME( Str:D $key ) { say %!x{ $key } }\n   method AT-KEY($key) { self.($key) }\n}\n\n```\n\nOr you can do so by delegating `AT-KEY` to `%!x` using the [`handles`](https:\/\/docs.raku.org\/language\/typesystem#trait_handles) trait:\n\n\n\n```\nclass A {\n   has %.x handles <AT-KEY>;\n   method CALL-ME( Str:D $key ) { say %!x{ $key } }\n}"}
{"questionId":"c18607033f0d42fdaf3e1915e5addf45","question":"Is there any way to disable \"resource saver mode\" in Docker desktop?\nWith Docker [4.22](https:\/\/www.docker.com\/blog\/docker-desktop-4-22\/) came a new \"Resource Saver Mode\". However, this is considerably slowing me down as it now takes **several minutes** to do any operations, such as running a container, when Docker is running in saver mode.\n\n\n**So, is there a way to disable Resource Saver Mode? Or is the only option to downgrade Docker desktop?**\n\n\n*Note: I am running Docker desktop version 4.22.1 on macOS.*","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"other"},"answer":"In Docker Desktop *for Windows*, you can go to **Settings** \/ **Features in development** \/ **Experimental features**, and uncheck **Access experimental features**, then **Apply & restart**.\n\n\nMy guess is that on Mac you have the same setting."}
{"questionId":"406624fede35436c9bd5b01a66fd3395","question":"Google Play Account Deletion Requirement\nMy app allows users to create accounts. I noticed in the Google Play console a message that I should provide the ability of account deletion. I have this functionality in my mobile app in the account section (the user should be logged in to delete his account obviously ). But the message says there should be a web URL with the same feature.\n\n\nIf your app allows users to create an account from within your app, then it must also allow users to request for their account to be deleted. Users must have a readily discoverable option to initiate app account deletion from within your app and outside of your app (for example, by visiting your website). A link to this web resource must be entered in the designated URL form field within Play Console.\n\n\nI don't understand clearly how it should be: just a link to the profile page with the delete account button, like it is in the mobile app, or some kind of form where the user may apply the deletion request?","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"other"},"answer":"It says that you should also provide a \"Delete account\" functionality on the website, where user can delete the account.\n\n\nWell, it can be possible in both ways. You either make it possible only for logged in users or directly just by entering the email or username or whatever credentials are needed for you to get the unique user account.\n\n\nSo, user can request their account to be deleted without logging in - I think this might be the right way to \"Account deletion feature\".\n\n\nIf user deletes his account from the app, then it's alright, but you should also provide the other option as well that I mentioned above because, what if user has forgotten the password and instead of creating a new password for continue using the app, he just wants to delete the account with just a username or email.\n\n\nSo, give it a try and it must work if done correctly."}
{"questionId":"7060062d21094f60b85e0347e7ac5d49","question":"Java 21 guarded pattern exhaustiveness\nI started to play with the new [Java 21](https:\/\/en.wikipedia.org\/wiki\/Java_version_history#Java_21) feature - [pattern matching](https:\/\/openjdk.org\/jeps\/441).\n\n\n\n```\npublic class Main {\n\n  public static void main(String[] args) {\n\n    RecordB recordB = new RecordB(true);\n\n    switch(recordB) {\n      case RecordB b when b.bool() -> System.out.println(\"It's true\");\n      case RecordB b when !b.bool() -> System.out.println(\"It's false\");\n    }\n\n  }\n\n  record RecordB(boolean bool) { }\n}\n\n```\n\nWhen compiling the code above, the compiler yields information that `the switch statement does not cover all possible input values`\n\n\nThis is not necessarily true from my perspective. So here is my question: does any guarded pattern always make the switch expression non-exhaustive for the compiler or I am missing something here?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"java"},"answer":"> \n> does any guarded pattern always make the switch expression non-exhaustive for the compiler or I am missing something here?\n> \n> \n> \n\n\nNo, it's not that \"*any* guarded pattern makes the switch statement non-exhaustive\", but there must be *at least* one unguarded pattern covering the type. If there is not *any* such unguarded pattern, it makes it non-exhaustive.\n\n\nMore formally (see [JLS 14.11.1.1. Exhaustive Switch Blocks](https:\/\/docs.oracle.com\/javase\/specs\/jls\/se21\/html\/jls-14.html#jls-14.11.1.1)),\n\n\n\n> \n> The switch block of a `switch` expression or `switch` statement is exhaustive for a selector expression `e` if one of the following cases applies:\n> \n> \n> - There is a `default` label associated with the switch block.\n> - There is a `case null, default` label associated with the switch block.\n> - The **set containing all the** `case` constants and **`case` patterns appearing**\n> **in an unguarded `case` label** (collectively known as `case` elements)\n> associated with the switch block **is non-empty** and covers the type of\n> the selector expression `e`.\n> \n> \n> \n\n\nThe key point here is **non-empty**.\n\n\nIn your case the *set containing the case patterns appearing in an unguarded case label is **empty***.\n\n\n\n\n---\n\n\nSide note:\n\n\nAfter [JEP 455 Primitive types in Patterns, instanceof, and switch](https:\/\/openjdk.org\/jeps\/455) is implemented, the switch statement could be exhaustive if a nested pattern is used to access `bool` value directly (see comments below from davidalayachew)."}
{"questionId":"52302eb883534539b75850619b6b9616","question":"SwiftUI DatePicker issue iOS 17.1\nThe code below is working fine in **iOS 17.0.1**, but in the latest update, **17.1** DatePicker does not open when we tap on it.\n\n\nAfter several tries, I found that on the long press, around 2-3 seconds, it opens the date picker.\n\n\nAny solution for this issue? How do I open only on tap?\n\n\n\n```\nVStack(alignment: .leading){\nText(\"Date\".localized())\n    .applyFont(style: .Medium, size: 18)\n    .foregroundColor(Colors.grayLight)\n        HStack{\n            let lastYear = Calendar.current.date(byAdding: .year, value: -1, to: Date()) ?? Date()\n                if let date = checkLowerBound(){\n                    DatePicker(\"\",selection:$selectedDate, in: date...Date(), displayedComponents: [.date])\n                        .frame(width: 100)\n                    }else{\n                    DatePicker(\"\",selection:$selectedDate, in: lowestDate...Date(), displayedComponents: [.date])\n                        .frame(width: 100)\n                    }\n                    Spacer()\n                    }\n                    .padding(.horizontal, 10)\n                }\n    .cardProperty()\n    .allowsHitTesting(!(eventTransaction != nil && eventTransaction?.by != UserDefaults.userID))","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"swift"},"answer":"I encountered a similar issue and found a solution. In my case, the problem arose when I used `onTapGesture` on the entire `ZStack` containing my content and called `view.endEditing()`. This issue occurred in version 17.1 but not in earlier versions."}
{"questionId":"a6eebf10fdc94efcac31b78360228245","question":"sorting a very large dataset in R\nI have this vector and I would like to sort it.\n\n\n`v1<-c(1,10,2,6,8,1,3,5,\"a\",\"ab\",\"c\",\"d\",\"aa\",\"abc\",\"a a\",\"a bc\")`.\n\n\nHowever the vector has both numeric and characters in it. So i need sorting numerically after that with the characters.\nthe code `library(gtools); mixedsort(v1)` solves the purpose exactly as required.\n\n\nBut I am applying it to a dataset of around 3million. So it isnt efficient and I need a code which processes it faster.\n\n\nSolution should be like this.\n\n\n\n```\n> mixedsort(v1)\n [1] \"1\"    \"1\"    \"2\"    \"3\"    \"5\"    \"6\"    \"8\"    \"10\"   \"a\"    \"a a\"  \"a bc\" \"aa\"   \"ab\"   \"abc\"  \"c\"    \"d\"  \n\n```\n\nThanks a lot","questionMetadata":{"Type":"Optimization","Level":"Intermediate","Tag":"r"},"answer":"Curiously, this kludgy dplyr\/readr approach is 27x faster than `mixedsort` and 3x faster than `stringr::str_sort`, so it's clear there is lots more room for doing this faster.\n\n\nSince then, I have tried `mixed_sort2` from @one and it's typically a smidge faster. Faster still is to use `as.numeric` in my original approach, to get about 100x as fast as `mixedsort`.\n\n\n\n```\n# fake data, 3 million rows of characters and numbers\nv1 <- sample(c(1:99, LETTERS), 3E6, replace = TRUE)\n\nlibrary(dplyr)\ndf_parse <- function(a) {\n  data.frame(a) %>%\n    mutate(num = readr::parse_number(a)) %>% # 4x faster if we use `as.numeric` instead\n    arrange(num, a) %>%\n    pull(a)\n}\n\n```\n\nedit: added from the comments @one's\n\n\n\n```\nmixed_sort2 <- function(vec){   \n  numeric_sort <- sort(as.numeric(vec))\n  character_sort <- sort(vec[is.na(as.numeric(vec))])\n  c(numeric_sort,character_sort)\n}\n\n```\n\nand using the dplyr approach + as.numeric is even faster:\n\n\n\n```\ndf_asnumeric <- function(a) {\n  data.frame(a) %>%\n    mutate(num = as.numeric(a)) %>%\n    arrange(num, a) %>%\n    pull(a)\n}\n\n```\n\n\n\n---\n\n\n\n```\nmicrobenchmark::microbenchmark(\n  gtools::mixedsort(v1),\n  stringr::str_sort(v1, numeric = TRUE),\n  df_parse(v1),\n  mixed_sort2(v1),\n  df_asnumeric(v1),\n  times = 10, check = 'identical'\n)\n\nUnit: milliseconds\n                                  expr        min         lq       mean     median         uq       max neval\n                 gtools::mixedsort(v1) 80676.6222 81825.6986 84672.9726 83671.3667 87335.5948 92225.717    10\n stringr::str_sort(v1, numeric = TRUE)  4309.6739  4327.6529  4371.6771  4373.5337  4411.2573  4450.610    10\n                          df_parse(v1)   777.4925  1031.4233  1133.8790  1052.9858  1073.2195  1683.851    10\n                       mixed_sort2(v1)  3505.1529  3539.8577  3747.5667  3613.3492  4033.8569  4141.554    10\n                      df_asnumeric(v1)   619.2090   635.7193   812.5554   864.3629   908.5028  1026.983    10"}
{"questionId":"ad329efffba843dfab9574667fd8f35f","question":"Are function parameter objects temporary objects?\nThe current C++ standard draft in [[class.temporary]\/7](https:\/\/eel.is\/c++draft\/class.temporary#7) contains the phrase\n\n\n\n> \n> a temporary object other than a function parameter object\n> \n> \n> \n\n\nI was under the impression that function parameter objects are not temporary objects. However, this phrase was added relatively recently. So am I mistaken or misunderstanding the context?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"An [example](https:\/\/eel.is\/c++draft\/stmt.ranged#example-2) in [stmt.ranged] illustrates CWG's intent:\n\n\n\n```\nusing T = std::list<int>;\nconst T& f1(const T& t) { return t; }\nconst T& f2(T t)        { return t; }\nT g();\n\nvoid foo() {\n  for (auto e : f1(g())) {}     \/\/ OK, lifetime of return value of g() extended\n  for (auto e : f2(g())) {}     \/\/ undefined behavior\n}\n\n```\n\nSo the \"other than a function parameter object\" wording is referring to the parameter `t` of `f2`.\n\n\nI think the OP is right that this isn't actually a temporary object; it doesn't fall under any of the categories mentioned in [[class.temporary]\/1](https:\/\/eel.is\/c++draft\/class.temporary#1). However, on some implementations a function parameter object has temporary-like characteristics in that it might be destroyed at the end of the full-expression containing the call, rather than at the point you would expect for a block variable. And I think that was the case that CWG was trying to address here."}
{"questionId":"712c6594464a424e96c6b4e93a19e3fe","question":"Error parsing time with two decimal digits in Java 17, but succeeds in Java 8\nThe following code snippet runs without error in Java 8. However, when I run the same code in Java 17 it fails.\n\n\n\n```\nimport java.time.OffsetDateTime;\nimport java.time.format.DateTimeFormatter;\n\npublic class Main2 {\n    public static void main(String[] args) {\nDateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH[:mm[:ss[.SSS]]]X\");\n        OffsetDateTime offSetDateTime = OffsetDateTime.parse(\"2021-10-09T08:59:00.00Z\", formatter);\n        System.out.println(offSetDateTime);\n    }\n}\n\n```\n\nOutput when run on Java 17:\n\n\n\n```\nException in thread \"main\" java.time.format.DateTimeParseException: Text '2021-10-09T08:59:00.00Z' could not be parsed at index 19\n    at java.base\/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)\n    at java.base\/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1954)\n    at java.base\/java.time.OffsetDateTime.parse(OffsetDateTime.java:404)\n    at Main2.main(Main2.java:9)\n\n```\n\nHowever, if I run the same code against Java 8 it gives the following result:\n\n\n\n```\n2021-10-09T08:59Z\n\n```\n\nIf I change the test data from `\"2021-10-09T08:59:00.00Z\"` to `\"2021-10-09T08:59:00.000Z\"`, it works in Java 17. Any idea what changes in Java 17 has causes it it fail?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"java"},"answer":"This behaviour seems to be caused by the fix for [JDK-8230136](https:\/\/bugs.openjdk.org\/browse\/JDK-8230136).\n\n\nJDK-8230136 is the issue that the fraction-of-second field does not check its minimum width. If you do `appendFraction(NANO_OF_SECOND, 3, 3, false)` in a `DateTimeFormatterBuilder`, 1, 2, or 3 digits will all be accepted by that field.\n\n\n`DateTimeFormatter` uses `DateTimeFormatterBuilder` under the hood, and so it is also affected. The `SSS` in your pattern eventually leads to a call like `appendFraction(NANO_OF_SECOND, 3, 3, false)` on a `DateTimeFormatterBuilder`.\n\n\nIf you need the old behaviour, you can call `appendFraction` with your own minimum digit count.\n\n\n\n```\nDateTimeFormatter formatter = new DateTimeFormatterBuilder()\n        .appendPattern(\"yyyy-MM-dd'T'HH[:mm[:ss[\")\n        .appendFraction(ChronoField.NANO_OF_SECOND, 1, 3, true)\n        .appendPattern(\"]]]X\")\n        .toFormatter();"}
{"questionId":"d728267b25534bbc89782b4aac3d6ff4","question":"Xcode 15 locking up my app from running: 1 log\/signpost messages lost due to high rates in live mode recording?\nThis just started today in Xcode 15. I haven't added any new logs or print statements but when I build and run on my iPhone 14 Pro, I tap on a view in my app, the console is printing:\n\n\n\n> \n> 1 log\/signpost messages lost due to high rates in live mode recording. To guarantee delivery of all logs, set IDELogRedirectionPolicy to oslogToStdio in the environment of the executable.\n> \n> \n> \n\n\ntons of times. When I build and run the exact same code on my iPhone 13 mini, it doesn't do it?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"swift"},"answer":"I had the same thing, but only when I ran my app on a physical device. I couldn't debug anymore because my debug area in XCode kept on showing these messages.\n\n\nSo I did what the log message said and added the environment variable `IDELogRedirectionPolicy` with value `oslogToStdio` to the environment of the executable. Now I don't see these messages anymore in my debug area."}
{"questionId":"bdc239608a9a4b5497e408d2522ee45d","question":"NoSuchMethodError: 'java.util.Set org.junit.platform.engine.TestDescriptor.getAncestors()\nI get this error while running a JUnit test in Spring Tool Suite (4.20.0).\n\n\nTest runs fine, but this error is thrown at the end -\n\n\n\n```\njava.lang.NoSuchMethodError: 'java.util.Set org.junit.platform.engine.TestDescriptor.getAncestors()'\nat org.junit.platform.launcher.core.StackTracePruningEngineExecutionListener.getTestClassNames(StackTracePruningEngineExecutionListener.java:50)\nat org.junit.platform.launcher.core.StackTracePruningEngineExecutionListener.executionFinished(StackTracePruningEngineExecutionListener.java:39)\nat org.junit.platform.launcher.core.DelegatingEngineExecutionListener.executionFinished(DelegatingEngineExecutionListener.java:46)\nat org.junit.platform.launcher.core.OutcomeDelayingEngineExecutionListener.reportEngineFailure(OutcomeDelayingEngineExecutionListener.java:83)\nat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:203)\n\n```\n\nThis is my Gradle plugins section -\n\n\n\n```\nplugins {\n    id 'java'\n    id 'eclipse'\n    id 'eclipse-wtp'\n    id 'org.springframework.boot' version \"3.1.4\"\n    id 'io.spring.dependency-management' version \"1.1.3\"\n}\n\n```\n\nThis is my Gradle dependencies section -\n\n\n\n```\nimplementation(\"org.springframework.boot:spring-boot-starter-parent:3.1.4\")\nimplementation(\"org.springframework.boot:spring-boot-starter-web\")\nimplementation(\"org.springframework.boot:spring-boot-configuration-processor\")\nimplementation(\"org.springframework.boot:spring-boot-starter-test\")\n\n```\n\nWhat am I doing wrong?\n\n\nMy test class is very basic -\n\n\n\n```\n@ExtendWith(SpringExtension.class)\n@ContextConfiguration(initializers = ConfigDataApplicationContextInitializer.class, classes = { TransformService.class})\n@Slf4j\npublic class TransformServiceTest {\n\n    @Autowired\n    TransformService transformService;\n\n    @Test\n    public void transformXmlTest() throws Exception {\n        transformService.transformXml();\n    }\n}\n\n```\n\nSo, is my service class -\n\n\n\n```\n@Service\n@Slf4j\npublic class TransformService {\n\n    public void transformXml() {\n        log.info(\"Transforming XML\");\n    }\n}\n\n```\n\nI am running the test in STS by right-clicking on \"TransformXmlTest()\" in the Package Explorer and clicking \"Run as - JUnit Test\". (When I run it outside STS using Gradle task, everything looks clean).\n\n\nThis is the full log entry -\n\n\n\n```\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\n2023-10-06 10:00:33.097  INFO Transforming XML | com.dowill.xsl.service.TransformService\njava.lang.NoSuchMethodError: 'java.util.Set org.junit.platform.engine.TestDescriptor.getAncestors()'\n    at org.junit.platform.launcher.core.StackTracePruningEngineExecutionListener.getTestClassNames(StackTracePruningEngineExecutionListener.java:50)\n    at org.junit.platform.launcher.core.StackTracePruningEngineExecutionListener.executionFinished(StackTracePruningEngineExecutionListener.java:39)\n    at org.junit.platform.launcher.core.DelegatingEngineExecutionListener.executionFinished(DelegatingEngineExecutionListener.java:46)\n    at org.junit.platform.launcher.core.OutcomeDelayingEngineExecutionListener.reportEngineFailure(OutcomeDelayingEngineExecutionListener.java:83)\n    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:203)\n    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:169)\n    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:93)\n    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:58)\n    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:141)\n    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:57)\n    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:103)\n    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:94)\n    at org.junit.platform.launcher.core.DelegatingLauncher.execute(DelegatingLauncher.java:52)\n    at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:70)\n    at org.eclipse.jdt.internal.junit5.runner.JUnit5TestReference.run(JUnit5TestReference.java:98)\n    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:40)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:529)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:756)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:452)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:210)\n\n```\n\nThank you for the help","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"java"},"answer":"I switched to STS 4.19.0 and not seeing this error anymore. Wonder if this is an issue with STS 4.20.0.\n\n\nI have tagged spring-tools-4 here hoping that someone from the STS community will take a look."}
{"questionId":"7f0c7d51126246079ac0224c9d5221d1","question":"Angular NG5002: @for loop must have a \"track\" expression\nAngular 17 introduced a new control flow syntax moving from\n\n\n\n```\n<div *ngFor=\"let item of items\"><\/div>\n\n```\n\nto\n\n\n\n```\n@for (item of items) {\n    <div><\/div>\n}\n\n```\n\nWhen directly translating my loops, I get the following error:\n\n\n\n```\n NG5002: @for loop must have a \"track\" expression","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"typescript"},"answer":"# Background\n\n\nAs part of the new control flow structure introduced in [angular](\/questions\/tagged\/angular \"show questions tagged 'angular'\") v17 the `track` expression needs to be defined.\n\n\nTrack is used to keep track of items in an array and their association to the DOM.\n\n\nFrom the Angular documentation:\n\n\n\n> \n> The value of the `track` expression determines a key used to associate array items with the views in the DOM. Having clear indication of the item identity allows Angular to execute a minimal set of DOM operations as items are added, removed or moved in a collection.\n> \n> \n> \n\n\n\n> \n> Loops over immutable data without `trackBy` as one of the most common causes for performance issues across Angular applications. Because of the potential for poor performance, the `track` expression is required for the `@for` loops. When in doubt, using `track $index` is a good default.\n> \n> \n> \n\n\nFrom :<https:\/\/angular.dev\/guide\/templates\/control-flow#track-for-calculating-difference-of-two-collections>\n\n\n# Solutions\n\n\n## Use a property of the element\n\n\nIf the elements in your array have any kind of property attached to them that allows identifying them you can use it for the tracking. Any kind of ID works great.\n\n\n\n```\nlet items = [\n    { id: \"id1\", content: []},\n    { id: \"id2\", content: []},\n]\n\n```\n\n\n```\n@for (item of items; track item.id) {\n    <div><\/div>\n}\n\n```\n\n## Use `track $index`\n\n\nThis uses the index of the current row to associate the row with the element in the DOM. This can be used as default, if no better property is available and the array itself is immutable.\n\n\n\n```\n@for (item of items; track $index) {\n    <div><\/div>\n}\n\n```\n\n`$index` will be recalculated if an item is added to the array, so it could lead to some unexpected behaviour or performance issues, and should not be used if a unique identifier is available."}
{"questionId":"d9eb78ec23e4414fb2cb5bf878577105","question":"Material Design 3 with Angular\nI am new to material design and I am researching whether I can use Material Design 3 with Angular version 17.\n\n\nI have followed the steps mentioned in [Material Design 3](https:\/\/github.com\/material-components\/material-web\/blob\/main\/docs\/quick-start.md) and successfully installed but I have no idea on how to use its components. Any help would be appreciated. Thanks..!!","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"other"},"answer":"Material 3 support for Angular has been [released in v17.2](https:\/\/blog.angular.io\/material-3-experimental-support-in-angular-17-2-8e681dde650e).\n\n\nYou can now use design tokens, customization becomes scary easy !\n\n\n\n```\n<mat-checkbox class=\"scary-setting\">Delete my account<\/mat-checkbox>\n\n.scary-setting {\n  \/\/ No need to target any internal checkbox selectors! \ud83c\udf89\n  - mdc-checkbox-unselected-hover-state-layer-color: red;\n  - mdc-checkbox-unselected-hover-icon-color: red;\n}"}
{"questionId":"59191ad436844e3a9bd3583e5e77faa4","question":"Why `Monad` is required to use `pure`?\nI'm learning Haskell with [Luis Morillo's Haskell challenge for beginners (snake-fury)](https:\/\/github.com\/lsmor\/snake-fury\/).\nOne of the tasks in [exercise 4](https:\/\/github.com\/lsmor\/snake-fury\/blob\/main\/exercises\/refactor-4.md) is \"to implement `Applicative` instance for the new type\" where the new type is the following:\n\n\n\n```\nnewtype GameStep m a = GameStep {runGameStep :: ReaderT BoardInfo (StateT GameState m) a}\n\n```\n\nHere is my first attempt, which did not compile.\n\n\n\n```\ninstance (Applicative m) => Applicative (GameStep m) where\n  pure a = GameStep $ pure a\n  (GameStep r) <*> (GameStep s) = GameStep $ r <*> s\n\n```\n\nGHC says\n\n\n\n```\n    \u2022 Could not deduce (Monad m) arising from a use of \u2018pure\u2019\n      from the context: Applicative m\n        bound by the instance declaration at src\/GameState.hs:55:10-52\n      Possible fix:\n        add (Monad m) to the context of\n          the type signature for:\n            pure :: forall a. a -> GameStep m a\n          or the instance declaration\n\n```\n\n(and same for `<*>` use)\n\n\nIt compiles if I replace `(Applicative m) => ...` with `(Monad m) => ...` as suggested. However, I do not understand this error message because `pure` (as well as `<*>`) is defined for `Applicative`, not for `Monad`.\n\n\nWhy does `Monad` require `pure`?","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"haskell"},"answer":"It's because your `m` is wrapped inside of `StateT` (and then in `ReaderT`, but that doesn't add anything).\n\n\nWhen you write `GameStep $ pure a`, that `pure a` must return not just `m a`, but `ReaderT _ (StateT _ m) a`\n\n\nAnd if you look at [the docs for `StateT`](https:\/\/www.stackage.org\/haddock\/lts-22.3\/transformers-0.6.1.0\/Control-Monad-Trans-State-Lazy.html#t:StateT), you'll see that `Applicative` is defined as:\n\n\n\n```\ninstance (Functor m, Monad m) => Applicative (StateT s m)\n\n```\n\nSo it's not your own instance that requires `Monad`, it's the one of `StateT`.\n\n\n\n\n---\n\n\nNow, the *reason* `StateT` requires `Monad` is that, in order to implement `<*>`, it needs to execute both arguments, but for `StateT` the order of execution matters, because each execution potentially modifies the state. So `StateT`'s implementation of `<*>` has to use the underlying `Monad`'s implementation of `>>=` to define the order in which the arguments are executed and thread the state between them."}
{"questionId":"611f18f4550f4869b4716433624883f8","question":"Should I create private properties for primary constructor parameters in C# 12?\nI'm using C# 12. In C# 12 I can use primary constructor:\n\n\n**Implementation 1 :**\n\n\n\n```\npublic class Calculation(int a,int b)\n{\n  public int Addition() => a + b;\n  public int Subtraction() => a - b;\n}\n\n```\n\n**Implementation 2 :**\n\n\n\n```\npublic class Calculation(int a,int b)\n{\n private int _a { get; init; } = a;\n private int _b { get; init; } = b;\n public int Addition() => _a + _b;\n public int Subtraction() => _a - _b;\n}\n\n```\n\nWhen I'm calling this method like :\n\n\n\n```\nconsole.WriteLine(new Calculation(10,25).Addition());\n\n```\n\nBoth implementation working fine, so I'd like to know whether there is any advantage to using the longer Implementation 2 over the shorter Implementation 1.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"csharp"},"answer":"There is no point in creating fields\/properties yourself this way, since the compiler will create them for you in this case (decompilation [@sharplab.io](https:\/\/sharplab.io\/#v2:EYLgtghglgdgPgAQEwEYCwAoTCUE4AUARIDwbgIPuECUA3JtgMwAEyDAwhADYDGAruxAC5QA9jHyx+DCABpxDYBUwBvTAyaNZAQQAmWqIJH4KDALwA+SQwDUcmhlUJ1MCQGVuwfgCcInfaKNmLAFobTABfWgwHJiRWDh4+XxQxJ0kZFPklFQAHDygANwEAUwZZAH0IBkUGAHNC\/ioSmD0G0JNJWwYc\/KLGiVLgSpq6hthmhlbjELsozR09YT8Tc3KrBn6OmZTXdy8fBcMltYrg9bCgA=)).\n\n\nFrom the [Tutorial: Explore primary constructors](https:\/\/learn.microsoft.com\/en-us\/dotnet\/csharp\/whats-new\/tutorials\/primary-constructors#create-mutable-state) doc (they use `struct` but the handling of the primary ctor for classes is basically the same):\n\n\n\n> \n> \n> ```\n> public struct Distance(double dx, double dy)\n> {\n>    public readonly double Magnitude => Math.Sqrt(dx * dx + dy * dy);\n>    public readonly double Direction => Math.Atan2(dy, dx);\n> \n>    public void Translate(double deltaX, double deltaY)\n>    {\n>       dx += deltaX;\n>       dy += deltaY;\n>    }\n> \n>    public Distance() : this(0,0) { }\n> }\n> \n> ```\n> \n> \n\n\n\n> \n> In the previous example, the primary constructor properties are accessed in a method. Therefore the compiler creates hidden fields to represent each parameter. The following code shows approximately what the compiler generates. The actual field names are valid CIL identifiers, but not valid C# identifiers.\n> \n> \n> \n\n\n\n> \n> \n> ```\n> public struct Distance\n> {\n>    private double __unspeakable_dx;\n>    private double __unspeakable_dy;\n> \n>    public readonly double Magnitude => Math.Sqrt(__unspeakable_dx * __unspeakable_dx + __unspeakable_dy * __unspeakable_dy);\n>    public readonly double Direction => Math.Atan2(__unspeakable_dy, __unspeakable_dx);\n> \n>    public void Translate(double deltaX, double deltaY)\n>    {\n>        __unspeakable_dx += deltaX;\n>        __unspeakable_dy += deltaY;\n>    }\n> \n>    public Distance(double dx, double dy)\n>    {\n>        __unspeakable_dx = dx;\n>        __unspeakable_dy = dy;\n>    }\n>    public Distance() : this(0, 0) { }\n> }\n> \n> ```\n> \n> \n\n\nThe potential downside is that the generated field is mutable:\n\n\n\n```\npublic class Calculation(int a, int b)\n{\n    public void Mutate() => a = 100;\n    \/\/ ...\n}\n\n```\n\nThough your approach does not fix that but introduces some clutter when you can confuse `_a` and `a`. There is no way to mark the generated field as `readonly` for now (but there are plans for the feature - see [this LDM notes doc](https:\/\/github.com\/dotnet\/csharplang\/blob\/main\/meetings\/2023\/LDM-2023-07-31.md#primary-constructor-parameters-and-readonly) and [this proposal](https:\/\/github.com\/dotnet\/csharplang\/blob\/main\/proposals\/readonly-parameters.md)) but you can shadow the ctor parameter by using field with the same name (or property):\n\n\n\n```\npublic class Calculation2(int a, int b)\n{\n    private readonly int a = a;    \n    private readonly int b = b;\n    \/\/ public void Mutate() => a = 100; \/\/ does not compile\n    public int Addition() => a + b;\n    public int Subtraction() => a - b;\n}\n\n```\n\nSee also:\n\n\n- [Why do C# 12 primary constructors execute in opposite order?](https:\/\/stackoverflow.com\/q\/77589989\/2501279)\n- [Null checking with primary constructor in C# 12](https:\/\/stackoverflow.com\/q\/77556444\/2501279)"}
{"questionId":"e9fb468b59aa4bbcb6fa9238bbf396cd","question":"Ambiguous constructor error in gcc but not in msvc\nI recently learnt that we can have more than one default constructor in a class. Then I wrote the following program that compiles with msvc but both clang and gcc fails to compile this.\n\n\n\n```\nstruct A\n{\n\u00a0 explicit A(int = 10);\n\u00a0 A()= default;\n};\n\nA a = {}; \/\/msvc ok but gcc and clang fails here\n\n```\n\n[Live demo](https:\/\/godbolt.org\/z\/Y3x8nP11M)\n\n\nI want to know which compiler is correct here as per the C++17 standard.\n\n\nGCC says:\n\n\n\n```\n<source>:8:8: error: conversion from '<brace-enclosed initializer list>' to 'A' is ambiguous\n    8 | A a = {}; \/\/msvc ok but gcc and clang fails here\n      |        ^\n<source>:5:3: note: candidate: 'constexpr A::A()'\n    5 |   A()= default;\n      |   ^\n<source>:4:12: note: candidate: 'A::A(int)'\n    4 |   explicit A(int = 10);","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"cpp"},"answer":"**TLDR;**\n\n\nThis is [CWG 2856](https:\/\/cplusplus.github.io\/CWG\/issues\/2856.html) and the program is **well-formed** as per the current wording in the standard as descirbed below.\nBasically, only one of the ctors(`A::A()` and `explicit A::A(int)`) is a converting constructor. Thus only the former `A::A()` is the viable option and can be used.\n\n\n\n\n---\n\n\nFirst note that `A a = {};` is [copy-initialization](https:\/\/timsong-cpp.github.io\/cppwp\/n4659\/dcl.init#15).\n\n\n\n> \n> 15. **The initialization that occurs in the = form of a brace-or-equal-initializer** or condition ([stmt.select]), as well as in argument passing, function return, throwing an exception ([except.throw]), handling an exception ([except.handle]), and aggregate member initialization ([dcl.init.aggr]), **is called copy-initialization**.\n> \n> \n> \n\n\nNext we move to the [semantics of the initializer](https:\/\/timsong-cpp.github.io\/cppwp\/n4659\/dcl.init#17).\n\n\n\n> \n> 17. The semantics of initializers are as follows. The destination type is the type of the object or reference being initialized and the source type is the type of the initializer expression. If the initializer is not a single (possibly parenthesized) expression, the source type is not defined.\n> \n> \n> - **If the initializer is a (non-parenthesized) braced-init-list or is = braced-init-list, the object or reference is list-initialized ([dcl.init.list])**.\n> - \n> - \n> \n> \n> \n\n\nThe above means that, the object is to be [list-initialized](https:\/\/timsong-cpp.github.io\/cppwp\/n4659\/dcl.init.list).\n\n\nFrom [list initialization](https:\/\/timsong-cpp.github.io\/cppwp\/n4659\/dcl.init.list):\n\n\n\n> \n> List-initialization is initialization of an object or reference from a braced-init-list.\n> Such an initializer is called an initializer list, and the comma-separated initializer-clauses of the initializer-list or designated-initializer-clauses of the designated-initializer-list are called the elements of the initializer list.\n> An initializer list may be empty. List-initialization can occur in direct-initialization or copy-initialization contexts; list-initialization in a direct-initialization context is called direct-list-initialization and **list-initialization in a copy-initialization context is called copy-list-initialization.**\n> \n> \n> \n\n\nThe above means that `A a = {};` is copy-list initiaization. Next we see the [effect](https:\/\/timsong-cpp.github.io\/cppwp\/n4659\/dcl.init.list#3) of list initialization:\n\n\n\n> \n> 3. List-initialization of an object or reference of type T is defined as follows:\n> \n> \n> - \n> - \n> - Otherwise, **if the initializer list has no elements and T is a class type with a default constructor, the object is value-initialized.**\n> \n> \n> \n\n\nThe above means that the object will be [value initialized](https:\/\/timsong-cpp.github.io\/cppwp\/n4659\/dcl.init#8), so we move on to value-initialization:\n\n\n\n> \n> 8. To value-initialize an object of type T means:\n> \n> \n> - if T has either no default constructor ([class.default.ctor]) or a **default constructor that is user-provided** or deleted, **then the object is default-initialized**;\n> \n> \n> \n\n\nThis means that the object will be [default initialized](https:\/\/timsong-cpp.github.io\/cppwp\/n4659\/dcl.init#7):\n\n\n\n> \n> 7. To default-initialize an object of type T means:\n> \n> \n> - If T is a (possibly cv-qualified) class type ([class]), constructors are considered.\n> **The applicable constructors are enumerated ([over.match.ctor]), and the best one for the `initializer ()`** is chosen through overload resolution ([over.match]). The constructor thus selected is called, with an empty argument list, to initialize the object.\n> \n> \n> \n\n\nSo we move on to [over.match.ctor](https:\/\/timsong-cpp.github.io\/cppwp\/n4659\/over.match.ctor) to get a list of candidate ctors. Also do note the `initializer()` part in the above quoted reference as it will be used at the end as an argument.\n\n\n\n> \n> 1. When objects of class type are direct-initialized, copy-initialized from an expression of the same or a derived class type ([dcl.init]), or default-initialized, overload resolution selects the constructor. For direct-initialization or default-initialization that is not in the context of copy-initialization, the candidate functions are all the constructors of the class of the object being initialized. **For copy-initialization, the candidate functions are all the converting constructors** of that class. The argument list is the expression-list or assignment-expression of the initializer.\n> \n> \n> \n\n\nThis means that only the non-explicit ctor `A::A()` is the candidate because it is a converting ctor while the other `explicit A::A(int)` is not. Thus the set of candidates and viable options only contains one ctor `A::A()`.\n\n\nFinally, since we only have one viable option, it is the one that is selected for the initializer ()."}
{"questionId":"e475ae9225484572a8bf852f068695ca","question":"Migrate PostgresDsn.build from pydentic v1 to pydantic v2\nI have simple Config class from FastAPI tutorial. But it seems like it uses old pydantic version. I run my code with pydantic v2 version and get a several errors. I fix almost all of them, but the last one I cannot fix yet. This is part of code which does not work:\n\n\n\n```\nfrom pydantic import AnyHttpUrl, HttpUrl, PostgresDsn, field_validator\nfrom pydantic_settings import BaseSettings\nfrom pydantic_core.core_schema import FieldValidationInfo\n\nload_dotenv()\n\n\nclass Settings(BaseSettings):\n    ...\n    POSTGRES_SERVER: str = 'localhost:5432'\n    POSTGRES_USER: str = os.getenv('POSTGRES_USER')\n    POSTGRES_PASSWORD: str = os.getenv('POSTGRES_PASSWORD')\n    POSTGRES_DB: str = os.getenv('POSTGRES_DB')\n    SQLALCHEMY_DATABASE_URI: Optional[PostgresDsn] = None\n\n    @field_validator(\"SQLALCHEMY_DATABASE_URI\", mode='before')\n    @classmethod\n    def assemble_db_connection(cls, v: Optional[str], info: FieldValidationInfo) -> Any:\n        if isinstance(v, str):\n            return v\n        postgres_dsn = PostgresDsn.build(\n            scheme=\"postgresql\",\n            username=info.data.get(\"POSTGRES_USER\"),\n            password=info.data.get(\"POSTGRES_PASSWORD\"),\n            host=info.data.get(\"POSTGRES_SERVER\"),\n            path=f\"{info.data.get('POSTGRES_DB') or ''}\",\n        )\n        return str(postgres_dsn)\n\n```\n\nThat is the error which I get:\n\n\n\n```\nsqlalchemy.exc.ArgumentError: Expected string or URL object, got MultiHostUrl('postgresql:\/\/user:password@localhost:5432\/database')\n\n```\n\nI check a lot of places, but cannot find how I can fix that, it looks like `build` method pass data to the sqlalchemy `create_engine` method as a `MultiHostUrl` instance instead of string. How should I properly migrate this code to use pydantic v2?\n\n\n**UPDATE**\n\n\nI have fixed that issue by changing typing for `SQLALCHEMY_DATABASE_URI: Optional[PostgresDsn] = None` to `SQLALCHEMY_DATABASE_URI: Optional[str] = None`.\nBecause pydantic makes auto conversion of result for some reason. But I am not sure if that approach is the right one, maybe there are better way to do that?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"You can use `unicode_string()` to stringify your `URI` as follow:\n\n\n\n```\nfrom sqlalchemy.ext.asyncio import create_async_engine\n\ncreate_async_engine(settings.POSTGRES_URI.unicode_string())\n\n\n```\n\nCheck documentation page [here](https:\/\/docs.pydantic.dev\/latest\/api\/pydantic_core\/#pydantic_core._pydantic_core.Url.unicode_string) for additional explanation."}
{"questionId":"ac445b1ae05244de9651eb24443f0de5","question":"Vectorize an sapply function\nI am trying to vectorize the following function to drop the sapply loop. I am calculating cumulative skewness.\n\n\n\n```\ncskewness <- function(.x) {\n  skewness <- function(.x) {\n    sqrt(length(.x)) * sum((.x - mean(.x))^3) \/ (sum((.x - mean(.x))^2)^(3 \/ 2))\n  }\n  sapply(seq_along(.x), function(k, z) skewness(z[1:k]), z = .x)\n}\n\n```\n\nNot getting my algebra right. Have this which is wrong:\n\n\n\n```\nskewness2 <- function(.x) {\n  n <- length(.x)\n  csum <- cumsum(.x)\n  cmu <- csum \/ 1:length(.x)\n  num <- cumsum(.x - cmu)^3\n  den <- cumsum((.x - cmu)^2)^(3\/2)\n  sqrt(n) * num \/ den\n}\n\n```\n\nCorrect code produces:\n\n\n\n```\nx <- c(1,2,4,5,8)\n\n> cskewness(x)\n[1]       NaN 0.0000000 0.3818018 0.0000000 0.4082483\n> skewness2(x)\n[1]      NaN 1.000000 1.930591 3.882748 4.928973","questionMetadata":{"Type":"Optimization","Level":"Advanced","Tag":"r"},"answer":"Using a single-pass `for` loop will be efficient:\n\n\n\n```\ncumskewness <- function(x) {\n  n <- length(x)\n  \n  if (n == 0L) {\n    return(x)\n  } else if (n == 1L) {\n    return(0)\n  }\n  \n  m2 <- m3  <- term1 <- 0\n  out <- numeric(n)\n  out[1] <- NaN\n  m1 <- x[1]\n  \n  for (i in 2:n) {\n    n0 <- i - 1\n    delta <- x[i] - m1\n    delta_n <- delta\/i\n    m1 <- m1 + delta_n\n    term1 <- delta*delta_n*n0\n    m3 <- m3 + term1*delta_n*(n0 - 1) - 3*delta_n*m2\n    m2 <- m2 + term1\n    out[i] <- sqrt(i)*m3\/m2^1.5\n  }\n  \n  out\n}\n\n```\n\nIt is also straightforward to write up with `Rcpp`:\n\n\n\n```\nlibrary(Rcpp)\n\ncppFunction('\n  NumericVector cumskewnessCpp(const NumericVector& x) {\n    const int n = x.size();\n    \n    if (n == 0) {\n      return(x);\n    } else if (n == 1) {\n      return(0);\n    }\n    \n    int n1;\n    double m1 = x(0);\n    double m2, m3, delta, delta_n, term1;\n    NumericVector out(n);\n    out(0) = R_NaN;\n    \n    for (int i = 1; i < n; i++) {\n      n1 = i + 1;\n      delta = x(i) - m1;\n      delta_n = delta\/n1;\n      m1 += delta_n;\n      term1 = delta*delta_n*i;\n      m3 += term1*delta_n*(i - 1) - 3*delta_n*m2;\n      m2 += term1;\n      out(i) = sqrt(n1)*m3\/pow(m2, 1.5);\n    }\n    \n    return out;\n  }\n')\n\n```\n\n## Testing\n\n\n\n```\ncumskewness(x)\n#> [1] NaN 0.0000000 0.3818018 0.0000000 0.4082483\ncumskewnessCpp(x)\n#> [1] NaN  0.000000e+00  3.818018e-01 -2.808667e-17  4.082483e-01\n\n```\n\n## Benchmarking\n\n\nIncluding the vectorized solution from @ThomisIsCoding:\n\n\n\n```\ncskewness_tic <- function(y) {\n  # cumulative length of y\n  k <- seq_along(y)\n  # cumulative n-th order moments of y\n  m3 <- cumsum(y^3)\n  m2 <- cumsum(y^2)\n  m1 <- cumsum(y)\n  u <- m1 \/ k\n  # expand cubic terms and refactor skewness in terms of num\/den\n  num <- (m3 - 3 * u * m2 + 3 * u^2 * m1 - k * u^3) \/ k\n  den <- sqrt((m2 + k * u^2 - 2 * u * m1) \/ k)^3\n  num \/ den\n}\n\nset.seed(0)\nx <- sample(1e3)\n\nmicrobenchmark::microbenchmark(\n  cskewness_tic = cskewness_tic(x),\n  cumskewness = cumskewness(x),\n  cumskewnessCpp = cumskewnessCpp(x),\n  check = \"equal\",\n  unit = \"relative\"\n)\n#> Unit: relative\n#>            expr      min      lq     mean   median       uq       max neval\n#>   cskewness_tic 2.035272 2.07954 2.006118 2.388216 2.282022 0.7228815   100\n#>     cumskewness 3.930424 3.96835 4.003956 3.939762 3.711879 1.1339946   100\n#>  cumskewnessCpp 1.000000 1.00000 1.000000 1.000000 1.000000 1.0000000   100\n\n```\n\n## A note of caution using `cskewness_tic`\n\n\nCatastrophic cancellation can result in precision errors for higher-order moments. It occurs when the standard deviation is small relative to the mean. Demonstrating:\n\n\n\n```\nset.seed(0)\nx <- sample(1e3) + 1e8\n\ny1 <- cskewness(x)\ny2 <- cumskewness(x)\ny3 <- cumskewnessCpp(x)\ny4 <- cskewness_tic(x); y4[1] <- NaN\n\nall.equal(y1, y2)\n#> [1] TRUE\nall.equal(y1, y3)\n#> [1] TRUE\nall.equal(y1, y4)\n#> [1] \"Mean relative difference: 270.1872\"\n\n```\n\n## Original Answer\n\n\n\n```\nskewness2 <- function(.x) {\n  d <- outer(.x, cumsum(.x)\/(1:length(.x)), \"-\")\n  d[lower.tri(d)] <- 0\n  sqrt(1:length(.x))*colSums(d^3)\/colSums(d^2)^(3\/2)\n}\n\nx <- c(1,2,4,5,8)\ncskewness(x)\n#> [1]       NaN 0.0000000 0.3818018 0.0000000 0.4082483\nskewness2(x)\n#> [1]       NaN 0.0000000 0.3818018 0.0000000 0.4082483"}
{"questionId":"c14486ebf5b6444ea502510f8eb9740e","question":"How to overwrite a warning in R\nI have a function that generates an unwanted warning, but keeps the value.\n\n\n\n```\nf <- function(a) { \n  \n  if (a > 1) { \n    warning(\"an uninformative warning\")\n  }\n  a\n}\n\ng1 <- function(b) {\n  \n  withCallingHandlers(\n    x <-f(b),\n    warning = function(w) {\n      warning(\"A more informative warning\")\n    })\n  \n  x\n}\ng1(2)\n#> Warning in (function (w) : A more informative warning\n#> Warning in f(b): an uninformative warning\n#> [1] 2\n\n```\n\nCreated on 2023-12-12 with [reprex v2.0.2](https:\/\/reprex.tidyverse.org)\n\n\nUnfortunately, this thows 2 warnings.\n\n\nWith `tryCatch()` x is not kept. and with `withCallingHandlers()`, both warnings are thrown.","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"r"},"answer":"You can get your `warning` handler to emit its own warning, then invoke the `muffleWarning` restart:\n\n\n\n```\nf <- function(a) { \n  \n  if (a > 1) { \n    warning(\"an uninformative warning\")\n  }\n  a\n}\n\ng1 <- function(b) {\n  \n  withCallingHandlers(\n    x <- f(b),\n    warning = function(w) {\n      warning('A more informative warning', call. = FALSE)\n      tryInvokeRestart(\"muffleWarning\")\n    })\n  x\n}\n\n```\n\nTesting:\n\n\n\n```\ng1(2)\n#> [1] 2\n#> Warning message:\n#> A more informative warning"}
{"questionId":"7e7916d3849f43428f336c1b8c5f20fe","question":"Why can't I have properties called \"id\" and \"set\\_id\" in the same class?\nI'm trying to consume a Web API. I created a class so that I can deserialize the data I get back. The problem is that the returned object has these two properties:\n\n\n\n```\npublic string id { get; set; }\npublic string set_id { get; set; }\n\n```\n\nThe `set_id` related to \"sets\". The compiler is throwing an error on the `set;` from the `id` property, saying that it already contains a definition for `set_id`.\n\n\n\n> \n> CS0102 The type 'MyClass' already contains a definition for 'set\\_id'\n> \n> \n> \n\n\nIs there any way to solve this without renaming the property?","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"csharp"},"answer":"I would recommend using JsonPropertyName\n\n\nFor example:\n\n\n\n```\nusing System.Text.Json.Serialization;\n\n    namespace ConsoleApp1\n    {\n        public class Class1\n        {\n            public string id { get; set; }\n            [JsonPropertyName(\"set_id\")]\n            public string setId { get; set; }\n        }\n    }\n\n```\n\nthis attribute based on your json library , if you use newtonsoft it would be `JsonProperty`\n\n\n\n\n---\n\n\n**Lets findout why we can't have a 2 variable with names id and set\\_id together**\n\n\nFor understanding this we are going to see IL generated code by c# compiler\n\n\nfor class with 1 property id generated IL looks like this :\n\n\n\n```\n.class public auto ansi beforefieldinit ConsoleApp1.Test\n    extends [System.Runtime]System.Object\n{\n    .custom instance void System.Runtime.CompilerServices.NullableContextAttribute::.ctor(uint8) = (\n        01 00 01 00 00\n    )\n    .custom instance void System.Runtime.CompilerServices.NullableAttribute::.ctor(uint8) = (\n        01 00 00 00 00\n    )\n    \/\/ Fields\n    .field private string '<id>k__BackingField'\n    .custom instance void [System.Runtime]System.Runtime.CompilerServices.CompilerGeneratedAttribute::.ctor() = (\n        01 00 00 00\n    )\n    .custom instance void [System.Runtime]System.Diagnostics.DebuggerBrowsableAttribute::.ctor(valuetype [System.Runtime]System.Diagnostics.DebuggerBrowsableState) = (\n        01 00 00 00 00 00 00 00\n    )\n\n    \/\/ Methods\n    .method public hidebysig specialname \n        instance string **get_id** () cil managed \n    {\n        .custom instance void [System.Runtime]System.Runtime.CompilerServices.CompilerGeneratedAttribute::.ctor() = (\n            01 00 00 00\n        )\n        \/\/ Method begins at RVA 0x20b8\n        \/\/ Header size: 1\n        \/\/ Code size: 7 (0x7)\n        .maxstack 8\n\n        IL_0000: ldarg.0\n        IL_0001: ldfld string ConsoleApp1.Test::'<id>k__BackingField'\n        IL_0006: ret\n    } \/\/ end of method Test::get_id\n\n    .method public hidebysig specialname \n        instance void set_id (\n            string 'value'\n        ) cil managed \n    {\n        .custom instance void [System.Runtime]System.Runtime.CompilerServices.CompilerGeneratedAttribute::.ctor() = (\n            01 00 00 00\n        )\n        \/\/ Method begins at RVA 0x20c0\n        \/\/ Header size: 1\n        \/\/ Code size: 8 (0x8)\n        .maxstack 8\n\n        IL_0000: ldarg.0\n        IL_0001: ldarg.1\n        IL_0002: stfld string ConsoleApp1.Test::'<id>k__BackingField'\n        IL_0007: ret\n    } \/\/ end of method Test::set_id\n\n    .method public hidebysig specialname rtspecialname \n        instance void .ctor () cil managed \n    {\n        \/\/ Method begins at RVA 0x20c9\n        \/\/ Header size: 1\n        \/\/ Code size: 8 (0x8)\n        .maxstack 8\n\n        IL_0000: ldarg.0\n        IL_0001: call instance void [System.Runtime]System.Object::.ctor()\n        IL_0006: nop\n        IL_0007: ret\n    } \/\/ end of method Test::.ctor\n\n    \/\/ Properties\n    .property instance string id()\n    {\n        .get instance string ConsoleApp1.Test::get_id()\n        .set instance void ConsoleApp1.Test::set_id(string)\n    }\n\n} \/\/ end of class ConsoleApp1.Test\n\n```\n\nIn this code you can see getter and setter method are transformed to some 2 methods named set\\_id and get\\_id (set\\_id = set , get\\_id = get)\n\n\nNow we have set\\_id and get\\_id **reserved** so we can't have other field with name set\\_id or get\\_id"}
{"questionId":"5b4346c31bce4c789d63f9fa38ff6929","question":"Performance Comparison - Mojo vs Python\nMojo, a programming language, claims to be 65000x faster than python. I am eager to understand if is there any concrete benchmark data that supports this claim? Also, how does it differ in real world problems?\n\n\nI am primarily encountered this claim on their website and have watched several videos discussing Mojo's speed. However I am seeking concrete benchmark data that substantiates this assertion.","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"other"},"answer":"**TL;DR:** The claim directly comes from a [blog on the Mojo website](https:\/\/www.modular.com\/blog\/mojo-a-journey-to-68-000x-speedup-over-python-part-3). The benchmark is a computation of the Mandelbrot set. It is **not a rigorous benchmark nor one representative of most Python applications**. It is also clearly **biased** (e.g. sequential VS parallel codes).\n\n\nThey choose it because it has the following properties: \"Simple to express\", \"Pure compute\" (ie. compute-bound), \"Irregular computation\", \"Embarrassingly parallel\", \"Vectorizable\". They also states \"Mandelbrot is a nice problem that demonstrates the capabilities of Mojo\". This is the kind of problem where Python does not shine (and it not meant to be used for), but on which Mojo shine well (pretty optimal use-case for it). Thus, the speed-up can be pretty huge. In fact, this benchmark is rather a maximum speed-up you can get and not an average of many real-world applications.\n\n\nFirst things first, it means nothing to compare language performance. We compare **implementations**. CPython is the standard Python implementation, but not the only one. CPython is an **interpreter** so a code is very slow when a it is fully interpreted. Optimized Python codes tends not to run much interpreted code but *vectorized* ones (eg. the script mostly calls Numpy optimized functions written in C).\n\n\nPyPy is an alternative implementation which uses a **JIT-compiler** to run code faster. It [claims 4.8x faster performance compared to CPython](https:\/\/speed.pypy.org\/) with a detailed set of benchmarks (geometric average). There are some benchmark that are very hard to make faster, even using a native language. Symbolic and large string computations are hard to make faster (CPython string are already well optimized in C). It tends to be faster for numerical codes.\n\n\nIn the Mojo benchmark, the naive Python codes, Numpy ones and PyPy ones are **sequential** while the final Mojo code is multi-threaded. This is **not a fair comparison**. One should have to use at least the *multiprocessing* module to compare parallel codes together. This is a critical point since they run the code on a **88-Core Intel Xeon** CPU. Indeed, since the computation is compute-bound, one can expect a speed up close to 88 using multiple threads. In fact, their parallel Mojo implementation is 85 times faster than their sequential Mojo one. Without a parallel Python implementation, It would be more fair to claim that Mojo is 874 times faster than a naive CPython implementation, 175 times faster than a (rather naive) Numpy code, and 40 times faster than a PyPy implementation (on this specific Mandelbrot set computation).\n\n\nIn sequential, most of the speed-up comes from the use of [**SIMD**](https:\/\/fr.wikipedia.org\/wiki\/Single_instruction_multiple_data) instructions and [instruction-level parallelism](https:\/\/en.wikipedia.org\/wiki\/Instruction-level_parallelism). The Python implementations tends not to use them. While Numpy can do that, not all functions are well vectorized (the one operating on Complex numbers tends not to be AFAIK) and Numpy codes tends to be memory-bound due to the creation of many large temporary arrays.\n\n\nNote that tools like **Numba and Cython** are not shown in the benchmark while they are frequently used to speed up numerical codes. It would be more fair to add them (or at least mention them)."}
{"questionId":"4ec73186af224f31bde4d28ff6a59696","question":"Does noexcept matter with explicitly defaulted move constructor\/assignment operator?\nclass C\n{\n    public:\n        C(C&&) = default; \/\/ (1)\n        C& operator=(C&&) = default; \/\/ (1)\n\n        C(C&&) noexcept = default; \/\/ (2)\n        C& operator=(C&&) noexcept = default; \/\/ (2)\n}\n\n```\n\nAs far as I know, if move constructor\/assignment operator are either implicitly generated or explicitly defaulted by user `(1)`, the compiler will decide whether these special member functions should be `noexcept` or not depending on whether all the members of a class provide `noexcept` guarantees for move operations. But what if I want to use default move special members and force them to be `noexcept` regardless of the underlying members exception guarantees for move operations? Is there any difference for the compiler between `(1)` and `(2)` declarations?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"Ok, I found the answer in Nico's Josuttis book \"C++ Move Semantics - The Complete Guide\":\n\n\n\n> \n> When you have a defaulted special member function you can explicitly\n> specify a different noexcept guarantee than the generated one. For\n> example:\n> \n> \n> \n> ```\n> class C\n> {\n>   ...\n>  public:\n>   C(const C&) noexcept = default;     \/\/ guarantees not to throw (OK since C++20)\n>   C(C&&) noexcept(false) = default;   \/\/ specifies that it might throw (OK since C++20)\n>   ...\n> };\n> \n> ```\n> \n> Before C++20, if the generated and specified noexcept condition\n> contradict, the defined function was deleted.\n> \n> \n> \n\n\nI asked this question because I want STL algorithms to always apply move semantics to my classes regardless of their members' exception guarantees, and I am ready to deal with `std::abort` if something goes wrong. But it seems that prior to C++ 20 the only way to force `noexcept` guarantees on move special member functions was to explicitly define them, even if you don't need your own implementation. It turns out C++20 solved this by allowing us to specify `noexcept` guarantees for defaulted special member functions. Of course, as in case with any other `noexcept` function, if any of the members of a class throws during a move operation, `std::abort` will be called if move constructor\/assignment operator are declared `noexcept` for that class, as it was kindly pointed out by **@Micha\u00eblRoy** in the comments section."}
{"questionId":"cc5ec19132f8488d8d54026f1aae9fea","question":"Why does a regex using an unescaped dash `-` character not seem to work in a `pattern` attribute in HTML but it works when testing on regex101?\n<div class=\"formRow\">\n    <label for=\"phone\">Phone number<\/label>\n    <input name=\"custPhone\" id=\"phone\" type=\"tel\" placeholder=\"(nnn) nnn-nnnn\" \n     pattern=\"^\\d{10}$|^(\\(\\d{3}\\)\\s*)?\\d{3}[\\s-]?\\d{4}$\" > \n<\/div>\n\n```\n\nThis comes from a textbook (albeit a few years old) and I don't know why the browser just accepts anything.\n\n\nCan anyone see what might be preventing the regex from doing its job to get 7 or 10 digit phone numbers to be validated and other patterns to be rejected?\n\n\nWhen I take just the expression and test at regex101.com, the expression seems to do its job there.\n\n\n`^\\d{10}$|^(\\(\\d{3}\\)\\s*)?\\d{3}[\\s-]?\\d{4}$`\n\n\nOn regex101.com 1234567 will match and 1234567890 will match and other patterns with fewer than 7, more than 10 and patterns with 8, and 9 digits will be rejected.\n\n\nThe textbook's other regex validation is fine.\nIt doesn't seem to be related to the browser as it fails with two different browsers.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"## Why is it invalid\n\n\nThe value provided to a `pattern` attribute is converted into a regular expression with the [`v` flag](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/RegExp\/RegExp#flags) for [unicode sets](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/RegExp\/unicodeSets).\n\n\nThe [HTML standard](https:\/\/html.spec.whatwg.org\/multipage\/input.html#attr-input-pattern) defines that:\n\n\n\n> \n> The compiled pattern regular expression of an input element, if it exists, is a JavaScript RegExp object. It is determined as follows:\n> \n> \n> 1. If the element does not have a `pattern` attribute specified, then return nothing. The element has no compiled pattern regular expression.\n> 2. Let *pattern* be the value of the `pattern` attribute of the element.\n> 3. Let *regexpCompletion* be RegExpCreate(*pattern*, \"v\").\n> 4. If *regexpCompletion* is an abrupt completion, then return nothing. The element has no compiled pattern regular expression.\n> 5. Let *anchoredPattern* be the string \"^(?:\", followed by *pattern*, followed by \")$\".\n> 6. Return ! RegExpCreate(*anchoredPattern*, \"v\").\n> \n> \n> \n\n\nIn `v` (unicode sets) mode, [it is mandatory to escape a single dash inside a character class](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Regular_expressions\/Character_class#v-mode_character_class) (among other values that are valid outside `v`) even if it is next to a bracket. Demo:\n\n\n\n\n\n```\nconst nonV = new RegExp(\/[a-]\/);\n\nconsole.log('nonV match \"a\"', nonV.test(\"a\"));\nconsole.log('nonV match \"-\"', nonV.test(\"-\"));\nconsole.log('nonV match \"b\"', nonV.test(\"b\"));\n\ntry {\n  const v = new RegExp(\/[a-]\/, \"v\"); \/\/ throws an error\n\n  console.log('v match \"a\"', v.test(\"a\"));\n  console.log('v match \"-\"', v.test(\"-\"));\n  console.log('v match \"-\"', v.test(\"b\"));\n  \n} catch (error) {\n  console.error(\"cannot use unescaped dash:\", error.message)\n}\n\nconst vEscaped = new RegExp(\/[a\\-]\/, \"v\");\n\nconsole.log('vEscaped match \"a\"', vEscaped.test(\"a\"));\nconsole.log('vEscaped match \"-\"', vEscaped.test(\"-\"));\nconsole.log('vEscaped match \"b\"', vEscaped.test(\"b\"));\n```\n\n\n```\n.as-console-wrapper { max-height: 100% !important }\n```\n\n\n\n\n\n\n### When did it become invalid?\n\n\nThis was changed in April 2023 from the previous [`u` (unicode)](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/RegExp\/unicode) flag to `v` (unicode sets), thus resulting in a breaking change for HTML that uses the `pattern` attribute with certain unescaped values. From [the GutHub Pull Request for this change to the HTML standard](https:\/\/github.com\/whatwg\/html\/pull\/7908), Mathias Bynens said:\n\n\n\n> \n> **[BREAKING CHANGE]** Some previously valid patterns are now errors, specifically those with a character class including either an unescaped [special character](https:\/\/arai-a.github.io\/ecma262-compare\/snapshot.html?pr=2418#prod-ClassSetSyntaxCharacter) `(` `)` `[` `]` `{` `}` `\/` `-` `\\` `|` or [a double punctuator](https:\/\/arai-a.github.io\/ecma262-compare\/snapshot.html?pr=2418#prod-ClassSetReservedDoublePunctuator):\n> \n> \n> \n> ```\n> pattern=\"[(]\"\n> pattern=\"[)]\"\n> pattern=\"[[]\"\n> pattern=\"[{]\"\n> pattern=\"[}]\"\n> pattern=\"[\/]\"\n> pattern=\"[-]\"\n> pattern=\"[|]\"\n> pattern=\"[&&]\"\n> pattern=\"[!!]\"\n> pattern=\"[##]\"\n> pattern=\"[$$]\"\n> pattern=\"[%%]\"\n> pattern=\"[**]\"\n> pattern=\"[++]\"\n> pattern=\"[,,]\"\n> pattern=\"[..]\"\n> pattern=\"[::]\"\n> pattern=\"[;;]\"\n> pattern=\"[<<]\"\n> pattern=\"[==]\"\n> pattern=\"[>>]\"\n> pattern=\"[??]\"\n> pattern=\"[@@]\"\n> pattern=\"[``]\"\n> pattern=\"[~~]\"\n> pattern=\"[_^^]\"\n> \n> ```\n> \n> Throwing patterns result in `inputElement.validity.valid === true` for any input value, so the only compatibility risk is that some value\/pattern combinations that would previously result in `inputElement.validity.valid === false` now result in `inputElement.validity.valid === true`.\n> \n> \n> \n\n\nThe impact of if invalidating previously valid patterns was considered to be low and the trade off allows using more powerful patterns to be used. Examples from the PR:\n\n\n\n> \n> \n> ```\n> pattern=\"[\\p{ASCII_Hex_Digit}--[Ff]]\"\n> pattern=\"\\p{RGI_Emoji}\"\n> pattern=\"[_\\q{a|bc|def}]\"\n> \n> ```\n> \n> \n\n\n## Fix\n\n\nTherefore, the `-` in `[\\s-]` needs to be escaped in HTML:\n\n\n\n\n\n```\n:invalid {\n  background-color: red;\n  color: white;\n}\n```\n\n\n```\n<div class=\"formRow\">\n    <label for=\"phone\">Phone number<\/label>\n    <input name=\"custPhone\" id=\"phone\" type=\"tel\" placeholder=\"(nnn) nnn-nnnn\" \n     pattern=\"^\\d{10}$|^(\\(\\d{3}\\)\\s*)?\\d{3}[\\s\\-]?\\d{4}$\" > \n<\/div>\n```\n\n\n\n\n\n\nThe same fix would need to be applied to any other pattern that is now invalid with `v` (unicode sets) mode.\n\n\nFor more information see [Valid with the RegExp u flag, but not with the v flag](https:\/\/stackoverflow.com\/questions\/76285652\/valid-with-the-regexp-u-flag-but-not-with-the-v-flag)"}
{"questionId":"6a3830361dc241ffb3d9f19c39bf6b93","question":"Exact number of elements in std::array in function call\nI have a function that takes an `std::array` of given size `N`\n\n\n\n```\nvoid func(std::array<int,3> x) {\n  \/\/ do something\n}\n\nint main() {\n  func({4,4,4}) \/\/ works\n  func({4}) \/\/ works as well\n}\n\n```\n\nI understand why the second call works as well, my question is: is there a way to check at compile time how many arguments I've actually passed?\n\n\nThe background: I don't want to second call to be allowed, I want the user to pass exactly `N` arguments.","questionMetadata":{"Type":"Debugging","Level":"Advanced","Tag":"cpp"},"answer":"You can make the function more restrictive like this :\n\n\n\n```\n#include <type_traits>\n    \n\/\/ let sfinae disable mismatches\ntemplate<std::size_t N>\nauto func(const int (&v)[N]) -> std::enable_if_t<N==3,void>\n{\n  \/\/ do something\n}\n        \nint main() \n{\n    func({4,4,4}); \/\/ works\n    func({4}); \/\/ no longer compiles\n}\n\n```\n\nDemo : <https:\/\/onlinegdb.com\/FHlRINqCZ>"}
{"questionId":"322256199d6b44bf8e40f66533b3cd12","question":"Is psycopg3 a fork of psycopg2, or a replacement upgrade?\nI see references to both psycopg2 and psycopg3, but no clear guidance wrt a roadmap for transitioning between the two. I see that over time there is a large body of SO questions regarding psycopg2.\n\n\nIs psycopg3 intended to be a replacement for psycopg2? Has there been a significant uptake of this version?\n\n\nWill there be a long-lived version of psycopg2? Are there any compelling reasons to choose one version over the other?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"From the [documentation of psycopg3](https:\/\/www.psycopg.org\/psycopg3\/docs\/):\n\n\n\n> \n> Psycopg 3 is a newly designed PostgreSQL database adapter for the Python programming language.\n> \n> \n> Psycopg 3 presents a familiar interface for everyone who has used Psycopg 2 or any other DB-API 2.0 database adapter, but allows to use more modern PostgreSQL and Python features, such as:\n> \n> \n> - Asynchronous support\n> - COPY support from Python objects\n> - A redesigned connection pool\n> - Support for static typing\n> - Server-side parameters binding\n> - Prepared statements\n> - Statements pipeline\n> - Binary communication\n> - Direct access to the libpq functionalities\n> \n> \n> \n\n\nFrom a glance, psycopg3 appears to support more modern python and postgresql features like typing and async. Doing so likely required a lot of backwards-incompatible changes from psycopg2, hence the new version and forked development."}
{"questionId":"7adeaa9cf2b4432d8d9ead78ce3073c4","question":"Searching palindromes with grep -E (egrep)\n**UPD**: according to [Ed Morton's answer](https:\/\/stackoverflow.com\/a\/77861446\/23204874) the issue isn't related to `-E` option only.\n\n\nReading The Unix Programming Environment by Brian W. Kernighan and Rob Pike.\n\n\nWorking on an exercise, where one should find all palindromes in a wordlist using `grep -E`.\n\n\nThe most concise solution I've been able to find online and understand would be\n\n\n`'^(.?)(.?)(.?)(.?)(.?).?\\5\\4\\3\\2\\1$'`, which should find all palindromes up to 11 characters.\n\n\nFor some reason though, grep in my terminal gives me results that I can't explain:\n\n\n\n```\nme@machine:~\/test$ cat sample\na\nab\nabba\nabcdef\nabcba\nzufolo\nme@machine:~\/test$ grep -E '^(.?)(.?)(.?)(.?)(.?).?\\5\\4\\3\\2\\1$' sample\na\nabba\nabcba\nzufolo\nme@machine:~\/test$ grep -E '^(.?)(.?)(.?)(.?).?\\4\\3\\2\\1$' sample\na\nabba\nabcba\n\n\n```\n\nWhy would it add `zufolo` to the results (while not coloring any matches as it does with the actual palindromes)? Why does `zufolo` disappear when I reduce the regex to up to 9 characters?\n[output](https:\/\/i.stack.imgur.com\/zweqe.png)\n\n\nDoes it have anything to do with my setup (the most standard one)?\n\n\n\n```\nme@machine:~\/test$ ls -l \/proc\/$$\/exe\nlrwxrwxrwx 1 me me 0 jan 15 18:37 \/proc\/3528\/exe -> \/usr\/bin\/bash\nme@machine:~\/test$ cat \/proc\/version\nLinux version 6.5.0-14-generic (buildd@lcy02-amd64-110) (x86_64-linux-gnu-gcc-12 (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #14~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Nov 20 18:15:30 UTC 2\n\n```\n\nP.S. `zufolo` is from `wamerican-insane` where I'm actually testing it. There are plenty of others like `xenian`, `tartar`.\n\n\nI tried researching online, modifying regex, but can't get my head around this one.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"Strangely enough, removing the `$` from the end of the regexp (i.e. making it less restrictive) produces fewer matches, which is the opposite of what it should do:\n\n\na) With the `$` at the end of the regexp:\n\n\n\n```\n$ grep -E '^(.?)(.?)(.?)(.?)(.?).?\\5\\4\\3\\2\\1$' sample\na\nabba\nabcba\nzufolo\n\n```\n\nb) Without the `$` at the end of the regexp:\n\n\n\n```\n$ grep -E '^(.?)(.?)(.?)(.?)(.?).?\\5\\4\\3\\2\\1' sample\na\nabba\nabcba\n\n```\n\nIt's not just GNU grep that behaves strangely, GNU sed has the same behavior from the question when just matching with `sed -nE '\/...\/p' sample` as GNU `grep` does AND sed behaves differently if we're just doing a match vs if we're doing a match + replace.\n\n\nFor example here's `sed` doing a match+replacement and behaving the same way as `grep` above:\n\n\na) With the `$` at the end of the regexp:\n\n\n\n```\n$ sed -nE 's\/^(.?)(.?)(.?)(.?)(.?).?\\5\\4\\3\\2\\1$\/&\/p' sample\na\nabba\nabcba\nzufolo\n\n```\n\nb) Without the `$` at the end of the regexp:\n\n\n\n```\n$ sed -nE 's\/^(.?)(.?)(.?)(.?)(.?).?\\5\\4\\3\\2\\1\/&\/p' sample\na\nabba\nabcba\n\n```\n\nbut here's sed just doing a match and behaving differently from any of the above:\n\n\na) With the `$` at the end of the regexp (note the extra `ab` in the output):\n\n\n\n```\n$ sed -nE '\/^(.?)(.?)(.?)(.?)(.?).?\\5\\4\\3\\2\\1$\/p' sample\na\nab\nabba\nabcba\nzufolo\n\n```\n\nb) Without the `$` at the end of the regexp (note the extra `ab` and `abcdef` in the output):\n\n\n\n```\n$ sed -nE '\/^(.?)(.?)(.?)(.?)(.?).?\\5\\4\\3\\2\\1\/p' sample\na\nab\nabba\nabcdef\nabcba\nzufolo\n\n```\n\nAlso interestingly this:\n\n\n\n```\n$ sed -nE 's\/^(.?)(.?)(.?)(.?)(.?).?\\5\\4\\3\\2\\1$\/<&>\/p' sample\n\n```\n\noutputs:\n\n\n\n```\n<a>\n<abba>\n<abcba>\n<>zufolo\n\n```\n\nthe last line of which means the regexp is apparently matching the start of the line and ignoring the `$` end-of-string metachar present in the regexp!\n\n\nThe odd behavior isn't just associated with using `-E`, though, if I remove `-E` and just use [POSIX compliant BREs](https:\/\/pubs.opengroup.org\/onlinepubs\/9699919799\/basedefs\/V1_chap09.html#tag_09_03) then:\n\n\na) With the `$` at the end of the regexp:\n\n\n\n```\n$ grep '^\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\).\\{0,1\\}\\5\\4\\3\\2\\1$' sample\na\nabba\nabcba\nzufolo\n\n```\n\n\n\n```\n$ sed -n 's\/^\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\).\\{0,1\\}\\5\\4\\3\\2\\1$\/&\/p' sample\na\nabba\nabcba\nzufolo\n\n```\n\nb) Without the `$` at the end of the regexp:\n\n\n\n```\n$ grep '^\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\).\\{0,1\\}\\5\\4\\3\\2\\1' sample\na\nabba\nabcba\n\n```\n\n\n\n```\n$ sed -n 's\/^\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\).\\{0,1\\}\\5\\4\\3\\2\\1\/&\/p' sample\na\nabba\nabcba\n\n```\n\nand again just doing a match in sed below behaves differently from the sed match+replacements above:\n\n\na) With the `$` at the end of the regexp:\n\n\n\n```\n$ sed -n '\/^\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\).\\{0,1\\}\\5\\4\\3\\2\\1$\/p' sample\na\nab\nabba\nabcba\nzufolo\n\n```\n\nb) Without the `$` at the end of the regexp:\n\n\n\n```\n$ sed -n '\/^\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\)\\(.\\{0,1\\}\\).\\{0,1\\}\\5\\4\\3\\2\\1\/p' sample\na\nab\nabba\nabcdef\nabcba\nzufolo\n\n```\n\nThe above shows that, given the same regexp, sed is apparently matching different strings depending on whether it's doing a substitution or not.\n\n\nSo, there appear to be bugs in GNU grep and GNU sed without the `-E`. Or maybe the above is all just undefined behavior given those regexps and so any grep or sed can do whatever they like with them.\n\n\n\n```\n$ grep --version | head -1\ngrep (GNU grep) 3.11\n\n$ sed --version | head -1\nsed (GNU sed) 4.9"}
{"questionId":"a87496493a3a4071a1b1e3b6b05bf03e","question":"Azure AD B2C vs Azure Active Directory External Identities vs Microsoft Entra External ID\nWe need an auth service like [Auth0](https:\/\/auth0.com\/) or [Clerk](https:\/\/clerk.com\/). So Customer Identity and Access Management (CIAM)\n\n\nWe're building all our services in Azure so we'd like to user their authentication service for the closest integration with Web Apps, APIs and Front Door.\n\n\nHowever the Azure product naming seems incredibly complicated.\n\n\nReading this article titled [**Billing model for Azure Active Directory B2C**](https:\/\/learn.microsoft.com\/en-us\/azure\/active-directory-b2c\/billing) links to this pricing page titled **[Azure Active Directory External Identities pricing](https:\/\/azure.microsoft.com\/en-gb\/pricing\/details\/active-directory-external-identities\/)**\n\n\nSo is AD B2C the same as AD External Identities?\n\n\nThen within the Azure Portal we are often told about 'Entra' when building a AD B2C tenant and services and one of those pages takes me to this article titled **[Microsoft Entra External ID](https:\/\/www.microsoft.com\/en-us\/security\/business\/identity-access\/microsoft-entra-external-id)** which states in the FAQ at the bottom:\n\n\n\"Microsoft Entra External ID is a next-generation customer identity and access management (CIAM) solution for managing all external identities.\"\n\n\nand then:\n\n\n\"Azure AD B2C is our current generation customer identity and access management product. Azure AD B2C will continue to remain a fully supported customer solution. There are no requirements for customers to migrate at this time and no plans to discontinue our current B2C product. Microsoft is committed to continued investment in the Azure AD B2C product. We encourage you to try out the next-generation platform, Microsoft Entra External ID, and give us your feedback while sharing your priorities. Get started with Azure AD B2C\"\n\n\nHowever when we click on the text **Get started with Azure AD B2C** we are taken to a page titled [**Azure Active Directory External Identities**](https:\/\/azure.microsoft.com\/en-gb\/products\/active-directory-external-identities).\n\n\nSo would I be correct in saying that:\n\n\n1. Originally 'Azure Active Directory B2C' was called 'Azure Active Directory External Identities'\n2. That 'Azure Active Directory B2C' and 'Azure Active Directory External Identities' are the same product\n3. That this product is going to be replaced by 'Microsoft Entra External ID'\n4. That 'Microsoft Entra External ID' is effectively a re-factored version of 'Active Directory B2C' which will not rely on old Active Directory architecture and is built as a CIAM solution more similar to Auth0 or Clarke rather than an extension of B2B Active Directory?","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"other"},"answer":"> \n> 1. Originally 'Azure Active Directory B2C' was called 'Azure Active Directory External Identities'\n> \n> \n> \n\n\nThis is confusing for me as well. But it was definitely called AAD B2C from the start, AAD External Identities is a newer name.\nIt's been used to refer to features supporting external users in the \"regular\" AAD as well.\n\n\n\n> \n> 2. That 'Azure Active Directory B2C' and 'Azure Active Directory External Identities' are the same product\n> \n> \n> \n\n\nNo. AAD B2C is AAD B2C. But honestly this is very confusing.\n\n\n\n> \n> 3. That this product is going to be replaced by 'Microsoft Entra External ID'\n> \n> \n> \n\n\nYeah, like they say in the docs the development of new features is focused on Entra External Identities. See: <https:\/\/learn.microsoft.com\/en-us\/entra\/external-id\/customers\/faq-customers#as-a-new-customer-which-solution-is-a-better-fit-azure-ad-b2c-or-microsoft-entra-external-id-preview>.\n\n\n\n> \n> 4. That 'Microsoft Entra External ID' is effectively a re-factored version of 'Active Directory B2C' which will not rely on old Active Directory architecture and is built as a CIAM solution more similar to Auth0 or Clarke rather than an extension of B2B Active Directory?\n> \n> \n> \n\n\nIt's definitely built on Azure AD (now called Entra ID).\nIt uses the same URLs, the same management UIs partially, there is a \"b2c-extensions-app\" app registration in the tenant, just like in AAD B2C."}
{"questionId":"6a9ce9e82c2e441fa7e014cd456262f6","question":"Error: Type 'FontFeature' not found. Flutter google\\_fonts package error\nWhen using the latest version of google\\_fonts (6.2.0) package in flutter project, I'm facing this 'Type FontFeature not found' issue:\n\n\n\n```\n\/C:\/Users\/Dell\/AppData\/Local\/Pub\/Cache\/hosted\/pub.dev\/google_fonts-6.2.0\/lib\/src\/google_fonts_base.dart:69:8: \nError: 'FontFeature' isn't a type.\n  List<FontFeature>? fontFeatures,\n       ^^^^^^^^^^^\nTarget kernel_snapshot failed: Exception\n\n\nFAILURE: Build failed with an exception.\n\n* What went wrong:\nExecution failed for task ':app:compileFlutterBuildDebug'.\n> Process 'command 'C:\\flutter\\flutter\\bin\\flutter.bat'' finished with non-zero exit value 1\n\n```\n\nI tried downgrading the package version but the issue persists.\nAlso tried flutter clean and pub get..","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"dart"},"answer":"To everyone facing this issue, downgrade to an older version 6.1.0 (recommended) to get this issue solved.\nAlso, keep this in mind:\n\n\n\n> \n> When you downgrade packages, make sure to remove the caret (^) in front of the version number. For example: google\\_fonts: 6.1.0\n> \n> \n> \n\n\nThanks to [@Dhafin Rayhan](https:\/\/stackoverflow.com\/users\/13625293\/dhafin-rayhan) for pointing it out."}
{"questionId":"4419f68d0b774011b8112e81dd2dc40f","question":"How to migrate from textScaleFactor to TextScalar.scale\ntextScaleFactor` was recently deprecated and one is supposed to use the `TextScaler.scale(double fontSize)` instead, but not all places that take a `textScaleFactor` seemingly supports the new way.\n\n\nFor example `ButtonStyleButton.scaledPadding` is taking in a `textScaleFactor`, but what `fontSize` are you supposed to call `TextScalar.scale` with to pass in as the scale factor there?\n\n\n\n```\n    ButtonStyleButton.scaledPadding(\n      const EdgeInsets.symmetric(horizontal: 16),\n      const EdgeInsets.symmetric(horizontal: 8),\n      const EdgeInsets.symmetric(horizontal: 4),\n      MediaQuery.maybeOf(context)?.textScaler.scale(??),\n    );\n\n```\n\nPreviously:\n\n\n\n```\n    ButtonStyleButton.scaledPadding(\n      const EdgeInsets.symmetric(horizontal: 16),\n      const EdgeInsets.symmetric(horizontal: 8),\n      const EdgeInsets.symmetric(horizontal: 4),\n      MediaQuery.maybeOf(context)?.textScaleFactor,\n    );","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"dart"},"answer":"I have same problem. For the alternative maybe we can do this\n\n\n\n```\nfinal textScaleFactor = TextScaler.scale(10) \/ 10;"}
{"questionId":"01e6c63a3f59497eac44ac2e11e8891a","question":"How do I choose default python homebrew version?\nI have all three versions of python 3.10, 3.11 and 3.12 using homebrew. but somehow homebrew defaults to using 3.11 as the default.\n\n\nwhen i type `which python3`, it shows 3.11.6 as the version instead of 3.12. why did it default to 3.11 and how do i change this to 3.12?\n\n\nI was expecting the latest version 3.12 to be the default.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"It seems to me that Homebrew defaults to Python 3.11.6 due to the current configuration of your system PATH or the way Homebrew has linked the Python versions. To change the default version to Python 3.12, you might want to try the following steps:\n\n\n1. Check Homebrew Linking:   \n\nCheck the linking status of Python 3.12 using Homebrew:\n\n\n\n```\nbrew info python@3.12\n\n```\n\n2. Alter the System PATH (if necessary):   \n\nIf Python 3.12 is correctly linked but not coming first in the PATH, you may need to alter the system PATH. Update your shell profile (e.g., `~\/.zshrc` or `~\/.bash_profile`) to put the directory containing Python 3.12 executable before others:\n\n\n\n```\nexport PATH=\"\/usr\/local\/opt\/python@3.12\/bin:$PATH\"\n\n```\n\n3. Utilize Homebrew Linking (with caution):   \n\nAlternatively, you might try using Homebrew's `link` command to change the default Python version. Be cautious as this could lead to broken dependencies:\n\n\n\n```\nbrew link --overwrite python@3.12\n\n```\n\n4. Consider Using `pyenv`:   \n\nFor a more robust solution, consider using pyenv to manage multiple Python versions:\n\n\n\n```\nbrew install pyenv\npyenv install 3.12.0\npyenv global 3.12.0"}
{"questionId":"fffcff66f261498e889e42d59f06d90a","question":"How to recursively rename a list based on its list items\nI'd like to recursively rename (or name, as those items are currently unnamed) a `list()` based on its items (here `text`). There are several similar questions, however I haven't found one with a list structure as follows and I can't seem to find a general recursive approach to solve this.\n\n\nThe example data comes from [here](https:\/\/github.com\/stla\/jsTreeR\/blob\/master\/inst\/examples\/contextMenu\/global.R):\n\n\n\n```\nnodes <- list(\n  list(\n    text = \"RootA\",\n    children = list(\n      list(\n        text = \"ChildA1\"\n      ),\n      list(\n        text = \"ChildA2\"\n      )\n    )\n  ),\n  list(\n    text = \"RootB\",\n    children = list(\n      list(\n        text = \"ChildB1\"\n      ),\n      list(\n        text = \"ChildB2\"\n      )\n    )\n  )\n)\n# hard coded solution:\nnames(nodes) <- c(nodes[[1]]$text, nodes[[2]]$text)\nnames(nodes[[1]]$children) <- c(nodes[[1]]$children[[1]]$text, nodes[[1]]$children[[2]]$text)\nnames(nodes[[2]]$children) <- c(nodes[[2]]$children[[1]]$text, nodes[[2]]$children[[2]]$text)\nstr(nodes)\n\n```\n\nExpected output:\n\n\n\n```\nList of 2\n $ RootA:List of 2\n  ..$ text    : chr \"RootA\"\n  ..$ children:List of 2\n  .. ..$ ChildA1:List of 1\n  .. .. ..$ text: chr \"ChildA1\"\n  .. ..$ ChildA2:List of 1\n  .. .. ..$ text: chr \"ChildA2\"\n $ RootB:List of 2\n  ..$ text    : chr \"RootB\"\n  ..$ children:List of 2\n  .. ..$ ChildB1:List of 1\n  .. .. ..$ text: chr \"ChildB1\"\n  .. ..$ ChildB2:List of 1\n  .. .. ..$ text: chr \"ChildB2\"\n\n```\n\n\n\n---\n\n\n**Edit:** I just benchmarked the three answer given on my system. The function provided by @knitz3 seems to be the fastest. Thanks everyone - I learned a lot.\n\n\n\n```\nUnit: microseconds\n                  expr     min        lq     mean   median        uq     max neval\n list_rename_recursive  46.200   64.7010  458.389   79.601   95.2510 36040.6   100\n           modify_tree 886.102 1929.4005 2787.664 2302.801 2779.1010 18778.5   100\n            names_text 101.001  207.8015  575.603  246.852  305.9505 30270.8   100","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"r"},"answer":"This was fun. I went for something very interpretable. This function loops through every item of the list provided, and calls on itself if an item is itself another list. Should be able to handle unnamed list items as well.\n\n\n\n```\nlist_rename_recursive <- function(x) {\n\n    # If not a list, return the item\n    if (!is.list(x)) {\n\n        return(x)\n\n    } else {\n\n        # If a list, iterate through the items of the list\n        for (i in seq_along(x)) {\n\n            # If the list item i itself is a list, call\n            # the function again. The list item is updated\n            # with the returned value with proper name\n            # $text if found\n            if (is.list(x[[i]])) {\n\n                name_item <- NA\n                if (!is.null(x[[i]]$text)) name_item <- x[[i]]$text\n                x[[i]] <- list_rename_recursive(x[[i]])\n                if (!is.na(name_item)) names(x)[i] <- name_item\n\n            }\n\n        }\n\n        return(x)\n\n    }\n\n}\n\nnodes_new <- list_rename_recursive(nodes)\nstr(nodes_new)\n\n```\n\n\n```\nList of 2\n $ RootA:List of 2\n  ..$ text    : chr \"RootA\"\n  ..$ children:List of 2\n  .. ..$ ChildA1:List of 1\n  .. .. ..$ text: chr \"ChildA1\"\n  .. ..$ ChildA2:List of 1\n  .. .. ..$ text: chr \"ChildA2\"\n $ RootB:List of 2\n  ..$ text    : chr \"RootB\"\n  ..$ children:List of 2\n  .. ..$ ChildB1:List of 1\n  .. .. ..$ text: chr \"ChildB1\"\n  .. ..$ ChildB2:List of 1\n  .. .. ..$ text: chr \"ChildB2\""}
{"questionId":"f53b3a0bf2d2459d8fd542783add2d7e","question":"Xcode 15: Undefined symbols: Linker command failed with exit code 1 (use -v to see invocation)\nupon upgrading to Xcode 15, my widget extension is causing this error:\n`Undefined symbols: Linker command failed with exit code 1 (use -v to see invocation)`\n\n\nWhen pressing the error, nothing happens. It doesn't show me which symbols are undefined.\n\n\nHow should I fix this?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"try adding -ld\\_classic to the linker options\n\n\nfrom\n\n\n<https:\/\/developer.apple.com\/forums\/thread\/731089>\n\n\nthis worked in my case, It forces use of the old linker, the new linker is used by default in Xcode 15.\n\n\nAddition info here..\n\n\n<https:\/\/developer.apple.com\/forums\/thread\/715385>"}
{"questionId":"7fc3ef67c5f742bd8b800907457ca038","question":"Cannot locate tasks that match ':composeApp:compileJava' as task 'compileJava' is ambiguous in project ':composeApp'\nCannot locate tasks that match ':composeApp:compileJava' as task 'compileJava' is ambiguous in project ':composeApp'. Candidates are: 'compileDebugAndroidTestJavaWithJavac', 'compileDebugJavaWithJavac', 'compileDebugUnitTestJavaWithJavac', 'compileReleaseJavaWithJavac', 'compileReleaseUnitTestJavaWithJavac'.\n\n* Try:\n> Run gradle tasks to get a list of available tasks.\n> For more on name expansion, please refer to https:\/\/docs.gradle.org\/8.4\/userguide\/command_line_interface.html#sec:name_abbreviation in the Gradle documentation.\n> Run with --stacktrace option to get the stack trace.\n> Run with --info or --debug option to get more log output.\n> Run with --scan to get full insights.\n> Get more help at https:\/\/help.gradle.org.\nDeprecated Gradle features were used in this build, making it incompatible with Gradle 9.0.\nYou can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins.\nFor more on this, please refer to https:\/\/docs.gradle.org\/8.4\/userguide\/command_line_interface.html#sec:command_line_warnings in the Gradle documentation.\nBUILD FAILED in 189ms\n\n```\n\nI have created the project from the jetbrains website.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"java"},"answer":"It looks like a bug in the template. According to [Multiplatform: Cannot locate tasks that match ':composeApp:compileJava'](https:\/\/youtrack.jetbrains.com\/issue\/KT-63092\/Multiplatform-Cannot-locate-tasks-that-match-composeAppcompileJava), calling `gradle :composeApp:run` from Run Anything works on my machine."}
{"questionId":"1bbb16380cf649ffaa0025d216cc3365","question":"ModuleNotFoundError: No module named 'jupyter\\_server.contents'\nI got this error:\n\n\n\n```\nTraceback (most recent call last):\nFile \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\notebook\\traittypes.py\", line 235, in _resolve_classes\nklass = self._resolve_string(klass)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\Cristian Valiante\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\traitlets.py\", line 2025, in _resolve_string\nreturn import_item(string)\n^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\Cristian Valiante\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\utils\\importstring.py\", line 31, in import_item\nmodule = __import__(package, fromlist=[obj])\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'jupyter_server.contents'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\nFile \"C:\\ProgramData\\anaconda3\\Scripts\\jupyter-notebook-script.py\", line 10, in \nsys.exit(main())\n^^^^^^\nFile \"C:\\Users\\Cristian Valiante\\AppData\\Roaming\\Python\\Python311\\site-packages\\jupyter_core\\application.py\", line 280, in launch_instance\nsuper().launch_instance(argv=argv, **kwargs)\nFile \"C:\\Users\\Cristian Valiante\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1051, in launch_instance\napp = cls.instance(**kwargs)\n^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\Cristian Valiante\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\configurable.py\", line 575, in instance\ninst = cls(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\Cristian Valiante\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\traitlets.py\", line 1311, in __new__\ninst.setup_instance(*args, **kwargs)\nFile \"C:\\Users\\Cristian Valiante\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\traitlets.py\", line 1354, in setup_instance\nsuper(HasTraits, self).setup_instance(*args, **kwargs)\nFile \"C:\\Users\\Cristian Valiante\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\traitlets.py\", line 1330, in setup_instance\ninit(self)\nFile \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\notebook\\traittypes.py\", line 226, in instance_init\nself._resolve_classes()\nFile \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\notebook\\traittypes.py\", line 238, in _resolve_classes\nwarn(f\"{klass} is not importable. Is it installed?\", ImportWarning)\nTypeError: warn() missing 1 required keyword-only argument: 'stacklevel'\n\n```\n\nThank you for help ! :)\n\n\nI have tried installing and uninstalling and it didnt work.","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"python"},"answer":"This is a known error:\n\n\nThe Workaround:\nUninstall the Recent Problematic Release (v5.10.0) and Install the Prior Version (v5.9.0).\n\n\nCommand Line:\n\n\n\n```\npip uninstall traitlets\npip install traitlets==5.9.0\n\n```\n\nGit links:\n\n\n<https:\/\/github.com\/microsoft\/azuredatastudio\/issues\/24436#issuecomment-1723328100>\n\n\n<https:\/\/github.com\/jupyter\/notebook\/issues\/7048>"}
{"questionId":"7e832ec468b34292983b62bea6ac848f","question":"Purpose of integer literal suffix in left shift\nIn C, many operations employ bit shifting, in which an integer literal is often used. For example, consider the following code snippet:\n\n\n\n```\n#define test_bit(n, flag) (1UL << (n) & (flag))\n\n```\n\nAs far as I know, the integer literal suffix `UL` is supposed to suppress unwanted behavior in a shift, e.g. sign-extending a signed integer may result in multiple bits being set. However, if the case is doing a left shift only, as shown above, do we still need the integer literal suffix?\n\n\nAs a left shift won't cause unintended behavior, I can't figure what its purpose is. Code like the above often appears in projects such as Linux kernel, which makes me think that there must be a need for it. Does anyone know the purpose of the `UL` suffix in this case?","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"c"},"answer":"Sign extending only applies to right shifts, so that's not applicable.\n\n\n\n\n---\n\n\n`<<` is defined as follows:\n\n\n\n> \n> C23 \u00a76.5.7 \u00b64 The result of `E1 << E2` is `E1` left-shifted `E2` bit positions; vacated bits are filled with zeros. If `E1` has an unsigned type, the value of the result is `E1 \u00d7 2E2`, wrapped around. If `E1` has a signed type and nonnegative value, and `E1 \u00d7 2E2` is representable in the result type, then that is the resulting value; otherwise, the behavior is undefined.\n> \n> \n> \n\n\nThere are two ways in which left-shifting values can result in undefined behaviour based on `E1`:[1]\n\n\n- `E1` has a signed type and negative value.\n- `E1` has a signed type and nonnegative value, and `E1 \u00d7 2E2` is unrepresentable.\n\n\nIn our case, `E1` is a positive value, so the former isn't applicable. However, the latter could apply depending on the type of `E1`.\n\n\nLet's look at what results we get for different types on two systems.\n\n\n- System \"L\" has a 32-bit `int` and a 64-bit `long` (e.g. Linux on x86-64).\n- System \"W\" has a 32-bit `int` and a 32-bit `long` (e.g. Windows on x86-64).\n\n\n\n\n\n| Implementation | Usage | Result on \"L\" | Result on \"W\" |\n| --- | --- | --- | --- |\n| `1 << (n)` | `test_bit( 31, flag )` | Undefined behaviour | Undefined behaviour |\n| `1L << (n)` | `test_bit( 31, flag )` | ok (since long is 64 bits) | Undefined behaviour |\n| `1U << (n)` | `test_bit( 31, flag )` | ok | ok |\n| `1U << (n)` | `test_bit( 63, flag )` | Incorrect result | \u2014 |\n| `1L << (n)` | `test_bit( 63, flag )` | Undefined behaviour | \u2014 |\n| `1UL << (n)` | `test_bit( 63, flag )` | ok | \u2014 |\n\n\n\nSo, assuming you want to be able to test any of the bits of `flag`\n\n\n- `1U` is needed if `flag` can be a `signed int` or an `unsigned int` or shorter.\n- `1UL` is needed if `flag` can also be a `signed long` or an `unsigned long`.\n\n\n\n\n---\n\n\n1. Undefined behaviour can also result based on the value of `E2`. This happens if `E2` is negative, equal to the width of `E1`, or greater than the width of `E1`. This puts a constraint on the valid values for `test_bit`'s first argument."}
{"questionId":"1d563ee38ff0431e97739d2a8fabbbaa","question":"MongoDB via Brew Services \"undefined method `plist\\_startup'\"\nWhen I run MongoDB, I usually start it manually (i.e. it's not part of my login startup items) and I'm good about stopping the service before I shutdown.\n\n\nI recently restarted my laptop and received an error upon running:\n\n\n\n```\nbrew services run mongodb\/brew\/mongodb-community\n\n```\n\nThe error message reads:\n\n\n\n> \n> Error: undefined method `plist\\_startup' for #<Formula mongodb-community (stable) \/usr\/local\/Homebrew\/Library\/Taps\/mongodb\/homebrew-brew\/Formula\/mongodb-community.rb>\n> \n> \n> \n\n\nI'm not entirely sure what happened. I haven't installed any major updates or made any modifications to my environment.\n\n\n## What I've Tried\n\n\nI completely uninstalled MongoDB before installing it again:\n\n\n\n```\n# Uninstall each component of mongodb-community...\nbrew uninstall mongodb\/brew\/mongodb-community\nbrew uninstall mongodb\/brew\/mongodb-database-tools\nbrew uninstall mongosh\n\n# Reinstall all of the above...\nbrew install mongodb\/brew\/mongodb-community\n\n```\n\n#### Current Install\n\n\n`mongod --version`\n\n\n\n```\ndb version v7.0.2\nBuild Info: {\n    \"version\": \"7.0.2\",\n    \"gitVersion\": \"02b3c655e1302209ef046da6ba3ef6749dd0b62a\",\n    \"modules\": [],\n    \"allocator\": \"system\",\n    \"environment\": {\n        \"distarch\": \"x86_64\",\n        \"target_arch\": \"x86_64\"\n    }\n}\n\n```\n\n#### Could be Homebrew...?\n\n\nI'll be honest, sometimes on these late nights, I'm on autopilot. I may or may not have run a `brew upgrade` at some point. I'll look into whether or not this happened. In the meantime, when I get the log for MongoDB in Homebrew, I don't even see a commit that would have impacted me:\n\n\n\n```\nbrew log mongodb\/brew\/mongodb-community\n\n# Yields...\n\ncommit f33a59b6642f6a9f47f84b390dd71c386998cce6\nAuthor: Zack Winter <zack.winter@mongodb.com>\nDate:   Wed Oct 4 23:24:26 2023 +0000\n\n    Update paths with mr script\n\ncommit 7f3db6dbe9231300cc61645c68686a970b575f1f\nAuthor: Zack Winter <zack.winter@mongodb.com>\nDate:   Tue Oct 3 20:00:06 2023 +0000\n\n    SERVER-80537 update formulas for 7.0.1 release\n\ncommit 5055c34131148681885ea2241abccfc603596295\nAuthor: Alexander Neben <alexander.neben@mongodb.com>\nDate:   Fri Aug 18 10:54:02 2023 -0700","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"Recently Homebrew updated to 4.2, which changed the way it starts services. It required the mongodb formula to explicitly define a service block, even if just points to the service file.\n\n\nThe fix was merged in <https:\/\/github.com\/mongodb\/homebrew-brew\/commit\/2d0bfe19214d1f5071decd238b29306a6e82fff2>"}
{"questionId":"31ebc31882934d4d80f8bbb3d96c3cc3","question":"When is \"use lib PATH\" evaluated?\nI would like to set the library load paths for Raku and Inline::Perl5 modules from within my script while minimizing the runtime impact.\n\n\nI tried to replace\n\n\n\n```\nuse lib $*PROGRAM.resolve.parent(2) ~ '\/lib';\nuse lib $*PROGRAM.resolve.parent(2) ~ '\/Inline\/perl5';\n\n```\n\nwhich works with the following:\n\n\n\n```\nBEGIN {\n    my $root = $*PROGRAM.resolve.parent(2);\n    use lib \"$root\/lib\";\n    use lib \"$root\/Inline\/perl5\";\n\n```\n\nin order to save the second `.resolve.parent(2)` call.\nThis does not work as `$root` seems to be undefined when the `use lib` lines are being evaluated.","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"perl"},"answer":"use lib` is compile time. When you write\n\n\n\n```\nBEGIN {\n    my $root = $*PROGRAM.resolve.parent(2);\n    use lib \"$root\/lib\";\n}\n\n```\n\nthe `use lib \"$root\/lib\"` inside a `BEGIN` is essentially compile-compile time. What you would need to write is\n\n\n\n```\nBEGIN {\n    BEGIN my $root = $*PROGRAM.resolve.parent(2);\n    use lib \"$root\/lib\";\n}\n\n```\n\nor more succinctly\n\n\n\n```\nBEGIN my $root = $*PROGRAM.resolve.parent(2);\nuse lib \"$root\/lib\";"}
{"questionId":"d436eccca17b4245915ea84293effc4d","question":"How can a big number fit precisely into `double` with 32-bit GCC?\nConsider the following code:\n\n\n\n```\n#include <iostream>\nint main() {\n    long long x = 123456789123456789;\n    std::cout << std::fixed;\n    auto y = static_cast<double>(x);  \/\/ (1)\n    std::cout << static_cast<long long>(y) << \"\\n\";  \/\/ (2)\n    std::cout << y << \"\\n\";\n    std::cout << (x == static_cast<long long>(y)) << \"\\n\";  \/\/ (3)\n    std::cout << static_cast<long long>(static_cast<double>(x)) << \"\\n\";  \/\/ (4)\n    std::cout << (x == static_cast<long long>(static_cast<double>(x))) << \"\\n\";  \/\/ (5)\n}\n\n```\n\nWhen compiled with 32-bit GCC on Linux (`g++ -m32 a.cpp`), [it prints as follows](https:\/\/godbolt.org\/z\/MWxa3c4Wd):\n\n\n\n```\n123456789123456784\n123456789123456784.000000\n0\n123456789123456789\n1\n\n```\n\nNote that result of converting `long long` to `double` and then back to `long long` is different depending on how it was done. If I do it via a separate variable `double y` (lines `(1)` and `(2)`), the result ends with 4. But if I do everything in one expression (line `(4)`), the result ends with 9 just as the original value.\n\n\nThis is quite inconvenient: there exists no `double` that results in `123456789123456789` when converted to `long long`, and the check in line `(3)` confirms that. However, the check in the line `(5)` passes as if there is one. Is it a bug in GCC or my program?\n\n\nThis behavior started in GCC 9 according to Godbolt link above, GCC 8 works fine. Even funnier, if I add `-O2`, all expressions are optimized away during compilation and the output is:\n\n\n\n```\n123456789123456784\n123456789123456784.000000\n0\n123456789123456784\n0\n\n```\n\nAnd if I read `x` from `std::cin` and keep `-O2`, the intermediate variable ends in `4`, but after conversion to `long long` a wild `9` appears.\n\n\n\n```\n123456789123456789\n123456789123456784.000000\n1\n123456789123456789\n1\n\n```\n\nI believe there is no undefined behavior in the program above:\n\n\n- Conversion from an integer type to a floating-point type is kind of defined: it results in a rounded floating-point value. So either `123456789123456784` or `123456789123456790`.\n- Both of these values fit into `long long`.","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"GCC does document this non-standard behavior in <https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/Optimize-Options.html>.\n\n\nWhen no `-std=c++XX` option is given (i.e. if a default `-std=gnu++XX` is used), then `-fexcess-precision` is set to `fast`.\n\n\nThe effect of this is that, in violation of the C and C++ standards, GCC assumes that operations can always be carried out in higher precision than the types permit and whether this will happen in any given instance is unspecified.\n\n\nThe C and C++ standards however require casts and assignment to round to a value representable in the actual target type. Therefore the result of your checks is not permitted to be `1` as you describe. (However, for other, i.e. arithmetic, operations in a single expression the standards *do* explicitly permit operation in higher precision.)\n\n\nYou get the standard-conforming behavior with `-fexcess-precision=standard` which is automatically enabled with a `-std=c++XX` option. However, for C++, it has been implemented only since GCC 13.\n\n\nSimilarly GCC defaults to `-ffp-contract=fast`, which is also non-conforming and permits GCC to contract floating point operations across statements, i.e. to assume infinite precision intermediate results, while the standards again don't permit this across statements, casts or assignment. The standard-conforming option is `-ffp-contract=on` (or `-ffp-contract=off` to completely disable it).\n\n\n(This however doesn't mean that there aren't still bugs that make the behavior non-conforming as discussed in the bug reports linked in the other answer.)"}
{"questionId":"2f86a95881df47d783ef398f140f322a","question":"How can i fix warning that Class 'Shape' is exposed outside its defined visibility scope? (JAVA 17)\nI made function 'warn' in line 17 whose parameter is enum Shape.\nWhy warning about visibility scope and how can I fix it?\n\n\n\n```\nimport java.util.Scanner;\n\npublic class AreaCalculator {\n\n    enum Shape {TRIANGLE, RECTANGLE, CIRCLE}\n    static Scanner scanner = new Scanner(System.in);\n\n    public static void main(String[] args) {\n        String str = scanner.next();\n\n        while (!str.equals(\"quit\")){\n            str = str.toUpperCase();\n            warn(Shape.valueOf(str));\n        }\n    }\n\n    public static void warn(Shape shape) { \/\/warning\n\n    }\n\n```\n\nIntellij recommend generate overloaded method with default parameter values like following code.\n\n\n\n```\npublic static void warn(){\n    warn(null);\n}\n\n```\n\nBut I think it doesn't look intuitive.","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"java"},"answer":"Why is there a warning `Class 'Shape' is exposed outside its defined visibility scope`?\n\n\nBecause the `enum` `AreaCalculator.Shape` is only visible to classes in the same package, but the method `public static void warn(Shape shape)` is visible to any class.\n\n\nSo if we write a class:\n\n\n\n```\npackage a;\n\nimport b.AreaCalculator;\n\npublic class AreaCalculatorClient {\n    public static void main(String[] args) {\n        AreaCalculator.warn(AreaCalculator.Shape.CIRCLE);\n    }\n}\n\n```\n\nIt will fail to compile, because `'b.AreaCalculator.Shape' is not public in 'b.AreaCalculator'. Cannot be accessed from outside package`.\n\n\nThe fix is to with make `Shape` public or `warn` package-private, depending on your intent.\n\n\nThe fix suggested by IntelliJ IDEA is something you might do if you're convinced that you've chosen the correct visibility for `Shape`, and yet you want to call something like the `warn` method from arbitrary classes."}
{"questionId":"625baea6d0874e99b514540fb1ec0539","question":"Broker could not satisfy the silent request, Visual Studio\nI recently updated Visual Studio 2022 to version 17.7.3 and It logged me out of Visual Studio, and I am unable to use source control (We use TFS).\n\n\nafter entering the credentials, it pops up a message saying:\n\n\n\n> \n> We could not add the account\n> Broker could not satisfy the silent request\n> \n> \n> \n\n\nNote:\nWe're on a Amazon Workspace and Using Active Directory\n\n\ntried resetting user settings, restarting visual studio and the machine","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"other"},"answer":"A fix for me was authenticating outside visual studio embedded browser, go to\nTools -> Options -> Environment -> Accounts\nAnd change embedded browser to System Web browser and try again.\n\n\nCheck Image below:\n\n\n[Tools -> Options -> Environment -> Accounts](https:\/\/i.stack.imgur.com\/Fn1Q6.png)"}
{"questionId":"77584dc981894ba49ed28dd0ac15b894","question":"After installing new version of XCode 15.0 unable to run app XCode\nI have installed new version of Xcode 15.0. After this I am unable to run my flutter app. It is showing me following error:\n\n\n\n```\nError (Xcode): DT_TOOLCHAIN_DIR cannot be used to evaluate LIBRARY_SEARCH_PATHS, use TOOLCHAIN_DIR instead","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"I have solved those issues in this way:\n\n\n\n```\npost_install do |installer|\n  installer.pods_project.targets.each do |target| \n    flutter_additional_ios_build_settings(target)\n    target.build_configurations.each do |config|\n      xcconfig_path = config.base_configuration_reference.real_path\n      xcconfig = File.read(xcconfig_path)\n      xcconfig_mod = xcconfig.gsub(\/DT_TOOLCHAIN_DIR\/, \"TOOLCHAIN_DIR\")\n      File.open(xcconfig_path, \"w\") { |file| file << xcconfig_mod }\n      end\n  end\nend"}
{"questionId":"e99fe462bf2e4e20acdd9506f368c72a","question":"Problems installing libraries via pip after installing Python 3.12\ntoday I installed the new Python 3.12 on my Ubuntu 22.04 from the ppa repository **ppa:deadsnakes\/ppa**, everything works, but when I try to install some library with the command **python3.12 -m pip install somelibrary**, I get the following error\n\n\n\n```\nERROR: Exception:\nTraceback (most recent call last):\n  File \"\/usr\/lib\/python3\/dist-packages\/pip\/_internal\/cli\/base_command.py\", line 165, in exc_logging_wrapper\n    status = run_func(*args)\n             ^^^^^^^^^^^^^^^\n  File \"\/usr\/lib\/python3\/dist-packages\/pip\/_internal\/cli\/req_command.py\", line 205, in wrapper\n    return func(self, options, args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\/usr\/lib\/python3\/dist-packages\/pip\/_internal\/commands\/install.py\", line 285, in run\n    session = self.get_default_session(options)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\/usr\/lib\/python3\/dist-packages\/pip\/_internal\/cli\/req_command.py\", line 75, in get_default_session\n    self._session = self.enter_context(self._build_session(options))\n                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\/usr\/lib\/python3\/dist-packages\/pip\/_internal\/cli\/req_command.py\", line 89, in _build_session\n    session = PipSession(\n              ^^^^^^^^^^^\n  File \"\/usr\/lib\/python3\/dist-packages\/pip\/_internal\/network\/session.py\", line 282, in __init__\n    self.headers[\"User-Agent\"] = user_agent()\n                                 ^^^^^^^^^^^^\n  File \"\/usr\/lib\/python3\/dist-packages\/pip\/_internal\/network\/session.py\", line 157, in user_agent\n    setuptools_dist = get_default_environment().get_distribution(\"setuptools\")\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\/usr\/lib\/python3\/dist-packages\/pip\/_internal\/metadata\/__init__.py\", line 24, in get_default_environment\n    from .pkg_resources import Environment\n  File \"\/usr\/lib\/python3\/dist-packages\/pip\/_internal\/metadata\/pkg_resources.py\", line 9, in <module>\n    from pip._vendor import pkg_resources\n  File \"\/usr\/lib\/python3\/dist-packages\/pip\/_vendor\/pkg_resources\/__init__.py\", line 2164, in <module>\n    register_finder(pkgutil.ImpImporter, find_on_path)\n                    ^^^^^^^^^^^^^^^^^^^\nAttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n\n```\n\nAny suggestions why this is happening?\n\n\nEDIT: This problem doesn't exist when I use venv, it seems to me that the problem is that pip uses \/usr\/lib\/python3 instead of \/usr\/lib\/python3.12","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"python3.12 -m ensurepip --upgrade` fixed my problem!\n[solution](https:\/\/stackoverflow.com\/a\/77364602\/22307474)"}
{"questionId":"2821091fdf1248259979e33152350998","question":"fmt format %H:%M:%S without decimals\nI am trying to format a `std::chrono::duration` object to the format HH:MM::SS, e.g. 16:42:02 being the hours (16), the minutes (42), and the seconds (2).\n\n\nthe library `fmt` offers useful formatting specifiers for this:\n\n\n\n```\nusing namespace std::chrono;\n\nauto start = high_resolution_clock::now();\nauto end = start + 4s;\nfmt::print(\"{:%H:%M:%S} \\n\", end);\n\n```\n\nwhich unfortuantely prints the seconds in decimals\n\n\n\n```\n16:58:55.359425076 \n\n```\n\nI want to round this to the nearest integer, but cannot figure out where to place the precision specifier (precision 2 merely test-wise):\n\n\n\n```\nfmt::print(\"{:.2%H:%M:%S} \\n\", end);  \/\/ error\nfmt::print(\"{:.2f%H:%M:%S} \\n\", end);  \/\/ error \nfmt::print(\"{:%H:%M:.2%S} \\n\", end);  \/\/ nonsense: 17:07:.202.454873454 \n\n```\n\nI am a bit lost staring at the details of the [formatspec](https:\/\/fmt.dev\/latest\/syntax.html#formatspec) for chrono...\n\n\nA compiler explorer example of the above is [here](https:\/\/godbolt.org\/z\/zGcb6W84Y).","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"cpp"},"answer":"To round to the nearest second, convert your time point to seconds precision using `round<seconds>(tp)`. Also, `high_resolution_clock` has no portable relationship to the calendar. You need to use `system_clock` instead. For gcc, `high_resolution_clock` is a type alias of `system_clock`, so it works by accident. But this will fail to compile using MSVC or LLVM tools.\n\n\n\n```\n#include <fmt\/chrono.h>\n#include <fmt\/format.h>\n\n#include <chrono>\n#include <iostream>\n#include <thread>\n#include <vector>\n\nint main() {\n    using namespace std::chrono;\n\n    auto start = round<seconds>(system_clock::now());\n    auto end = start + 4s;\n    fmt::print(\"{:%H:%M:%S} \\n\", end);\n}\n\n```\n\n[Demo.](https:\/\/godbolt.org\/z\/qa5cdKPoc)\n\n\nIf you would like other rounding modes you can use `floor` or `ceil` as well."}
{"questionId":"cfd5630946fa4498a23589843307b4ec","question":"\"\\*s = 0\" being optimized out. Possible GCC 13 bug? Or some undefined behaviour?\nWith GCC 13.2, the output of the following code depends on the optimization level:\n\n\n\n```\n#include <ctype.h>\n#include <stdio.h>\n\nchar *SkipAName(char *s) {\n  if (('A' <= *s && *s <= 'Z') || ('a' <= *s && *s <= 'z') || *s == '_' ||\n      *s == '$') {\n    if (*s == '$') {\n      s++;\n    }\n    while (isalnum(*s)) {\n      s++;\n    }\n    if (*s == '_') {\n      s++;\n    }\n  }\n  return s;\n}\n\nint TestName(char *name) {\n  while (*name) {\n    name++;\n  }\n  return 0;\n}\n\nint StrICmp(char *s1, char *s2) {\n  while (*s1 && tolower(*s1) == tolower(*s2)) {\n    s1++;\n    s2++;\n  }\n  return tolower(*s1) - tolower(*s2);\n}\n\nint DoTable(char *s) {\n  char *name, c;\n\n  do {\n    name = s;\n    s = SkipAName(s);\n    c = *s;\n    *s = 0;\n    TestName(name);\n    *s = c;\n    if (*s == '(') {\n      break;\n    }\n    if (*s != ',') {\n      printf(\"Error 1\\n\");\n      return 1;\n    }\n    *s = 0;\n\n    if (StrICmp(name, \"sparse\") == 0) {\n    } else {\n      printf(\"Error 2\\n\");\n      return 1;\n    }\n    *s++ = ',';\n    while (*s == ',') {\n      s++;\n    }\n  } while (*s);\n\n  printf(\"OK\\n\");\n  return 0;\n}\n\nint main() {\n  char buf[] = \"sparse,C(1)\";\n  DoTable(buf);\n  return 0;\n}\n\n```\n\n\n```\n$ gcc-13 -O0 test.c && .\/a.out\nOK\n$ gcc-13 -O1 test.c && .\/a.out\nOK\n$ gcc-13 -O2 test.c && .\/a.out\nError 2\n$ gcc-13 -O3 test.c && .\/a.out\nError 2\n\n```\n\nThe code comes from [this project](https:\/\/github.com\/vermaseren\/form) and I have tried to make a minimal reproducible example; that is why this code may look awkward. See also [this issue](https:\/\/github.com\/vermaseren\/form\/issues\/461).\n\n\nI am wondering if this is a bug in GCC 13 or if I have hit some undefined behaviour. With [Compiler Explorer](https:\/\/gcc.godbolt.org\/z\/WYdcK5aMo), it looks like somehow `*s = 0` at line 52 is optimized away. The code works fine with GCC 12.3.","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"c"},"answer":"> \n> I am wondering if this is a bug in GCC 13 or if I have hit some undefined behaviour.\n> \n> \n> \n\n\n- The arguments to `isalnum()` and `tolower()` should be `char`s *that have been converted to type `unsigned char`* (before automatically being converted to `int` to match the parameter type). Example: `isalnum((unsigned char) *s)`. That issue could cause a similar program to manifest UB, but the input provided by the particular program in your example will not have that effect.\n- The code assumes that the uppercase Latin letters are encoded as a contiguous range of numbers in the execution character set, and the same for the lowercase Latin letters. C does not guarantee that this will be so, and if it isn't, then the program will not work as intended. However, that issue is not at all likely to be a problem for you in your test environment.\n- `SkipAName()` also seems to have weird rules for what it considers a name to skip, but that's not erroneous.\n- `TestName()` has no caller-observable effect. But that's not erroneous, and if the observed misbehavior depends on that function and the call to it being present in the program then that's very curious.\n- `StrICmp()` can run off the end of the second string, thus producing UB, but that will not happen with your particular input.\n- It's a bit nasty that `DoTable()` modifies its input, and especially that it does not consistently reverse its changes, since they seem intended only to serve its internal purposes. Among other things, that makes it unsafe to use with string literals. But your example input is not a string literal, and it is not inherently erroneous that `DoTable()` modifies it.\n- I do not like `DoTable()`'s use of parameter `s`. I am not altogether against functions modifying their parameters, but I am very much for using descriptive names. It's altogether unclear what the name \"`s`\" means that is both adequately descriptive and consistent with all the uses to which it is put. But of course, that doesn't make the code erroneous.\n\n\n**Overall**, this code has some issues, and I dislike it overall, but I see no undefined behavior in it as presented. The outputs you show for GCC 13 builds at higher optimization levels are wrong. That is, they reflect a bug in GCC 13."}
{"questionId":"b1116e2ccc094c16a2ee7c885e96c9fd","question":"Compare number to string with ranges and single numbers\nI have the following tibble\n\n\n\n```\nlibrary(tidyverse)\ntest <- tibble(A = c(\"1994:2020, 2021\"), B = 1995)\n\n```\n\nI would like to check if the year in B is in the years given in column A. The years in column A are a string (the data is read from an Excel file).\nThe following clearly doesn't work (it gives \"No\", but I would like to have \"Yes\"):\n\n\n\n```\ntest %>%\n  mutate(InA = ifelse(B %in% A, \"Yes\", \"No\"))\n\n> test\n# A tibble: 1 x 2\n  A                   B\n  <chr>           <dbl>\n1 1994:2020, 2021  1995\n\n```\n\nI assume that I have to separate the string in A. However, A can contain more than one range and\/or more than one year (e.g. ( \"1994:2012, 2014, 2016:2020, 2021\") and using \"separate\" for different structures gets complicated. Perhaps there is more straightforward way.","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"r"},"answer":"Tidyverse equivalent of @SamR\u2019s strategy\n\n\n\n```\nlibrary(tidyverse)\n\ntest <- tibble(A = c(\"1994:2020, 2021\"), B = 1995)\n\ntest %>%\n  mutate(InA = map2_lgl(A, B, ~ .y %in% eval(str2lang(\n    paste0(\"c(\", .x, \")\")\n  ))))\n#> # A tibble: 1 \u00d7 3\n#>   A                   B InA  \n#>   <chr>           <dbl> <lgl>\n#> 1 1994:2020, 2021  1995 TRUE\n\n```\n\nCreated on 2024-03-08 with [reprex v2.0.2](https:\/\/reprex.tidyverse.org)"}
{"questionId":"322a45950cfd494ebc332d35642959c5","question":"ovsdb-server.service from no where\ni update my system by:\n\n\n\n```\n$ apt-get upgrade\n\n```\n\nthen bad things happened, when i reboot the system, i had it get a timeout about network connection.\n\n\ni am pretty sure that, my network connection is fine (it unchanged during update), i can get ip allocated (both ethernet and wlan)\n\n\ni have consulted google:\n\n\n\n```\n# anyway, i was told to run\n$ sudo netplan apply\n# and i get\nWARNING:root:Cannot call Open vSwitch: ovsdb-server.service is not running.\n\n```\n\ni have never installed this ovsdb stuff in my server, but this warning is really annoying\n\n\n\n> \n> it may related to network timeout, or not\n> \n> \n> \n\n\nhow can i fix this (to erase this waring or just help me to solve network connection problem)\n\n\ni tried:\n\n\n\n```\n$ systemctl status systemd-networkd-wait-online.service\n\n```\n\nand i get:\n\n\n\n```\n\u00d7 systemd-networkd-wait-online.service - Wait for Network to be Configured\n     Loaded: loaded (\/lib\/systemd\/system\/systemd-networkd-wait-online.service; enabled; vendor preset: disabled)\n     Active: failed (Result: timeout) since Tue 2023-08-22 05:12:01 CST; 2 months 3 days ago\n       Docs: man:systemd-networkd-wait-online.service(8)\n    Process: 702 ExecStart=\/lib\/systemd\/systemd-networkd-wait-online (code=exited, status=0\/SUCCESS)\n   Main PID: 702 (code=exited, status=0\/SUCCESS)\n        CPU: 22ms\n\nAug 22 05:11:59 ubuntu systemd[1]: Starting Wait for Network to be Configured...\nAug 22 05:12:01 ubuntu systemd[1]: systemd-networkd-wait-online.service: start operation timed out. Terminating.\nAug 22 05:12:01 ubuntu systemd[1]: systemd-networkd-wait-online.service: Failed with result 'timeout'.\nAug 22 05:12:01 ubuntu systemd[1]: Failed to start Wait for Network to be Configured.","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"other"},"answer":"i have solved this problem\n\n\n`netplan apply` says ovsdb-server.service is not running, then i just install this `openvswitch`\n\n\nsince i run ubuntu server in raspberry pi, i need to install extra lib:\n\n\n\n```\n# run this first\n$ sudo apt-get install linux-modules-extra-raspi\n# run this then\n$ sudo apt-get install openvswitch-switch-dpdk\n\n```\n\nyou may need to check installation by run these command again\n\n\nafter the installation complete, no annoying WARNING shows again:\n\n\n\n```\n$ sudo netplan try\n\n```\n\nhowever, systemd-networkd-wait-online.service still timeout, no matter how many times you restart it\n\n\ni have consulted the [man page](https:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/systemd-networkd-wait-online.service.html) for systemd-networkd-wait-online.service usage\n\n\nthis service is just to wait all interface managed by [systemd-networkd](https:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/systemd-networkd.service.html#) are ready\n\n\nin fact, i only use ethernet interface and wlan interface, these interfaces work well\n\n\n\n```\n$ ip a\n# status of my interfaces\n\n```\n\nso i asked chatgpt about how to wait specific interfaces for systemd-networkd-wait-online.service\n\n\nit told my to add args in `\/lib\/systemd\/system\/systemd-networkd-wait-online.service`\n\n\n\n```\n$ vim \/lib\/systemd\/system\/systemd-networkd-wait-online.service\n[Service]\nType=oneshot\n# flag `--interface` is used to wait specific interface\n# in this case, i need to wait wlan interface and ethernet interface\nExecStart=\/lib\/systemd\/systemd-networkd-wait-online --interface=wlan0 --interface=eth0\nRemainAfterExit=yes\n# this parameter is used to set timeout, 30s is enough for my pi\nTimeoutStartSec=30sec\n\n```\n\nafter edition, you need to reload this script and restart service\n\n\n\n```\n$ systemctl daemon-reload\n$ systemctl restart systemd-networkd-wait-online.service\n\n```\n\nthat is all, everything gonna be fine (maybe)"}
{"questionId":"198e761e015547d884142951d98225a6","question":"How to suppress empty outputs of cargo test when running a specific test?\nFollowing up on Q\/A: [How to run a specific unit test in Rust?](https:\/\/stackoverflow.com\/a\/54586148\/1804173)\n\n\nWhen using the described solution `cargo test path::to::some_test -- --exact` in a project that mixes a library with binaries, how to suppress all the \"0 passed\" noise in the output?\n\n\nTo demonstrate the problem: I've create a minimal test project via `cargo new test-project --lib`. Then I've created the following files:\n\n\n\n```\n.\n\u251c\u2500\u2500 Cargo.toml\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 bin\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 bar.rs\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 baz.rs\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 foo.rs\n    \u2514\u2500\u2500 lib.rs\n\n```\n\nThe `lib.rs` contains:\n\n\n\n```\n#[cfg(test)]\nmod tests {\n    #[test]\n    fn test_foo() {\n        assert!(true);\n    }\n\n    #[test]\n    fn test_bar() {\n        assert!(true);\n    }\n}\n\n```\n\nAll three binaries are just `fn main() {}` dummies.\n\n\nWhen running `cargo test tests::test_foo -- --exact` I can see that filtering the unit tests to the specific `test_foo` works, but the output gets littered with \"running 0 tests\" from the binaries:\n\n\n\n```\n    Finished test [unoptimized + debuginfo] target(s) in 0.00s\n     Running unittests src\/lib.rs (target\/debug\/deps\/test_project-72d9bd396ce43ab9)\n\nrunning 1 test\ntest tests::test_foo ... ok\n\ntest result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 1 filtered out; finished in 0.00s\n\n     Running unittests src\/bin\/bar.rs (target\/debug\/deps\/bar-1d7d5b4c3ec748b3)\n\nrunning 0 tests\n\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n     Running unittests src\/bin\/baz.rs (target\/debug\/deps\/baz-ebd229498f923881)\n\nrunning 0 tests\n\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n     Running unittests src\/bin\/foo.rs (target\/debug\/deps\/foo-f88dc03912c7d051)\n\nrunning 0 tests\n\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n```\n\nMy real life project is a [cargo workspace](https:\/\/doc.rust-lang.org\/cargo\/reference\/workspaces.html) project with many workspace members, each containing various utility binaries. As a result of the behavior demonstrated above, running specific tests isn't very usable: Depending on the filter the matching results gets sandwiched by ~3 screen pages of \"running 0 tests\", and I when I misspell a test filter, and I get ~3 pages of actual 0 tests matched (which happily passes of course!) -- which is hard to tell apart.\n\n\nIs there a way to suppress the output from non-matching modules altogether?","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"rust"},"answer":"1. To reduce noise *when filtering tests*, pass a Cargo target selection option (and package selection if you are in a workspace), so you're only running the target that has the test you want, not all the targets. In this case, you would use `--lib`.\n\n\n\n```\ncargo test --lib -- --exact path::to::some_test\n\n```\n\n(It's not currently possible for Cargo to do this for you, because Cargo has no idea what tests each target contains.)\n2. To reduce noise *in general*, and save some build time too, you can disable testing for targets that don't have any tests. For each such target add `test = false` to the `Cargo.toml`:\n\n\n\n```\n[[bin]]\nname = \"foo\"\ntest = false\n\n[[bin]]\nname = \"bar\"\ntest = false\n\n[[bin]]\nname = \"baz\"\ntest = false\n\n```\n\nOf course, this comes with a risk of forgetting you wrote this config, and thus writing tests that don't get run."}
{"questionId":"e812d544d865430abbe71fde3fb55ea9","question":"How, and how best, to select a method\/sub based on a boolean?\nI have a method in a class to return a hash, with the aim that children classes will over-ride the method. But for testing purposes, I want a different hash.\nWhen the class is instantiated, I would prefer for the test option to be assumed to be False, so `:test` can be omitted when not needed.\n\n\nAt present I do this:\n\n\n\n```\nclass A {\n  has %.tmps;\n  submethod BUILD( :$test = False ) {\n    %!tmps = $test ?? self.test-tmps !! self.default-tmps\n  }\n  method test-tmps { ... }\n  method default-tmps { ... }\n}\n\n```\n\nThis does what I want, in that `my A $x .= new` calls default-tmps, and `my A $y .=new(:test)` calls test-tmps.\n\n\nBut I wondered about removing the explicit check of $test via multi-dispatch. Except I could not work out the appropriate syntax. I tried\n\n\n\n```\nclass A {\n  has %.tmps;\n  submethod BUILD( :$test = False ) {\n    %!tmps = self.get-tmps( :$test )\n  }\n  multi method get-tmps( :test($)! where * ) {\n     # return the hash for test values\n  }\n  multi method get-tmps( :test($)! where ! * ) {\n     # return the hash for default values\n  }\n}\n\n```\n\nThis does not work because I always get the test values, whether or not I specify `:test` in `new`. So my questions:\n\n\n1. How to select the multi method candidate based solely on a boolean's value?\n2. If what I am trying to do is possible, is there a reason (eg. run time \/ compile time checks) why an explicit check in `BUILD`, called by `new`, would be better than multi dispatch candidate selection?\n\n\nAnd then, if multi-dispatch works, would the following work, or would `$!test` be False because its undefined when `%.tmps` is built?\n\n\n\n```\nclass A {\n  has Bool $!test;\n  has %.tmps = self.get-tmps( :$!test );\n  submethod BUILD( :$!test = False ) {}\n  multi method get-tmps( :test($)! where * ) { ... }\n # replacing the following by whatever is the correct syntax\n  multi method get-tmps( :test($) where ! * ) { ... }\n}","questionMetadata":{"Type":"Implementation","Level":"Advanced","Tag":"perl"},"answer":"First of all, nowadays I would recommend using the `TWEAK` method, rather than the `BUILD` method. The TWEAK semantics are generally more DWIM.\n\n\nSecondly, there is nothing special about the `TWEAK` method (or the `BUILD` method, for that matter). So they can be `multi`!\n\n\nSo that brings me to the following solution:\n\n\n\n```\nclass A {\n    has %.temps;\n    multi submethod TWEAK(--> Nil) {\n        %!temps = a => 42;\n    }\n    multi submethod TWEAK(:$test! --> Nil) {\n        $test\n          ?? (%!temps = a => 666)  # testing\n          !! self.TWEAK(|%_);      # NOT testing\n    }\n}\n\nsay A.new(:test).temps;   # {a => 666}\nsay A.new(:!test).temps;  # {a => 42}\nsay A.new.temps;          # {a => 42}\n\n```\n\nNote the `!` in `:$test!`. This makes the named argument mandatory. So that candidate will be selected whenever the `test` named argument is specified. But this also includes when it is specified with a `False` value, as in `:!test`. That's why it needs to be tested for in that candidate.\n\n\nAlso note the `%_` in `self.TWEAK(|%_)`. All methods in Raku have an *implicit* `*%_` (slurpy hash) parameter defined. So you can use `%_` inside a method to indicate all arguments that were not caught by an explicit named parameter (such as `:$test` in this case). So `self.TWEAK(|%_)` is basically re-dispatching *without* all explicit named parameters.\n\n\nFinally: the `--> Nil` is just there to indicate that the method will not return a value. This may allow the compiler to produce more efficient bytecode."}
{"questionId":"b931bac30e4c490eb2ad9ed93ade780d","question":"How to ngIf in angular 17 with async pipe as a variable\nI have this in Angular 16\n\n\n\n```\n<div *ngIf=\"post$ | async as post\">\n  <h1>{{ post.attributes.title }}<\/h1>\n<\/div>\n\n```\n\nType of `post$` is  `Observable<ContentFile<BlogPost | Record<string, never>>>`\n\n\nI want to rewrite this with the new built-in syntax for control flow.\n\n\n\n```\n@if(post$|async as post) {\n\n}\n\n```\n\n`as post` part results in\n\n\n\n> \n> Parser Error: Unexpected token 'as' at column 13 in [post$|async as post] in ...\n> \n> \n> \n\n\nHow to get around this?","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"typescript"},"answer":"You're missing the semi colon :\n\n\n\n```\n@if(post$ | async; as post) {\n\n}"}
{"questionId":"245567659e884496b7c09489dc298bce","question":"Use @ngx-translate in standalone components in Angular 17\nI use standalone components in Angular 17. When I used module architecture, I didn't have this problem. I added it to import in `AppModule` and it works good.\n\n\n\n```\nimports: [\n TranslateModule.forRoot(loader)\n],\n\n```\n\nBut if I add `TranslateModule.forRoot(loader)` in standalone components\n\n\n\n```\n@Component({\n selector: 'app-main-work-space',\n standalone: true,\n imports: [\n  \/\/ @ts-ignore\n  TranslateModule.forRoot(loader)\n ],\n templateUrl: '.\/main-work-space.component.html',\n styleUrl: '.\/main-work-space.component.scss'\n})\n\n```\n\nIn result I have this mistake.\n\n\n`\u2718 [ERROR] TS-992012: 'imports' contains a ModuleWithProviders value, likely the result of a 'Module.forRoot()'-style call. These calls are not used to configure components and are not valid in standalone component imports - consider importing them in the application bootstrap instead.` \n\n\nI try add `@ts-ignore` but it doesn't help.\n\n\nHow can I fix it?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"typescript"},"answer":"I think you missed the `importProvidersFrom` wrapper for `translation module`, please find below working example along with extra sample code, to help you resolve your problem!\n\n\n\n```\nimport { Component, importProvidersFrom } from '@angular\/core';\nimport { TranslateService } from '@ngx-translate\/core';\nimport { CommonModule } from '@angular\/common';\nimport { provideHttpClient } from '@angular\/common\/http';\nimport { bootstrapApplication } from '@angular\/platform-browser';\nimport { AppModule } from '.\/app\/app.module';\nimport { BrowserModule } from '@angular\/platform-browser';\nimport { HttpClient, HttpClientModule } from '@angular\/common\/http';\nimport { TranslateModule, TranslateLoader } from '@ngx-translate\/core';\nimport { TranslateHttpLoader } from '@ngx-translate\/http-loader';\nimport 'zone.js';\n\n\/\/ AoT requires an exported function for factories\nexport function HttpLoaderFactory(httpClient: HttpClient) {\n  return new TranslateHttpLoader(httpClient);\n}\n\n@Component({\n  selector: 'app-root',\n  imports: [CommonModule, TranslateModule],\n  standalone: true,\n  template: `\n    <div>\n      <h2>{{ 'HOME.TITLE' | translate }}<\/h2>\n      <label>\n        {{ 'HOME.SELECT' | translate }}\n        <select #langSelect (change)=\"translate.use(langSelect.value)\">\n          <option *ngFor=\"let lang of translate.getLangs()\" [value]=\"lang\" [selected]=\"lang === translate.currentLang\">{{ lang }}<\/option>\n        <\/select>\n      <\/label>\n    <\/div>\n  `,\n})\nexport class AppComponent {\n  constructor(public translate: TranslateService) {\n    translate.addLangs(['en', 'fr']);\n    translate.setDefaultLang('en');\n\n    const browserLang = translate.getBrowserLang();\n    translate.use(browserLang.match(\/en|fr\/) ? browserLang : 'en');\n  }\n}\n\nbootstrapApplication(AppComponent, {\n  providers: [\n    provideHttpClient(),\n    importProvidersFrom(\n      TranslateModule.forRoot({\n        loader: {\n          provide: TranslateLoader,\n          useFactory: HttpLoaderFactory,\n          deps: [HttpClient],\n        },\n      })\n    ),\n  ],\n});\n\n```\n\n[stackblitz](https:\/\/stackblitz.com\/edit\/github-axafwc?file=src%2Fmain.ts,src%2Fapp%2Fapp.module.ts,src%2Fpolyfills.ts)\n\n\n[Stackblitz for reference not by me](https:\/\/stackblitz.com\/edit\/angular-rxe78f?file=src%2Fmain.ts)"}
{"questionId":"e242b7787e0d43aab415719f6feaaafd","question":"Compare all columns to each other one by one in a smart way\nThere are variables `x`, `y`, `z`. I want to filter any of them have no huge gap (the gap less then 5%). Below code can simulate, but if I want more variables for comparing, the code will be boring. Is there any smart way for this? Thanks!\n\n\n\n```\nlibrary(tidyverse)\n\ndiamonds %>%\n  select(x, y, z) %>%\n  filter(abs((x - y)\/ max(x, y)) < 0.05,\n         abs((x - z)\/ max(x, z)) < 0.05,\n         abs((y - z)\/ max(y, z)) < 0.05)","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"r"},"answer":"You can try `combn` + `rowMeans` like below\n\n\n\n```\ndiamonds %>%\n    filter(\n        rowMeans(\n            combn(\n                select(., x, y, z),\n                2,\n                \\(v) abs(v[[1]] - v[[2]]) \/ max(unlist(v)) < 0.05\n            )\n        ) == 1\n    )\n\n```\n\nor a base R option using `subset` + `Reduce` + `combn`\n\n\n\n```\nsubset(\n    diamonds,\n    Reduce(\n        `&`,\n        combn(\n            list(x, y, z),\n            2,\n            \\(v) abs(v[[1]] - v[[2]]) \/ max(unlist(v)) < 0.05,\n            simplify = FALSE\n        )\n    ) \n\n```\n\nwhich gives\n\n\n\n```\n# A tibble: 2,023 \u00d7 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 4  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 5  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 6  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 7  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 8  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n 9  0.3  Good      J     SI1      64      55   339  4.25  4.28  2.73\n10  0.23 Ideal     J     VS1      62.8    56   340  3.93  3.9   2.46\n# \u2139 2,013 more rows\n# \u2139 Use `print(n = ...)` to see more rows\n\n```\n\n# Benchmark\n\n\n\n```\nf1 <- function() {\n    diamonds %>%\n        filter(\n            rowMeans(\n                combn(\n                    select(., x, y, z),\n                    2,\n                    \\(v) abs(v[[1]] - v[[2]]) \/ max(unlist(v)) < 0.05\n                )\n            ) == 1\n        )\n}\n\n\nf2 <- function() {\n    subset(\n        diamonds,\n        Reduce(\n            `&`,\n            combn(\n                list(x, y, z),\n                2,\n                \\(v) abs(v[[1]] - v[[2]]) \/ max(unlist(v)) < 0.05,\n                simplify = FALSE\n            )\n        )\n    )\n}\n\nmicrobenchmark(\n    f1 = f1(),\n    f2 = f2(),\n    unit = \"relative\",\n    check = \"equivalent\",\n    times = 50L\n)\n\n```\n\nshows\n\n\n\n```\nUnit: relative\n expr      min       lq     mean   median       uq      max neval\n   f1 25.91684 23.46633 17.96225 22.78245 19.55927 7.241132    50\n   f2  1.00000  1.00000  1.00000  1.00000  1.00000 1.000000    50"}
{"questionId":"54b1d7fbdfcc4d449a9337bca71ac12e","question":"C# sync over async implementations\nI believe we all would agree that sync over async is not a good idea. The TAP model greatly simplifies the asynchronous calls with async\/await, and that's the way to go.\n\n\nUnfortunately, for anyone stuck with some old technologies, this is not an option. For example, web services (asmx) in .NET Framework, have no support for TAP. In large legacy solutions, maintaining duplicate call chains (sync and async) can be very intimidating and tiresome. Moreover, you may be using \"sync over async\" approaches even if you're not aware. A lot of libraries that provide sync APIs, in essence internally are just wrappers over their async APIs. Examples Rebus, RestSharp, and others.\n\n\nThat's the main purpose of this post\/question. Even though I have a fair understanding of async\/await, I won't pretend I know all the corner cases (and there are a lot). So, I'd like to know the opinion of the experts on this topic. What are the possible drawbacks of this implementation?\n\n\n<https:\/\/github.com\/restsharp\/RestSharp\/blob\/dev\/src\/RestSharp\/AsyncHelpers.cs>\n\n\nUnlike many \"sync over async\" implementations, where usually they just spin up a new thread to avoid possible deadlock `Task.Run(async () => {await ...}).Result`; this implementation uses custom synchronization context to avoid deadlocks while remaining on the same thread. This offers the ability to even access `HttpContext.Current` (for NETFX apps).\n\n\nI've tried this helper in various scenarios and it works fairly well. But, I'd like further opinion where it may go wrong.\n\n\nSo, how wrong is this? What are the pitfalls?","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"csharp"},"answer":"> \n> Unfortunately, for anyone stuck with some old technologies, this is not an option.\n> \n> \n> \n\n\nI do have an [article](https:\/\/learn.microsoft.com\/en-us\/archive\/msdn-magazine\/2015\/july\/async-programming-brownfield-async-development?WT.mc_id=DT-MVP-5000058) discussing several possible approaches of sync-over-async along with a short discussion of the drawbacks of each.\n\n\n\n> \n> For example, web services (asmx) in .NET Framework, have no support for TAP.\n> \n> \n> \n\n\nNo, but ASMX does support APM, and you can write a [short interop layer](https:\/\/learn.microsoft.com\/en-us\/dotnet\/standard\/asynchronous-programming-patterns\/interop-with-other-asynchronous-patterns-and-types?WT.mc_id=DT-MVP-5000058#from-tap-to-apm) to expose your core TAP logic to ASMX and remain async-all-the-way. There's some [examples on SO](https:\/\/stackoverflow.com\/a\/24082534\/263693) on how to do that and some [TAP-to-APM helpers](https:\/\/github.com\/StephenCleary\/AsyncEx\/blob\/0361015459938f2eb8f3c1ad1021d19ee01c93a4\/src\/Nito.AsyncEx.Tasks\/Interop\/ApmAsyncFactory.cs) in Nito.AsyncEx that make the interop layer very clean.\n\n\nThere are very few scenarios that *require* sync-over-async; it's usually a pragmatic decision to leave some of the code sync that really should be async, when converting the code isn't a business priority.\n\n\n\n> \n> In large legacy solutions, maintaining duplicate call chains (sync and async) can be very intimidating and tiresome.\n> \n> \n> \n\n\n100% agree.\n\n\nMy preferred solution is a more modern version of the \"boolean argument hack\" in the [Brownfield Async article](https:\/\/learn.microsoft.com\/en-us\/archive\/msdn-magazine\/2015\/july\/async-programming-brownfield-async-development?WT.mc_id=DT-MVP-5000058), which was originally shown to me by Stephen Toub during a technical review of that article. It's a technique that keeps the code either async-all-the-way or sync-all-the-way, but without requiring any duplication of logic.\n\n\nMore recently, Stephen Toub showed off the more modern version of the \"boolean argument hack\" in his [article on .NET 7 performance improvements](https:\/\/devblogs.microsoft.com\/dotnet\/performance_improvements_in_net_7\/?WT.mc_id=DT-MVP-5000058#:%7E:text=One%20final%20change%20related%20to%20reading%20and%20writing%20performance) (it's a huge article; search for \"One final change related to reading and writing performance\" to find the relevant section). I pulled out that gem and wrote a [more specific blog post about it](https:\/\/blog.stephencleary.com\/2022\/10\/modern-csharp-techniques-3-generic-code-generation.html). The code looks weird at first but it's a really powerful technique and that's what I recommend for all modern libraries.\n\n\n\n> \n> What are the possible drawbacks of this implementation?\n> \n> \n> \n\n\nThis implementation installs a custom `SynchronizationContext` that contains a queue of work, queues the initial work item, and then synchronously waits for its queue of work to be complete. It's similar to `AsyncContext` from my AsyncEx library. I believe that implementation is originally sourced from [this old forum post](https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/163ef755-ff7b-4ea5-b226-bbe8ef5f4796\/is-there-a-pattern-for-calling-an-async-method-synchronously), which I have seen copied around a few places, usually with a comment like \"I have no idea how this works\", which to be honest is kind of terrifying to me. I take code from SO and other places, but *only* after fully understanding it.\n\n\nYou could say it's a variant of the \"nested message loop hack\" from [the Brownfield Async article](https:\/\/learn.microsoft.com\/en-us\/archive\/msdn-magazine\/2015\/july\/async-programming-brownfield-async-development?WT.mc_id=DT-MVP-5000058). `AsyncHelpers.RunSync` takes control of the current thread and turns it into a message pump, processing its own queue of work. It installs its own `SynchronizationContext` to capture `async` continuations ([which by default resume on the captured `SynchronizationContext` or `TaskScheduler`](https:\/\/blog.stephencleary.com\/2012\/02\/async-and-await.html) as I describe on my blog).\n\n\nSo, the corner cases you're going to run into all have to do with that `SynchronizatonContext` swap. An exhaustive list may not be possible, but here's some concerns off the top of my head:\n\n\n1. Some components require a specific `SynchronizationContext`. One example is in the pre-Core ASP.NET days, some ASP.NET APIs will just *hang* if `SynchronizationContext.Current` is not an `AspNetSynchronizationContext`. I don't really know why they do that, but it is behavior I've observed when attempting this hack many years ago. As another example, some UI components will verify they're on the correct synchronization context (others verify they're on the correct *thread*, which still works fine if the SyncCtx is swapped).\n2. Some components capture a `SynchronizationContext` for later, but the `SynchronizationContext` used here has a limited lifetime; once the tasks complete, the whole SyncCtx is torn down. So anything like `Progress<T>` or an Rx observable observing on that SyncCtx must not be used after the SyncCtx is torn down.\n3. This solution installs a single-threaded context and then synchronously blocks on it. So, it solves a class of sync-over-async issues, but if anything it calls does a blocking-style sync-over-async, it will deadlock for sure.\n4. The final one is one of my biggest concerns, but the hardest to explain. In my article I kind of hand-wave it as \"unexpected reentrancy\". This kind of approach may have surprising results if run on a UI thread (or more specifically, an STA thread). CBrumme had some great blog posts about how the .NET runtime would do *some* STA pumping when blocking; those posts were sadly taken down a few years ago when MS changed their blog URI schemes. Essentially, it means that some Windows messages may be processed by the UI thread even if it's \"blocked\" from a managed perspective; this may cause parts of your code to run that you're not expecting to (in this case, running as part of `RunSync` instead of the window's main message processing loop). Now, these posts were taken down, and the [modern .NET Core `AutoResetEvent.WaitOne` ends up at `WaitForMultipleObjectsIgnoringSyncContext`](https:\/\/github.com\/dotnet\/runtime\/blob\/8e1e6f5e1c80fae44ce3b6e703d5b30d45122676\/src\/libraries\/System.Private.CoreLib\/src\/System\/Threading\/WaitHandle.Windows.cs#L21), which from the name *sounds* like it might not be partially pumping, so maybe that's just not true anymore. But for myself, I'd be very wary of doing something like this on an STA\/GUI thread.\n\n\nIn summary, it's not an approach I recommend. I recommend using the generic-value-type-constrained-interface approach instead. But, if you have a strong understanding of all the code that will ever be run synchronously and are sure you won't run into the situations above, then it would be acceptable."}
{"questionId":"14b4c5289ad44ed48d48e8db280a2d6c","question":"Build function before passing it to other functions\nMy goal is to take additional arguments passed through the ellipsis `...` (see `?dots` for more info) and build a new generic function with the parameters already set and pass this to another function.\n\n\nFor example, given the two functions:\n\n\n\n```\nfoo <- function(v, FUN, ...) {\n    ## code here to build NEWFUN\n    SomeFun(v, NEWFUN)\n}\n\nbar <- function(v, FUN) {\n    SomeFun(v, FUN)\n}\n\n```\n\nI would like to be able to do this in `foo`:\n\n\n\n```\nbar(x, FUN = \\(x) paste(x, collapse = \", \"))\n\n```\n\nBy calling `foo(x, paste, collapse = \", \")`.\n\n\n## My Attempt\n\n\nWe start with a simple function takes a base `R` function (here `paste`) and applys it to a vector. Note, I'm trying to make this as simple as possible, so I have removed sanity checks. Also, I wrote this to be demonstrated only with the base `R` function `paste`.\n\n\n\n```\nFunAssign <- function(f, x) f(x)\n\n```\n\nAnd here is my naive attempt:\n\n\n\n```\nfoo <- function(v, FUN, ...) {\n    FUN <- \\(x) FUN(x, ...)\n    FunAssign(FUN, v)\n}\n\n```\n\nAnd calling it, we get the error:\n\n\n\n```\nfoo(letters[1:5], paste, collapse = \", \")\n#> Error in FUN(x, ...) : unused argument (collapse = \", \")\n\n```\n\nAs explained above, the desired output when calling `foo(letters[1:5], paste, collapse = \", \")` can be emulated by calling `bar` like so:\n\n\n\n```\nbar <- function(v, FUN) FunAssign(FUN, v)\n\nbar(letters[1:5], FUN = \\(x) paste(x, collapse = \", \"))\n#> [1] \"a, b, c, d, e\"\n\n```\n\nI thought my attempt with `foo` would work because if we do something like the below it appears that we are on the right track:\n\n\n\n```\nbaz <- function(FUN, ...) \\(x) FUN(x, ...)\n\nbaz(paste, collapse = \", \")(letters[1:5])\n#> [1] \"a, b, c, d, e\"\n\n```\n\nI have found several resources that almost get at what I'm after, but nothing that quite hits the spot.\n\n\n- [R: using a list for ellipsis arguments](https:\/\/stackoverflow.com\/q\/3142731\/4408538). This one is good, however `do.call` evaluates the expression.\n- And then there is [How to create an R function programmatically?](https:\/\/stackoverflow.com\/q\/12982528\/4408538). But again, this looks like it is evaluating the function and thus returning a value.","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"r"},"answer":"I think your first idea would work if you didn't overwrite the `FUN` variable in `foo()`. That is, this works for me:\n\n\n\n```\nFunAssign <- function(f, x) f(x)\n\nfoo <- function(v, FUN, ...) {\n    force(FUN)\n    FUN2 <- \\(x) FUN(x, ...)\n    FunAssign(FUN2, v)\n}\n\nfoo(letters[1:5], paste, collapse = \", \")\n\n```\n\nBesides renaming the second `FUN` in `foo()`, I added the `force(FUN)` command. This is not necessary in this example, but generally speaking it's a good idea to make sure arguments that are needed in a created function are evaluated. I think that automatically happens in this code (since `FunAssign` will use it), but I'm superstitious about things like that.\n\n\nEdited to add: the explanation for this is fairly simple.\n\n\n- The variable `FUN` in the function `FUN2()` is not evaluated until `FUN2()` is called.\n- When `FUN2` is called within `FunAssign(FUN2, v)`, R tries to evaluate `FUN(x, ...)`.\n- Since `FUN` is not defined in that function, R looks in the parent environment, and finds `FUN` in the evaluation frame of `foo()` as an argument to the function call.\n- In your original code where you had both objects named `FUN`, that lookup happened after you had overwritten the argument with a different variable (the function I called `FUN2`).\n- When R tries to evaluate `FUN(x, ...)`, in your code the `FUN` it is working with is the one that was defined right there, and R attempts a recursive call, but gives an error because that function only has one argument named `x`.\n\n\nThe key thing here is that the body of a function is just an unevaluated expression. It doesn't get evaluated until you call the function, and that's when R tries to find all the objects it uses, including the functions it calls."}
{"questionId":"4ac30fba8ee64a0ba6ce09b9858c8db8","question":"When satisfation of SFINAE condition depends on the function being declared, is that condition guaranteed to be false?\nConsider this code:\n\n\n\n```\n#include <type_traits>\n\nstruct A {};\n\nstruct B\n{\n    template <typename T, std::enable_if_t<!std::is_constructible_v<B, T &&>, std::nullptr_t> = nullptr>\n    B(T &&) {}\n};\n\nint main()\n{\n    B b(A{});\n}\n\n```\n\nBoth Clang, GCC, and MSVC [accept it](https:\/\/gcc.godbolt.org\/#g:!((g:!((g:!((h:codeEditor,i:(filename:%271%27,fontScale:14,fontUsePx:%270%27,j:1,lang:c%2B%2B,selection:(endColumn:2,endLineNumber:12,positionColumn:2,positionLineNumber:12,selectionStartColumn:2,selectionStartLineNumber:12,startColumn:2,startLineNumber:12),source:%27%23include+%3Ctype_traits%3E%0A%0Astruct+A+%7B%7D%3B%0A%0Astruct+B%0A%7B%0A++++template+%3Ctypename+T,+std::enable_if_t%3C!!std::is_constructible_v%3CB,+T+%26%26%3E,+std::nullptr_t%3E+%3D+nullptr%3E%0A++++B(T+%26%26)+%7B%7D%0A%7D%3B%0A%0Aint+main()%0A%7B%0A++++B+b(A%7B%7D)%3B%0A%7D%27),l:%275%27,n:%270%27,o:%27C%2B%2B+source+%231%27,t:%270%27)),k:50,l:%274%27,n:%270%27,o:%27%27,s:0,t:%270%27),(g:!((h:conformance,i:(compilers:!((compilerId:gsnapshot,options:%27-std%3Dc%2B%2B23+-Wall+-Wextra+-pedantic-errors%27),(compilerId:clang_trunk,options:%27-std%3Dc%2B%2B23+-Wall+-Wextra+-pedantic-errors%27),(compilerId:vcpp_v19_latest_x64,options:%27\/std:c%2B%2Blatest%27)),editorid:1,langId:c%2B%2B,libs:!()),l:%275%27,n:%270%27,o:%27Conformance+Viewer+(Editor+%231)+3\/15%27,t:%270%27)),header:(),k:50,l:%274%27,n:%270%27,o:%27%27,s:0,t:%270%27)),l:%272%27,n:%270%27,o:%27%27,t:%270%27)),version:4).\n\n\nIs this code valid, or should the compilers reject it?\n\n\nInterestingly, if I use `requires` instead of `enable_if_t`, GCC and Clang start [rejecting it](https:\/\/gcc.godbolt.org\/#g:!((g:!((g:!((h:codeEditor,i:(filename:%271%27,fontScale:14,fontUsePx:%270%27,j:1,lang:c%2B%2B,selection:(endColumn:48,endLineNumber:8,positionColumn:5,positionLineNumber:8,selectionStartColumn:48,selectionStartLineNumber:8,startColumn:5,startLineNumber:8),source:%27%23include+%3Ctype_traits%3E%0A%0Astruct+A+%7B%7D%3B%0A%0Astruct+B%0A%7B%0A++++template+%3Ctypename+T%3E%0A++++requires(!!std::is_constructible_v%3CB,+T+%26%26%3E)%0A++++B(T+%26%26)+%7B%7D%0A%7D%3B%0A%0Aint+main()%0A%7B%0A++++B+b(A%7B%7D)%3B%0A%7D%27),l:%275%27,n:%270%27,o:%27C%2B%2B+source+%231%27,t:%270%27)),k:50,l:%274%27,n:%270%27,o:%27%27,s:0,t:%270%27),(g:!((h:conformance,i:(compilers:!((compilerId:gsnapshot,options:%27-std%3Dc%2B%2B23+-Wall+-Wextra+-pedantic-errors%27),(compilerId:clang_trunk,options:%27-std%3Dc%2B%2B23+-Wall+-Wextra+-pedantic-errors%27),(compilerId:vcpp_v19_latest_x64,options:%27\/std:c%2B%2Blatest%27)),editorid:1,langId:c%2B%2B,libs:!()),l:%275%27,n:%270%27,o:%27Conformance+Viewer+(Editor+%231)+3\/15%27,t:%270%27)),header:(),k:50,l:%274%27,n:%270%27,o:%27%27,s:0,t:%270%27)),l:%272%27,n:%270%27,o:%27%27,t:%270%27)),version:4), while MSVC still accepts. The error boils down to `satisfaction of constraint depends on itself`.\n\n\nBut shouldn't the same thing cause an error in the first code? Or is this undefined?\n\n\n\n```\n#include <type_traits>\n\nstruct A {};\n\nstruct B\n{\n    template <typename T>\n    requires(!std::is_constructible_v<B, T &&>)\n    B(T &&) {}\n};\n\nint main()\n{\n    B b(A{});\n}","questionMetadata":{"Type":"Debugging","Level":"Advanced","Tag":"cpp"},"answer":"is_constructible_v<T...>` is an `inline constexpr bool`.\n\n\n`B b(A{});` does an overload resolution to pick a viable constructor.\n\n\nWhen checking if `template <typename T, std::enable_if_t<!std::is_constructible_v<B, T &&>, std::nullptr_t> = nullptr> B(T &&)` is viable, `std::is_constructible_v<B, T &&>` is evaluated, where `T` is deduced as `A`.\n\n\nTo evaluate `std::is_constructible_v<B, A &&>`, the variable template is instantiated and it's initializer is evaluated. Overload resolution is done again for `B t(declval<A&&>());` ([[meta.unary.prop]p9](https:\/\/wg21.link\/meta.unary.prop#9)).\n\n\nThis again tries the same constructor template and has to check for the value of `std::is_constructible_v<B, A &&>`. However, this refers to the *same* variable `std::is_constructible_v<B, A &&>` from beforehand. Since the initialization isn't finished, the variable is not in its lifetime ([[basic.life]p(1.2)](https:\/\/wg21.link\/basic.life#1.2)), so reading it is not a constant expression, so `std::is_constructible_v<B, A &&>` isn't a constant expression in this case. This is a SFINAE-able error, so the constructor isn't viable.\n\n\nThis means that the original `std::is_constructible_v<B, A &&>` is now initialized to `false`, making your original constructor viable.\n\n\nA similar thing happens if you have `std::is_constructible<B, T &&>::value`, except `std::is_constructible<B, A &&>` is an incomplete type the second time around and `::value` is the SFINAE error.\n\n\n\n\n---\n\n\nI think this is still IF-NDR. The second overload resolution depends on the value of `std::is_constructible_v<B, A &&>`, which would have a different result depending on where it is evaluated (`true` or `false` instead of \"not yet in lifetime\"). And [[temp.point]p7](https:\/\/wg21.link\/temp.point#7.sentence-4):\n\n\n\n> \n> If two different points of instantiation give a template specialization different meanings according to the one-definition rule, the program is ill-formed, no diagnostic required.\n> \n> \n> \n\n\nThough I could not find how points of instantiation are defined for variable templates (or the initializer for a variable template)."}
{"questionId":"5ef28118067c49c280bd258161bb8f2b","question":"How to make a Raku test optional if a module is not installed\nI am adding a feature to a package that optionally uses `DBIish`. I would like to write a few unit tests to ensure that the functionality is working. However, `DBIish` should be an *optional* dependency, meaning you don't need it to install or use the package. How can I skip `DBIish` tests if the user does not have `DBIish` installed?\n\n\nSomething like:\n\n\n\n```\n{\n    require ::('DBIish');\n    CATCH {\n        default {\n            skip-testing;\n        }\n    }\n}\n\n```\n\n... would be ideal, but an optional test flag would also work, but I can't seem to find any documentation on the topic.","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"perl"},"answer":"If it's about importing a class that should be installed (or not), I always use the following \"trick\":\n\n\n\n```\nmy \\Foo = try \"use Foo; Foo\".EVAL;\n\n```\n\nIf it worked, then `Foo` will be the class. If it didn't, then `Foo` will be `Nil`."}
{"questionId":"dfce2c492e8946b491a02a8f63399aa4","question":"AttributeError: cython\\_sources\nI am using:\n\n\n\n```\npython: 3.12\n\nOS: Windows 11 Home\n\n```\n\nI tried to install `catboost==1.2.2`\n\n\nI am getting this error:\n\n\n\n```\nC:\\Windows\\System32>py -3 -m pip install catboost==1.2.2\nCollecting catboost==1.2.2\n  Downloading catboost-1.2.2.tar.gz (60.1 MB)\n     ---------------------------------------- 60.1\/60.1 MB 5.1 MB\/s eta 0:00:00\n  Installing build dependencies ... error\n  error: subprocess-exited-with-error\n\n  \u00d7 pip subprocess to install build dependencies did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [135 lines of output]\n      Collecting setuptools>=64.0\n        Using cached setuptools-68.2.2-py3-none-any.whl (807 kB)\n      Collecting wheel\n        Using cached wheel-0.41.3-py3-none-any.whl (65 kB)\n      Collecting jupyterlab\n        Downloading jupyterlab-4.0.8-py3-none-any.whl (9.2 MB)\n           ---------------------------------------- 9.2\/9.2 MB 7.8 MB\/s eta 0:00:00\n      Collecting conan<=1.59,>=1.57\n        Downloading conan-1.59.0.tar.gz (780 kB)\n           -------------------------------------- 781.0\/781.0 kB 4.9 MB\/s eta 0:00:00\n        Installing build dependencies: started\n        Installing build dependencies: finished with status 'done'\n        Getting requirements to build wheel: started\n        Getting requirements to build wheel: finished with status 'done'\n        Preparing metadata (pyproject.toml): started\n        Preparing metadata (pyproject.toml): finished with status 'done'\n      Collecting async-lru>=1.0.0 (from jupyterlab)\n        Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n      Collecting ipykernel (from jupyterlab)\n        Downloading ipykernel-6.26.0-py3-none-any.whl (114 kB)\n           -------------------------------------- 114.3\/114.3 kB 6.5 MB\/s eta 0:00:00\n      Collecting jinja2>=3.0.3 (from jupyterlab)\n        Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n           -------------------------------------- 133.1\/133.1 kB 7.7 MB\/s eta 0:00:00\n      Collecting jupyter-core (from jupyterlab)\n        Downloading jupyter_core-5.5.0-py3-none-any.whl (28 kB)\n      Collecting jupyter-lsp>=2.0.0 (from jupyterlab)\n        Downloading jupyter_lsp-2.2.0-py3-none-any.whl (65 kB)\n           ---------------------------------------- 66.0\/66.0 kB 3.7 MB\/s eta 0:00:00\n      Collecting jupyter-server<3,>=2.4.0 (from jupyterlab)\n        Downloading jupyter_server-2.10.1-py3-none-any.whl (378 kB)\n           -------------------------------------- 378.6\/378.6 kB 4.7 MB\/s eta 0:00:00\n      Collecting jupyterlab-server<3,>=2.19.0 (from jupyterlab)\n        Downloading jupyterlab_server-2.25.1-py3-none-any.whl (58 kB)\n           ---------------------------------------- 59.0\/59.0 kB 3.0 MB\/s eta 0:00:00\n      Collecting notebook-shim>=0.2 (from jupyterlab)\n        Downloading notebook_shim-0.2.3-py3-none-any.whl (13 kB)\n      Collecting packaging (from jupyterlab)\n        Downloading packaging-23.2-py3-none-any.whl (53 kB)\n           ---------------------------------------- 53.0\/53.0 kB 2.7 MB\/s eta 0:00:00\n      Collecting tornado>=6.2.0 (from jupyterlab)\n        Downloading tornado-6.3.3-cp38-abi3-win_amd64.whl (429 kB)\n           -------------------------------------- 429.2\/429.2 kB 9.1 MB\/s eta 0:00:00\n      Collecting traitlets (from jupyterlab)\n        Downloading traitlets-5.13.0-py3-none-any.whl (84 kB)\n           ---------------------------------------- 85.0\/85.0 kB 4.7 MB\/s eta 0:00:00\n      Collecting requests<3.0.0,>=2.25 (from conan<=1.59,>=1.57)\n        Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n           ---------------------------------------- 62.6\/62.6 kB ? eta 0:00:00\n      Collecting urllib3<1.27,>=1.26.6 (from conan<=1.59,>=1.57)\n        Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n           -------------------------------------- 143.8\/143.8 kB 4.3 MB\/s eta 0:00:00\n      Collecting colorama<0.5.0,>=0.3.3 (from conan<=1.59,>=1.57)\n        Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n      Collecting PyYAML<=6.0,>=3.11 (from conan<=1.59,>=1.57)\n        Downloading PyYAML-6.0.tar.gz (124 kB)\n           -------------------------------------- 125.0\/125.0 kB 3.6 MB\/s eta 0:00:00\n        Installing build dependencies: started\n        Installing build dependencies: finished with status 'done'\n        Getting requirements to build wheel: started\n        Getting requirements to build wheel: finished with status 'error'\n        error: subprocess-exited-with-error\n\n        Getting requirements to build wheel did not run successfully.\n        exit code: 1\n\n        [54 lines of output]\n        running egg_info\n        writing lib\\PyYAML.egg-info\\PKG-INFO\n        writing dependency_links to lib\\PyYAML.egg-info\\dependency_links.txt\n        writing top-level names to lib\\PyYAML.egg-info\\top_level.txt\n        Traceback (most recent call last):\n          File \"C:\\Users\\talta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n            main()\n          File \"C:\\Users\\talta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n            json_out['return_val'] = hook(**hook_input['kwargs'])\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n          File \"C:\\Users\\talta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n            return hook(config_settings)\n                   ^^^^^^^^^^^^^^^^^^^^^\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 355, in get_requires_for_build_wheel\n            return self._get_build_requires(config_settings, requirements=['wheel'])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 325, in _get_build_requires\n            self.run_setup()\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 341, in run_setup\n            exec(code, locals())\n          File \"<string>\", line 288, in <module>\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 103, in setup\n            return distutils.core.setup(**attrs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 185, in setup\n            return run_commands(dist)\n                   ^^^^^^^^^^^^^^^^^^\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 201, in run_commands\n            dist.run_commands()\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 969, in run_commands\n            self.run_command(cmd)\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\dist.py\", line 989, in run_command\n            super().run_command(command)\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\n            cmd_obj.run()\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\command\\egg_info.py\", line 318, in run\n            self.find_sources()\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\command\\egg_info.py\", line 326, in find_sources\n            mm.run()\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\command\\egg_info.py\", line 548, in run\n            self.add_defaults()\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\command\\egg_info.py\", line 586, in add_defaults\n            sdist.add_defaults(self)\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\command\\sdist.py\", line 113, in add_defaults\n            super().add_defaults()\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\command\\sdist.py\", line 251, in add_defaults\n            self._add_defaults_ext()\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\command\\sdist.py\", line 336, in _add_defaults_ext\n            self.filelist.extend(build_ext.get_source_files())\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n          File \"<string>\", line 204, in get_source_files\n          File \"C:\\Users\\talta\\AppData\\Local\\Temp\\pip-build-env-w9d6umo6\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 107, in __getattr__\n            raise AttributeError(attr)\n        AttributeError: cython_sources\n        [end of output]\n\n        note: This error originates from a subprocess, and is likely not a problem with pip.\n      error: subprocess-exited-with-error\n\n      Getting requirements to build wheel did not run successfully.\n      exit code: 1\n\n      See above for output.\n\n      note: This error originates from a subprocess, and is likely not a problem with pip.\n\n      [notice] A new release of pip is available: 23.1.2 -> 23.3.1\n      [notice] To update, run: C:\\Users\\talta\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n\u00d7 pip subprocess to install build dependencies did not run successfully.\n\u2502 exit code: 1\n\u2570\u2500> See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n\n```\n\nAny workaround or solutions?\n\n\nComments and answers are much appreciated.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"**Edit: Adding the work arounds which worked for people.**\n\n\n**Two workarounds exist**:\n\n\n1.Preinstall `cython<3`, then install `pyyaml` without build isolation, then install the rest of your dependencies \"AttributeError: cython\\_sources\" with Cython 3.0.0a10 #601 (comment)\n\n\n\n```\n$ pip install \"cython<3.0.0\" wheel\n$ pip install \"pyyaml==5.4.1\" --no-build-isolation\n$ pip install -r requirements.txt\n\n```\n\n2.Use a constraints file to force pip to use cython<3 at build time \"AttributeError: cython\\_sources\" with Cython 3.0.0a10 #601 (comment)\n\n\n\n```\n$ echo \"cython<3\" > \/tmp\/constraint.txt\n$ PIP_CONSTRAINT=\/tmp\/constraint.txt pip install -r requirements.txt\n\n```\n\nCredit to @astrojuanlu\n\n\n<https:\/\/github.com\/yaml\/pyyaml\/issues\/601#issuecomment-1813963845>\n\n\n\n\n---\n\n\n\n\n---\n\n\nLooks like there is an ongoing issue for installation of `catboost==1.2.2`\n\n\ngit links:\n\n\n<https:\/\/github.com\/catboost\/catboost\/issues\/2520>\n\n\n<https:\/\/github.com\/catboost\/catboost\/issues\/2469>\n\n\n`PyYAML` and `Cython` are the culprit.\n\n\nHere is the main git : <https:\/\/github.com\/yaml\/pyyaml\/issues\/601>"}
{"questionId":"fb71bea6a5af41849512da412e3149d9","question":"error: 'rust-analyzer' is not installed for the toolchain 'stable-x86\\_64-unknown-linux-gnu'\nI have installed rust-analyzer following the instructions at <https:\/\/rust-analyzer.github.io\/manual.html#installation> :\n\n\n\n```\n\u2714 ~> rustup component add rust-src\ninfo: component 'rust-src' is up to date\n\n```\n\nHowever, I get:\n\n\n\n```\n\u2714 ~> rust-analyzer -v\nerror: 'rust-analyzer' is not installed for the toolchain 'stable-x86_64-unknown-linux-gnu'\n\n```\n\nI have tried to install for this specific toolchain too:\n\n\n\n```\n\u2718-1 ~> rustup component add rust-src --toolchain stable-x86_64-unknown-linux-gnu\ninfo: component 'rust-src' is up to date\n\n```\n\nBut I still get the same error.\n\n\nI have looked for the error on the internet, and found for example discussions at <https:\/\/github.com\/rust-lang\/rust-analyzer\/issues\/14776> . However, checking the executable location returns\n\n\n\n```\n\u2714 ~> which rust-analyzer\n\/home\/jrmet\/.cargo\/bin\/rust-analyzer\n\n```\n\nwhich is well within my PATH. Also, I can do:\n\n\n\n```\n\u2714 ~> \/home\/jrmet\/.cargo\/bin\/rust-analyzer\nerror: 'rust-analyzer' is not installed for the toolchain 'stable-x86_64-unknown-linux-gnu'\n\n```\n\nI have been working also with other targets:\n\n\n\n```\n\u2714 ~> rustup target list | grep installed\nthumbv7em-none-eabihf (installed)\nthumbv8m.main-none-eabihf (installed)\nx86_64-unknown-linux-gnu (installed)\n\n```\n\nbut I do not think this should play a role?\n\n\nAny idea why I have this issue and how to fix it? I have a feeling I may have messed up somehow with targets and toolchains, but no idea how \/ where \/ how to fix it.\n\n\nI am on Ubuntu 22.04.","questionMetadata":{"Type":"Debugging","Level":"Beginner","Tag":"rust"},"answer":"I had misunderstood the instructions on how to install rust-analyzer: what I had installed were just the dependencies, not rust-analyzer itself.\n\n\nTo fix this, following the instruction page:\n\n\n\n```\n\u2714 ~> rustup component add rust-analyzer\ninfo: downloading component 'rust-analyzer'\ninfo: installing component 'rust-analyzer'\n\u2714 ~> rust-analyzer --version\nrust-analyzer 1.73.0 (cc66ad4 2023-10-03)\n\n```\n\nMy confusion stemmed from the fact that there was an executable rust-analyzer that was detected by `which` and was launched automatically, even though rust-analyzer was not actually installed..."}
{"questionId":"6cdc4b244ec54731b0a8bc9e0a0c69c5","question":"Why TypeScript cannot resolve assignment in object destructuring?\nLet's have a following code:\n\n\n\n```\ninterface Foo {\n    a?: number\n    b?: number\n}\n\nfunction foo(options?: Foo) {\n    const {\n        a, \/\/ <-- error here\n        b = a\n    } = (options ?? {})\n\n    return [a, b]\n}\n\n```\n\nWhy this code fails with:\n\n\n\n> \n> 'a' implicitly has type 'any' because it does not have a type annotation and is referenced directly or indirectly in its own initializer.\n> \n> \n> \n\n\nDoesn't `a` have type `number | undefined`?\n\n\nHowever, if I rewrite the `foo` function to:\n\n\n\n```\nfunction foo(options?: Foo) {\n    const f = options ?? {}\n    \n    const {\n        a,\n        b = a\n    } = f\n\n    return [a, b]\n}\n\n```\n\nNo error is emitted.\n\n\nWhy? Is it a bug?\n\n\n[TypeScript playground](https:\/\/www.typescriptlang.org\/play?ts=5.3.2#code\/JYOwLgpgTgZghgYwgAgGIHt3IN4FgBQyRycA-AFzIgCuAtgEbQHHL0VV2NQEC+BBMaiARhg6EMhiYAFOgAOo8QGd2GdAEoczYgmVgthFsTgAabUdbIAvCXPIe15LIViQS5KVI4e6-oaJQEGDUUBIA2qasALq8BEA)","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"typescript"},"answer":"This was a bug in TypeScript, as described in [microsoft\/TypeScript#49989](https:\/\/github.com\/microsoft\/TypeScript\/issues\/49989). A pull request at [microsoft\/TypeScript#56753](https:\/\/github.com\/microsoft\/TypeScript\/pull\/56753) fixes it, so as of TS5.4 this particular problem should no longer be an issue:\n\n\n[Playground link, TS version 5.4.0-dev.20240111](https:\/\/tsplay.dev\/WJBgVW)\n\n\n\n\n---\n\n\nIn general, however, TypeScript's type inference algorithm is limited in its ability to avoid circularities. See [microsoft\/TypeScript#45213](https:\/\/github.com\/microsoft\/TypeScript\/issues\/45213) for a similar situation that runs into this. If, when analyzing `const { a, b = a } = options ?? {}`, the compiler decides to, for whatever reason, defer the analysis of `options ?? {}` until after it knows the [context](https:\/\/www.typescriptlang.org\/docs\/handbook\/type-inference.html#contextual-typing) in which it is assigned, then `const { a, b = a }` will be seen as circular since the type of `a` depends on the type of `{a, b}` which depends on the type of `b` which depends on the type of `a`. Of course a human being would never get caught in such a situation, but the type checker doesn't have the luxury of seeing \"the big picture\".\n\n\nWhile the particular problem you found is indeed considered a bug in TypeScript and had a straightforward fix, there are many similar scenarios classified as a design limitation of the language, because fixing it would require a massive refactoring. As mentioned in [this comment on microsoft\/TypeScript#45213](https:\/\/github.com\/microsoft\/TypeScript\/issues\/45213#issuecomment-1018043918):\n\n\n\n> \n> It's instructive to look at the call stack at the point that we log the circularity error and see how every caller in that path (of which the incoming call stack is could be practically any function in the checker) would have to be basically completely rewritten to handle the answer of \"ask me again later\".\n> \n> \n> We also need to be able to *detect* circularities instead of looping forever. [...] then any sort of work-deferring mechanism needs to be able to detect when it's looping forever so that it doesn't attempt to infer an infinite sequence [...].\n> \n> \n> The relevant codepaths in the checker here are not especially complicated (relatively speaking) and I'd encourage you to try a PR to understand the problem more deeply. Not to be glib, but if this were low-hanging fruit, we would have done it already \ud83d\ude09. Again, it's not *impossible*, but it's not at all a straightforward fix - we'd have to rewrite very large portions of the entire checker.\n> \n> \n>"}
{"questionId":"07eb85215e7c459bb0faa48b5d3ce1c2","question":"How do I match a for loop with vector declaring in it's body?\nI hope to match all for-loop with vector declaring in it's body (might extend to while-loop and do-while-loop later):\n\n\n\n```\n#include <vector>\n\nint main() {\n  for (int i = 0; i < 10; i++) {\n    std::vector<int> foo;\n  }\n\n  return 0;\n}\n\n```\n\nThe AST I got from this program is\n\n\n\n```\n`-FunctionDecl 0x55c1d33c8bc8 <main.cpp:3:1, line:9:1> line:3:5 main 'int ()'\n  `-CompoundStmt 0x55c1d3402a48 <col:12, line:9:1>\n    |-ForStmt 0x55c1d34029e0 <line:4:3, line:6:3>\n    | |-DeclStmt 0x55c1d33c8d40 <line:4:8, col:17>\n    | | `-VarDecl 0x55c1d33c8cb8 <col:8, col:16> col:12 used i 'int' cinit\n    | |   `-IntegerLiteral 0x55c1d33c8d20 <col:16> 'int' 0\n    | |-<<<NULL>>>\n    | |-BinaryOperator 0x55c1d33c8db0 <col:19, col:23> 'bool' '<'\n    | | |-ImplicitCastExpr 0x55c1d33c8d98 <col:19> 'int' <LValueToRValue>\n    | | | `-DeclRefExpr 0x55c1d33c8d58 <col:19> 'int' lvalue Var 0x55c1d33c8cb8 'i' 'int'\n    | | `-IntegerLiteral 0x55c1d33c8d78 <col:23> 'int' 10\n    | |-UnaryOperator 0x55c1d33c8df0 <col:27, col:28> 'int' postfix '++'\n    | | `-DeclRefExpr 0x55c1d33c8dd0 <col:27> 'int' lvalue Var 0x55c1d33c8cb8 'i' 'int'\n    | `-CompoundStmt 0x55c1d34029c8 <col:32, line:6:3>\n    |   `-DeclStmt 0x55c1d34029b0 <line:5:5, col:25>\n    |     `-VarDecl 0x55c1d33cd388 <col:5, col:22> col:22 foo 'std::vector<int>' callinit destroyed\n    |       `-CXXConstructExpr 0x55c1d3402988 <col:22> 'std::vector<int>' 'void () noexcept'\n    `-ReturnStmt 0x55c1d3402a38 <line:8:3, col:10>\n      `-IntegerLiteral 0x55c1d3402a18 <col:10> 'int' 0\n\n```\n\nNow how do I write a matcher (in clang-tidy customized check) to match such pattern?\n\n\nI read the document for [customizing clang-tidy checks](https:\/\/clang.llvm.org\/extra\/clang-tidy\/Contributing.html) and [Matching the Clang AST](https:\/\/clang.llvm.org\/docs\/LibASTMatchers.html), but they don't seem to provide enough information on how I should actually combine each APIs.\n\n\n\n\n---\n\n\nupdate: with @Scott McPeak's answer, I can match the for loop with clang-query in terminal but I have trouble transplanting this matcher into my clang-tidy check:\n\n\n\n```\nvoid LowPerfLoopCheck::registerMatchers(MatchFinder *Finder) {\n  Finder->addMatcher(\n      forStmt(hasDescendant(varDecl(hasType(hasDeclaration(cxxRecordDecl(\n                                        matchesName(\"^::std::vector$\")))))\n                                .bind(\"vector-in-for\"))),\n      this);\n}\n\n```\n\nWhen building clang-tidy, it says `call of overloaded 'hasType()' is ambiguous`:\n\n\n\n```\nllvm-project\/clang-tools-extra\/clang-tidy\/misc\/LowPerfLoopCheck.cpp: In member function \u2018virtual void clang::tidy::misc::LowPerfLoopCheck::registerMatchers(clang::ast_matchers::MatchFinder*)\u2019:\n\/home\/wentao\/Desktop\/llvm-project\/clang-tools-extra\/clang-tidy\/misc\/LowPerfLoopCheck.cpp:19:44: error: call of overloaded \u2018hasType(clang::ast_matchers::internal::PolymorphicMatcher<clang::ast_matchers::internal::HasDeclarationMatcher, void(clang::ast_matchers::internal::TypeList<clang::CallExpr, clang::CXXConstructExpr, clang::CXXNewExpr, clang::DeclRefExpr, clang::EnumType, clang::ElaboratedType, clang::InjectedClassNameType, clang::LabelStmt, clang::AddrLabelExpr, clang::MemberExpr, clang::QualType, clang::RecordType, clang::TagType, clang::TemplateSpecializationType, clang::TemplateTypeParmType, clang::TypedefType, clang::UnresolvedUsingType, clang::ObjCIvarRefExpr>), clang::ast_matchers::internal::Matcher<clang::Decl> >)\u2019 is ambiguous\n   19 |       forStmt(hasDescendant(varDecl(hasType(hasDeclaration(cxxRecordDecl(\n      |                                     ~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n   20 |                                         matchesName(\"^::std::vector$\")))))\n      |                                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n```\n\nWhy does the same code works for clang-query but not for clang-tidy checks?","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"cpp"},"answer":"You can find all `for` loops that contain, somewhere, a declaration of\na `std::vector` variable using this AST matcher:\n\n\n\n```\n#!\/bin\/sh\n\nPATH=$HOME\/opt\/clang+llvm-16.0.0-x86_64-linux-gnu-ubuntu-18.04\/bin:$PATH\n\n# In this query, the comments are ignored because clang-query (not the\n# shell) recognizes and discards them.\nquery='m\n  forStmt(                        # Report any \"for\" statement\n    hasDescendant(                # where a descendant\n      varDecl(                    # is a variable declaration\n        hasType(                  # whose type\n          cxxRecordDecl(          # is a struct\/class\/union (possibly templatized)\n            matchesName(          # whose name matches the regex\n              \"^::std::vector$\"   # given here.\n            )\n          )\n        )\n      ).bind(\"vector_var_decl\")   # Bind to the declaration AST node.\n    )\n  )\n'\n\nclang-query \\\n  -c=\"$query\" \\\n  test.cc -- -w\n\n# EOF\n\n```\n\nThis matcher is expressed as a\n[`clang-query`](https:\/\/firefox-source-docs.mozilla.org\/code-quality\/static-analysis\/writing-new\/clang-query.html)\nshell script for ease of experimentation, but the matcher expression can\nbe copied directly into a C++ `clang-tidy` checker implementation like\nthis:\n\n\n\n```\nvoid HelloCheck::registerMatchers(MatchFinder *Finder) {\n  Finder->addMatcher(\n    forStmt(                        \/\/ Report any \"for\" statement\n      hasDescendant(                \/\/ where a descendant\n        varDecl(                    \/\/ is a variable declaration\n          hasType(                  \/\/ whose type\n            cxxRecordDecl(          \/\/ is a struct\/class\/union (possibly templatized)\n              matchesName(          \/\/ whose name matches the regex\n                \"^::std::vector$\"   \/\/ given here.\n              )\n            )\n          )\n        ).bind(\"vector_var_decl\")   \/\/ Bind to the declaration AST node.\n      )\n    ),\n    this\n  );\n}\n\n```\n\nTo create this matcher, I used the list of matchers at\n[AST Matcher Reference](https:\/\/clang.llvm.org\/docs\/LibASTMatchersReference.html).\nIt takes some trial and error to find the right combination; I wish the\ndocumentation was better. One small tip I can give is you can use the\n`anything()` matcher as a placeholder that matches anything while\ndeveloping the matcher incrementally.\n\n\nIn this case, the first hard part (for me, despite some prior\nexperience) was realizing I needed to use the `cxxRecordDecl` matcher\nbefore `matchesName` could be used. Originally, I tried\n`hasDeclaration`, but that matches declarations that don't necessarily\nhave a name, and consequently `matchesName` will just silently fail\n(`clang-query` could be a lot more informative in cases like this). By\nusing `cxxRecordDecl`, we're guaranteed to be working with a declaration\nthat has a name to match, so it works.\n\n\nThe second problem, when translating my original `clang-query` matcher\ninto a C++ `clang-tidy` check, was an error about an ambiguous call to\n`hasType`. The original matcher had:\n\n\n\n```\n  varDecl(hasType(hasDeclaration(cxxRecordDecl(...))))\n\n```\n\nEvidently, `hasDeclaration` in that spot causes problems in the C++\ncontext (although I don't really understand why), and my solution was to\njust remove it:\n\n\n\n```\n  varDecl(hasType(cxxRecordDecl(...)))\n\n```\n\nTo expand to other kinds of loops, the `whileStmt` and `doStmt` matchers\ncould be substituted for `forStmt`.\n\n\nExample input:\n\n\n\n```\n#if 1\n  #include <vector>\n#else\n  \/\/ Simplified stand-in for easier AST inspection.\n  namespace std {\n    template <class T>\n    class vector {};\n  }\n#endif\n\nclass SomethingElse {};\n\nint main() {\n  for (int i = 0; i < 10; i++) {       \/\/ Report.\n    std::vector<int> foo;              \/\/ Original example.\n  }\n\n  for (int i = 0; i < 10; i++) {       \/\/ Do not report.\n    SomethingElse se;                  \/\/ Not a std::vector.\n  }\n\n  for (;;) {                           \/\/ Report.\n    {\n      std::vector<int> foo;            \/\/ Not an immediate child.\n    }\n  }\n\n  return 0;\n}\n\n```\n\nCorresponding output:\n\n\n\n```\n$ .\/cmd.sh\n\nMatch #1:\n\n$PWD\/test.cc:14:3: note: \"root\" binds here\n  for (int i = 0; i < 10; i++) {       \/\/ Report.\n  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n$PWD\/test.cc:15:5: note: \"vector_var_decl\" binds here\n    std::vector<int> foo;              \/\/ Original example.\n    ^~~~~~~~~~~~~~~~~~~~\n\nMatch #2:\n\n$PWD\/test.cc:22:3: note: \"root\" binds here\n  for (;;) {                           \/\/ Report.\n  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n$PWD\/test.cc:24:7: note: \"vector_var_decl\" binds here\n      std::vector<int> foo;            \/\/ Not an immediate child.\n      ^~~~~~~~~~~~~~~~~~~~\n2 matches."}
{"questionId":"2fdf0be7ae0b45078df38108c1a28c56","question":"Is it possible to determine whether an object is of a class type or not using a trait through a non-type template parameter in C++20?\nConsider this code:\n\n\n\n```\n#include <type_traits>\n\ntemplate <typename T>\nstruct wrapper {\n    T& ref;\n    constexpr wrapper(T& ref) : ref(ref) {}\n};\n\n\/\/ Trait that checks whether a type is of the form `wrapper<T>`\ntemplate <typename T>\nstruct is_wrapper_type : std::false_type {};\n\ntemplate <typename T>\nstruct is_wrapper_type<wrapper<T>> : std::true_type {};\n\n\/\/ Trait that checks whether an object is of the type `wrapper<T>`\ntemplate <auto& Value>\nstruct is_wrapper_object;\n\ntemplate <auto& Value>\n    requires (!is_wrapper_type<std::decay_t<decltype(Value)>>::value)\nstruct is_wrapper_object<Value> : std::false_type {};\n\ntemplate <auto& Value>\n    requires is_wrapper_type<std::decay_t<decltype(Value)>>::value\nstruct is_wrapper_object<Value> : std::true_type {};\n\nint main() {\n    static constexpr int v = 42;\n    static_assert(!is_wrapper_object<v>::value);\n    static constexpr wrapper w {v};\n    static_assert(is_wrapper_object<w>::value);\n}\n\n```\n\nThe above [fails to compile in Clang with the errors shown below but compiles successfully in GCC](https:\/\/godbolt.org\/z\/n3ozovd67).\n\n\n\n```\n<source>:30:20: error: implicit instantiation of undefined template 'is_wrapper_object<v>'\n   30 |     static_assert(!is_wrapper_object<v>::value);\n      |                    ^\n<source>:18:8: note: template is declared here\n   18 | struct is_wrapper_object;\n      |        ^\n<source>:32:19: error: implicit instantiation of undefined template 'is_wrapper_object<w>'\n   32 |     static_assert(is_wrapper_object<w>::value);\n      |                   ^\n<source>:18:8: note: template is declared here\n   18 | struct is_wrapper_object;\n      |        ^\n\n```\n\nWhich compiler is correct here?\n\n\nI believe GCC should be correct here as the above code *should* be legal on paper, however, I am not sure whether that is really the case.\n\n\nMoreover, if I change the definition of the trait `is_wrapper_object` to this:\n\n\n\n```\ntemplate <auto& Value>\nstruct is_wrapper_object : std::false_type {};\n\ntemplate <auto& Value>\n    requires is_wrapper_type<std::decay_t<decltype(Value)>>::value\nstruct is_wrapper_object<Value> : std::true_type {};\n\n```\n\nIt [compiles on Clang as well, albeit with wrong output](https:\/\/godbolt.org\/z\/Yxax4xEzY).","questionMetadata":{"Type":"Debugging","Level":"Advanced","Tag":"cpp"},"answer":"GCC is correct.\n\n\nNote that simply replacing your `auto&` with `const auto&` makes the code compile for both. Whatever the issue is, it has something to do with placeholder type specifiers in partial specializations.\n\n\nNote that:\n\n\n\n> \n> If the type `T` of a [template-parameter](https:\/\/eel.is\/c++draft\/temp.param#nt:template-parameter) contains a [placeholder type](https:\/\/eel.is\/c++draft\/dcl.spec.auto) or a placeholder for a deduced class type ([dcl.type.class.deduct]), the type of the parameter is the type deduced for the variable `x` in the invented declaration\n> \n> \n> \n> ```\n> T x = E ;\n> \n> ```\n> \n> \n\n\n- [[temp.arg.nontype] p1](https:\/\/eel.is\/c++draft\/temp.arg.nontype#1)\n\n\nIn other words, `auto&` in a template parameter should be `const int&` and `const wrapper&` after such deduction.\nIt should not be necessary for the user to provide `const` themselves.\n\n\nI was unable to find a relevant LLVM bug report, so I have submitted one: [LLVM Bug 77189](https:\/\/github.com\/llvm\/llvm-project\/issues\/77189).\n\n\n### Minimal Reproducible Example\n\n\nSee <https:\/\/godbolt.org\/z\/rhKsWKdPz>\n\n\n\n```\n#include <type_traits>\n\ntemplate <auto& Value, int>\nstruct test : std::false_type {};\n\ntemplate <auto& Value>\nstruct test<Value, 0> : std::true_type {};\n\nint main() {\n    static constexpr int v = 42;\n    static_assert(test<v, 0>::value); \/\/ fails for clang\n}\n\n```\n\nFrom this, we can tell that clang is *completely* unable to deduce `Value` within the partial specialization of `test`."}
{"questionId":"d575b2f22b184c98a3c06e45f4cd2559","question":"How can I calculate the number of uniques in a row within a species matrix?\nI am trying to identify which rows have uniques (a species that was only observed in that row and not in any other row of my species matrix). I have my data matrix set up with columns as individual species and rows as individual sampling units (in our case, transects).\n\n\nSay, for example, species 1 was only found in row 8 and nowhere else in the dataset, then I would like to know that row 8 contains 1 unique. If species 4 was also only found in row 8, then the number of uniques would be 2 etc. Note: some of the abundances of the uniques I have found in the dataset have been greater than 1, meaning they were found on the transect more than once, but they were still only found in that one transect (still considered a unique).\n\n\nHere is some example data where row 3 has two uniques and row 5 has 1 unique:\n\n\n\n```\n example_data <- data.frame(Species1 = c(1, 2, 3, 4, 5),\n                             Species2 = c(6, 7, 8, 9, 10),\n                             Species3 = c(0, 0, 13, 0, 0),\n                             Species4 = c(0, 0, 0, 0, 20),\n                             Species5 = c(0, 0, 23, 0, 0))\n\n```\n\nTried with ChatGPT and got nowhere, I was able to figure out VIA excel that the data set does have uniques. I also figured out how to get R to tell me which species were only found once, but I am more interested in which rows (transects) **have** uniques and how many, not which species were uniques.\n\n\nAdditionally is there a way I can make a list of all the rows and the number of uniques? For the example data it would be 0,0,2,0,1. I am interested in which rows also don't have uniques (0 uniques).","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"r"},"answer":"Since you are interested in which rows have uniques and how many, not which species were uniques:\n\n\nYou can find both the row and column indices of those with unique values by first finding which species are unique, then using `which(..., ind.arr = TRUE)`:\n\n\n\n```\nuniques <- vapply(example_data, \\(x) sum(x !=0) == 1, logical(1L))\n\n# Species1 Species2 Species3 Species4 Species5 \n#    FALSE    FALSE     TRUE     TRUE     TRUE \n\nrowcol_uniques <- which(example_data[uniques] != 0, arr.ind = TRUE)\nrownames(rowcol_uniques) <- names(uniques[uniques])\n\n#            row col\n# Species3   3   1\n# Species4   5   2\n# Species5   3   3\n\n```\n\nYou can find how many by `table`:\n\n\n\n```\ntable(rowcol_uniques[,1])\n\n# 3 5 \n# 2 1 \n\n```\n\nIn your edit, you mentioned you are interested rows that also don't have uniques - for this you could `merge`, which would create a second column `Freq` that identifies the number of uniques per row for all rows:\n\n\n\n```\nanyuniques <- merge(data.frame(row = seq_len(nrow(example_data))), \n                    as.data.frame(table(row = rowcol_uniques[,1])), \n                    all.x = TRUE)\nanyuniques[is.na(anyuniques)] <- 0\n\n#   row Freq\n# 1   1    0\n# 2   2    0\n# 3   3    2\n# 4   4    0\n# 5   5    1"}
{"questionId":"2d80e7101ed5410eb44d49ae7d5aabe4","question":"Why does Rust stop detecting dead structs if they implement\/derive some trait?\nUnder normal circumstances, Rust warns if a struct is dead by generating a warning like:\n\n\n\n```\nstruct `Foo` is never constructed\n\n```\n\nThis is nice to detect unused types that can be removed.\n\n\nHowever, I noticed that the compiler stops warning about dead structs as soon as they implement or derive some trait. For instance, for a source file consisting purely of:\n\n\n\n```\nstruct DeadStruct1;\n\n#[derive(Clone)]\nstruct DeadStruct2;\n\n```\n\nThe compiler will only detect that `DeadStruct1` is unused, but not `DeadStruct2`, which is obviously not used in this program either (example on [Rust Playground](https:\/\/play.rust-lang.org\/?version=stable&mode=debug&edition=2021&gist=8c25c3ce9e9785951e0a10cd7abed0ad)):\n\n\n\n```\nwarning: struct `DeadStruct1` is never constructed\n --> src\/lib.rs:1:8\n  |\n1 | struct DeadStruct1;\n  |        ^^^^^^^^^^^\n  |\n  = note: `#[warn(dead_code)]` on by default\n\n```\n\nI'm wondering why this lint depends on whether or not a struct implements a trait?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"rust"},"answer":"It is because of a known bug, it is discussed [here](https:\/\/github.com\/rust-lang\/rust\/issues\/57613) and [here](https:\/\/github.com\/rust-lang\/rust\/issues\/47851).\n\n\nBut... it's weird.\n\n\nI tested the January 1st builds of each year between 2018 and 2023, and the problem got *worse* over time.\n\n\n- The problem you're mentioning (with `#[derive(Clone)]`, and also for `PartialEq`, `Default`; but not for `Debug` and `Hash`, and also not if the trait was manually implemented) had already been there in `2018-01-01`. But at that point, only derived implementations caused it, manual implementations still had the warning.\n- In `2019-01-01`, nothing seems to have changed.\n- But then, by `2020-01-01`, some other cases stopped producing the warning too. This code demonstrates the behavior (from [this post](https:\/\/github.com\/rust-lang\/rust\/issues\/47851#issuecomment-629852474), also in the discussion linked above):\n\n\n\n```\nstruct Foo;\n\n\/\/ Does not prevent the warning\ntrait Bar0 {}\nimpl Bar0 for Foo {}\n\n\/\/ Does not prevent the warning\ntrait Bar1 { fn dummy(); }\nimpl Bar1 for Foo { fn dummy () {} }\n\n\/\/ Prevents the warning\ntrait Bar2 { fn dummy(&self); }\nimpl Bar2 for Foo { fn dummy (&self) {} }\n\n\/\/ Prevents the warning\ntrait Bar3 { fn dummy() -> Self; }\nimpl Bar3 for Foo { fn dummy () -> Self { todo!() } }\n\n\/\/ Prevents the warning\ntrait Bar4 { type Dummy; }\nimpl Bar4 for Foo { type Dummy = Self; }\n\n\/\/ Prevents the warning\ntrait Bar5 { const DUMMY: Self; }\nimpl Bar5 for Foo { const DUMMY: Self = Self; }\n\n```\n- And, as we arrive to `2021-01-01`, now *any* (non-blanket) trait implementation silences the warning, no matter if derived or manual, mentions `Self` or not!\n\n\nAnd this seems to be the behavior we have to this day."}
{"questionId":"8e5c3d0c198c4d99875a413ab5bc124d","question":"Will compilers optimize \"strnlen(mystring, 32) > 2\" to stop looping as soon as the running length exceeds 2?\nDo modern compilers (or perhaps these have been in place since C89) substitute in *short circuit evaluated* code for cases like the one below during conditional expression evaluations?\n\n\n\n```\nchar mystring[32] = \"this is a long line\";\nif((strnlen(mystring, 32)) > 2)\n{\n    return 1;\n}\n\n```\n\nAs in the right operand is taken into account during processing `strnlen(...)`, and the moment the running length of the C string within `strnlen(...)` exceeds the right operand of the outer conditional expression (2 in this case), `strnlen(...)` breaks out?\n\n\n- Would it have mattered if I hadn't preassigned the string length?\n- Would it have mattered if I had removed the parentheses from the **IF** inner expression?\n- Would it have mattered if I had switched the operands and the operator to a `<`?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"c"},"answer":"*Maybe*, depending on the compiler. Let's look at some examples, compiled with gcc 13.2.0 and clang 17.0.1, both at optimization level `-O3` and [with extensions enabled](https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/C-Dialect-Options.html#index-std-1) (note that `strnlen` is POSIX, not standard C).\n\n\n\n```\nint p() {\n    char mystring[32] = \"this is a long line\";\n    return strnlen(mystring, 32) > 2;\n}\n\n```\n\nBoth clang and gcc [optimize](https:\/\/godbolt.org\/z\/54s7s4az7) this to `mov eax, 1; ret`. This is because they know the behavior of `strnlen` and can substitute in the return value of the call without needing to evaluate it at runtime. (In gcc this is [implemented via `__builtin_strnlen`](https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/Other-Builtins.html#Other-Built-in-Functions-Provided-by-GCC)).\n\n\nIf the `strnlen` function isn't a known builtin, but can be inlined:\n\n\n\n```\ninline size_t my_strnlen(char const* s, size_t n) {\n    for (size_t i = 0; i != n; ++i)\n        if (s[i] == 0)\n            return i;\n    return n;\n}\nint p() {\n    char mystring[32] = \"this is a long line\";\n    return my_strnlen(mystring, 32) > 2;\n}\n\n```\n\nHere clang [optimizes](https:\/\/godbolt.org\/z\/qPe9TYMqn) to `mov eax, 1` but gcc emits a loop.\n\n\nFinally, for an unknown predicate that is marked [`pure`](https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/Common-Function-Attributes.html#index-pure-function-attribute) to tell the optimizer that it has no side effects:\n\n\n\n```\n__attribute__((pure)) int f(char);\ninline size_t my_strnlen_f(char const* s, size_t n) {\n    for (size_t i = 0; i != n; ++i)\n        if (f(s[i]))\n            return i;\n    return n;\n}\nint p() {\n    char mystring[32] = \"this is a long line\";\n    return my_strnlen_f(mystring, 32) > 2;\n}\n\n```\n\ngcc again [emits](https:\/\/godbolt.org\/z\/5nzzhMqf6) a loop; clang emits some rather clumsy code (what's up with `ebx`?) that nevertheless shows that it knows that `f` needs to be called no more than 3 times, with the character codes of the first 3 characters - it optimizes out the full string:\n\n\n\n```\np:                                      # @p\n        push    rbx\n        mov     edi, 116 # 't'\n        call    f@PLT\n        xor     ebx, ebx\n        test    eax, eax\n        je      .LBB2_1\n.LBB2_3:\n        mov     eax, ebx\n        pop     rbx\n        ret\n.LBB2_1:\n        mov     edi, 104 # 'h'\n        call    f@PLT\n        test    eax, eax\n        jne     .LBB2_3\n        mov     edi, 105 # 'i'\n        call    f@PLT\n        xor     ebx, ebx\n        test    eax, eax\n        sete    bl\n        mov     eax, ebx\n        pop     rbx\n        ret\n\n```\n\n\n> \n> - Would it have mattered if I hadn't preassigned the string length?\n> \n> \n> \n\n\nNo, in that case the C language would just set the buffer size to the size of the string literal (string length + 1 for the terminator).\n\n\n\n> \n> - Would it have mattered if I had removed the parentheses from the IF inner expression?\n> \n> \n> \n\n\nNo, the optimizer runs on a program representation that does not include these details of syntax.\n\n\n\n> \n> - Would it have mattered if I had switched the operands and the operator to a `<`?\n> \n> \n> \n\n\nAlmost certainly not, the optimizer is capable of understanding that these are equivalent."}
{"questionId":"c6a59b84248748af8fd2c98424068792","question":"How to handle authentication in Flutter with go\\_router and Firebase Authentication when already on a route?\nIn my Flutter application, I'm using the `go_router` package to manage routes and `Firebase Authentication` for user authentication. I have several screens that require users to be authenticated to access, such as account management, transactions, and details.\n\n\nCurrently, I have implemented redirects with `go_router` to ensure that unauthenticated users are redirected correctly. However, I face a challenge when the user is already on one of these screens and logs out or the session expires.\n\n\nI'm considering using a BlocListener to detect changes in the authentication state on each screen, but this seems to lead to code duplication. I also have a Stream that notifies changes in the authentication state, thanks to Firebase Authentication, and updates the `contex.isUserSignIn` variable.\n\n\nWhat would be the best practice for handling logout or session expiration events in Flutter with go\\_router and Firebase Authentication efficiently?","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"dart"},"answer":"I\u2019ve been struggling with the exact same question and have researched several alternatives and options.\n\n\n**TL;DR:** Option 3 is my preferred choice, which uses the `GoRouter.refresh()` method at the `main()` level to dynamically update the GoRouter state based on events from the auth stream.\n\n\n### Option 1: Follow the go\\_router async\\_redirection.dart example\n\n\nSee here for the example: <https:\/\/github.com\/flutter\/packages\/blob\/main\/packages\/go_router\/example\/lib\/async_redirection.dart>\n\n\nThis wraps the top level app, typically `MyApp` (as returned by `main()` in `runApp()` ) in an InheritedNotifer widget, which they call `StreamAuthScope` and which creates a dependency between the notifier `StreamAuthNotifier` and go\\_router's parsing pipeline. This in turn will rebuild `MyApp` (or `App` in the example) when the auth status changes (as communicated by `StreamAuthNotifier` via `notifyListeners()`).\n\n\nI implemented a similar model based on the Provider package where the `ChangeProviderNotifier` replaces `StreamAuthScope` and wraps the top level `MyApp` returned by `main()`. However this doesn\u2019t allow the creation of a monitored `Provider.of<>` inside the `GoRouter( redirect: )` enclosure. To solve this I created a `getRouter` function that passed in `isUserSignIn` which was monitored with a `Provider.of<>` in the main body of `MyApp` but before the `build` function. This works but feels cumbersome and\ncauses the main `MyApp` to be rebuilt each time auth status changes. If desired, I\u2019m sure you could do something similar with a BLoC model in place of Provider.\n\n\n### Option 2: Use `GoRouter`\u2019s `refreshListenable:` parameter\n\n\nThis is based on this go\\_router redirection.dart example: <https:\/\/github.com\/flutter\/packages\/blob\/main\/packages\/go_router\/example\/lib\/redirection.dart>\n\n\nIn the question you mentioned you have a stream that notifies auth state changes. You can wrap this in a class with `extends ChangeNotifier` to make it Listenable. Then in the constructor you can instantiate and monitor the stream with `.listen`, and in the enclosure issue a `notifyListerners()` each time there is an auth state change (probably each time there is a stream event). In my case I called this class `AuthNotifier` This can then be used as the listenable with `GoRouter`\u2019s `refreshListenable:` parameter simply as: `refreshListenable: AuthNotifier()`\n\n\nExample `AuthNotifier` class\n\n\n\n```\nclass AuthNotifier extends ChangeNotifier {\n  AuthNotifier() {\n    \/\/ Continuously monitor for authStateChanges\n    \/\/ per: https:\/\/firebase.google.com\/docs\/auth\/flutter\/start#authstatechanges\n    _subscription =\n        FirebaseAuth.instance.authStateChanges().listen((User? user) {\n          \/\/ if user != null there is a user logged in, however\n          \/\/ we can just monitor for auth state change and notify\n          notifyListeners();\n        });\n  } \/\/ End AuthNotifier constructor\n\n  late final StreamSubscription<dynamic> _subscription;\n\n  @override\n  void dispose() {\n    _subscription.cancel();\n    super.dispose();\n  }\n}\n\n```\n\nNote: to avoid multiple streams being created and monitored, you need to ensure this constructor is only called once in your app (in this case as part of `GoRouter`\u2019s `refreshListenable:`), or else modify it to be a singleton.\n\n\n### Option 3: Use `GoRouter`\u2019s `.refresh()` method\n\n\nA similar, but more direct approach to option 2 is to use `GoRouter`\u2019s `.refresh()` method. This directly calls an internal `notifyListerners()` that refreshes the GoRouter configuration. We can use a similar class to the `AuthNotifier` above but we don\u2019t need `extends ChangeNotifier` and would call `router.refresh()` in place of `notifyListeners()`, where `router` is your `GoRouter()` configuration. This new class would be instantiated in `main()`.\n\n\nGiven its so simple (2-3 lines of code), we can also skip the class definition and instantiation and implement the functionality directly in the `main()` body, as follows:\n\n\n\n```\nFuture<void> main() async {\n  WidgetsFlutterBinding.ensureInitialized();\n  await Firebase.initializeApp(options: DefaultFirebaseOptions.currentPlatform);\n\n  \/\/ Listen for Auth changes and .refresh the GoRouter [router]\n  FirebaseAuth.instance.authStateChanges().listen((User? user) {\n    router.refresh();\n  });\n\n  runApp(\n    const MyApp(),\n  );\n}\n\n```\n\nSince this appears to be the most direct and simplest solution, it is my preferred solution and the one I have implemented. However there is a lot of confusing and dated information out there and I don\u2019t feel I have enough experience to claim it as any sort of 'best practice', so will leave that for others to judge and comment.\n\n\nI hope all this helps you and others as it\u2019s taken me a long time with work out these various options and wade through the wide range of materials and options out there. I feel there is a definitely an opportunity to improve the official go\\_router documentation in this area !"}
{"questionId":"1ac56b1beac5477b8e35897faf63df23","question":"Docker \"Failed to solve: Canceled: context canceled\" when loading build context\nWhen running `docker-compose up --build` I'm constantly getting this error.\n\n\n\n```\n => [web internal] load build definition from Dockerfile                                                           0.0s\n => => transferring dockerfile: 1.58kB                                                                             0.0s\n => [web internal] load .dockerignore                                                                              0.0s\n => => transferring context: 279B                                                                                  0.0s\n => [web internal] load metadata for docker.io\/library\/python:3.8                                                  0.8s\n => CANCELED [web  1\/14] FROM docker.io\/library\/python:3.8@sha256:7a82536f5a2895b70416ccaffc49e6469d11ed8d9bf6bcf  0.3s\n => => resolve docker.io\/library\/python:3.8@sha256:7a82536f5a2895b70416ccaffc49e6469d11ed8d9bf6bcfc52328faeae7c77  0.2s\n => => sha256:795c73a8d985b6d1b7e5730dd2eece7f316ee2607544b0f91841d4c4142d9448 7.56kB \/ 7.56kB                     0.0s\n => => sha256:7a82536f5a2895b70416ccaffc49e6469d11ed8d9bf6bcfc52328faeae7c7710 1.86kB \/ 1.86kB                     0.0s\n => => sha256:129534c722d189b3baf69f6e3289b799caf45f75da37035c854100852edcbd7d 2.01kB \/ 2.01kB                     0.0s\n => CANCELED [web internal] load build context                                                                     0.1s\n => => transferring context: 26.00kB                                                                               0.0s\nfailed to solve: Canceled: context canceled\n\n```\n\nThis has been happening since yesterday, before my images would build and my application would run just fine.\n\n\n1. Tried identifying any processes\/apps that interfere with the docker build process\n2. Reinstalled Docker Desktop completely\n3. Tried changing Python versions inside my Dockerfile\n\n\nI'm quite lost as to what is going wrong, I've used `python3.8` and `python3.9` without issues before. Below is my dockerfile and docker-compose.\n\n\n\n```\n# Use an official Python runtime as the base image\nFROM python:3.8\n\n# Set environment variables\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\nENV NODE_ENV production\n\n# Create and set the working directory\nWORKDIR \/app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        default-mysql-client \\\n        software-properties-common \\\n        curl \\\n        gnupg \\\n    && curl -fsSL https:\/\/deb.nodesource.com\/gpgkey\/nodesource.gpg.key | gpg --dearmor -o \/usr\/share\/keyrings\/nodesource.gpg \\\n    && echo \"deb [signed-by=\/usr\/share\/keyrings\/nodesource.gpg] https:\/\/deb.nodesource.com\/node_20.x bullseye main\" | tee \/etc\/apt\/sources.list.d\/nodesource.list \\\n    && apt-get update && apt-get install -y nodejs \\\n    && apt-get clean \\\n    && rm -rf \/var\/lib\/apt\/lists\/*\n\n# Verify that Node.js and npm are installed\nRUN node --version\nRUN npm --version\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install --upgrade pip && \\\n    pip install -r requirements.txt\n\n# Install Tailwind CSS and its peer dependencies\nCOPY package.json package-lock.json .\/\nRUN npm install\n\n# Copy the current directory contents into the container\nCOPY . .\n\n# Build the Tailwind CSS\nRUN npm run build:css\n\nRUN npm run watch:css\n\nCOPY entrypoint.sh .\/entrypoint.sh\nRUN chmod +x .\/entrypoint.sh\n\nENTRYPOINT [\".\/entrypoint.sh\"]\n\n# The main command to run when the container starts\nCMD [\"gunicorn\", \"--bind\", \":8000\", \"bmlabs.wsgi:application\"]\n\n```\n\nDocker-compose\n\n\n\n```\n\nservices:\n  db:\n    image: mysql:5.7\n    volumes:\n      - db_data:\/var\/lib\/mysql\n    restart: always\n    environment:\n      MYSQL_ROOT_PASSWORD: \n      MYSQL_DATABASE:\n      MYSQL_USER: \n      MYSQL_PASSWORD:\n    networks:\n      - backend\n\n  web:\n    build: .\n    volumes:\n      - .:\/app\n      - static_volume:\/app\/staticfiles\n      - media_volume:\/app\/mediafiles\n    depends_on:\n      - db\n    networks:\n      - backend\n    expose:\n      - \"8000\"\n\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - .\/nginx\/nginx.conf:\/etc\/nginx\/nginx.conf:ro\n      - .\/nginx\/bmlabs.conf:\/etc\/nginx\/conf.d\/bmlabs.conf:ro\n      - static_volume:\/app\/staticfiles\n      - media_volume:\/app\/mediafiles\n    depends_on:\n      - web\n    networks:\n      - backend\n\nnetworks:\n  backend:\n\nvolumes:\n  db_data:\n  cache_data:\n  static_volume:\n  media_volume:\n\n```\n\nI am on Windows 10.","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"python"},"answer":"Altough downgrading docker worked, the actual problem was that I didn't exclude node\\_modules in Dockerignore. I had been running the containers fine for quite a long time.\n\n\nAfter purging all containers and images and adding the `node_modules\/` line to my `.dockerignore` it fixed it.\n\n\nI'm guessing the error had to do with the amount of files inside certain directories."}
{"questionId":"17cbeff1597a448097517117a61e030d","question":"ImportError: cannot import name 'mock\\_s3' from 'moto'\nimport pytest\nfrom moto import mock_s3\n\n\n@pytest.fixture(scope='module')\ndef s3():\n    with mock_s3():\n        os.environ['AWS_ACCESS_KEY_ID'] = 'test'\n        os.environ['AWS_SECRET_ACCESS_KEY'] = 'test'\n        os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n        s3 = boto3.resource('s3')\n        s3.create_bucket(Bucket='test_bucket')\n        yield s3\n\n```\n\nThis code was working, but is now throwing an exception `Cannot import name mock_s3 from moto`. What am I doing wrong?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"Simply replace your import with `from moto import mock_aws` and use `with mock_aws():` instead.\n\n\nMoto was recently bumped to version 5.0, and you were probably running 4.x before.\n\n\n<https:\/\/github.com\/getmoto\/moto\/blob\/master\/CHANGELOG.md>\n\n\nIf you check the change log, you will see that an important breaking change was made:\n\n\n\n> \n> All decorators have been replaced with a single decorator: `mock_aws`\n> \n> \n>"}
{"questionId":"8950443e247144aaac16d5fb77381694","question":"Haskell complexity of an algorithm\nI encounter a simple problem on Codeforces, the problem is [this](https:\/\/codeforces.com\/problemset\/problem\/1915\/E). The thing I wanted to discuss is not about the problem, it about the languages we used, in this case, Python3 and Haskell.\n\n\nIn details, I have 2 version of my algorithm, one in Haskell and the other in Python3. Both of them in Functional programming style. The code looks like this\n\n\n**Python3**\n\n\n\n```\nfrom operator import add\nfrom itertools import accumulate\nfrom functools import reduce\n \n \ndef floss(l):\n    def e(u):\n        a, b = u\n        return b if a % 2 == 1 else -b\n \n    return map(e, enumerate(l))\n \n \ndef flock(l):\n    return accumulate(l, add)\n \n \ndef search(l):\n    b = zip(l, l[1:])\n \n    def equal(u):\n        x, y = u\n        return x == y\n \n    c = any(map(equal, b))\n    return 'YES\\n' if c else 'NO\\n'\n \n \ndef main():\n    t = int(input())\n \n    def solution(x):\n        return search(sorted(list(flock(floss(x)))))\n \n    def get():\n        _ = input()\n        b = [0] + [int(x) for x in input().split()]\n        return b\n \n    all_data = [get() for _ in range(t)]\n    all_solution = map(solution, all_data)\n    print(reduce(add, all_solution))\n \n \nmain()\n\n```\n\n**Haskell**\n\n\n\n```\nmodule Main (main) where\nimport Data.List (sort)\n \nmain :: IO ()\nmain = do\n  x <- des\n  putStrLn x\n \nreadInts :: IO [Int]\nreadInts = fmap (map read.words) getLine\n \nflock :: [Int] -> [Int]\nflock l = scanr (+) 0 l \n  \nfloss :: [Int] -> [Int]\nfloss l = map (e :: (Int, Int) -> Int) $ zip [0..] l where {\n  e (u, v) = if mod u 2 == 0 then v else -v\n  }\n  \nsearch :: [Int] -> String \nsearch l = if c then \"YES\\n\" else \"NO\\n\" where {\n  b = zip l $ tail l;\n  c = any (\\(x, y) -> x == y) b;\n  }\n  \nsolution :: [Int] -> String \nsolution = search.sort.flock.floss\n \ndes :: IO String\ndes = do \n  io <- readInts\n  let t = head io\n  all_data <- sequence $ replicate t $ do\n    _ <- readInts\n    b <- readInts\n    return b\n  let all_solution = map solution all_data\n  let output = foldr (++) \"\" all_solution\n  return output\n\n```\n\nBoth of them relatively the same in term of algorithm. In fact, the Python3 passed the testcases with high complexity while Haskell code cannot. I wonder why my code in Haskell run slower than Python3 and I wanted to know the operation of Haskell that make the fault. One thing I found suspicious is the memory usage of my Haskell code is incredibly higher (2-8 times) than Python3 code.\n\n\nI have just started study about FP recently, so there maybe some mistake that I made in the post.\n\n\nUpdate #1: One thing that I found might be useful for bug detecting is that in the Haskell code is `let output = foldr (++) \"\" all_solution`. In a much worse code, I used `foldl` rather than `foldr`, which made the code extremely slow. I think that might make bug detecting task a little easier.","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"haskell"},"answer":"From profiling, it looks like most of the time is being spent in `readInts`. A really stupid reimplementation that doesn't invoke the full overhead of `read` already triples the speed of the program in my tests:\n\n\n\n```\nreadIntDumb :: String -> IO Int\nreadIntDumb = go 0 1 where\n    go n sgn [] = pure (sgn * n)\n    go n sgn (c:cs) = case c of\n        '-' -> go n (negate sgn) cs\n        d | '0' <= d && d <= '9' -> go (10*n + fromEnum d - fromEnum '0') sgn cs\n        _ -> fail \"whoops\"\n\n```\n\nMoving to `ByteString`-based `IO` gets another factor of two:\n\n\n\n```\nimport Data.ByteString.Char8 (ByteString)\nimport qualified Data.ByteString.Char8 as BS8\n\nreadIntsBS :: ByteString -> [Int]\nreadIntsBS bs = case BS8.readInt bs of\n    Nothing -> []\n    Just (n, bs') -> n : readIntsBS (BS8.dropWhile isSpace bs')\n\nreadInts :: IO [Int]\nreadInts = readIntsBS <$> BS8.getLine\n\n```\n\nAt this point, profiling reveals that about half the runtime is due to `sort`. Switching to [nubInt](https:\/\/hackage.haskell.org\/package\/containers-0.7\/docs\/Data-Containers-ListUtils.html#v:nubInt) speeds that step up a lot:\n\n\n\n```\nimport Data.Containers.ListUtils\nsearch l = if nubInt l \/= l then \"YES\" else \"NO\"\nsolution = search.flock.floss\n\n```\n\nOr you could implement a custom uniqueness check, though this gets only a small savings over using `nubInt`:\n\n\n\n```\nimport qualified Data.IntSet as IS\n\nsearch :: [Int] -> String \nsearch l = if uniqInt l then \"NO\" else \"YES\"\n\nuniqInt :: [Int] -> Bool\nuniqInt = go IS.empty where\n    go seen [] = True\n    go seen (n:ns) = case IS.alterF (,True) n seen of\n        (False, seen') -> go seen' ns\n        _ -> False\n\n```\n\nYou'll also want to switch over from `scanr` to `scanl`. (As a rule of thumb, for folds you generally are choosing from `foldr` and `foldl'` based on what operation you're folding, but for scans you almost always want `scanl` or a minor variant of it like `scanl1`.) This almost doubles the speed.\n\n\n\n```\nflock = scanl (+) 0\n\n```\n\nAt this point, the cumulative savings have dropped the runtime on my machine+test file from 25s to 0.8s; perhaps this is going far enough. Here's the complete final result, with a few minor tweaks (for style, not performance) not explicitly discussed above.\n\n\n\n```\nimport Control.Monad\nimport Data.Bool\nimport Data.ByteString.Char8 (ByteString)\nimport Data.Char\nimport Data.Containers.ListUtils\nimport qualified Data.ByteString.Char8 as BS8\nimport qualified Data.IntSet as IS\n\nmain :: IO ()\nmain = do\n    t <- readLn\n    replicateM_ t $ do\n        BS8.getLine\n        putStrLn . solution . readIntsBS =<< BS8.getLine\n\nsolution :: [Int] -> String\nsolution = bool \"YES\" \"NO\" . uniqInt . scanl (+) 0 . zipWith ($) (cycle [id, negate])\n\nreadIntsBS :: ByteString -> [Int]\nreadIntsBS bs = case BS8.readInt bs of\n    Nothing -> []\n    Just (n, bs') -> n : readIntsBS (BS8.dropWhile isSpace bs')\n\nuniqInt :: [Int] -> Bool\nuniqInt = go IS.empty where\n    go seen [] = True\n    go seen (n:ns) = case IS.alterF (,True) n seen of\n        (False, seen') -> go seen' ns\n        _ -> False"}
{"questionId":"c593ad788b514903bbcec7fbe0c2a90c","question":"Will sending `kill -11` to java process raises a NullPointerException?\nFor example, the HotSpot JVM implement null-pointer detection by catching SIGSEGV signal. So if we manually generate a SIGSEGV from external, will that also be recognized as `NullPointerException` **in some circumstances** ?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"java"},"answer":"> \n> Will sending `kill -11` to java process raises a NullPointerException?\n> \n> \n> \n\n\nIt should not: a `NullPointerException` is a specific exception that occurs when an application tries to use an object reference that has the null value.\n\n\nYet, from [JavaSE 17 \/ Troubleshooting guide \/ Handle Signals and Exceptions](https:\/\/docs.oracle.com\/en\/java\/javase\/17\/troubleshoot\/handle-signals-and-exceptions.html#GUID-CBA7BA16-F89F-4BE2-B653-139BABA997D4)\n\n\n\n> \n> The Java HotSpot VM installs signal handlers to implement various features and to handle fatal error conditions.\n> \n> \n> For example, in an optimization to avoid explicit null checks in cases where `java.lang.NullPointerException` will be thrown rarely, the `SIGSEGV` signal is caught and handled, and the `NullPointerException` is thrown.\n> \n> \n> In general, there are two categories where signal\/traps happen:\n> \n> \n> - When signals are expected and handled, like implicit null-handling. Another example is the safepoint polling mechanism, which protects a page in memory when a safepoint is required. Any thread that accesses that page causes a `SIGSEGV`, which results in the execution of a stub that brings the thread to a safepoint.\n> - Unexpected signals. That includes a `SIGSEGV` when executing in VM code, Java Native Interface (JNI) code, or native code. In these cases, the signal is unexpected, so fatal error handling is invoked to create the error log and terminate the process.\n> \n> \n> \n\n\nThat approach allows the JVM to optimize performance by reducing the overhead of explicit null checks in the code, relying instead on the operating system's memory protection mechanisms to detect access to null references. When such access occurs, the operating system generates a `SIGSEGV` signal, which the JVM then interprets as an attempt to dereference a null pointer, leading to the throwing of a `NullPointerException`.\n\n\nHowever, it is important to note that this is an *internal* mechanism of the JVM and is distinct from externally generated `SIGSEGV` signals, such as those sent using the `kill` command. External `SIGSEGV` signals are generally used to indicate serious errors, including invalid memory access, and are more likely to result in a JVM crash or core dump rather than a `NullPointerException`.\n\n\n\n```\n+---------------------+         +-----------------------------------+\n| External Process    |         | Java Process running on HotSpot   |\n| sending SIGSEGV     | ------> | JVM                               |\n| (kill -11)          |         | Likely JVM Crash or Core Dump     |\n+---------------------+         +-----------------------------------+\n\n```\n\n\n\n---\n\n\n\n> \n> Is the JVM always capable of detecting whether an external `SIGSEGV` is an external `SIGSEGV` or is it possible to confuse an external `SIGSEGV` for a null access when it happens at a specific time, i.e. when a potential null access is expected?\n> \n> \n> \n\n\nAgain, it should not, but this is an *implementation*-specific aspect of JVM behavior.  \n\nThat means the likelihood of such confusion happening in practice may vary depending on the JVM version, the specific code being executed, and the state of the JVM at the time of the signal.\n\n\nSee for instance \"[How does the JVM know when to throw a NullPointerException](https:\/\/stackoverflow.com\/a\/36955888\/6309)\"\n\n\n\n> \n> The JVM could implement the null check using virtual memory hardware. The JVM arranges that page zero in its virtual address space is mapped to a page that is unreadable + unwriteable.\n> \n> \n> Since null is represented as zero, when Java code tries to dereference null this will try to access a non-addressible page and will lead to the OS delivering a \"segfault\" signal to the JVM.\n> \n> \n> The JVM's segfault signal handler could trap this, figure out where the code was executing, and create and throw an NPE on the stack of the appropriate thread.\n> \n> \n> \n\n\nIn that scenario, it should be easy to distinguish a trapped signal from within the code execution, from a received signal from the OS.\n\n\nAlso: \"[Can a `SIGSEGV` in Java not crash the JVM?](https:\/\/stackoverflow.com\/a\/43346525\/6309)\"\n\n\n\n> \n> There are definitely scenarios where the JVM's `SIGSEGV` signal handler may turn the `SIGSEGV` event into a Java exception.  \n> \n> You will only get a JVM hard crash if that cannot happen; e.g. if the thread that triggered the `SIGSEGV` was executing code in a native library when the event happened.\n> \n> \n> \n\n\n[For instance](https:\/\/stackoverflow.com\/questions\/44501096\/avoid-nullpointerexception-on-jni-vm-startup#comment76037746_44501096):\n\n\n\n> \n> [HotSpot JVM deliberately generates SIGSEGV at startup](https:\/\/hg.openjdk.org\/jdk8u\/jdk8u\/hotspot\/file\/68758c5ab0c1\/src\/cpu\/x86\/vm\/vm_version_x86.cpp#l259) to check certain CPU features. There is no switch to turn it off. I suggest skipping `SIGSEGV` in `gdb` altogether, because JVM uses it for its own purpose in many cases.\n> \n> \n> \n\n\n\n\n---\n\n\n\n> \n> What if the stack happens to locate at accessing an address when the `SIGSEGV` is triggered externally?\n> \n> \n> \n\n\nThe hotspot had a major refactoring around signal handling in [JDK-8255711](https:\/\/bugs.openjdk.org\/browse\/JDK-8255711), resulting in [commit dd8e4ff](https:\/\/github.com\/openjdk\/jdk\/commit\/dd8e4ffbe559568222fc21c09d162a867df2e805).\n\n\nThe current code is [`os_linux_x86.cpp#PosixSignals::pd_hotspot_signal_handler`](https:\/\/github.com\/openjdk\/jdk\/blob\/9902d2eb177072c108933056cba544cc5a34bb54\/src\/hotspot\/os_cpu\/linux_x86\/os_linux_x86.cpp#L207-L421)\n\n\n\n```\n  \/\/ decide if this trap can be handled by a stub\n  address stub = nullptr;\n\n  address pc          = nullptr;\n\n  \/\/%note os_trap_1\n  if (info != nullptr && uc != nullptr && thread != nullptr) {\n    pc = (address) os::Posix::ucontext_get_pc(uc);\n\n    if (sig == SIGSEGV && info->si_addr == 0 && info->si_code == SI_KERNEL) {\n      \/\/ An irrecoverable SI_KERNEL SIGSEGV has occurred.\n      \/\/ It's likely caused by dereferencing an address larger than TASK_SIZE.\n      return false;\n    }\n\n    \/\/ Handle ALL stack overflow variations here\n    if (sig == SIGSEGV) {\n      address addr = (address) info->si_addr;\n\n      \/\/ check if fault address is within thread stack\n      if (thread->is_in_full_stack(addr)) {\n        \/\/ stack overflow\n        if (os::Posix::handle_stack_overflow(thread, addr, pc, uc, &stub)) {\n          return true; \/\/ continue\n        }\n      }\n    }\n\n    if ((sig == SIGSEGV) && VM_Version::is_cpuinfo_segv_addr(pc)) {\n      \/\/ Verify that OS save\/restore AVX registers.\n      stub = VM_Version::cpuinfo_cont_addr();\n    }\n\n    if (thread->thread_state() == _thread_in_Java) {\n      \/\/ Java thread running in Java code => find exception handler if any\n      \/\/ a fault inside compiled code, the interpreter, or a stub\n\n      if (sig == SIGSEGV && SafepointMechanism::is_poll_address((address)info->si_addr)) {\n        stub = SharedRuntime::get_poll_stub(pc);\n      } else if (sig == SIGBUS \/* && info->si_code == BUS_OBJERR *\/) {\n        \/\/ BugId 4454115: A read from a MappedByteBuffer can fault\n        \/\/ here if the underlying file has been truncated.\n        \/\/ Do not crash the VM in such a case.\n        CodeBlob* cb = CodeCache::find_blob(pc);\n        CompiledMethod* nm = (cb != nullptr) ? cb->as_compiled_method_or_null() : nullptr;\n        bool is_unsafe_arraycopy = thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc);\n        if ((nm != nullptr && nm->has_unsafe_access()) || is_unsafe_arraycopy) {\n          address next_pc = Assembler::locate_next_instruction(pc);\n          if (is_unsafe_arraycopy) {\n            next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n          }\n          stub = SharedRuntime::handle_unsafe_access(thread, next_pc);\n        }\n      }\n      else\n\n#ifdef AMD64\n      if (sig == SIGFPE  &&\n          (info->si_code == FPE_INTDIV || info->si_code == FPE_FLTDIV)) {\n        stub =\n          SharedRuntime::\n          continuation_for_implicit_exception(thread,\n                                              pc,\n                                              SharedRuntime::\n                                              IMPLICIT_DIVIDE_BY_ZERO);\n#else\n      if (sig == SIGFPE \/* && info->si_code == FPE_INTDIV *\/) {\n        \/\/ HACK: si_code does not work on linux 2.2.12-20!!!\n        int op = pc[0];\n        if (op == 0xDB) {\n          \/\/ FIST\n          \/\/ TODO: The encoding of D2I in x86_32.ad can cause an exception\n          \/\/ prior to the fist instruction if there was an invalid operation\n          \/\/ pending. We want to dismiss that exception. From the win_32\n          \/\/ side it also seems that if it really was the fist causing\n          \/\/ the exception that we do the d2i by hand with different\n          \/\/ rounding. Seems kind of weird.\n          \/\/ NOTE: that we take the exception at the NEXT floating point instruction.\n          assert(pc[0] == 0xDB, \"not a FIST opcode\");\n          assert(pc[1] == 0x14, \"not a FIST opcode\");\n          assert(pc[2] == 0x24, \"not a FIST opcode\");\n          return true;\n        } else if (op == 0xF7) {\n          \/\/ IDIV\n          stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::IMPLICIT_DIVIDE_BY_ZERO);\n        } else {\n          \/\/ TODO: handle more cases if we are using other x86 instructions\n          \/\/   that can generate SIGFPE signal on linux.\n          tty->print_cr(\"unknown opcode 0x%X with SIGFPE.\", op);\n          fatal(\"please update this code.\");\n        }\n#endif \/\/ AMD64\n      } else if (sig == SIGSEGV &&\n                 MacroAssembler::uses_implicit_null_check(info->si_addr)) {\n          \/\/ Determination of interpreter\/vtable stub\/compiled code null exception\n          stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::IMPLICIT_NULL);\n      }\n    } else if ((thread->thread_state() == _thread_in_vm ||\n                thread->thread_state() == _thread_in_native) &&\n               (sig == SIGBUS && \/* info->si_code == BUS_OBJERR && *\/\n               thread->doing_unsafe_access())) {\n        address next_pc = Assembler::locate_next_instruction(pc);\n        if (UnsafeCopyMemory::contains_pc(pc)) {\n          next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n        }\n        stub = SharedRuntime::handle_unsafe_access(thread, next_pc);\n    }\n\n    \/\/ jni_fast_Get<Primitive>Field can trap at certain pc's if a GC kicks in\n    \/\/ and the heap gets shrunk before the field access.\n    if ((sig == SIGSEGV) || (sig == SIGBUS)) {\n      address addr = JNI_FastGetField::find_slowcase_pc(pc);\n      if (addr != (address)-1) {\n        stub = addr;\n      }\n    }\n  }\n\n```\n\nThe JVM uses various checks to determine the context of a `SIGSEGV` signal. However, I do not see a straightforward mechanism to distinguish an externally sent `SIGSEGV` from one internally generated due to a null reference access.\n\n\nThe signal handler examines the execution context, including the program counter and the stack, to infer the cause of the `SIGSEGV`. In case of a null reference, it looks for specific patterns that suggest a null pointer exception. But if an external `SIGSEGV` happens to coincide precisely with a situation where the JVM's execution state resembles that of a null pointer access, distinguishing between the two can be challenging.\n\n\nHowever, such a scenario is relatively unlikely due to the level of precision required in timing."}
{"questionId":"9ee1e16c89fb48bb8bb6fe65df95181b","question":"No provider for \\_HttpClient\nI'm working on a personal project with Angular 17, and there are some settings that I get from the backend of my application. But my Angular `HttpClient` does not work and honestly I don't know why not.\n\n\nThe error that I get is this:\n\n\n\n> \n> ERROR Error [NullInjectorError]: R3InjectorError(Standalone[\\_AppComponent])[\\_GetSettingsService -> \\_GetSettingsService -> \\_HttpClient -> \\_HttpClient]:\n> NullInjectorError: No provider for \\_HttpClient!\n> at NullInjector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:5465:13)\n> at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:5916:22)  \n> \n> at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:5916:22)  \n> \n> at injectInjectorOnly (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:912:12)\n> at Module.\u0275\u0275inject (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:998:53)  \n> \n> at Object.GetSettingsService\\_Factory (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\src\\app\\shared\\services\\get-settings.service.ts:9:32)\n> at eval (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:6028:10)\n> at runInInjectorProfilerContext (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:873:5)\n> at R3Injector.hydrate (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:6027:11)\n> at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:5907:7) {  \n> \n> ngTempTokenPath: null,\n> ngTokenPath: [\n> '\\_GetSettingsService',\n> '\\_GetSettingsService',\n> '\\_HttpClient',\n> '\\_HttpClient'\n> ]\n> }\n> Error [NullInjectorError]: R3InjectorError(Standalone[\\_AppComponent])[\\_GetSettingsService -> \\_GetSettingsService -> \\_HttpClient -> \\_HttpClient]:\n> NullInjectorError: No provider for \\_HttpClient!\n> at NullInjector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:5465:13)\n> at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:5916:22)  \n> \n> at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:5916:22)  \n> \n> at injectInjectorOnly (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:912:12)\n> at Module.\u0275\u0275inject (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:998:53)  \n> \n> at Object.GetSettingsService\\_Factory (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\src\\app\\shared\\services\\get-settings.service.ts:9:32)\n> at eval (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:6028:10)\n> at runInInjectorProfilerContext (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:873:5)\n> at R3Injector.hydrate (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:6027:11)\n> at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:5907:7) {  \n> \n> ngTempTokenPath: null,\n> ngTokenPath: [\n> '\\_GetSettingsService',\n> '\\_GetSettingsService',\n> '\\_HttpClient',\n> '\\_HttpClient'\n> ]\n> }\n> 14:23:23 [vite] Internal server error: R3InjectorError(Standalone[\\_AppComponent])[\\_GetSettingsService -> \\_GetSettingsService -> \\_HttpClient -> \\_HttpClient]:\n> NullInjectorError: No provider for \\_HttpClient!\n> at NullInjector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\co14:23:23 [vite] Internal server error: R3InjectorError(Standalone[\\_AppComponent])[\\_GetSettingsService -> \\_GetSettingsService -> \\_HttpClient -> \\_HttpClient]:\n> NullInjectorError: No provider for \\_HttpClient!\n> at NullInjector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:5465:13)\n> at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:5916:22)\n> at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:5916:22)\n> at injectInjectorOnly (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:912:12)\n> at Module.\u0275\u0275inject (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:998:53)\n> at Object.GetSettingsService\\_Factory (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\src\\app\\shared\\services\\get-settings.service.ts:9:32)\n> at eval (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:6028:10)\n> at runInInjectorProfilerContext (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:873:5)\n> at R3Injector.hydrate (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:6027:11)\n> at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000.angular\\vite-root\\pip-boy-3000\\node\\_modules@angular\\core\\fesm2022\\core.mjs:5907:7) (x2)\n> \n> \n> \n\n\nMy app.component.ts :\n\n\n\n```\nimport { GetSettingsService } from '.\/shared\/services\/get-settings.service';\nimport { NavheaderComponent } from '.\/components\/navheader\/navheader.component';\nimport { Component, NgModule, OnInit } from '@angular\/core';\nimport { CommonModule } from '@angular\/common';\nimport { RouterOutlet } from '@angular\/router';\nimport { ConfigComponent } from '.\/components\/config\/config.component';\nimport { TerminalComponent } from '.\/components\/terminal\/terminal.component';\nimport { StaticEffectComponent } from '.\/components\/static-effect\/static-effect.component';\nimport { UserConfig } from '.\/shared\/models\/userConfigs.model';\nimport { HttpClientModule } from '@angular\/common\/http';\n\n@Component({\n  selector: 'app-root',\n  standalone: true,\n  imports: [\n    CommonModule,\n    RouterOutlet,\n    NavheaderComponent,\n    ConfigComponent,\n    TerminalComponent,\n    StaticEffectComponent,\n  ],\n  providers: [\n    HttpClientModule\n  ],\n  templateUrl: '.\/app.component.html',\n  styleUrls: ['.\/app.component.scss']\n})\nexport class AppComponent implements OnInit {\n  title = 'pip-boy-3000';\n\n  constructor(private getSettingsService: GetSettingsService) {}\n\n\n  \/\/Ambient Variables\n  public userConfig: UserConfig = new UserConfig();\n\n\n  \/\/Used for activating the Config Panel;\n  public configIsActivated: boolean = false;\n  public noisyBackground: boolean = true;\n\n  public setNoisyBackground(e: boolean) {\n    this.noisyBackground = e;\n  }\n\n  public activateOrDeactivateConfig(e: any): void {\n    console.log(e);\n    this.configIsActivated = e;\n  }\n\n  ngOnInit(): void {\n    this.getSettingsService.getSettingsInfos(1).subscribe({\n      error: () => {\n        console.log(\"error\")\n      },\n      next: (data: UserConfig) => {\n        this.userConfig = data;\n      }\n    })\n  }\n}\n\n\n```\n\nMy service\n\n\n\n```\nimport { HttpClient } from '@angular\/common\/http';\nimport { Injectable } from '@angular\/core';\nimport { Observable } from 'rxjs';\nimport { UserConfig } from '..\/models\/userConfigs.model';\n\n@Injectable({\n  providedIn: 'root',\n})\nexport class GetSettingsService {\n\n  constructor(private httpClient: HttpClient) { }\n\n  public getSettingsInfos(userId: number): Observable<UserConfig> {\n    return this.httpClient.get<UserConfig>(`\/settings\/${userId}`);\n  }\n}\n\n```\n\ni tried multiple times changing some things, creating modules and other sort of things, and i dont know what can i do\n\n\n**EDIT**\n\n\nIve also changed the location of the HttpClientModule from providers to imports, but then this error comes in:\n\n\n\n```\nERROR Error [NullInjectorError]: R3InjectorError(Standalone[_AppComponent])[_GetSettingsService -> _GetSettingsService -> _HttpClient -> _HttpClient]: \n  NullInjectorError: No provider for _HttpClient!\n    at NullInjector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:5465:13)\n    at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:5916:22)\n    at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:5916:22)\n    at injectInjectorOnly (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:912:12)\n    at Module.\u0275\u0275inject (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:998:53)\n    at Object.GetSettingsService_Factory (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\src\\app\\shared\\services\\get-settings.service.ts:9:32)\n    at eval (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:6028:10)\n    at runInInjectorProfilerContext (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:873:5)\n    at R3Injector.hydrate (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:6027:11)\n    at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:5907:7) {\n  ngTempTokenPath: null,\n  ngTokenPath: [\n    '_GetSettingsService',\n    '_GetSettingsService',\n    '_HttpClient',\n    '_HttpClient'\n  ]\n}\nError [NullInjectorError]: R3InjectorError(Standalone[_AppComponent])[_GetSettingsService -> _GetSettingsService -> _HttpClient -> _HttpClient]: \n  NullInjectorError: No provider for _HttpClient!\n    at NullInjector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:5465:13)\n    at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:5916:22)\n    at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:5916:22)\n    at injectInjectorOnly (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:912:12)\n    at Module.\u0275\u0275inject (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:998:53)\n    at Object.GetSettingsService_Factory (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\src\\app\\shared\\services\\get-settings.service.ts:9:32)\n    at eval (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:6028:10)\n    at runInInjectorProfilerContext (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:873:5)\n    at R3Injector.hydrate (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:6027:11)\n    at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:5907:7) {\n  ngTempTokenPath: null,\n  ngTokenPath: [\n    '_GetSettingsService',\n    '_GetSettingsService',\n    '_HttpClient',\n    '_HttpClient'\n  ]\n}\n16:37:53 [vite] Internal server error: R3InjectorError(Standalone[_AppComponent])[_GetSettingsService -> _GetSettingsService -> _HttpClient -> _HttpClient]: \n  NullInjectorError: No provider for _HttpClient!\n      at NullInjector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:5465:13)\n      at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:5916:22)\n      at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:5916:22)\n      at injectInjectorOnly (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:912:12)\n      at Module.\u0275\u0275inject (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:998:53)\n      at Object.GetSettingsService_Factory (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\src\\app\\shared\\services\\get-settings.service.ts:9:32)\n      at eval (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:6028:s:6028:10)\n      at runInInjectorProfilerContext (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:873:5)\n      at R3Injector.hydrate (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:6027:11)\n      at R3Injector.get (C:\\Users\\user\\Documents\\Works\\pip-boy-3000\\.angular\\vite-root\\pip-boy-3000\\node_modules\\@angular\\core\\fesm2022\\core.mjs:5907:7)","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"typescript"},"answer":"I was dealing with the same issue until I had a close look at the new documentation [here at angular.dev](https:\/\/angular.dev\/guide\/http\/setup#providing-httpclient-through-dependency-injection), which tells you how to provide `HttpClient` to a standalone app:\n\n\n\n> \n> `HttpClient` is provided using the `provideHttpClient` helper function, which most apps include in the application `providers` in `main.ts`.\n> \n> \n> \n> ```\n> boostrapApplication(App, {providers: [\n>   provideHttpClient(),\n> ]});\n> \n> ```\n> \n> \n\n\nLastly, since it's a standalone app you're dealing with, importing `HttpClientModule` doesn't do anything that would've made Angular provide `HttpClient` to your app to begin with, so feel free to remove the `HttpClientModule` import from `app.component.ts` afterwards."}
{"questionId":"334ab191ffbe477d8cd9fb3b961452ad","question":"Percentage of total counts by group in R\nI'm trying to create an output that calculates the percentage of counts, out of total counts (in a data frame), by factor level, but can't seem to figure out how to retain the grouping structure in the output.\n\n\nI can get the total counts that I want to divide by...\n\n\n\n```\ndf %>% summarise(sum(num))\n# 15\n\n```\n\n...and the total by group...\n\n\n\n```\ndf %>% group_by(species) %>% summarise(sum(num))\n# A tibble: 3 \u00d7 2\n#   species                  `sum(num)`\n#   <chr>                         <int>\n# 1 Farfantepenaeus duorarum          4\n# 2 Farfantepenaeus notialis          0\n# 3 Farfantepenaeus spp              11\n\n```\n\nBut I can't get it to get it to look like this...\n\n\n\n```\n# ???\n#   species                     Percent\n#   <chr>                         <int>\n# 1 Farfantepenaeus duorarum       4 \/ 15 = 0.267\n# 2 Farfantepenaeus notialis       0 \/ 15 = 0.000\n# 3 Farfantepenaeus spp           11 \/ 15 = 0.733\n\n```\n\nThe closest I got was this, but because I used reframe() it returns the ungrouped data\n\n\n\n```\ndf %>% group_by(species) %>% \n  summarise(factor_count=sum(num)) %>% \n  # ungroup() %>% \n  # Wanring: # Please use `reframe()` instead., When switching from `summarise()` \n  # to `reframe()`, remember that `reframe()` always returns an ungrouped data\n  reframe(percent=factor_count\/sum(df$num))\n\n# A tibble: 3 \u00d7 1\n  percent\n    <dbl>\n1   0.267\n2   0    \n3   0.733\n\n```\n\nData:\n\n\n\n```\n> dput(df)\nstructure(list(species = c(\"Farfantepenaeus notialis\", \"Farfantepenaeus spp\", \n\"Farfantepenaeus notialis\", \"Farfantepenaeus notialis\", \"Farfantepenaeus duorarum\", \n\"Farfantepenaeus duorarum\", \"Farfantepenaeus notialis\", \"Farfantepenaeus spp\", \n\"Farfantepenaeus duorarum\", \"Farfantepenaeus spp\", \"Farfantepenaeus notialis\", \n\"Farfantepenaeus duorarum\", \"Farfantepenaeus spp\", \"Farfantepenaeus notialis\", \n\"Farfantepenaeus notialis\", \"Farfantepenaeus spp\", \"Farfantepenaeus duorarum\", \n\"Farfantepenaeus spp\", \"Farfantepenaeus spp\", \"Farfantepenaeus duorarum\", \n\"Farfantepenaeus duorarum\", \"Farfantepenaeus spp\", \"Farfantepenaeus spp\", \n\"Farfantepenaeus spp\", \"Farfantepenaeus notialis\"), num = c(0L, \n0L, 0L, 0L, 1L, 0L, 0L, 2L, 0L, 3L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, \n0L, 0L, 0L, 3L, 0L, 2L, 4L, 0L)), row.names = c(159897L, 174698L, \n236857L, 190237L, 327321L, 272931L, 304567L, 75538L, 109206L, \n351373L, 280332L, 163966L, 282183L, 341197L, 316962L, 354703L, \n343971L, 95333L, 244258L, 254061L, 87561L, 186908L, 221318L, \n258688L, 97737L), class = \"data.frame\")","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"r"},"answer":"Two steps: summarize group-totals, then percent-calcs on everything combined.\n\n\n\n```\nlibrary(dplyr)\ndf %>%\n  summarize(Percent = sum(num), .by = species) %>%\n  mutate(Percent = Percent \/ sum(Percent))\n#                    species   Percent\n# 1 Farfantepenaeus notialis 0.0000000\n# 2      Farfantepenaeus spp 0.7333333\n# 3 Farfantepenaeus duorarum 0.2666667\n\n```\n\nFor your code:\n\n\n- `reframe` is not necessary (mostly when the number of rows *changes*, it can often be used in place of `summarise`, but I haven't verified if\/where the two differ significantly), and in fact here it will drop the `species` column\n- (Almost) *Never* use `df$` in a pipe that starts with `df`: using `df$num` ignores anything you've done since the start of the pipe, meaning that grouping, filtering, additions\/changes, etc, are not available in that version of `df`. There are certainly times when it is useful and even necessary, but they are few and far-between."}
{"questionId":"93d97ea013e5402da8053e2a75fbbac1","question":"warning C4996: 'Py\\_OptimizeFlag': deprecated in 3.12 aiohttp\/\\_websocket.c(3042): error C2039: 'ob\\_digit': is not a member of '\\_longobject'\nNewbie here.\n\n\nI have been trying to installen the openai library into python, but I keep running into problems. I have already installed C++ libraries.\n\n\nIt seems to have problems specific with aio http, and I get the error below.\nI a running a Windows 11 laptop without admin restrictions.\n\n\n**Error**\n\n\n\n```\n\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.37.32822\\bin\\HostX86\\x64\\cl.exe\" \/c \/nologo \/O2 \/W3 \/GL \/DNDEBUG \/MD -IC:\\Users\\sande\\AppData\\Local\\Programs\\Python\\Python312\\include -IC:\\Users\\sande\\AppData\\Local\\Programs\\Python\\Python312\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.37.32822\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" \/Tcaiohttp\/_websocket.c \/Fobuild\\temp.win-amd64-cpython-312\\Release\\aiohttp\/_websocket.obj\n      _websocket.c\n      aiohttp\/_websocket.c(1475): warning C4996: 'Py_OptimizeFlag': deprecated in 3.12\n      aiohttp\/_websocket.c(3042): error C2039: 'ob_digit': is not a member of '_longobject'\n\n[end of output]\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n\nERROR: Failed building wheel for aiohttp\n\nFailed to build aiohttp\n\nERROR: Could not build wheels for aiohttp, which is required to install pyproject.toml-based projects","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"python"},"answer":"Either use `python 3.11`\n\n\nor\n\n\n\n```\npip install aiohttp==3.9.0b0\n\n```\n\n\n> \n> installs their current beta release that supports python 3.12.x\n> \n> \n> \n\n\nthen try `openai` installation\n\n\n\n\n---\n\n\nLink to git :<https:\/\/github.com\/KillianLucas\/open-interpreter\/issues\/581>"}
{"questionId":"6498badbee654c4e921ddf67ec16923b","question":"Using starlette TestClient causes an AttributeError : '\\_UnixSelectorEventLoop' object has no attribute '\\_compute\\_internal\\_coro'\nUsing FastAPI : `0.101.1`\n\n\nI run this `test_read_aynsc` and it pass.\n\n\n\n```\n# app.py\nfrom fastapi import FastAPI\n\n\napp =  FastAPI()\napp.get(\"\/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n# conftest.py\n\nimport pytest\nfrom typing import Generator\nfrom fastapi.testclient import TestClient\n\nfrom server import app\n@pytest.fixture(scope=\"session\")\ndef client() -> Generator:\n    with TestClient(app) as c:\n        yield c\n\n# test_root.py\n\ndef test_read_aynsc(client):\n    response = client.get(\"\/item\")\n\n```\n\nHowever, executing this test in DEBUG mode (in pycharm) will cause an error. Here is the Traceback :\n\n\n\n```\ntest setup failed\ncls = <class 'anyio._backends._asyncio.AsyncIOBackend'>\nfunc = <function start_blocking_portal.<locals>.run_portal at 0x1555c51b0>\nargs = (), kwargs = {}, options = {}\n\n    @classmethod\n    def run(\n        cls,\n        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval]],\n        args: tuple[Unpack[PosArgsT]],\n        kwargs: dict[str, Any],\n        options: dict[str, Any],\n    ) -> T_Retval:\n        @wraps(func)\n        async def wrapper() -> T_Retval:\n            task = cast(asyncio.Task, current_task())\n            task.set_name(get_callable_name(func))\n            _task_states[task] = TaskState(None, None)\n    \n            try:\n                return await func(*args)\n            finally:\n                del _task_states[task]\n    \n        debug = options.get(\"debug\", False)\n        loop_factory = options.get(\"loop_factory\", None)\n        if loop_factory is None and options.get(\"use_uvloop\", False):\n            import uvloop\n    \n            loop_factory = uvloop.new_event_loop\n    \n        with Runner(debug=debug, loop_factory=loop_factory) as runner:\n>           return runner.run(wrapper())\n\n..\/..\/..\/Library\/Caches\/pypoetry\/virtualenvs\/kms-backend-F9vGicV3-py3.10\/lib\/python3.10\/site-packages\/anyio\/_backends\/_asyncio.py:1991: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n..\/..\/..\/Library\/Caches\/pypoetry\/virtualenvs\/kms-backend-F9vGicV3-py3.10\/lib\/python3.10\/site-packages\/anyio\/_backends\/_asyncio.py:193: in run\n    return self._loop.run_until_complete(task)\n..\/..\/..\/Library\/Application Support\/JetBrains\/Toolbox\/apps\/PyCharm-P\/ch-0\/233.13763.11\/PyCharm.app\/Contents\/plugins\/python\/helpers-pro\/pydevd_asyncio\/pydevd_nest_asyncio.py:202: in run_until_complete\n    self._run_once()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _run_once(self):\n        \"\"\"\n        Simplified re-implementation of asyncio's _run_once that\n        runs handles as they become ready.\n        \"\"\"\n        ready = self._ready\n        scheduled = self._scheduled\n        while scheduled and scheduled[0]._cancelled:\n            heappop(scheduled)\n    \n        timeout = (\n            0 if ready or self._stopping\n            else min(max(\n                scheduled[0]._when - self.time(), 0), 86400) if scheduled\n            else None)\n        event_list = self._selector.select(timeout)\n        self._process_events(event_list)\n    \n        end_time = self.time() + self._clock_resolution\n        while scheduled and scheduled[0]._when < end_time:\n            handle = heappop(scheduled)\n            ready.append(handle)\n    \n>       if self._compute_internal_coro:\nE       AttributeError: '_UnixSelectorEventLoop' object has no attribute '_compute_internal_coro'\n\n..\/..\/..\/Library\/Application Support\/JetBrains\/Toolbox\/apps\/PyCharm-P\/ch-0\/233.13763.11\/PyCharm.app\/Contents\/plugins\/python\/helpers-pro\/pydevd_asyncio\/pydevd_nest_asyncio.py:236: AttributeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture(scope=\"session\")\n    def client() -> Generator:\n>       with TestClient(app) as c:\n\ntests\/fixtures\/common\/http_client_app.py:10: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n..\/..\/..\/Library\/Caches\/pypoetry\/virtualenvs\/kms-backend-F9vGicV3-py3.10\/lib\/python3.10\/site-packages\/starlette\/testclient.py:730: in __enter__\n    self.portal = portal = stack.enter_context(\n..\/..\/..\/.pyenv\/versions\/3.10.12\/lib\/python3.10\/contextlib.py:492: in enter_context\n    result = _cm_type.__enter__(cm)\n..\/..\/..\/.pyenv\/versions\/3.10.12\/lib\/python3.10\/contextlib.py:135: in __enter__\n    return next(self.gen)\n..\/..\/..\/Library\/Caches\/pypoetry\/virtualenvs\/kms-backend-F9vGicV3-py3.10\/lib\/python3.10\/site-packages\/anyio\/from_thread.py:454: in start_blocking_portal\n    run_future.result()\n..\/..\/..\/.pyenv\/versions\/3.10.12\/lib\/python3.10\/concurrent\/futures\/_base.py:451: in result\n    return self.__get_result()\n..\/..\/..\/.pyenv\/versions\/3.10.12\/lib\/python3.10\/concurrent\/futures\/_base.py:403: in __get_result\n    raise self._exception\n..\/..\/..\/.pyenv\/versions\/3.10.12\/lib\/python3.10\/concurrent\/futures\/thread.py:58: in run\n    result = self.fn(*self.args, **self.kwargs)\n..\/..\/..\/Library\/Caches\/pypoetry\/virtualenvs\/kms-backend-F9vGicV3-py3.10\/lib\/python3.10\/site-packages\/anyio\/_core\/_eventloop.py:73: in run\n    return async_backend.run(func, args, {}, backend_options)\n..\/..\/..\/Library\/Caches\/pypoetry\/virtualenvs\/kms-backend-F9vGicV3-py3.10\/lib\/python3.10\/site-packages\/anyio\/_backends\/_asyncio.py:1990: in run\n    with Runner(debug=debug, loop_factory=loop_factory) as runner:\n..\/..\/..\/Library\/Caches\/pypoetry\/virtualenvs\/kms-backend-F9vGicV3-py3.10\/lib\/python3.10\/site-packages\/anyio\/_backends\/_asyncio.py:133: in __exit__\n    self.close()\n..\/..\/..\/Library\/Caches\/pypoetry\/virtualenvs\/kms-backend-F9vGicV3-py3.10\/lib\/python3.10\/site-packages\/anyio\/_backends\/_asyncio.py:141: in close\n    _cancel_all_tasks(loop)\n..\/..\/..\/Library\/Caches\/pypoetry\/virtualenvs\/kms-backend-F9vGicV3-py3.10\/lib\/python3.10\/site-packages\/anyio\/_backends\/_asyncio.py:243: in _cancel_all_tasks\n    loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))\n..\/..\/..\/Library\/Application Support\/JetBrains\/Toolbox\/apps\/PyCharm-P\/ch-0\/233.13763.11\/PyCharm.app\/Contents\/plugins\/python\/helpers-pro\/pydevd_asyncio\/pydevd_nest_asyncio.py:202: in run_until_complete\n    self._run_once()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _run_once(self):\n        \"\"\"\n        Simplified re-implementation of asyncio's _run_once that\n        runs handles as they become ready.\n        \"\"\"\n        ready = self._ready\n        scheduled = self._scheduled\n        while scheduled and scheduled[0]._cancelled:\n            heappop(scheduled)\n    \n        timeout = (\n            0 if ready or self._stopping\n            else min(max(\n                scheduled[0]._when - self.time(), 0), 86400) if scheduled\n            else None)\n        event_list = self._selector.select(timeout)\n        self._process_events(event_list)\n    \n        end_time = self.time() + self._clock_resolution\n        while scheduled and scheduled[0]._when < end_time:\n            handle = heappop(scheduled)\n            ready.append(handle)\n    \n>       if self._compute_internal_coro:\nE       AttributeError: '_UnixSelectorEventLoop' object has no attribute '_compute_internal_coro'\n\n\n```\n\nI am not sure to understand what causes the error\nSince I can see the `_UnixSelectorEventLoop`, I need to precise that my operating system is MacOS M1.","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"python"},"answer":"To support async debugging, PyCharm patches a bunch of asyncio APIs with custom wrapped functions. Notably, it patches `asyncio.new_event_loop()` in `~\/Applications\/PyCharm Professional Edition.app\/Contents\/plugins\/python\/helpers-pro\/pydevd_asyncio\/pydevd_nest_asyncio.py:169`.\n\n\nStarlette uses anyio, with the asyncio backend by default. Anyio is eventually gonna try to get its event loop from `asyncio.events.new_event_loop()`, which is unpatched. Subsequent calls to patched asyncio APIs are gonna throw errors because they assume a patched event loop.\n\n\nUntil it's fixed properly, you can resolve this issue by forcing anyio to use the patched `new_event_loop` with\n\n\n\n```\nTestClient(app, backend_options={'loop_factory': asyncio.new_event_loop})\n\n```\n\n\n\n---\n\n\n### Update\n\n\nJetBrains dev says the fix is on the way, see <https:\/\/youtrack.jetbrains.com\/issue\/PY-70245>. Until then, they recommend disabling `python.debug.asyncio.repl` in **Help | Find Actions | Registry**"}
{"questionId":"8a1eb513d61243688464bc9d9eeedeaa","question":"Ignore specific rules in specific directory with Ruff\nI am using [Ruff](https:\/\/docs.astral.sh\/ruff\/), a formatter or linter tool for Python code.\n\n\nI want to ignore some specific rules, and to write that config in `pyproject.toml`.\n\n\nMy package structure is as follows.\n\n\n\n```\n.\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 src\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 mypackage\/mymodule.py\n\u2514\u2500\u2500 tests\n \u00a0\u00a0 \u251c\u2500\u2500 doc.md\n \u00a0\u00a0 \u2514\u2500\u2500 test_mymodule.py\n\n```\n\nAnd, I want to ignore rules [pydocstyle (D)](https:\/\/docs.astral.sh\/ruff\/rules\/#pydocstyle-d) in the `tests\/` directory.","questionMetadata":{"Type":"Implementation","Level":"Beginner","Tag":"python"},"answer":"You can use Ruff's `per-file-ignores` and specify individual files, or select a directory tree with a wildcard (star). You can ignore a \"letter class\" by specifying only the letter.\n\n\n### Example: Ignore specific rule in a specific file\n\n\nTo ignore line-length violations in your tests, add this to pyproject.toml:\n\n\n\n```\n[tool.ruff.lint.per-file-ignores]\n\"foofile.py\" = [\"E501\"]\n\n```\n\n### Example: Ignore a class of rules in a whole directory tree\n\n\nTo ignore all pydocstyle errors (starting with \"D\") in your tests, add this to pyproject.toml:\n\n\n\n```\n[tool.ruff.lint.per-file-ignores]\n\"tests\/*\" = [\"D\"]\n\n```\n\nThe related Ruff docs are [here](https:\/\/docs.astral.sh\/ruff\/configuration\/)."}
{"questionId":"348b4f64094a420bae9c3ee5816460a5","question":"How to reuse the return type of a function call inside a requires expression?\nI'm writing a concept that checks if a type can be used in an expression that composes 2 functions:\n\n\n\n```\ntemplate<typename T>\nconcept C = requires(T t) {\n    f(g(t));\n};\n\n```\n\ni.e., I want to check if for a given object `t` of type `T`, I can call `g(t)` and then use the resulting value as an argument to `f`. e.g.,\n\n\n\n```\nauto g(int)  -> float;   \nauto g(char) -> double; \n\nvoid f(float);\nvoid f(double) = delete;\n\nstatic_assert(C<int>);       \/\/ g(int) returns a float, so f(float) is called.\nstatic_assert(not C<char>);  \/\/ g(char) returns a double, but f(double) is deleted\n\n```\n\nThis works just fine.\n\n\nHowever, I want to split up the calls to `f` and `g` because 1) `g` may take additional arguments (that don't depend on `T`) resulting in a verbose call, and 2) I might want to use the return type\/value of `g` multiple times inside the concept, so I don't want to repeat the call multiple times.\n\n\nNaive attempts like the following\n\n\n\n```\nauto res = g(t);\nf(res);\n\n```\n\nand\n\n\n\n```\nusing ret_t = decltype(g(t));\nf(ret_t{});\n\n```\n\ndon't work inside concepts.\n\n\nIs there some way to achieve the splitting up of `f(g(t))` in a way that doesn't require writing `g(t)` inside the call to `f`?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"A nested requirement can be an option:\n\n\n\n```\ntemplate<typename T>\nconcept C = requires(T t) {\n    g(t);   \/\/ Not strictly needed, I think, but explicit verification might be more readable if the concept fails in a template\n    requires requires(decltype(g(t)) s) { \/\/ Can reuse s how many time we want\n        f(s);\n    };\n};\n\n```\n\nAnd yes, requires requires is required here to make sure `f(s)` is verified as well."}
{"questionId":"4b4ab9b08fbb43418f3328bfdfc009a0","question":"Longest Repeating Subarray (With Overlapping)\nGiven an array of arbitrary numbers, I am looking to quickly [with time and space complexity lower than O(n\u00b2) at minimum, n = length of array] find the longest subarray (a contiguous segment of elements within array) that is repeated (appears more than once, or at least twice).  \n\nPlease don't assume any bounds on the numbers in the array apart from all the values are valid JavaScript numbers and `isFinite` is true for every number in the array.\n\n\nOverlapping is allowed, that is partial occurrences of a subarray within itself is allowed. for ex, in the array `[1, 2, 1, 2, 1, 2]` the longest repeating subarray is `[1, 2, 1, 2]`. Look here\n\n\n\n```\n[1, 2, 1, 2, 1, 2]\n^         ^        occurrence #1\n       ^        ^  occurrence #2\n\n```\n\nThe two occurrences overlap, but that is a-ok as long as the two occurrence differ in either starting or ending index.\n\n\nIn case there are multiple distinct repeated subarrays that all have the longest length, the answer can be any of them. However, I suspect that any way that can find one answer can find all of them with the same ease.  \n\nA sample for this scenario is the array `[1, 1, 2, 2]`. The subarray `[1]` and `[2]` both appear twice. Either of those may be returned as the result.\n\n\nFinally, if no subarray occurs more than once, the answer is `[]` (the empty array).\n\n\nI recently came across the question [Find Repeat Sequence in an Array](https:\/\/stackoverflow.com\/questions\/77828008\/find-repeat-sequence-in-an-array) but all the answers there are ridiculously slow. Everyone's using o(n\u00b3) solutions, with only one answer in o(n\u00b2).\n\n\nI'm seeking a method that can handle arrays with length up to a couple hundred thousand (100,000) in a few seconds. Sub quadratic time complexity at least. It'd be great if you can provide a sample implementation in JS of the solution too because my understanding of some data structures is not that solid\n\n\nSorry for the long question, let me know if anything is unclear. If you have an idea but not a full solution just drop it in the comments, it can still be helpful to me.\n\n\nI made some more examples if it helps:\n\n\n\n\n\n| Array | Result |\n| --- | --- |\n| `[1, 2, 5, 7, 1, 2, 4, 5]` | `[1, 2]` |\n| `[9, 7, 0, 1, 6, 3, 6, 5, 2, 1, 6, 3, 8, 3, 6, 1, 6, 3]` | `[1, 6, 3]` |\n| `[1, 2, 1, 2, 7, 0, -1, 7, 0, -1]` | `[7, 0, -1]` |\n| `[1, 1, 1, 1]` | `[1, 1, 1]` |\n| `[1, 1, 1]` | `[1, 1]` |\n| `[1, 2, 3, 4, 2, 5]` | `[2]` |\n| `[1, 2, 3, 4, 5]` | `[]` |\n| `[1, 6, 2, 1, 6, 2, 1, 5]` | `[1, 6, 2, 1]` |\n\n\n\nHere's a function to generate large test case with array size 100,000 for reference, the answer is [1, 2, 3, ..., 100] (the 100 consecutive integers from 1 to 100)\n\n\n\n```\nfunction make_case() {\n    let array = [];\n    let number = 200;\n    for (let i = 0; i < 500; i++) {\n        array.push(number); array.push(number);\n        number++;\n    }\n    for (let i = 1; i < 101; i++) {\n        array.push(i);\n    }\n    for (let i = 0; i < 1700; i++) {\n        array.push(number); array.push(number);\n        number++;\n    }\n    for (let i = 1; i < 101; i++) {\n        array.push(i);\n    }\n    for (let i = 0; i < (100_000 - 500 * 2 - 100 - 1700 * 2 - 100) \/ 2; i++) {\n        array.push(number); array.push(number);\n        number++;\n    }\n    return array;\n}\n\n```\n\nAnother important case is `new Array(100_000).fill(0)`, the answer is the array with 99,999 zeros.","questionMetadata":{"Type":"Implementation","Level":"Advanced","Tag":"javascript"},"answer":"This can be solved by building a suffix array over a general alphabet. The longest repeating subarray will be the longest common prefix (LCP) between some two adjacent elements in the suffix array.\n\n\nThe following method has a time complexity of O(N log^2 (N)) and space complexity of O(N). The running time is dominated by the creation of the suffix array, so it can optimized further by using a faster suffix array construction algorithm, e.g. DC3.\n\n\n\n\n\n```\nfunction longestRepeatingSubarray(arr) {\n    const n = arr.length, suff = [...arr.keys()], rank = [...arr], lcp = Array(n), tempRank = Array(n);\n    tempRank[0] = 0;\n    for (let jump = 1; jump < n; jump <<= 1) {\n        const comp = (i, j) => rank[i] - rank[j] || (Math.max(i, j) + jump < n ? rank[i + jump] - rank[j + jump] : j - i);\n        suff.sort(comp);\n        for (let i = 1; i < n; i++) tempRank[i] = tempRank[i - 1] + !!comp(suff[i - 1], suff[i]);\n        for (let i = 0; i < n; i++) rank[suff[i]] = tempRank[i];\n    }\n    for (let i = 0, k = 0; i < n; i++)\n        if (rank[i] !== n - 1) {\n            for (let j = suff[rank[i] + 1]; arr[i + k] === arr[j + k];) ++k;\n            lcp[rank[i]] = k;\n            if (k) --k;\n        }\n    let best = 0;\n    for (let i = 1; i < n - 1; i++) if (lcp[i] > lcp[best]) best = i;\n    return arr.slice(suff[best], suff[best] + lcp[best]);\n}\n\nconsole.log(...longestRepeatingSubarray([1, 2, 5, 7, 1, 2, 4, 5]));\nconsole.log(...longestRepeatingSubarray([1, 2, 1, 2, 7, 0, -1, 7, 0, -1]));\nfunction make_case(){let $=[],u=200;for(let e=0;e<500;e++)$.push(u),$.push(u),u++;for(let s=1;s<101;s++)$.push(s);for(let h=0;h<1700;h++)$.push(u),$.push(u),u++;for(let p=1;p<101;p++)$.push(p);for(let t=0;t<47700;t++)$.push(u),$.push(u),u++;return $}\nconst arr = make_case();\nconsole.time(`${arr.length} elements`);\nconsole.log(...longestRepeatingSubarray(arr));\nconsole.timeEnd(`${arr.length} elements`);"}
{"questionId":"4efa601ea3b64d77a242e4b0a4b8d4e3","question":"Java upgrade with Caucho Hessian v3\/v4\n**Problem:**  \n\nWe are currently using Caucho Hessian 3.2.0 in combination with Java 8 and Java 11 applications. This works fine, but we want to migrate to Java 17 and we get following systen.out message:\n\n\n\n```\njava.lang.reflect.InaccessibleObjectException: Unable to make field private java.lang.String java.lang.StackTraceElement.classLoaderName accessible: module java.base does not \"opens java.lang\" to unnamed module @78e03bb5\n    at java.base\/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)\n    at java.base\/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)\n    at java.base\/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)\n    at java.base\/java.lang.reflect.Field.setAccessible(Field.java:172)\n    at com.caucho.hessian.io.JavaDeserializer.getFieldMap(JavaDeserializer.java:299)\n    at com.caucho.hessian.io.JavaDeserializer.<init>(JavaDeserializer.java:77)\n    at com.caucho.hessian.io.StackTraceElementDeserializer.<init>(StackTraceElementDeserializer.java:60)\n    at com.caucho.hessian.io.SerializerFactory.<clinit>(SerializerFactory.java:627)\n    at com.caucho.hessian.io.AbstractHessianOutput.findSerializerFactory(AbstractHessianOutput.java:95)\n    at com.caucho.hessian.io.Hessian2Output.writeObject(Hessian2Output.java:486)\n    at Main.serializeWithHessian(Main.java:23)\n    at Main.main(Main.java:13)\n\n```\n\nThere is a work-around by adding the following JVM param:\n\n\n\n```\n--add-opens java.base\/java.lang=ALL-UNNAMED\n\n```\n\nWe dont want to use this flag in all our productive applications, so we tried to upgrade Hessian to its newest version 4.0.66.\nBasicly this works for the most common use cases but doesn't work when we serialize data having same object instances multiple times.\n\n\nI created a small example application: <https:\/\/github.com\/MatWein\/hessian-test>\n\n\nThis application reads some java serialized test data from its classpath and tries to serialize and deserialize it with Hessian.\nIf you run this application using Hessian 3.2.0 it works for all Java versions (8, 11 and 17 if you set the JVM flag above) in run&debug modes.\nBut I cannot get it work with Hessian 4.0.66 because following error occures:\n\n\n\n```\nException in thread \"main\" com.caucho.hessian.io.HessianFieldException: de.test.TestData.field29: de.test.PaymentType cannot be assigned from null\n    at com.caucho.hessian.io.FieldDeserializer2FactoryUnsafe.logDeserializeError(FieldDeserializer2FactoryUnsafe.java:538)\n    at com.caucho.hessian.io.FieldDeserializer2FactoryUnsafe$ObjectFieldDeserializer.deserialize(FieldDeserializer2FactoryUnsafe.java:169)\n    at com.caucho.hessian.io.UnsafeDeserializer.readObject(UnsafeDeserializer.java:237)\n    at com.caucho.hessian.io.UnsafeDeserializer.readObject(UnsafeDeserializer.java:148)\n    at com.caucho.hessian.io.Hessian2Input.readObjectInstance(Hessian2Input.java:2202)\n    at com.caucho.hessian.io.Hessian2Input.readObject(Hessian2Input.java:2123)\n    at com.caucho.hessian.io.CollectionDeserializer.readLengthList(CollectionDeserializer.java:93)\n    at com.caucho.hessian.io.Hessian2Input.readObject(Hessian2Input.java:2050)\n    at Main.deserializeWithHessian(Main.java:32)\n    at Main.main(Main.java:15)\nCaused by: java.lang.IndexOutOfBoundsException: Index 13 out of bounds for length 13\n    at java.base\/jdk.internal.util.Preconditions.outOfBounds(Preconditions.java:64)\n    at java.base\/jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Preconditions.java:70)\n    at java.base\/jdk.internal.util.Preconditions.checkIndex(Preconditions.java:266)\n    at java.base\/java.util.Objects.checkIndex(Objects.java:361)\n    at java.base\/java.util.ArrayList.get(ArrayList.java:427)\n    at com.caucho.hessian.io.Hessian2Input.readObject(Hessian2Input.java:1810)\n    at com.caucho.hessian.io.FieldDeserializer2FactoryUnsafe$ObjectFieldDeserializer.deserialize(FieldDeserializer2FactoryUnsafe.java:165)\n    ... 8 more\n\n```\n\nATTENTION: We have a different runtime behaviour between running and debugging the app (dont know why, but Hessian seems to use Unsafe class, Weak&Soft references and so on).\n\n\nFor me, it looks like an internal bug in Hessian. It is using the class com.caucho.hessian.util.IdentityIntMap on serialization site, but when deserializing the client cannot find the object reference index.\n\n\n**Question:**\nIs there any way to get Hessian 4.0.66 working with Java 8, 11 and 17 (or at least with Java 11 and 17)?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"java"},"answer":"Finally, found the issue after lot of debugging. The issue is with Enum fields having `null` values.\n\n\nIn Hessian `3.x.x`, it is allowing de-serialization of `null` value by default for Enum type as checked.\n\n\nBut in Hessian `4.x.x`, they have changed the approach of evaluating enum values and didn't allow the de-serialization of `null` value for Enum type as checked.\n\n\n**Note:** I have used **4.0.66** Hessian. It will work with all versions of (JDK 8, 11, 17) (already tested)\n\n\n**Fix:**\n\n\n1. **First solution:** use `transient` keyword for `PaymentType`, `GenderType` & `ActivationState` as they are coming with `null` values and should be avoided being part of de-serialization.\n\n\n**Note:** Before putting `transient` for Enum fields, please make sure to select fields that you think are not needed as part of serialization and you are sure that data will be coming as null for those Enum fields when de-serialization will happen.\n\n\n\n```\npublic class TestData implements Serializable {\n    private static final long serialVersionUID = 8316361431446795589L;\n\n    private transient ActivationState field18;\n    private transient PaymentType field29;\n    private transient GenderType field34;\n\n\n    ... \/\/ getters and setters\n}\n\n```\n\n**Output with JDK 11:**\n\n\n\n```\n\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-11.0.19\/Contents\/Home\/bin\/java -javaagent:\/Applications\/IntelliJ IDEA CE.app\/Contents\/lib\/idea_rt.jar=59693:\/Applications\/IntelliJ IDEA CE.app\/Contents\/bin -Dfile.encoding=UTF-8 -classpath \/Users\/anisb\/hessian-test\/target\/classes:\/Users\/anisb\/.m2\/repository\/com\/caucho\/hessian\/4.0.66\/hessian-4.0.66.jar Main\n[de.test.TestData@12f41634, de.test.TestData@7a3d45bd, de.test.TestData@87f383f, de.test.TestData@e720b71, de.test.TestData@27f723, de.test.TestData@612fc6eb, de.test.TestData@4d3167f4, de.test.TestData@4923ab24, de.test.TestData@50d0686, de.test.TestData@1a3869f4, de.test.TestData@3aeaafa6, de.test.TestData@13c27452, de.test.TestData@1060b431, de.test.TestData@7b69c6ba, de.test.TestData@46daef40, de.test.TestData@a38d7a3, de.test.TestData@11758f2a, de.test.TestData@5ed828d, de.test.TestData@670b40af, de.test.TestData@44c8afef, de.test.TestData@262b2c86, de.test.TestData@ed9d034, de.test.TestData@612679d6, de.test.TestData@76a3e297, de.test.TestData@4eb7f003, de.test.TestData@371a67ec, de.test.TestData@63440df3, de.test.TestData@6121c9d6, de.test.TestData@77f99a05, de.test.TestData@5f3a4b84, de.test.TestData@eafc191]\n\nProcess finished with exit code 0\n\n```\n\n**Output with JDK 8:**\n\n\n\n```\n\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/bin\/java -javaagent:\/Applications\/IntelliJ IDEA CE.app\/Contents\/lib\/idea_rt.jar=59763:\/Applications\/IntelliJ IDEA CE.app\/Contents\/bin -Dfile.encoding=UTF-8 -classpath \/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/jre\/lib\/charsets.jar:\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/jre\/lib\/ext\/cldrdata.jar:\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/jre\/lib\/ext\/dnsns.jar:\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/jre\/lib\/ext\/jaccess.jar:\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/jre\/lib\/ext\/localedata.jar:\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/jre\/lib\/ext\/nashorn.jar:\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/jre\/lib\/ext\/sunec.jar:\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/jre\/lib\/ext\/sunjce_provider.jar:\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/jre\/lib\/ext\/sunpkcs11.jar:\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/jre\/lib\/ext\/zipfs.jar:\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/jre\/lib\/jce.jar:\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/jre\/lib\/jfr.jar:\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/jre\/lib\/jsse.jar:\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/jre\/lib\/management-agent.jar:\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/jre\/lib\/resources.jar:\/Users\/anisb\/Library\/Java\/JavaVirtualMachines\/corretto-1.8.0_362\/Contents\/Home\/jre\/lib\/rt.jar:\/Users\/anisb\/hessian-test\/target\/classes:\/Users\/anisb\/.m2\/repository\/com\/caucho\/hessian\/4.0.66\/hessian-4.0.66.jar Main\n[de.test.TestData@7fbe847c, de.test.TestData@442d9b6e, de.test.TestData@4c98385c, de.test.TestData@1c655221, de.test.TestData@1b9e1916, de.test.TestData@1edf1c96, de.test.TestData@368102c8, de.test.TestData@1963006a, de.test.TestData@4f8e5cde, de.test.TestData@5eb5c224, de.test.TestData@1b701da1, de.test.TestData@58d25a40, de.test.TestData@53e25b76, de.test.TestData@ee7d9f1, de.test.TestData@3f8f9dd6, de.test.TestData@45fe3ee3, de.test.TestData@6996db8, de.test.TestData@759ebb3d, de.test.TestData@73a8dfcc, de.test.TestData@7e774085, de.test.TestData@504bae78, de.test.TestData@ba8a1dc, de.test.TestData@ea30797, de.test.TestData@4cdf35a9, de.test.TestData@17d10166, de.test.TestData@484b61fc, de.test.TestData@726f3b58, de.test.TestData@3b764bce, de.test.TestData@aec6354, de.test.TestData@15615099, de.test.TestData@5fcfe4b2]\n\nProcess finished with exit code 0\n\n```\n\n**Output with JDK 17:**\n\n\n\n```\n\/Library\/Java\/JavaVirtualMachines\/zulu-17.jdk\/Contents\/Home\/bin\/java -javaagent:\/Applications\/IntelliJ IDEA CE.app\/Contents\/lib\/idea_rt.jar=59813:\/Applications\/IntelliJ IDEA CE.app\/Contents\/bin -Dfile.encoding=UTF-8 -classpath \/Users\/anisb\/hessian-test\/target\/classes:\/Users\/anisb\/.m2\/repository\/com\/caucho\/hessian\/4.0.66\/hessian-4.0.66.jar Main\n[de.test.TestData@12bb4df8, de.test.TestData@11028347, de.test.TestData@39a054a5, de.test.TestData@2f333739, de.test.TestData@1f89ab83, de.test.TestData@14899482, de.test.TestData@3caeaf62, de.test.TestData@7382f612, de.test.TestData@21588809, de.test.TestData@2a33fae0, de.test.TestData@7b3300e5, de.test.TestData@71bc1ae4, de.test.TestData@6ed3ef1, de.test.TestData@77468bd9, de.test.TestData@e73f9ac, de.test.TestData@383534aa, de.test.TestData@311d617d, de.test.TestData@7b1d7fff, de.test.TestData@61064425, de.test.TestData@16f65612, de.test.TestData@6bc168e5, de.test.TestData@3b192d32, de.test.TestData@2437c6dc, de.test.TestData@2e5c649, de.test.TestData@707f7052, de.test.TestData@299a06ac, de.test.TestData@7c53a9eb, de.test.TestData@1055e4af, de.test.TestData@2aae9190, de.test.TestData@ed17bee, de.test.TestData@136432db]\n\nProcess finished with exit code 0\n\n```\n\n2. **Second solution:** Make sure to send the non-null values in those appropriate Enum fields to avoid facing the below error.\n\n\n\n```\nException in thread \"main\" com.caucho.hessian.io.HessianFieldException: <package>.<Class>.<field-name>: <package>.<Enum> cannot be assigned from null\n\n```\n\n\n**Update:**\n\n\n3. **Third Solution:** I found a way to fix the issue without a code change. After adding `jackson-databind` dependency in pom.xml of the app, Hessian surprisingly started to deserialize with `null` values for Enum type.\n\n\nJust add this dependency in **POM**:\n\n\n\n```\n<dependency>\n    <groupId>com.fasterxml.jackson.core<\/groupId>\n    <artifactId>jackson-databind<\/artifactId>\n    <version>{{version}}<\/version>\n<\/dependency>\n\n```\n\n4. **Forth Solution:** You can use `@HessianUnshared` (that you suggested) annotation will fix the issue."}
{"questionId":"53f5d29f7cbb4e96ab24ba24cb964a04","question":"Angular detected that `HttpClient` is not configured to use `fetch` APIs. (Angular 17 - SSR)\nI got this error after updating to angular 17:\n\n\n\n> \n> NG02801: Angular detected that `HttpClient` is not configured to use `fetch` APIs. It's strongly recommended to enable `fetch` for applications that use Server-Side Rendering for better performance and compatibility. To enable `fetch`, add the `withFetch()` to the `provideHttpClient()` call at the root of the application.\n> \n> \n> \n\n\nI think after coding my services and importing `HttpClient` via inject functions it reached to this.","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"typescript"},"answer":"NG02801` is not an error but an warning that encourage the developers to enable the fetch implementation of the `HttpClient`.\n\n\nTo do this, just call `provideHttpClient(withFetch())` in your `app.config.ts`."}
{"questionId":"13141bd084364ff9b7730ac1ec5badf1","question":"How to tap() an iterator?\nWhat's the Rust equivalent of [tap()](https:\/\/ramdajs.com\/docs\/#tap)?\n\n\nIt calls a function on each item in the iterator like `map()` but instead of passing the value returned by the function, `tap()` returns the original item.\n\n\nFor example, I'd like to call `println!()` in the middle of my stream somehow:\n\n\n\n```\nfoo.into_iter()\n  .filter(|x| x == target)\n  .tap(|x| println!(\"{:?}\", x)) \/\/ <-- what goes here?\n  .map(|c| c.result)\n\n```\n\nCorrection:\n\n\n`tap()` calls the closure once on the entire iterator  \n\n`inspect()` calls the closure on each item in the iterator","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"rust"},"answer":"This is on `Iterator` as [`.inspect()`](https:\/\/doc.rust-lang.org\/std\/iter\/trait.Iterator.html#method.inspect):\n\n\n\n```\nfoo.into_iter()\n  .filter(|x| x == target)\n  .inspect(|x| println!(\"{:?}\", x))\n  .map(|c| c.result)"}
{"questionId":"e369c07ac7e74b9e9c50c8609b6925e7","question":"Find out libwebp version from PHP (Windows)\nDue to a recent vulnerability in libwebp before version 1.3.2 ([CVE-2023-4863](https:\/\/cve.mitre.org\/cgi-bin\/cvename.cgi?name=CVE-2023-4863)) I want to find out what libwebp version the Windows builds of PHP (downloadable here: <https:\/\/windows.php.net\/>) is using. Is there any way to find out the version?\n\n\nI've tried to find this version in the following places already:\n\n\n- phpinfo() - only gives the info whether webp support is enabled or not\n- PHP Changelogs\n- Texts in the DLLs\n- Texts in the debug symbols\n- The dependencies the PHP SDK downloads when building PHP\n\n\nBut no luck - I've found out the relevant constants from libwebp are `MUX_MAJ_VERSION`, `MUX_MIN_VERSION` and `MUX_REV_VERSION`, but those are not contained in the debug symbols.\n\n\nAny idea how to find the linked libwebp version or at least if the vulnerability is present in the used version?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"You'll get the info when taking a look on the article [Build your own PHP on Windows](https:\/\/wiki.php.net\/internals\/windows\/stepbystepbuild_sdk_2) for PHP >= 7.2 resp. its [older version](https:\/\/wiki.php.net\/internals\/windows\/stepbystepbuild) for PHP < 7.2 in [The PHP.net wiki](https:\/\/wiki.php.net\/), which is \"mainly used to track internal development of the PHP project\", and is linked to also from the PHP Source Github repository in the section on [Building PHP source code](https:\/\/github.com\/php\/php-src#building-php-source-code)\n\n\nSo, there you will find the section *Download prerequisites*, subsection *Get the libraries on which PHP depends*, referring to <https:\/\/windows.php.net\/downloads\/php-sdk\/deps\/>.\n\n\nCorrespondingly you'll find the following:\n\n\n\n\n\n| Directory | PHP branch | Employed libwebp |\n| --- | --- | --- |\n| [vs16](https:\/\/windows.php.net\/downloads\/php-sdk\/deps\/vs16\/) | master | 1.1.0 |\n| [vc15](https:\/\/windows.php.net\/downloads\/php-sdk\/deps\/vc15\/) | 7.2-7.4 | 1.0.0 & 1.1.0 |\n| [vc14](https:\/\/windows.php.net\/downloads\/php-sdk\/deps\/vc14\/) | 7.0-7.1 | 1.0.0 |"}
{"questionId":"c8b0c6aeccca4802b051eed1cdff19e2","question":"How do I get a &-conjoined regular expression to be considered as long a match as its constituents?\nYou can specify that a regular expression should only match if all its constituents match by using a [conjunction](https:\/\/docs.raku.org\/language\/regexes#Conjunction:_&&), `&&` or `&`:\n\n\n\n```\n[4] >  'ready' ~~ \/ r..dy & .ea..  \/\n\uff62ready\uff63\n[5] >  'roody' ~~ \/ r..dy & .ea..  \/\nNil\n[6] >  'peaty' ~~ \/ r..dy & .ea..  \/\nNil\n\n```\n\nWhen choosing which side of an alternation matches, Raku chooses the \"better\" side. Two rules matter here: an alternative with a longer declarative prefix is better than one with a shorter declarative prefix, and alternatives that were declared earlier are better than their siblings.\n\n\nThe documentation indicates that part of the reason to use `&` instead of `&&` is that `&` is [considered declarative](https:\/\/docs.raku.org\/language\/regexes#Conjunction:_&). If I understand longest token matching correctly, that's what I want.\n\n\nHowever, something surprising is happening if I use a conjunction as one branch of an alternation: it is never chosen if its alternatives aren't conjoined. It seems to be considered shorter than matches that seem to me to be of equal length.\n\n\nThis is annoying because I'm writing a parser in which it would be natural to say \"if the text matches both these rules, it in fact is considered an example of the conjoint rule\". The grammar consistently prefers to find the constituent rules to the conjoint rule.\n\n\nThe following REPL examples are all variations on a pattern in which we have an alternation with a conjoined alternative and a non-conjoined alternative and we want to know why the non-conjoined alternative is being chosen:\n\n\n\n```\n'froody' ~~ \/ froody & froody | froody \/\n\n```\n\n(The REPL interactions have diagnostic code and extra brackets to make sure I'm not running into precedence problems.)\n\n\nHere, the two sides of the alternation look to me like they should be considered the same length, so I expect it to choose the left branch. It chooses the right branch.\n\n\n\n```\n[7] >  'froody' ~~ \/ [ [ froody & froody ] { say 'left' } ] | [ froody { say 'right' } ] \/\nright\n\n```\n\nIf I reverse the order, it still chooses the non-conjoined branch.\n\n\n\n```\n[7] >  'froody' ~~ \/ [ froody { say 'left' } | [ [ froody & froody ] { say 'right' } ] ] \/\nleft\n\n```\n\nIf I artificially shorten the declarative portion of the non-conjoined branch by prepending `{}`, it chooses the left branch...\n\n\n\n```\n[7] >  'froody' ~~ \/ [ [ froody & froody ] { say 'left' } ] | [ {}froody { say 'right' } ] \/\nleft\n\n```\n\n... and also if we flip them. This suggests that the conjoined branch is considered to have a declarative length of 0.\n\n\n\n```\n[7] >  'froody' ~~ \/ [ {}froody { say 'left' } | [ [ froody & froody ] { say 'right' } ] ] \/\nleft\n\n```\n\nSo: How do I get the an alternation with a conjunction to be considered to contain as long a match as its non-conjoined alternative without dumb hacks? Is this an unreasonable thing to want? Is `&` not supposed to be able to do this?","questionMetadata":{"Type":"Debugging","Level":"Advanced","Tag":"perl"},"answer":"*Disclaimer: This answer is quite plausibly wrong. That said, it's carefully researched, and I think it's at least half right.*\n\n\n**TL;DR** Some regex constructs terminate a pattern's LDP (Longest Declarative Prefix). `&` and `[...]` both do so at the start of the sub-expressions they produce. Others don't, including `&&` (and assertions like `<froody>`), so use those instead.\n\n\n# Example, and discussion\n\n\nI begin with a variant of your first example. This code...\n\n\n\n```\nsay 'foo' ~~ \/  foo & foo {print 'L '}  |  foo {print 'R '}  \/\n\n```\n\n...displays `R \uff62foo\uff63`. In other words, this has exactly the same behavior as your first example, choosing the RHS branch instead of the desired\/expected left branch.\n\n\nOf critical importance, I did not introduce `[...]` sub grouping, so have avoided quietly introducing double trouble. (In my tests, both `&` *and* `[...]` (and `(...)`) terminate the LDP at their start.)\n\n\nNow we can change the `&` to `&&` ...\n\n\n\n```\nsay 'foo' ~~ \/  foo && foo {print 'L '}  |  foo {print 'R '}  \/\n\n```\n\n...and get the desired result: `L \uff62foo\uff63`.\n\n\n# \"declarative\"\n\n\n\"declarative\" generically means [\"expresses the logic of a computation without describing its control flow\"](https:\/\/en.wikipedia.org\/wiki\/Declarative_programming). There are two specific meanings of it within Raku's current regex feature set that are relevant here:\n\n\n1. What almost all uses of the word \"declarative\" refer to, namely the \"longest declarative prefixes\" related to `|` alternations.\n2. What just one use of the word \"declarative\" refers to, namely the (lack of a specified) order in which the LHS and RHS of an `&` expression are to be processed.\n\n\n# From the docs\n\n\n[Speculative design doc S05](https:\/\/github.com\/Raku\/old-design-docs\/blob\/a4c36c683dafdad0bb55996e78b66a1f48fc703c\/S05-regex.pod#L57) uses the word \"declarative\" a *lot*:\n\n\n\n> \n> While the syntax of `|` does not change, the default semantics do change slightly. We are attempting to concoct a pleasing mixture of declarative and procedural matching so that we can have the best of both. In short, you need not write your own tokenizer for a grammar because [Raku] will write one for you. See the section below on \"Longest-token matching\".\n> \n> \n> ...\n> \n> \n> As with the disjunctions `|` and `||`, conjunctions come in both `&` and `&&` forms. The `&` form is considered declarative rather than procedural; it allows the compiler and\/or the run-time system to decide which parts to evaluate first, and it is erroneous to assume either order happens consistently.\n> \n> \n> \n\n\n\n\n---\n\n\nLikewise, in the Raku doc, specifically [the regex doc](https:\/\/docs.raku.org\/language\/regexes), we find:\n\n\n\n> \n> Briefly, what `|` does is this ... select the branch which has the longest **declarative** prefix. ... For more details, see the LTM strategy.\n> \n> \n> ...\n> \n> \n> `&` (unlike `&&`) is considered **declarative**, and notionally all the segments can be evaluated in parallel, or in any order the compiler chooses.\n> \n> \n> \n\n\nAgain, in all cases the term \"declarative\" is appropriate. But what it means in the phrase \"longest declarative prefix\" has absolutely nothing to do with what it means in \"The `&` form is considered declarative\".\n\n\n# The LHS of `&&` can specify a declarative prefix; why not `&`?\n\n\nAs explained in the previous section, the word \"declarative\" in relation to `&` should not be read to imply that it necessarily derives a declarative prefix (or pair of them?) out of its LHS (and RHS?). Furthermore, `&` clearly has a zero length prefix in current Rakudo. And there's nothing I've found in the speculation docs and IRC discussions in 2005 onward to suggest any intent for `&` to contribute to declaring a declarative prefix in the LTM sense.\n\n\nBut why not sweep all this confusion away by making `&` do at least as well as `&&` in playing nicely in the LTM game?\n\n\nMy current thinking is that that's because `&` wouldn't then be declarative in the sense of always leaving it up to the compiler in which order to attempt matching of its LHS and RHS. In fact it would *never* leave it up to the compiler because Raku(do) couldn't know whether an `&` expression would end up appearing dynamically in the context of an LTM alternation, and couldn't sometimes do it in whatever order it prefers when it sees a `&` *lexically*, and other times try the LHS first because it sees it *dynamically*, because that would mean that refactoring could alter behavior.\n\n\nSo `&` would have to always do exactly the same thing as `&&`. But if that's the case, why have it available at all? One reason for having it would be because it's nice to have the option of declaring an `&` pair declaratively, i.e. giving the compiler the freedom to decide which order to match them in. But in that case the LTM prefix that `&` implies has to always be zero length."}
{"questionId":"c1cd775540074e9a941811be184ba012","question":"Android Jetpack Glance 1.0.0 : problems updating widget\nI am implementing my first Jetpack Glance powered app widget for a simple todo list app I'm working on. I'm following the guide at <https:\/\/developer.android.com\/jetpack\/compose\/glance> and everything is fine until I get to the point where I need to update my widget to match the data updates that occured from within my application.\n\n\nFrom my understanding of [Manage and update GlanceAppWidget](https:\/\/developer.android.com\/jetpack\/compose\/glance\/glance-app-widget), one can trigger the widget recomposition by calling either of `update`, `updateIf` or `updateAll` methods on a widget instance from the application code itself. Specifically, calls to those functions should trigger the `GlanceAppWidget.provideGlance(context: Context, id: GlanceId)` method, which is responsible for fetching any required data and providing the widget content, as described on this snippet :\n\n\n\n```\nclass MyAppWidget : GlanceAppWidget() {\n\n    override suspend fun provideGlance(context: Context, id: GlanceId) {\n\n        \/\/ In this method, load data needed to render the AppWidget.\n        \/\/ Use `withContext` to switch to another thread for long running\n        \/\/ operations.\n\n        provideContent {\n            \/\/ create your AppWidget here\n            Text(\"Hello World\")\n        }\n    }\n}\n\n```\n\nBut in my case, it is not always working. Here is what I observed after a few tries :\n\n\n- It always works when first adding the widget to my dashboard. Data is always fresh there.\n- It always works the first time the update method is called from the application itself (I'm using `updateAll`). The widget is updated and shows up-to-date data,\n- Then it does not work if I call the update method **too soon after the last call**. In this case the `provideGlance` method is not triggered at all.\n\n\nI then looked into the `GlanceAppWidget` source code and noticed it relies on an `AppWidgetSession` class :\n\n\n\n```\n    \/**\n     * Internal version of [update], to be used by the broadcast receiver directly.\n     *\/\n    internal suspend fun update(\n        context: Context,\n        appWidgetId: Int,\n        options: Bundle? = null,\n    ) {\n        Tracing.beginGlanceAppWidgetUpdate()\n        val glanceId = AppWidgetId(appWidgetId)\n        if (!sessionManager.isSessionRunning(context, glanceId.toSessionKey())) {\n            sessionManager.startSession(context, AppWidgetSession(this, glanceId, options))\n        } else {\n            val session = sessionManager.getSession(glanceId.toSessionKey()) as AppWidgetSession\n            session.updateGlance()\n        }\n    }\n\n```\n\nIf a session is running, it will be used to trigger the glance widget update. Otherwise it will be started and used for the same purpose.\n\n\nI noticed that my problem occurs if and only if only a session is running, which would explain why it doesn't occur if I give it enough time between updates call : there are no more running sessions (whatever it means exactly) and a new one needs to be created.\n\n\nI tried digging further in Glance internals to understand why it does not work when using a running session, with no success so far. The only thing I noticed and thought is weird is that at some point the `AppWidgetSession` internaly uses a class called `GlanceStateDefinition`, that I didn't see mentionned on the official Android Glance guide but that a few other guides on the web use to implement a Glance widget (Though using alpha or beta versions of Jetpack Glance libs).\n\n\nDoes anyone has a clue on why it behaves like this ? Here is some more information, please let me know if you need something else. Thanks a lot !\n\n\n- I use version 1.0.0 of the `androidx.glance:glance-appwidget` lib, released a few days ago,\n- I did not forget to add the `<receiver>` tag in my `AndroidManifest.xml`, as well as the required `android.appwidget.provider` xml file in my res\/xml folder. I think I've correctly done everything that is mentioned on the [Glance setup](https:\/\/developer.android.com\/jetpack\/compose\/glance\/setup) page given I have no problem adding the widget on my home screen in the first place,\n- I use `SQLiteOpenHelper` under the hood to access my data, with a few helper classes of my creation on top of it, not using `Room` or any other ORM lib (I want to keep my application simple for now).\n- Here is what my `provideGlance` method looks like :\n\n\n\n```\n    override suspend fun provideGlance(context: Context, id: GlanceId) {\n\n        val todoDbHelper = TodoDbHelper(context)\n        val dbHelper = DbHelper(todoDbHelper.readableDatabase)\n        val todoDao = TodoDao(dbHelper)\n        val todos = todoDao.findAll()\n\n        provideContent {\n            TodoAppWidgetContent(todos)\n        }\n    }\n\n```\n\nThe `todoDao.findAll()` returns a plain List (it relies on a helper function that runs on `Dispatchers.IO` so that the main thread is not blocked)\n\n\n- I also don't use `Hilt` or any other DI lib.","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"kotlin"},"answer":"I spent a few more hours searching and found my answer :\n\n\nIt turns out I was wrong assuming that the `provideGlance` method, or even the `provideContent` should be triggered again when calling any of the aforementioned `update` methods. You can fetch some initialization data in there but you cannot rely on it to keep your widget updated, it is only called when no *Glance session* is currently running (When first adding the widget \/ when time has passed since adding it). Instead you can (\/should) rely on the **state** of your Glance Widget.\n\n\nI think this concept of **Glance state** is very poorly explained in the guide, to say the least, so I'll give it a short try hoping to help people having the same problem as I did :\n\n\n- When a **Glance session** is running, everytime any of the `GlanceAppWidget.update` methods are called from within the app, Glance will *recompose* your widget content using a fresh copy of the **Glance state**\n- This **Glance state** must be defined beforehand, by providing a `GlanceStateDefinition` instance to your class extending **GlanceAppWidget**. This instance is responsible for providing the type (class) of your state, as well as the `DataStore` that Glance will use internally to get an updated version of the **Glance state**\n\n\nA `DataStore` is an `interface` that provides two abstracts methods for getting and updating data (More info here : [DataStore](https:\/\/developer.android.com\/topic\/libraries\/architecture\/datastore)). There are 2 implementations provided, *Preferences DataStore* and *Proto DataStore*. The first one is intended to replace `SharedPreferences` as a mean to store key-value pairs, and the second can be used to store typed objects.\n\n\nMost of the Glance tutorials I found on the web make use of the *Preferences DataStore* in their examples, but for my purpose I chose to implement my own version of a `DataStore` as a *readonly proxy* to my Dao object, as follows :\n\n\n\n```\nclass TodoDataStore(private val context: Context): DataStore<List<TodoListData>> {\n    override val data: Flow<List<TodoListData>>\n        get() {\n            val todoDbHelper = TodoDbHelper(context)\n            val dbHelper = DbHelper(todoDbHelper.readableDatabase)\n            val todoDao = TodoDao(dbHelper)\n            return flow { emit(todoDao.findAll()) }\n        }\n\n    override suspend fun updateData(transform: suspend (t: List<TodoListData>) -> List<TodoListData>): List<TodoListData> {\n        throw NotImplementedError(\"Not implemented in Todo Data Store\")\n    }\n}\n\n```\n\nThe state definition in my class extending `GlanceAppWidget` looks like this :\n\n\n\n```\n    override val stateDefinition: GlanceStateDefinition<List<TodoListData>>\n        get() = object: GlanceStateDefinition<List<TodoListData>> {\n            override suspend fun getDataStore(\n                context: Context,\n                fileKey: String\n            ): DataStore<List<TodoListData>> {\n                return TodoDataStore(context)\n            }\n\n            override fun getLocation(context: Context, fileKey: String): File {\n                throw NotImplementedError(\"Not implemented for Todo App Widget State Definition\")\n            }\n        }\n\n```\n\nMeaning I can now rely on the *state* of my Glance Widget instead of using my Dao class directly, by using the `currentState()` method :\n\n\n\n```\n    override suspend fun provideGlance(context: Context, id: GlanceId) {\n        provideContent {\n            TodoAppWidgetContent(currentState())\n        }\n    }\n\n```\n\nIt works like a charm now !\n\n\nI intend to fill an issue regarding the lack of documentation regarding the **Glance state** and its relation to the concept of **Datastore** in the Glance guide."}
{"questionId":"5caa98b912c84a51829f2da1b2b2d9cd","question":"Use of template keyword before dependent template name\nConsidering the following code example, I would expect to have to use the `template` keyword here to guide the compiler to treat variable `v` as a template. However, MSVC rejects the use of the `template` keyword, whereas Clang and GCC actually require it. Which specific rule in the C++20 standard either mandates or forbids the use of the `template` keyword in this case?\n\n\n\n```\nstruct s {\n    template<typename...>\n    static constexpr auto v = true;\n};\n\/\/ all ok\nstatic_assert([](auto x){ return decltype(x)::template v<>; }(s{}));\n\/\/ clang ok, gcc ok, msvc nope\nstatic_assert([](auto x){ return x.template v<>; }(s{}));\n\/\/ clang nope, gcc nope, msvc ok\nstatic_assert([](auto x){ return x.v<>; }(s{}));\n\n```\n\n[Live example](https:\/\/godbolt.org\/z\/zxb3Eb4v7)\n\n\n\n\n---\n\n\nThe error message from Clang:\n\n\n\n```\n<source>:10:36: error: use 'template' keyword to treat 'v'\nas a dependent template name\n   10 | static_assert([](auto x){ return x.v<>; }(s{}));\n      |                                    ^\n      |                                    template \n\n```\n\nThe error message from MSVC:\n\n\n\n```\n<source>(8): error C2187: syntax error: 'template' was unexpected here\n<source>(8): note: see reference to function template instantiation\n'auto <lambda_2>::operator ()<s>(_T1) const' being compiled\n        with\n        [\n            _T1=s\n        ]","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"cpp"},"answer":"In short, `x` and `decltype(x)` are dependent, and `template` is allowed and necessary in all cases. There is an MSVC compiler bug.\n\n\n### C++ standard wording\n\n\nAs for dependent types, including `decltype`:\n\n\n\n> \n> A type is dependent if it is:\n> \n> \n> - a template parameter,\n> - [...]\n> - denoted by `decltype(expression)`, where `expression` is type-dependent.\n> \n> \n> \n\n\n- [[temp.dep.type] p7.10](https:\/\/eel.is\/c++draft\/temp.dep.type#7.10)\n\n\nFurthermore, `x` is type-dependent because (see [[temp.dep.expr] p3.1](https:\/\/eel.is\/c++draft\/temp.dep.expr#3.1)) it is:\n\n\n\n> \n> associated by name lookup with one or more declarations declared with a dependent type\n> \n> \n> \n\n\nThe `auto` parameter for the generic lambda makes the call operator a function template, so naturally, `x` is associated with a template parameter, which is dependent.\nTherefore `template` is necessary for `decltype(x)::template v<>`. All compilers agree and this is correct.\n\n\nIn the statement\n\n\n\n> \n> \n> ```\n> return x.template v<>;\n> \n> ```\n> \n> \n\n\n`x` is type-dependent because it is associated by name lookup with an implicit template parameter of the generic lambda's call operator. Therefore, `template` is necessary.\n\n\n\n\n---\n\n\n*Note: Besides the standardese, it's intuitively necessary because `x.v <` could be interpreted as *\"`x.v` is less than ...\"* if it isn't known that `v` is a template.\nThis is formally stated in [[temp.names] p3](https:\/\/eel.is\/c++draft\/temp.names#3).*\n\n\n*See also: [Where and why do I have to put the \"template\" and \"typename\" keywords?](https:\/\/stackoverflow.com\/q\/610245\/5740428)*\n\n\n### MSVC compiler bug\n\n\nThe fact that MSVC doesn't allow `.template v<>` is obviously a bug. It's worth noting that MSVC does accept:\n\n\n\n```\n[](s x){ return x.template v<>; }(s{})\n\n```\n\nHowever, using `auto` instead of `s` as the parameter type leads to a compiler error. This makes no sense at all, since `template` and `typename` are optional and adding them unnecessarily should never cause failure.\n\n\n\n\n---\n\n\n*Note: I was unable to find an existing bug report in the [Microsoft Developer Community](https:\/\/developercommunity.visualstudio.com\/cpp?q=generic+lambda+dependent+type), so maybe this hasn't been reported yet. You may want to report this bug.*"}
{"questionId":"00dbc9773c4f4879ba94c2836392ed67","question":"ChromeDriver Version 117+ Forces \"Save As\" Dialog - How to Bypass? (Selenium\/Java)\nI have been working on automating file downloads using Selenium WebDriver with ChromeDriver in Java. My code was working perfectly until I updated to ChromeDriver version 117+, the code worked fine till Chrome 116.0.5845.141, problem seems to start in Chrome 116.0.5845.188. Now, it seems that the browser is forcing the \"Save As\" dialog box to appear, even when I have set Chrome preferences to avoid it.\n\n\nHere is a snippet of my Java code:\n\n\n\n```\nimport org.openqa.selenium.By;\nimport org.openqa.selenium.WebDriver;\nimport org.openqa.selenium.chrome.ChromeDriver;\nimport org.openqa.selenium.chrome.ChromeOptions;\n\nimport java.util.HashMap;\n\npublic class FileDownloadHeadless {\n    public static void main(String[] args) throws InterruptedException {\n\n        System.setProperty(\"webdriver.chrome.driver\", \"path\/to\/chromedriver.exe\");\n\n        ChromeOptions options = new ChromeOptions();\n        options.setCapability(\"os\", \"Windows\");\n        options.setCapability(\"os_version\", \"10\");\n        options.setCapability(CapabilityType.ACCEPT_SSL_CERTS, true);\n        options.setCapability(CapabilityType.ForSeleniumServer.ENSURING_CLEAN_SESSION, true);\n        options.setCapability(\"chrome.switches\", Arrays.asList(\"--incognito\"));\n        options.setCapability(ChromeOptions.CAPABILITY, options);\n        options.addArguments(\"--headless\");\n        options.addArguments(\"--disable-gpu\");\n\n        HashMap<String, Object> chromePrefs = new HashMap<>();\n        chromePrefs.put(\"profile.default_content_settings.popups\", 0);\n        chromePrefs.put(\"download.default_directory\", \"C:\\\\local_files\");\n        chromePrefs.put(\"download.prompt_for_download\", false);\n        chromePrefs.put(\"profile.content_settings.exceptions.automatic_downloads.*.setting\", 1);\n        chromePrefs.put(\"profile.default_content_setting_values.automatic_downloads\", 1);\n        options.setExperimentalOption(\"prefs\", chromePrefs);\n\n        WebDriver driver = new ChromeDriver(options);\n\n        driver.get(\"http:\/\/my_site.com\/download\");\n\n        driver.findElement(By.id(\"id_button_download\")).click();\n\n        Thread.sleep(5000);\n\n        driver.quit();\n    }\n}\n\n```\n\nDespite these settings, the \"Save As\" dialog box still appears, and it's disrupting the automation flow. I've tried various combinations of Chrome preferences, but none seem to bypass the new behavior introduced in version 117.\n\n\nHas anyone else encountered this issue with ChromeDriver version 117+ or higher? If so, how have you managed to work around this update? Any insights would be greatly appreciated.\n\n\nRemoving the \"--incognito\" mode as suggested by @NhanTT, did work! but I wonder why and how the mode actually affects file download options. If anyone has another solution that works around the problem while maintaining incognito mode, I would appreciate it.\n\n\nThank you for your time and assistance.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"java"},"answer":"I just remove --incognito\nIt working"}
{"questionId":"e411ffeade4f4147ac9944e39499fcd1","question":"Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas\nI have the below code which for instance work as excepted but won't work in the Future :\n\n\n\n```\n    total.name = 'New_Row'\n    total_df = total.to_frame().T\n    total_df.at['New_Row', 'CURRENCY'] = ''\n    total_df.at['New_Row', 'MANDATE'] = Portfolio\n    total_df.at['New_Row', 'COMPOSITE'] = 'GRAND TOTAL'\n    total_df.set_index('COMPOSITE',inplace=True)\n\n```\n\nsince an error is thrown in\n\n\n\n```\nFutureWarning: Setting an item of incompatible dtype is deprecated and \nwill raise in a future error of pandas. Value 'GRAND TOTAL' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  total_df.at['New_Row', 'COMPOSITE'] = 'GRAND TOTAL'\n\n```\n\nHow to fix this ?\n\n\nvariable total is :\n\n\n\n```\nCURRENCY\nMANDATE             Mandate_Test\nUSD AMOUNT          123\nLOCAL AMOUNT        12\nBeg. Mkt            123\nEnd. Mkt            456\nName: New_Row, dtype: object","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"I think it is bug -  [BUG: incompatible dtype when creating string column with loc #55025](https:\/\/github.com\/pandas-dev\/pandas\/issues\/55025) . In next version of pandas should be solved."}
{"questionId":"5af702b95b1c447b9cbabe9e1254dd42","question":"How do you create a callable variable to call a class method with arguments?\nI'm trying to create a callable variable for a class method.\n\n\n\n```\nclass Person {\n    method walk(Str $direction) {\n        say \"Walking $direction\";\n    }\n}\n\n```\n\nI can create a callable variable for the method 'walk' that works as expected.\n\n\n\n```\nmy $person = Person.new;\nmy $callable = $person.^find_method('walk');\n$person.$callable('up'); # OUTPUT: \"Walking up\"\n\n```\n\nNow I want to create a callable that will call method 'walk' with the parameter 'up'.\n\n\n\n```\nmy $callable = $person.^find_method('walk').assuming('up');\n$person.$callable(); \n\n    # Type check failed in binding to parameter '$direction'; expected Str but got Person (Person.new)\n    #   in sub __PRIMED_ANON at EVAL_7 line 1\n    #   in block <unit> at <unknown file> line 1\n\n```\n\n`$person.$callable` (without the parentheses) gives the same error\n\n\nI tried calling it 'directly', but got a different error.\n\n\n\n```\n$person.^find_method('walk')('up')\n    # Too few positionals passed; expected 2 arguments but got 1\n    #   in method walk at <unknown file> line 2\n    #   in block <unit> at <unknown file> line 1\n\n```\n\nIs this possible?","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"perl"},"answer":"Yes, this is possible. The missing piece is that methods implicitly receive their invocant as their first positional parameter (that is, the signature of `Person`'s `walk` method is really `method walk(Person: Str $direction)`) and receive `self` as their first positional argument.\n\n\nThis means that you need to use `assuming` to supply the *second* positional argument instead of the first. Here's how that'd look:\n\n\n\n```\nclass Person {\n    method walk(Str $direction) {\n        say \"Walking $direction\";\n    }\n}\nmy $person = Person.new;\nmy $callable = $person.^find_method('walk').assuming(*, 'up');\n$person.$callable();  # # OUTPUT: \"Walking up\"\n\n```\n\nAs another option, instead of using `*` for the first positional parameter, you could use `$person`; if you did that, `$callable()` would return `\"Walking up\"` without needing to be invoked on a `Person`."}
{"questionId":"8e19771f9f1c4367931c779e22a3cabf","question":"How do I find all possible values of an enum in Raku?\nenum Direction <north east south west>;\nfor north, east, south, west -> $dir {\n    say $dir;\n    ...\n}\n\n```\n\nI don't want to repeat the list of Direction values here. How can I programmatically get this list?\n\n\nI haven't found anything in the documentation. Closest is `.enums`, but that returns a Map of (string) key to (integer) value, not enum values.","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"perl"},"answer":"If the order doesn't matter\n\n\n\n```\nenum Direction <north east south west>;\nDirection.pick(*).raku\n\n```\n\nor\n\n\n\n```\nenum Direction <north east south west>;\nDirection::.values.raku\n\n```\n\nSorted by value\n\n\n\n```\nenum Direction \u00ab:2north :1east :south(10) :west(9)\u00bb;\nDirection.pick(*).sort.raku\n\n```\n\nSorted by definition;\nif you know first and last element\n\n\n\n```\nenum Direction \u00ab:2north :1east :10south :9west\u00bb;\n(north, *.succ ... west).raku;\n\n```\n\nif you don't\n\n\n\n```\nenum Direction \u00ab:2north :1east :10south :9west\u00bb;\n(Direction.pick(*).first({ $_ === .pred }), *.succ ...^ * === *).raku"}
{"questionId":"b2f7b5817a2242f38911cc5041303703","question":"\"go mod tidy\" will auto upgrade go version in go.mod?\nPreviously the project was using `go 1.20` in `go.mod`. I've upgrade my go version to `1.21.0` on local linux machine.\n\n\nWhen I run `go mod tidy` for the project, it upgraded `go.mod` to use `go 1.21.0` automatically.\n\n\nIf this is the expected behavior, is there a way to disable this behaviour, aka. don't auto update the go version in `go.mod` when run `go mod tidy`? Because others might didn't upgrade locally yet.","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"other"},"answer":"The `go mod tidy` command does not (currently) alter the `go` directive in `go.mod` based only on the local toolchain version. It sets it based on dependencies, and does not need to track patch versions.\n\n\nIf you have a dependency that requires a minimum of `go1.21`, then your module *requires* `go1.21`, forcing the `go` tool to update the `go` directive in `go.mod` accordingly. It would not be of any use to have your module declare a version prior to `go1.21` when `go1.21` is actually required to build the module."}
{"questionId":"f90bc743401b42b7a6bb34d4cdefd31c","question":"Xcode attempted to install a beta profile without the proper entitlement\nXcode 15.0.1 15A507 with iPhone 14 Pro MQ0E3LL\/A on iOS 17.0.3 attached via USB Lightning trying to build and install iOS app on phone that had previously had no issues building.\n\n\nBuilding app to iPhone from Xcode gives error: \"xcode failed to installed embedded profile.\nAttempted to install a beta profile without the proper entitlement.\"\n\n\nSame with my iPad Pro over USB C, same error.\n\n\nHave tried cleaning build folder, deleting derived data, deleting provisioning profiles and downloading again. Tried removing iPhone from Devices and Simulators and adding back, trusting again. Recreated a new provisioning profile and removed old, installed new, and same error. Certificates, Profiles and Agreements are all up to date and not expired.\n\n\n\n```\nFull Xcode error is:\nUnable to Install \u201cAPPNAME\u201d\nDomain: IXUserPresentableErrorDomain\nCode: 14\nFailure Reason: This app cannot be installed because its integrity could not be verified.\nRecovery Suggestion: Failed to install embedded profile for BUNDLE ID : 0xe800801f \n(Attempted to install a Beta profile without the proper entitlement.) \nVerify that the Developer App certificate for your account is trusted on your device. \nOpen Settings on the device and navigate to General -> VPN & Device Management, then select \nyour Developer App certificate to trust it.\nUser Info: {\nDVTErrorCreationDateKey = \"2023-10-25 12:37:01 +0000\";\nIDERunOperationFailingWorker = IDEInstallCoreDeviceWorker;\nDomain: MIInstallerErrorDomain\nCode: 13\nUser Info: {\nFunctionName = \"-[MIInstallableBundle _installEmbeddedProfilesWithError:]\";\nLegacyErrorString = ApplicationVerificationFailed;\nLibMISErrorNumber = \"-402620385\";\nSourceFileLine = 200;\n\n```\n\nIn Xcode, going to Devices and Simulators and selecting Show Provisioning Profiles on any connected devices doesn't show any in the list. With \"no provisioning profiles installed.\" Choosing the + and trying to add my downloaded profile gives error \"failed to install one or more provisioning profiles on this device. Ensure the provisioning profile is configured for this device.\" Although, i never had to go in here and manually add profiles before.\n\n\nOn any device, going to Settings and navigating to General -> VPN & Device Management shows nothing.as the error suggests to select your Developer App certificate to trust it. Never had to that before either.","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"swift"},"answer":"My solution was to go to developer.apple.com \/ Devices and add my devices using Device ID (UDID). This was obtained by plugging device into a mac, selecting in finder, and clicking the top device banner to copy the UDID. I then had to create a new ad hoc profile and add the devices to the profile. Finally, I had to download and select the new profile in Targets \/ Signing and Capabilities.\n\n\nThis is strange because I never had to do this before when I was using my regular \"app store\" profile. So, I am not 100% sure this is the answer. Shouldn't a provisioning profile of \"app store\" type be able to run on connected devices without a specific separate ad hoc profile?"}
{"questionId":"67f01f14fdd24dfaae8d47cc92736e64","question":"Django built in Logout view `Method Not Allowed (GET): \/users\/logout\/\nMethod Not Allowed (GET): \/users\/logout\/\nMethod Not Allowed: \/users\/logout\/\n[10\/Dec\/2023 12:46:21] \"GET \/users\/logout\/ HTTP\/1.1\" 405 0\n\n```\n\nThis is happening when I went to url <http:\/\/127.0.0.1:8000\/users\/logout\/>\n\n\n`urls.py:`\n\n\n\n```\nfrom django.contrib.auth import views as auth_views\n\nurlpatterns = [\n    ...other urls...\n    path('users\/logout\/', auth_views.LogoutView.as_view(), name='logout'),\n]\n\n```\n\nI am expecting user to logout","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"Since [django-5](\/questions\/tagged\/django-5 \"show questions tagged 'django-5'\"), you need to do this through a **POST request**, since it has side-effects. The fact that it worked with a GET request was (likely) a violation of the HTTP protocol: it made it possible for certain scripts to log out users, without the user wanting to. So a POST request also protects against [*cross-site request forgery (CSRF)*\u00a0[wiki]](https:\/\/en.wikipedia.org\/wiki\/Cross-site_request_forgery).\n\n\nSo in the template, work with a mini-form:\n\n\n\n```\n<form method=\"post\" action=\"{% url 'logout' %}\">\n    {% csrf_token %}\n    <button type=\"submit\">logout<\/button>\n<\/form>"}
{"questionId":"4e129fad706847c38dd37e3b7c23d5ea","question":"QML plugin with static build\nI have Qt compiled\u00a0static, but when I run my application I get this error:\n\n\n\n```\nqrc:\/files\/particles\/particles.qml:2:1: module \"QtQuick.Particles\" plugin \"particlesplugin\" not found\u00a0\n\u00a0 \u00a0 \u00a0import QtQuick.Particles 2.0\u00a0\n\u00a0 \u00a0 \u00a0^\n\n```\n\nMy imports related to QML are these:\n\n\n\n```\nimport QtQuick 2.5\nimport QtQuick.Particles 2.0\n\n```\n\nI tried to follow the documentation:\n\n\n\n```\n#include <QtPlugin>\n\nint main(int argc, char *argv[])\n{\n\u00a0 \u00a0 QApplication a(argc, argv); \u00a0\u00a0 \u00a0\n    Q_IMPORT_PLUGIN(QtQuick2Plugin)\n \u00a0 \u00a0Q_IMPORT_PLUGIN(QtQuick2ParticlesPlugin) \/\/ <--- cause error\n  \u00a0 Q_IMPORT_PLUGIN(QtQmlMetaPlugin)\n\u00a0 \u00a0 Q_IMPORT_PLUGIN(QtQmlPlugin)\n\u00a0 \u00a0 Q_IMPORT_PLUGIN(QtQmlModelsPlugin)\n\u00a0 \u00a0 Q_IMPORT_PLUGIN(QtQmlWorkerScriptPlugin)\n\n```\n\nBut I'm getting this compilation error:\n\n\n\n```\n1>main.obj : error LNK2019: unresolved external symbol \"struct QStaticPlugin const __cdecl qt_static_plugin_QtQuick2ParticlesPlugin(void)\" (?qt_static_plugin_QtQuick2ParticlesPlugin@@YA?BUQStaticPlugin@@XZ) referenced in function main\n\n```\n\nI searched the source for proper way to import this plugin and I have found it at **C:\\Qt\\6.6.0\\_static\\qtdeclarative\\src\\particles**\n\n\nin the file **particlesplugin\\_init.cpp**:\n\n\n\n```\n#include <QtPlugin>\nQ_IMPORT_PLUGIN(QtQuick2ParticlesPlugin)\n\n```\n\nWhy is the compilation throwing that error?\n\n\nI'm compiling using **Visual Studio 2022**.\n\n\nI tested adding `#pragma comment(lib, \"particlesplugin.lib\")` into the `int main...` code above\n\n\nand the compiler complained about not finding the lib.\n\n\nI have them added to my project properties:\n\n\n\n```\nC\/C++ -> General -> Additional Include\u00a0Directories -> C:\\Qt\\6.6.0_static\\qml\\QtQuick\\Particles\n\nLinker -> Input -> particlesplugin.lib\n\n```\n\nWhen recompiled, now I'm getting a different error:\n\n\n\n```\n1>particlesplugin.lib(particlesplugin_QtQuick2ParticlesPlugin.cpp.obj) : error LNK2019: unresolved external symbol \"void __cdecl qml_register_types_QtQuick_Particles(void)\" (?qml_register_types_QtQuick_Particles@@YAXXZ) referenced in function \"public: static __cdecl `public: static void (__cdecl*__cdecl QtPrivate::QMetaTypeForType<class QtQuick2ParticlesPlugin>::getDefaultCtr(void))(class QtPrivate::QMetaTypeInterface const *,void *)'::`5'::<lambda_1>::<lambda_invoker_cdecl>(class QtPrivate::QMetaTypeInterface const *,void *)\" (?<lambda_invoker_cdecl>@<lambda_1>@?4??getDefaultCtr@?$QMetaTypeForType@VQtQuick2ParticlesPlugin@@@QtPrivate@@SAP6AXPEBVQMetaTypeInterface@4@PEAX@ZXZ@SA@01@Z)\n\n```\n\nI also get this external symbol error for **every** other **QtQuick** plugin that I try to import using `Q_IMPORT_PLUGIN` and not only the **particlesplugin**.\n\n\nI can't find anything about this in the documentation, what's going on?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"cpp"},"answer":"This issue is a bug, follow the bugreport:\n<https:\/\/bugreports.qt.io\/browse\/QTVSADDINBUG-1173>\n\n\n\n> \n> Qt\/MSBuild is not providing all the necessary link dependencies.\n> \n> \n>"}
{"questionId":"eccb7f6b62de40bea3a72ae466f6a05e","question":"Can't use sequence operator `...` on a separate line in Raku\nThe following code works:\n\n\n\n```\nmy @fibo = 1, 1, -> $a, $b { $a + $b } ...  *;\nsay @fibo[^10];\n\n```\n\noutput:\n\n\n\n```\n(1 1 2 3 5 8 13 21 34 55)\n\n```\n\nBut if you split this over multiple lines like this:\n\n\n\n```\nmy @fibo = 1,\n           1,\n           -> $a, $b { $a + $b }\n           ...\n           *;\nsay @fibo[^10];\n\n```\n\nit gives an error:\n\n\n\n```\n*\n  in block <unit> at .\/dotdotdot line 4\n\n```\n\nIt appears that the `...` is interpreted as the yada-operator in this case. But it shouldn't, should it? Both versions of the code should be equivalent. (This isn't Python, whitespace should be mostly irrelevant.)\n\n\nAs a workaround, the following does work correctly:\n\n\n\n```\nmy @fibo = 1,\n           1,\n           -> $a, $b { $a + $b } ...\n           *;\nsay @fibo[^10];\n\n```\n\n(Of course, this is a very artificial example, but I found this when [using a much more complicated sequence](https:\/\/github.com\/mscha\/aoc\/blob\/master\/aoc2023\/aoc10bb) that's too long to put on one line.)\n\n\nTested up to 2023.11.","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"perl"},"answer":"Raku has a very simple unforgiving rule that if the last thing in a line of code is a block that would make sense as the end of a statement then it's the end of a statement. There are many solutions, such as the one you suggested, or adding a `\\` after the closing curly brace. -raiph"}
{"questionId":"dad65e3b52aa4258b85fcc5e7cad637b","question":"C program compiled with gcc -msse2 contains AVX1 instructions\nI adapted a function I found on SO for SSE2 and included it in my program. The function uses SSE2 intrinsics to calculate the leading zero count of each of the 8 x 16bit integers in the vector. When I compiled the program, which produced no warnings, and ran it on my old laptop which I often use for development, it crashed with the message 'Illegal instruction (core dumped)'. On inspecting the assembly, I noticed my function appeared to have AVX1 'VEX' encoded SSE2 instructions as shown below.\n\n\n\n```\n    .globl  _mm_lzcnt_epi32\n    .type   _mm_lzcnt_epi32, @function\n_mm_lzcnt_epi32:\n.LFB5318:\n    .cfi_startproc\n    endbr64\n    vmovdqa64   %xmm0, %xmm1\n    vpsrld  $8, %xmm0, %xmm0\n    vpandn  %xmm1, %xmm0, %xmm0\n    vmovdqa64   .LC0(%rip), %xmm1\n    vcvtdq2ps   %xmm0, %xmm0\n    vpsrld  $23, %xmm0, %xmm0\n    vpsubusw    %xmm0, %xmm1, %xmm0\n    vpminsw .LC1(%rip), %xmm0, %xmm0\n    ret\n    .cfi_endproc\n\n```\n\nIf I change the header immintrin.h to emmintrin.h, it compiles my code properly to SSE2 instructions\n\n\n\n```\n    .globl  _mm_lzcnt_epi32\n    .type   _mm_lzcnt_epi32, @function\n_mm_lzcnt_epi32:\n.LFB567:\n    .cfi_startproc\n    endbr64\n    movdqa  %xmm0, %xmm1\n    psrld   $8, %xmm0\n    pandn   %xmm1, %xmm0\n    cvtdq2ps    %xmm0, %xmm1\n    movdqa  .LC0(%rip), %xmm0\n    psrld   $23, %xmm1\n    psubusw %xmm1, %xmm0\n    pminsw  .LC1(%rip), %xmm0\n    ret\n    .cfi_endproc\n\n```\n\nand runs successfully. My program is as follows.\n\n\n\n```\n#include <stdio.h>\n#include <string.h>\n#include <stdbool.h>\n#include <stdint.h>\n#include <immintrin.h>\n\n\/\/ gcc ssebug.c -o ssebug.bin -O3 -msse2 -Wall\n\n__m128i _mm_lzcnt_epi32(__m128i v) {\n    \/\/ Based on https:\/\/stackoverflow.com\/questions\/58823140\/count-leading-zero-bits-for-each-element-in-avx2-vector-emulate-mm256-lzcnt-ep\n    \/\/ prevent value from being rounded up to the next power of two\n    v = _mm_andnot_si128(_mm_srli_epi32(v, 8), v); \/\/ keep 8 MSB\n    v = _mm_castps_si128(_mm_cvtepi32_ps(v)); \/\/ convert signed integer to float ??\n    v = _mm_srli_epi32(v, 23); \/\/ shift down the exponent\n    v = _mm_subs_epu16(_mm_set1_epi32(158), v); \/\/ undo bias\n    v = _mm_min_epi16(v, _mm_set1_epi32(32)); \/\/ clamp at 32\n    return v;\n}\n\nint main(int argc, char **argv) {\n  uint32_t i, a[4];  \n  __m128i arg;\n  uint32_t argval = 123;\n  if (argc >= 2) argval = atoi(argv[1]);\n  arg = _mm_set1_epi32(argval);\n  arg = _mm_lzcnt_epi32(arg);\n  _mm_storeu_si128((void*)a, arg);\n  for(i=0; i<4; i++) {\n    printf(\"%u \", a[i]);\n  }\n  printf(\"\\n\");\n}\n\n```\n\nThis explanation, [Header files for x86 SIMD intrinsics](https:\/\/stackoverflow.com\/questions\/11228855\/header-files-for-x86-simd-intrinsics), appears to suggest that for gcc at least, it is safe to just use immintrin.h for everything, which appears to be false. My questions are as follows.\n\n\n1. Is it supposed to be safe to use immintrin.h for everything, or does using it tell the compiler to assume at least AVX1?\n2. Isn't it the compiler's responsibility to produce ONLY instructions which are valid for the target architecture? If not, why not?\n3. Why does it work (produce only SSE2) if I use immintrin.h but make my function static inline?\n4. Is there a way to scan an assembly file to reveal what CPU extensions it requires?\n5. Who should I contact about such issues in future?\n\n\nI think this is potentially quite a serious issue as it isn't always feasible to check the assembler contains only valid instructions for the target architecture. I only found this because my program crashed, and I was using an old machine which doesn't support AVX1. If the function was in some hardly ever executed branch, I might have missed it. You could argue that it isn't worth worrying about this issue specifically because nobody will be using such old hardware for anything serious, but the issues it raises could well apply to newer architectures too. Thanks for your time. I am using gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0.","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"c"},"answer":"### Rename your function to not clash with intrinsics\n\n\nLike `lzcnt_epi32_sse2` or just `lzcnt_epi32`. The `epi32` is already enough to remind you it's related to Intel intrinsics like taking an `__m128i` arg, but the lack of `_mm` in the name lets you know it's just a function, and not one of Intel's SVML functions or something.\n\n\nIf you want to mix vector widths and need to distinguish that in your helper functions (since C doesn't allow overloading), perhaps `__m1128i lzcntd_m128i( __m128i v );`. I've also seen names like `mm_lzcnt_epi32` without the leading `_`, but it would be very easy to miss that when reading.\n\n\n\n```\nstatic inline\n__m128i lzcnt_epi32(__m128i v) {\n#ifdef __AVX512VL__  \/\/ and __AVX512CD__ but that's effectively baseline\n    return _mm_lzcnt_epi32(v);  \/\/ use the HW version if build options allow\n#else\n    \/\/ Based on https:\/\/stackoverflow.com\/questions\/58823140\/count-leading-zero-bits-for-each-element-in-avx2-vector-emulate-mm256-lzcnt-ep\n    \/\/ prevent value from being rounded up to the next power of two\n    v = _mm_andnot_si128(_mm_srli_epi32(v, 8), v); \/\/ keep 8 MSB\n    v = _mm_castps_si128(_mm_cvtepi32_ps(v)); \/\/ convert signed integer to float ??\n    v = _mm_srli_epi32(v, 23); \/\/ shift down the exponent\n    v = _mm_subs_epu16(_mm_set1_epi32(158), v); \/\/ undo bias\n    v = _mm_min_epi16(v, _mm_set1_epi32(32)); \/\/ clamp at 32\n    return v;\n#endif\n}\n\n```\n\n\n\n---\n\n\n**Don't define your own functions with names that start with `_`, those are reserved for use by the implementation.** That reserved part of the namespace is a reasonable place for non-portable extensions that won't clash with any existing code, which is probably why Intel chose it for their intrinsics. (*[What are the rules about using an underscore in a C++ identifier?](https:\/\/stackoverflow.com\/questions\/228783\/what-are-the-rules-about-using-an-underscore-in-a-c-identifier)* - C has pretty much the same rules as C++ for this, IIRC. Since your definition isn't `static`, it's in the global namespace where `_anything` is reserved.\nNot that I'd recommend `static inline` with clashing names.)\n\n\nDon't follow their naming scheme for your own functions that take `__m128i` args, and ***definitely* never define your own version of an intrinsic**. Those do get defined even without `-mavx512vl` enabled globally, so they're usable inside functions that use `__attribute__((target(\"avx512vl\")))`, and unfortunately you end up with silent use of ISA extensions you didn't want, with no good way for GCC to detect a potential problem to even warn about it, I think.\n\n\n\n\n---\n\n\n### The intrinsic's definition\n\n\n`_mm_lzcnt_epi32` is a real intrinsic for an AVX-512 instruction. It's [declared and defined in a GCC header](https:\/\/github.com\/gcc-mirror\/gcc\/blob\/3a7dd24eadeb918f9990db7adcb3e63e76bd63fb\/gcc\/config\/i386\/avx512vlintrin.h#L13805) as an `extern inline` wrapper function (around a GNU C `__builtin`) inside a `#pragma GCC target(\"avx512vl,avx512cd\")` block, with `__attribute__((always_inline))`. (If `avx512vl` wasn't enabled globally, it will `#pragma GCC pop_options` afterwards so it's only enabled for that block of definitions.)\n\n\n**Apparently the target-attribute part of the declaration sticks, but not the always-inline attribute which normally makes inlining fail with a compile-time error.** This part may be a GCC bug. And somehow it's not an error to redefine the function, because of the `gnu_inline` attribute in the header's definition1. It is an error with clang which uses different headers.\n\n\nSo you get a `call _mm_lzcnt_epi32` in `main` to a non-inline function that uses AVX-512 instructions. (Yes, GCC9.4 uses EVEX `vmovdqa64 xmm1, xmm0` as well as VEX `vpsrld xmm0, xmm0, 8`, as you show in your code block. This is a missed-optimization bug that was fixed in GCC10: `vmovdqa xmm1, xmm0` is fewer bytes of machine code. Although I think the whole copy is avoidable by shifting into a separate destination so there is still a missed optimization, but GCC10 makes asm that will run on Godbolt's Zen 3 AWS instances, not just its SKX \/ Ice Lake instances.)\n\n\n\n\n---\n\n\n**This is what's supposed to happen with `arg = _mm_lzcnt_epi32(arg);` if you haven't defined your own version of it - a \"target-specific options mismatch\" error:**\n\n\n\n```\n\/opt\/compiler-explorer\/gcc-9.4.0\/lib\/gcc\/x86_64-linux-gnu\/9.4.0\/include\/avx512vlintrin.h:8376:1: error: inlining failed in call to always_inline '_mm_lzcnt_epi32': target specific option mismatch\n 8376 | _mm_lzcnt_epi32 (__m128i __A)\n      | ^~~~~~~~~~~~~~~\n<source>:28:9: note: called from here\n   28 |   arg = _mm_lzcnt_epi32(arg);\n      |         ^~~~~~~~~~~~~~~~~~~~\nIn file included from \/opt\/compiler-explorer\/gcc-9.4.0\/lib\/gcc\/x86_64-linux-gnu\/9.4.0\/include\/immintrin.h:63,\n                 from <source>:5:\n\/opt\/compiler-explorer\/gcc-9.4.0\/lib\/gcc\/x86_64-linux-gnu\/9.4.0\/include\/avx512vlintrin.h:8376:1: error: inlining failed in call to always_inline '_mm_lzcnt_epi32': target specific option mismatch\n 8376 | _mm_lzcnt_epi32 (__m128i __A)\n      | ^~~~~~~~~~~~~~~\n<source>:28:9: note: called from here\n   28 |   arg = _mm_lzcnt_epi32(arg);\n      |         ^~~~~~~~~~~~~~~~~~~~\nCompiler returned: 1\n\n```\n\nOr if you use the raw builtin manually:\n\n\n\n```\n<source>:29:18: error: '__builtin_ia32_vplzcntd_128_mask' needs isa option -mavx512vl -mavx512cd\n   29 |   arg = (__m128i)__builtin_ia32_vplzcntd_128_mask((__v4si)arg, (__v4si)_mm_setzero_si128(), -1);\n      |                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n```\n\n\n\n---\n\n\nNote that `-msse2` is baseline for x86-64. You only *need* to enable it if targeting `-m32` with a GCC config that doesn't do that by default. It doesn't do any harm for x86-64, but it also doesn't override AVX enabled by any earlier options like `-march=x86-64-v3` or `-mavx`. For that you want `-mno-avx`. But that just sets the baseline for all code: pragma and per-function `__attribute__` can still enable use of later ISA extensions for specific functions. `gcc -msse2 -mno-avx` is equivalent to the default and won't help work around this bug of naming a function that clashes with an intrinsic.\n\n\nSome Linux distros are planning to ship versions that are built with `-march=x86-64-v3` (Haswell baseline: AVX2+FMA+BMI2, [wikipedia](https:\/\/en.wikipedia.org\/wiki\/X86-64#Microarchitecture_levels)) although IDK if they're planning to configure GCC with that higher baseline as a no-options default the way many do for SSE2 with `gcc -m32`. But your GCC 9.4.0-1ubuntu1~20.04.1 is definitely not configured that way, and what I can see on Godbolt matches what you report your GCC doing.\n\n\n### Which CPUs is this relevant for?\n\n\n\n> \n> You could argue that it isn't worth worrying about this issue specifically because nobody will be using such old hardware for anything serious,\n> \n> \n> \n\n\nFirst of all, your code uses AVX-512 instructions (`vmovdqa64`) and will crash on Intel's latest desktop \/ laptop CPUs because they removed AVX-512 before defining a way (AVX10.1) to expose 128 and 256-bit EVEX instructions with all the great new features like masking, better shuffles, `vpternlogd`, and niche instructions like `vplzcntd`. They'll run fine on Zen 4, though.\n\n\nSecondly, low-power Intel CPUs based on Tremont and earlier lack AVX\/BMI, so there are recent low-power servers and low-end netbooks without AVX.\n\n\nAlso, Intel Pentium and Celeron before Ice Lake had AVX+BMI disabled. (BMI perhaps a victim of disabling decode of VEX prefixes as a way to disable AVX+FMA?) This was pretty bad, not helping the x86 ecosystem get closer to making BMI (or AVX) baseline. BMI1\/BMI2 are most useful if used everywhere for stuff like more efficient variable-count shifts, not just in a couple hot loops like SIMD.\n\n\n(Ice Lake Pentium\/Celeron are still half-width, but that means 256-bit so x86-64-v3 without AVX-512. Low-end \/ low-power Alder Lake N has all Gracemont E-cores but that's the same x86-64-v3 feature level as their P-cores, thanks to Intel crippling the AVX-512 on the P-cores even in systems with no E-cores, while enhancing their E-cores to add x86-64-v3 features.)\n\n\n\n\n---\n\n\n### Footnote 1: No redefinition error?\n\n\nIt seems that `__attribute__((__gnu_inline__))` is responsible for allowing a second definition. [GCC compiles this without complaint](https:\/\/godbolt.org\/#g:!((g:!((g:!((h:codeEditor,i:(filename:%271%27,fontScale:14,fontUsePx:%270%27,j:1,lang:___c,selection:(endColumn:14,endLineNumber:34,positionColumn:14,positionLineNumber:34,selectionStartColumn:14,selectionStartLineNumber:34,startColumn:14,startLineNumber:34),source:%27%23include+%3Cstdio.h%3E%0A%23include+%3Cstring.h%3E%0A%23include+%3Cstdbool.h%3E%0A%23include+%3Cstdint.h%3E%0A%23include+%3Cimmintrin.h%3E%0A%0A%23if+1%0A\/\/%23pragma+GCC+push_options%0A\/\/%23pragma+GCC+target(%22avx512vl,avx512cd%22)%0A\/\/__attribute__((target(%22sse4.1%22)))%0A__attribute__+((__gnu_inline__))%0Aextern+__inline+int+foo+(int+x)+%7B%0A++return+x%2B1%3B%0A%7D%0A\/\/%23pragma+GCC+pop_options%0A\/\/+it!%27s+normally+an+error+to+redefine+a+function+even+with+extern+inline%0A\/\/+even+with+attribute+target%0A\/\/__attribute__((target(%22default%22)))%0A+int+foo(int+x)+%7B+return+x+%2B+2%3B+%7D%0A%23endif%0A%0A\/\/+gcc+ssebug.c+-o+ssebug.bin+-O3+-msse2+-Wall%0A%0A%23if+1%0A\/\/__attribute__((target(%22default%22),+always_inline))+\/\/+doesn!%27t+change+anything%0A__m128i+_mm_lzcnt_epi32(__m128i+v)+%7B%0A++++\/\/+Based+on+https:\/\/stackoverflow.com\/questions\/58823140\/count-leading-zero-bits-for-each-element-in-avx2-vector-emulate-mm256-lzcnt-ep%0A++++\/\/+prevent+value+from+being+rounded+up+to+the+next+power+of+two%0A++++v+%3D+_mm_andnot_si128(_mm_srli_epi32(v,+8),+v)%3B+\/\/+keep+8+MSB%0A++++v+%3D+_mm_castps_si128(_mm_cvtepi32_ps(v))%3B+\/\/+convert+signed+integer+to+float+%3F%3F%0A++++v+%3D+_mm_srli_epi32(v,+23)%3B+\/\/+shift+down+the+exponent%0A++++v+%3D+_mm_subs_epu16(_mm_set1_epi32(158),+v)%3B+\/\/+undo+bias%0A++++v+%3D+_mm_min_epi16(v,+_mm_set1_epi32(32))%3B+\/\/+clamp+at+32%0A++++return+v%3B%0A%7D%0A%23endif%0A%0Aint+main(int+argc,+char+**argv)+%7B%0A++uint32_t+i,+a%5B4%5D%3B++%0A++__m128i+arg%3B%0A++uint32_t+argval+%3D+123%3B%0A++if+(argc+%3E%3D+2)+argval+%3D+atoi(argv%5B1%5D)%3B%0A++arg+%3D+_mm_set1_epi32(argval)%3B%0A++arg+%3D+_mm_lzcnt_epi32(arg)%3B%0A\/\/+you+get+a+more+direct+error+from+manually+uing+the+raw+builtin.%0A\/\/++arg+%3D+(__m128i)__builtin_ia32_vplzcntd_128_mask((__v4si)arg,+(__v4si)_mm_setzero_si128(),+-1)%3B%0A++_mm_storeu_si128((void*)a,+arg)%3B%0A++for(i%3D0%3B+i%3C4%3B+i%2B%2B)+%7B%0A++++printf(%22%25u+%22,+a%5Bi%5D)%3B%0A++%7D%0A++printf(%22%5Cn%22)%3B%0A%7D%0A%0A%27),l:%275%27,n:%270%27,o:%27C+source+%231%27,t:%270%27)),header:(),k:33.0965945313695,l:%274%27,m:100,n:%270%27,o:%27%27,s:0,t:%270%27),(g:!((g:!((h:compiler,i:(compiler:cg95,filters:(b:%270%27,binary:%271%27,binaryObject:%271%27,commentOnly:%270%27,debugCalls:%271%27,demangle:%270%27,directives:%270%27,execute:%270%27,intel:%270%27,libraryCode:%270%27,trim:%271%27),flagsViewOpen:%271%27,fontScale:14,fontUsePx:%270%27,j:3,lang:___c,libs:!(),options:%27-O3+-Wall%27,overrides:!(),selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1),l:%275%27,n:%270%27,o:%27+x86-64+gcc+9.5+(Editor+%231)%27,t:%270%27)),header:(),k:33.570072135297174,l:%274%27,m:74.10071942446042,n:%270%27,o:%27%27,s:0,t:%270%27),(g:!((h:output,i:(compilerName:%27x86-64+gcc+13.2%27,editorid:1,fontScale:14,fontUsePx:%270%27,j:3,wrap:%271%27),l:%275%27,n:%270%27,o:%27Output+of+x86-64+gcc+9.5+(Compiler+%233)%27,t:%270%27)),header:(),l:%274%27,m:25.899280575539574,n:%270%27,o:%27%27,s:0,t:%270%27)),k:33.570072135297174,l:%273%27,n:%270%27,o:%27%27,t:%270%27),(g:!((g:!((h:compiler,i:(compiler:cclang1701,filters:(b:%270%27,binary:%271%27,binaryObject:%271%27,commentOnly:%270%27,debugCalls:%271%27,demangle:%270%27,directives:%270%27,execute:%270%27,intel:%270%27,libraryCode:%270%27,trim:%271%27),flagsViewOpen:%271%27,fontScale:14,fontUsePx:%270%27,j:1,lang:___c,libs:!(),options:%27-O3+-Wall%27,overrides:!(),selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1),l:%275%27,n:%270%27,o:%27+x86-64+clang+17.0.1+(Editor+%231)%27,t:%270%27)),k:25,l:%274%27,m:63.10380267214799,n:%270%27,o:%27%27,s:0,t:%270%27),(g:!((h:output,i:(compilerName:%27x86-64+clang+17.0.1%27,editorid:1,fontScale:14,fontUsePx:%270%27,j:1,wrap:%271%27),l:%275%27,n:%270%27,o:%27Output+of+x86-64+clang+17.0.1+(Compiler+%231)%27,t:%270%27)),header:(),l:%274%27,m:36.89619732785201,n:%270%27,o:%27%27,s:0,t:%270%27)),k:33.333333333333336,l:%273%27,n:%270%27,o:%27%27,t:%270%27)),l:%272%27,n:%270%27,o:%27%27,t:%270%27)),version:4):\n\n\n\n```\n__attribute__ ((__gnu_inline__))\nextern __inline int foo (int x) {\n  return x+1;\n}\n\nint foo(int x) { return x + 2; }\n\n```\n\n(`__gnu_inline__` is a version of `gnu_inline` that doesn't pollute the namespace, for use in `-std=gnu11` mode, like `__asm__` vs. `asm`. Most GNU keywords have an `__x__` version so headers don't break even if user code did a `#define` on any non-reserved part of the namespace.)\n\n\nFrom the [GCC manual: function attributes](https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/Common-Function-Attributes.html#index-gnu_005finline-function-attribute):\n\n\n\n> \n> ##### `gnu_inline`\n> \n> \n> This attribute should be used with a function that is also declared with the inline keyword. It directs GCC to treat the function as if it were defined in gnu90 mode even when compiling in C99 or gnu99 mode.\n> \n> \n> If the function is declared extern, then this definition of the function is used only for inlining. In no case is the function compiled as a standalone function, not even if you take its address explicitly. Such an address becomes an external reference, as if you had only declared the function, and had not defined it. This has almost the effect of a macro. The way to use this is to put a function definition in a header file with this attribute, and put another copy of the function, without extern, in a library file. The definition in the header file causes most calls to the function to be inlined. If any uses of the function remain, they refer to the single copy in the library. Note that the two definitions of the functions need not be precisely the same, although if they do not have the same effect your program may behave oddly.\n> \n> \n> In C, if the function is neither extern nor static, then the function is compiled as a standalone function, as well as being inlined where possible.\n> \n> \n> This is how GCC traditionally handled functions declared inline. Since ISO C99 specifies a different semantics for inline, this function attribute is provided as a transition measure and as a useful feature in its own right. This attribute is available in GCC 4.1.3 and later. It is available if either of the preprocessor macros `__GNUC_GNU_INLINE__` or `__GNUC_STDC_INLINE__` are defined. See [An Inline Function is As Fast As a Macro](https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/Inline.html).\n> \n> \n> In C++, this attribute does not depend on extern in any way, but it still requires the inline keyword to enable its special behavior.\n> \n> \n> \n\n\nSo I guess the version in the header wasn't a candidate for inlining because of mismatching target options, but providing a non-`inline` definition let GCC call it anyway. So this might not be a GCC bug. And it's probably not something GCC should even warn about since most `.c` files that provide the non-inline definition (if there is one; not the case for intrinsics I assume) will include the header that defines the `extern inline` version.\n\n\nEven if it were or is a bug that GCC didn't error or warn about this, don't define your own functions in a reserved part of the namespace in the first place. The most we could hope for is GCC being more helpful like erroring at compile-time instead of silently making a binary you didn't intend.\n\n\nThe behaviour is undefined in this case (defining functions with reserved names). Perhaps GCC could warn if it differentiated based on path, knowing which headers were \"part of the implementation\" vs. 3rd-party libraries. But I think glibc also uses plenty of `__` names in headers in `\/usr\/include`, so I don't think that's viable.\n\n\n\n\n---\n\n\nAt first I thought GCC was allowing it because different target attributes on definitions for the same name is how GCC does [function multiversioning](https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/Function-Multiversioning.html). But this is different. If it was doing multiversioning, it would be using a non-AVX512 version because `main` was compiled with just SSE2 in effect. The test-case above compiles with just `gnu_inline`, no target-attribute stuff required."}
{"questionId":"2a708e9e22e54401a9dfb5bfa822dc72","question":"VS Code Python extension (circa v2018.19) no longer includes support for linters and formatters. Why?\nThe Python extension for VS Code used to provide builtin support for tools like formatters and linters, including:\n\n\n- Linting: Pylint, Flake8, Mypy, Bandit, Pydocstyle, Pycodestyle, Prospector, Pylama\n- Formatting: autopep8, Black, YAPF\n\n\nWhat's happening to the builtin support for these tools in the Python extension? How can I get integrated support for these tools in VS Code going forward?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"Basically, see <https:\/\/github.com\/microsoft\/vscode-python\/wiki\/Migration-to-Python-Tools-Extensions>. I'll try to summarize\/quote.\n\n\n\n> \n> As [announced on April 2022](https:\/\/devblogs.microsoft.com\/python\/python-in-visual-studio-code-april-2022-release\/#pylint-extension), our team has been working towards breaking the tools support we offer in the Python extension for Visual Studio Code into separate extensions, with the intent of improving performance, stability and no longer requiring the tools to be installed in a Python environment \u2013 as they can be shipped alongside an extension. This also allows the extensions to be shipped separately from the Python one once a new version of their respective tool becomes available.\n> \n> \n> \n\n\nThose extensions include: [`ms-python.pylint`](https:\/\/marketplace.visualstudio.com\/items?itemName=ms-python.pylint), [`ms-python.flake8`](https:\/\/marketplace.visualstudio.com\/items?itemName=ms-python.flake8), [`ms-python.mypy-type-checker`](https:\/\/marketplace.visualstudio.com\/items?itemName=ms-python.mypy-type-checker), [`ms-python.black-formatter`](https:\/\/marketplace.visualstudio.com\/items?itemName=ms-python.black-formatter), [`ms-python.autopep8`](https:\/\/marketplace.visualstudio.com\/items?itemName=ms-python.autopep8), [`ms-python.isort`](https:\/\/marketplace.visualstudio.com\/items?itemName=ms-python.isort), [`charliermarsh.ruff`](https:\/\/marketplace.visualstudio.com\/items?itemName=charliermarsh.ruff), [`matangover.mypy`](https:\/\/marketplace.visualstudio.com\/items?itemName=matangover.mypy), [`eeyore.yapf`](https:\/\/marketplace.visualstudio.com\/items?itemName=eeyore.yapf).\n\n\nPrompts, commands, and context menu items already started getting removed in [the 2018.18.0 release](https:\/\/github.com\/microsoft\/vscode-python\/releases\/tag\/v2023.18.0)- Ex. [Remove old linter and formatter prompts and commands #21979](https:\/\/github.com\/microsoft\/vscode-python\/pull\/21979) and [Remove sort imports from command palette and context menu #22058](https:\/\/github.com\/microsoft\/vscode-python\/pull\/22058). Lots of removals are in [the iteration plan for the October 2023 release](https:\/\/github.com\/microsoft\/vscode-python\/issues\/22173). Official VS Code Python docs [for linting](https:\/\/code.visualstudio.com\/docs\/python\/linting) and [for formatting](https:\/\/code.visualstudio.com\/docs\/python\/formatting) look to have been updated already- at least partially, which is nice.\n\n\nSettings related to linting and formatting features that are moving to their own extensions are accordingly being removed ([here's the full list with migration instructions](https:\/\/github.com\/microsoft\/vscode-python\/wiki\/Migration-to-Python-Tools-Extensions#linting-and-formatting-settings-deprecation)). That includes `python.linting.enabled`, `python.formatting.provider`, and a host of settings related to specific linters and formatters.\n\n\nNot all the linters and formatters that were supported before have extensions yet. If you want to create a linter or formatter tool extension yourself, the [Python Tools Extension Template](https:\/\/github.com\/microsoft\/vscode-python-tools-extension-template) will probably help. Or, you can take a look at [the list of alternatives for deprecated settings](https:\/\/github.com\/microsoft\/vscode-python\/wiki\/Migration-to-Python-Tools-Extensions#alternatives-for-deprecated-settings--), which includes trying another extension that supports multiple linters (Ex. [`charliermarsh.ruff`](https:\/\/marketplace.visualstudio.com\/items?itemName=charliermarsh.ruff)), [disabling extension auto-update](https:\/\/code.visualstudio.com\/docs\/editor\/extension-marketplace#_extension-autoupdate) and sticking with an older version of the Python extension \/ other extension that supports the tool you want to use, or writing a [task](https:\/\/code.visualstudio.com\/Docs\/editor\/tasks) to run the tool in the integrated terminal (you can also write custom problem matching)."}
{"questionId":"81a752bc82a648cdb4c04fb8a8f5aa67","question":"What are the rules for \\_ underscore variables in C++26, and what's a name-independent declaration?\nWhen I compile the following code, I get a warning (<https:\/\/godbolt.org\/z\/Tx7v6jWf1>):\n\n\n\n```\nvoid foo() {\n    int _;\n    \/\/ warning: name-independent declarations only available with\n    \/\/           '-std=c++2c' or '-std=gnu++2c' [-Wc++26-extensions]\n    int _;\n}\n\n```\n\nWhat exactly has changed about `_` variables in C++26, and what's a *name-indepent declaration*?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"[P2169: A nice placeholder with no name](https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/papers\/2023\/p2169r4.pdf) has been accepted into C++26, which makes `_` special in the following contexts:\n\n\n- local variables (e.g. `int _`)\n- local structured bindings (e.g. `auto [x, _]`)\n- [init-capture](https:\/\/eel.is\/c++draft\/expr.prim.lambda.capture#nt:init-capture)s (e.g. `[_ = 0] {}`)\n- non-static data members of other than an anonymous union (e.g. `struct S { int _; }`)\n\n\nIn such contexts, `_` makes the declaration [name-independent](https:\/\/eel.is\/c++draft\/basic.scope.scope#def:declaration,name-independent).\n\n\n## `_` suppresses warnings\n\n\nThe standard says:\n\n\n\n> \n> *Recommended practice*: Implementations should not emit a warning that a [name-independent](https:\/\/eel.is\/c++draft\/basic.scope.scope#def:declaration,name-independent) declaration is used or unused.\n> \n> \n> \n\n\nThis is very similar to the recommendation for `[[maybe_unused]]` in [[dcl.attr.unused] p4](https:\/\/eel.is\/c++draft\/dcl.attr.unused#4).\nNormally, you would get a warning for unused variables (`-Wunused` in GCC), but `_` and `[[maybe_unused]]` suppress this.\n\n\nHistorically, developers have used `_` as a \"placeholder\" for unused things, so this is just standardizing existing practice.\n\n\n## `_` can be declared multiple times\n\n\nFurthermore, [name-independent](https:\/\/eel.is\/c++draft\/basic.scope.scope#def:declaration,name-independent) declarations cannot [potentially conflict](https:\/\/eel.is\/c++draft\/basic.scope.scope#def:potentially_conflict).\nIn short, you can declare `_` multiple times. However, name lookup cannot be ambiguous.\n\n\n\n```\nvoid g() {\n  int _;\n  _ = 0;   \/\/ OK, and warning is not recommended\n  int _;   \/\/ OK, name-independent declaration does not potentially conflict with the first _\n  _ = 0;   \/\/ error: two non-function declarations in the lookup set\n}\n\n```\n\nThis code is taken from [[basic.scope.scope] example 3](https:\/\/eel.is\/c++draft\/basic.scope.scope#example-3).\n\n\nNote that `_` also has some special interactions with `using` declarations. See [[namespace.udecl] p10](https:\/\/eel.is\/c++draft\/namespace.udecl#10) for more details.\n\n\n## Two `_` are not the same entity from the linker's perspective\n\n\nEven with external linkage, two `_` are not considered the same entity:\n\n\n\n```\n\/\/ a.cpp\nint _ = 0;\n\n```\n\n\n```\n\/\/ b.cpp\nint _ = 0;\n\n```\n\nWhen linked, this program is OK.\nFor any name other than `_`, this would give you a \"multiple definitions\" linker error.\nSee also [[basic.link] p8](https:\/\/eel.is\/c++draft\/basic.link#8).\n\n\n## Compiler Support\n\n\nOnly GCC 14 and Clang 18 support this feature at the time of writing.\nSee [C++26 compiler support](https:\/\/en.cppreference.com\/w\/cpp\/compiler_support\/26) for more.\n\n\nIf you need to test for support, test for [`__cpp_placeholder_variables`](https:\/\/eel.is\/c++draft\/tab:cpp.predefined.ft):\n\n\n\n```\n#if __cpp_placeholder_variables >= 202306L"}
{"questionId":"49ad09c942cb4bbc94bc6e2c4d3b6ce3","question":"What does Haskell's Data.Function.on do?\nI am finding it hard to find examples of `on` in Haskell and I don't understand the explanation on Hoogle's `Data.Function` page. Please, can I have examples of its use and examples of problems\/code where using it makes a solution simpler or more efficient?","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"haskell"},"answer":"Suppose we have a function `f` that takes two arguments. We want a similar function which acts as `f`, but only after both its arguments have been modified by some other function `g`.\n\n\nWe could write a new definition\n\n\n\n```\nnew_f x y = f (g x) (g y)\n\n```\n\nor, exploiting `on`,\n\n\n\n```\nnew_f = f `on` g\n\n```\n\nNicely, the latter can be used directly, without defining a new symbol `new_f`. (Using a lambda is also viable, but not as nice)\n\n\nThis is typically used with `compare`, a library binary function that checks the ordering between its two arguments. So, we can use\n\n\n\n```\nsortBy (compare `on` name)   somePeopleList\nsortBy (compare `on` age)    somePeopleList\nsortBy (compare `on` height) somePeopleList\n...\n\n```\n\nto sort according different criteria. Above, we assume that `name`, etc. are functions that can extract the relevant information from a person.\n\n\n(In more modern Haskell we also have `sortBy (comparing age) somePeopleList`, or even `sortOn age somePeopleList` for the same task)"}
{"questionId":"62de7932021749acbe7496b29ff06c45","question":"Is it valid to use a reference to a local variable to initialize a constexpr variable?\nThe following code only compiles on GCC (checked it on 10.4 and 13.2 on godbolt.org) but not on Clang (fails on all versions I tried, for example 17.0.1 on godbolt.org):\n\n\n\n```\nstruct A {\n  static constexpr int b{1};\n};\n\nint main(int argc, char *argv[]) { \n    A a;\n    A& aref{a};\n    constexpr auto bb1{a.b};\n    constexpr auto bb2{aref.b};\n    return bb1+bb2; \n}\n\n```\n\nClang outputs:\n\n\n\n```\n<source>:9:20: error: constexpr variable 'bb2' must be initialized by a constant expression\n    9 |     constexpr auto bb2{aref.b};\n      |                    ^  ~~~~~~~~\n<source>:9:24: note: initializer of 'aref' is not a constant expression\n    9 |     constexpr auto bb2{aref.b};\n      |                        ^\n<source>:7:14: note: declared here\n    7 |     A& aref{a};\n      |   \n\n```\n\n<https:\/\/godbolt.org\/z\/nG4j3KefE>\n\n\nWhy?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"cpp"},"answer":"bb1` was always permitted.\n\n\n`bb2` is allowed since [P2280](https:\/\/github.com\/cplusplus\/papers\/issues\/973) which was accepted for C++23 as defect report for earlier revisions as well.\n\n\nOriginally there was a specific rule that in a constant expression forbade *any* use of a reference variable (but only *reference* variables!) that wasn't *usable in constant expressions* (i.e. `constexpr` initialized with constant expresison) or had its lifetime started during the constant expression evaluation. This held even if no lvalue-to-rvalue conversion was applied and nothing else depended on the specific referenced objects. This was inconsistent with usage of non-reference variables and the defect report fixes that. Your example demonstrates the inconsistency.\n\n\nGCC technically didn't behave conforming before the defect report and Clang isn't behaving conforming since it, but probably simply hasn't implemented it yet."}
{"questionId":"054d230a5800454d999c8d55e533a0c0","question":"Sorting words from argv by length\nI'm attempting to read words from the command line using `argv` in C and then sort them in descending order based on their lengths. However, my sorting algorithm is producing unexpected output.\n\n\nThe code I'm using is as follows:\n\n\n\n```\n#include <stdio.h>\n#include <string.h>\n\nint main(int argc, char *argv[]) {\n\n    for (int i = 1; i < argc - 1; i++) {\n        for (int j = 1; j < argc - i - 1; j++) {\n            if (strlen(argv[j]) < strlen(argv[j + 1])) {\n                char temp_word[20];\n                strcpy(temp_word, argv[j]);\n                strcpy(argv[j], argv[j + 1]);\n                strcpy(argv[j + 1], temp_word);\n            }\n        }\n    }\n\n    puts(\"\\n\");\n    for (int i = 1; i < argc; i++) {\n        printf(\"%s \", argv[i]);\n    }\n\n    return 0;\n}\n\n```\n\n\n```\ngcc test.c -o test\n.\/test I put this words\n\n\npuwordI wordI I  % \n\n```\n\nUnfortunately, the output is corrupted. I suspect there might be an issue with my sorting logic or how I'm handling the command line arguments. Could someone please review my code and provide guidance on how to correctly sort the words by length?","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"c"},"answer":"There are two issues with your code. First, you are trying to swap the *content* of the strings in the `argv` array: this won't work, because the buffer assigned the *shorter* of two strings to be swapped will not be large enough to hold the 'replacement' from the *longer*, and will lead to undefined behaviour because of buffer overrun. Instead, just swap the actual pointers.\n\n\nSecond, you are not testing the whole array; take out the `- 1` from the limits in the two `for` loops.\n\n\nHere's a corrected version of your code:\n\n\n\n```\n#include <stdio.h>\n#include <string.h>\n\nint main(int argc, char* argv[])\n{\n    for (int i = 1; i < argc; i++) { \/\/ Use \"argc\" (not argc - 1)\n        for (int j = 1; j < argc - i; j++) { \/\/ Similarly, no -1 here!\n            if (strlen(argv[j]) < strlen(argv[j + 1])) {\n                char* temp_word = argv[j];\n                argv[j] = argv[j + 1];    \/\/ Just swap the pointers,\n                argv[j + 1] = temp_word;  \/\/ not the string contents\n            }\n        }\n    }\n\n    puts(\"\\n\");\n    for (int i = 1; i < argc; i++) {\n        printf(\"%s \", argv[i]);\n    }\n\n    return 0;\n}\n\n```\n\nEDIT: In the light of the comments about whether or not modifying the `argv` elements (as in the above code) is safe (or if it can cause undefined behaviour), I offer a version, below, that first makes a *copy* of the array, then works with that:\n\n\n\n```\n#include <stdio.h>\n#include <string.h>\n#include <stdlib.h>\n#include <stddef.h>\n\nint main(int argc, char* argv[])\n{\n    \/\/ First, make a copy of the argv array ...\n    char** argvCopy = malloc((size_t)argc * sizeof(char*));\n    memcpy(argvCopy, argv, (size_t)argc * sizeof(char*));\n    \/\/ Now sort that copy ...\n    for (int i = 1; i < argc; i++) {\n        for (int j = 1; j < argc - i; j++) {\n            if (strlen(argvCopy[j]) < strlen(argvCopy[j + 1])) {\n                char* temp_word;\n                temp_word = argvCopy[j];\n                argvCopy[j] = argvCopy[j + 1];\n                argvCopy[j + 1] = temp_word;\n            }\n        }\n    }\n\n    puts(\"\\n\");\n    for (int i = 1; i < argc; i++) {\n        printf(\"%s \", argvCopy[i]);\n    }\n\n    free(argvCopy); \/\/ Don't forget to free the copy.\n    return 0;\n}\n\n```\n\nRefer also to the discussions here: [Is argv[n] writable?](https:\/\/stackoverflow.com\/q\/25737434\/10871073)"}
{"questionId":"b70aa9f3ef1d4b93bd2c534b2a52e554","question":"How to check if a string ONLY contains specific strings\nI want to test if a string ONLY contains specific substrings (as whole words) \/ spaces\n\n\nI've written some code and it works, but I am concerned that I don't understand the regex (I've copied from elsewhere)\n\n\nIs there a way of doing this without complex regex?\n\n\n\n\n\n```\nconst str1 = 'a\u266d apple a a a a a apple   a\u266d a'; \/\/ valid\nconst str2 = 'a\u266d apple a a a a a apple   a\u266d aa'; \/\/ invalid aa\nconst str3 = 'a\u266d apple ad  a a a apple   a\u266d a'; \/\/ invalid ad\nconst str4 = ' a\u266d apple a a a a a apple   a\u266d a'; \/\/ valid\nconst str5 = ' a\u266d apple a a a a a apple   a\u266d a '; \/\/ valid\nconst str6 = 'a\u266d apple a a a a a apple   a\u266d a '; \/\/ valid\nconst str7 = '      '; \/\/ invalid\nconst str8 = ''; \/\/ invalid\n\nconst allowedSubstrings = [\n  'a', 'a\u266d', 'apple'\n]\n\nconst isStringValid = str => {\n  if (str.trim() === '') return false\n  allowedSubstrings.forEach(sub => {\n    \/\/ https:\/\/stackoverflow.com\/a\/6713427\/1205871\n    \/\/ regex for whole words only\n    const strRegex = `(?<!\\\\S)${sub}(?!\\\\S)`\n    const regex = new RegExp(strRegex, 'g')\n    str = str.replace(regex, '')\n  })\n  str = str.replaceAll(' ', '')\n  \/\/ console.log(str)\n  return str === ''\n}\n\nconsole.log('str1', isStringValid(str1))\nconsole.log('str2', isStringValid(str2))\nconsole.log('str3', isStringValid(str3))\nconsole.log('str4', isStringValid(str4))\nconsole.log('str5', isStringValid(str5))\nconsole.log('str6', isStringValid(str6))\nconsole.log('str7', isStringValid(str7))\nconsole.log('str8', isStringValid(str8))","questionMetadata":{"Type":"Implementation","Level":"Beginner","Tag":"javascript"},"answer":"One approach which I can think of (which avoids complex `regex`) would be to:\n\n\n1. split the string based on one or more whitespace characters (space, tabs or others)\n2. check if every word in the `words` array (created by above split) is included in the `allowedSubstrings` array.\n\n\n\n\n\n```\nconst str1 = 'a\u266d apple a a a a a apple   a\u266d a'; \/\/ valid\nconst str2 = 'a\u266d apple a a a a a apple   a\u266d aa'; \/\/ invalid aa\nconst str3 = 'a\u266d apple ad  a a a apple   a\u266d a'; \/\/ invalid ad\nconst str4 = ' a\u266d apple a a a a a apple   a\u266d a'; \/\/ valid\nconst str5 = ' a\u266d apple a a a a a apple   a\u266d a '; \/\/ valid\nconst str6 = 'a\u266d apple a a a a a apple   a\u266d a '; \/\/ valid\nconst str7 = '      '; \/\/ invalid\nconst str8 = ''; \/\/ invalid\n\nconst allowedSubstrings = [\n  'a', 'a\u266d', 'apple'\n];\n\nconst isStringValid = (str) => {\n  const words = str.trim().split(\/\\s+\/);\n  \/\/ If the requirement would be to make them valid, we can use trim().\n  \/\/ const words = str.trim().split(\/\\s+\/);\n  return words.every(word => allowedSubstrings.includes(word));\n};\n\n\/\/ const isStringValid = (str) => str.split(\/\\s+\/).every(word => allowedSubstrings.includes(word));\n\nconsole.log('str1', isStringValid(str1));\nconsole.log('str2', isStringValid(str2));\nconsole.log('str3', isStringValid(str3));\nconsole.log('str4', isStringValid(str4));\nconsole.log('str5', isStringValid(str5));\nconsole.log('str6', isStringValid(str6));\nconsole.log('str7', isStringValid(str7));\nconsole.log('str8', isStringValid(str8));"}
{"questionId":"8314366cc9044d38a90a510a46dd46fd","question":"Element-wise comparison with certain precision\nI am looking for testing output of my function (which returns **array**) with the expected **output array**.\n\n\nExample:\n\n\n\n```\nuse Test;\nsub myoutput($n) {\n    (1..$n)>>.sqrt\n}\n\nis myoutput(3), (1, 1.4142135623730951, 1.7320508075688772);\n\n```\n\nThis looks fine but I want to set *precision* to `1e-12`.\n\n\nWhat I came out with is this:\n\n\n\n```\nsub element_wise_testing_with_precision(@functionoutput, @expectedoutput, $precision) {\n    die \"Provide equal elements!\" if +@functionoutput != +@expectedoutput;\n    loop (my $i = 0; $i < +@functionoutput; $i++)  {\n        is-approx @functionoutput[$i], @expectedoutput[$i], $precision;\n    }\n}\n\n\nmy @expected = (1, 1.4142135623730951, 1.7320508075688772);\nelement_wise_testing_with_precision(myoutput(3), @expected, 1e-12)\n\n\n```\n\nWorks (!) but not sure if it is the right way. Are there approaches to do this thing using [Z operator](https:\/\/docs.raku.org\/language\/operators#infix_Z) or [Hyper operator](https:\/\/docs.raku.org\/language\/operators#Hyper_operators) as they appear to do element-wise operation?","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"perl"},"answer":"Your instincts about hyper and zip\/the Z operator are exactly correct: it's possible to do what you want with either of them. The missing piece of the puzzle is [partial application](https:\/\/en.wikipedia.org\/wiki\/Partial_application) (which Raku often calls [priming](https:\/\/design.raku.org\/S06.html#Priming)). Raku offers two ways to prime \u2013 that is, partially apply \u2013 a function: the [`.assuming`](https:\/\/docs.raku.org\/routine\/assuming) method (to prime a subroutine) and [Whatever-priming](https:\/\/docs.raku.org\/type\/Whatever) (to prime an operator or method).\n\n\nSince `&is-approx` is a subroutine, `&assuming` is the priming method we're after. To specify precision as the third argument, we'd write `&is-approx.assuming(*, *, $precision)`. Or, since `&is-approx` allows specifying precision with a named argument, we can simplify that to `&is-approx.assuming: :abs-tol($precision)`.\n\n\nOnce we've done that, we can apply our new function element wise using the [hyper](https:\/\/docs.raku.org\/language\/operators#Hyper_operators) or Z metaoperators. Note that, because metaoperators expects an infix operator, we'll need to use the [infix form](https:\/\/docs.raku.org\/language\/functions#Infix_form) of our function by wrapping our function in square brackets.\n\n\nHere's your code with those minimal changes:\n\n\n\n```\nsub element_wise_testing_with_precision(@functionoutput, @expectedoutput, $precision) {\n    die \"Provide equal elements!\" if +@functionoutput != +@expectedoutput;\n\n    my &close-enough = &is-approx.assuming(*,*, $precision);\n    @functionoutput \u00ab[&close-enough]\u00bb @expectedoutput\n\n  # or this also works:\n  # @functionoutput Z[&close-enough] @expectedoutput\n}\n\n```\n\nHere's a version with some tangential changes to make it a bit more idiomatic:\n\n\n\n```\nsub element-wise-is-approx(@got, @expected, $abs-tol) {\n    PRE { +@got == +@expected }\n\n    my &close-enough = &is-approx.assuming: :$abs-tol;\n    @got \u00ab[&close-enough]\u00bb @expected\n}\n\nelement-wise-is-approx(myoutput(3), @expected, 1e-12);\n\n```\n\nIndeed, we could even do this inline, as shown below. Here, we switch to the non-DWIM version of hyper (`\u00bb \u00ab`), to enforce equal-sized arguments.\n\n\n\n```\nmyoutput(3) \u00bb[&(&is-approx.assuming(*,*,1e-12))]\u00ab @expected\n\n```\n\n(Note the extra `&( )` in in the infix operator is required to clarify our intent to the Raku compiler.)"}
{"questionId":"a15320dbc90a4238b418b5d7973a71dd","question":"Why is numba popcount code twice as fast as equivalent C code?\nI have this simple python\/numba code:\n\n\n\n```\nfrom numba import njit\nimport numba as nb\n@nb.njit(nb.uint64(nb.uint64))\ndef popcount(x): \n      b=0\n      while(x > 0):\n          x &= x - nb.uint64(1)   \n          b+=1\n      return b\n@njit\ndef timed_loop(n):\n    summand = 0\n    for i in range(n):\n        summand += popcount(i)\n    return summand\n\n```\n\nIt just adds the popcounts for integers 0 to n - 1.\n\n\nWhen I time it I get:\n\n\n\n```\n%timeit timed_loop(1000000)\n340 \u00b5s \u00b1 1.08 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\n\n```\n\nIt [turns out](https:\/\/stackoverflow.com\/questions\/77102860\/how-to-use-native-popcount-with-numba) that llvm cleverly converts the popcount function into the native CPU POPCNT instruction so we should expect it to be fast. But the question is, how fast.\n\n\nI thought I would compare it to a C version to see the speed difference.\n\n\n\n```\n #include <stdio.h>\n#include <time.h>\n\n\/\/ Function to calculate the population count (number of set bits) of an integer using __builtin_popcount\nint popcount(int num) {\n    return __builtin_popcount(num);\n}\n\nint main() {\n    unsigned int n;\n    printf(\"Enter the value of n: \");\n    scanf(\"%d\", &n);\n\n    \/\/ Variables to store start and end times\n    struct timespec start_time, end_time;\n\n    \/\/ Get the current time as the start time\n    clock_gettime(CLOCK_MONOTONIC, &start_time);\n\n    int sum = 0;\n    for (unsigned int i = 0; i < n; i++) {\n        sum += popcount(i);\n    }\n\n    \/\/ Get the current time as the end time\n    clock_gettime(CLOCK_MONOTONIC, &end_time);\n\n    \/\/ Calculate the elapsed time in microseconds\n    long long elapsed_time = (end_time.tv_sec - start_time.tv_sec) * 1000000LL +\n                            (end_time.tv_nsec - start_time.tv_nsec) \/ 1000;\n\n    printf(\"Sum of population counts from 0 to %d-1 is: %d\\n\", n, sum);\n    printf(\"Elapsed time: %lld microseconds\\n\", elapsed_time);\n\n    return 0;\n}\n\n```\n\nI then compiled this with `-march=native -Ofast`. I tried both gcc and clang and the results were very similar.\n\n\n\n```\n.\/popcount \nEnter the value of n: 1000000\nSum of population counts from 0 to 1000000-1 is: 9884992\nElapsed time: 732 microseconds\n\n```\n\nWhy is the numba twice as fast as the C code?","questionMetadata":{"Type":"Optimization","Level":"Advanced","Tag":"python"},"answer":"**TL;DR:** the performance gap between the GCC and the Clang version is due to the use of **scalar instructions versus SIMD instructions**. The performance gap between the Numba and the Clang version comes from the **size of the integers** that is not the same between the two version : **64-bit versus 32-bits**.\n\n\n\n\n---\n\n\n## Performance Results\n\n\nFirst of all, I am also able to reproduce the problem on my Intel i5-9600KF. Here are the results (and the versions):\n\n\n\n```\nNumba 0.56.4:  170.089 ms\nClang 14.0.6:  190.350 ms\nGCC 12.2.0:    328.133 ms\n\n```\n\nTo understand what happens, we need to analyze the assembly code produce by all compilers.\n\n\n\n\n---\n\n\n## Assembly code\n\n\nHere is the assembly code of the hot loop produced by GCC:\n\n\n\n```\n.L5:\n    xorl    %edx, %edx\n    popcntl %eax, %edx\n    incl    %eax\n    addl    %edx, %ebx\n    cmpl    %ecx, %eax\n    jne .L5\n\n```\n\nHere is the one produced by Clang:\n\n\n\n```\n.LBB1_3:                                # =>This Inner Loop Header: Depth=1\n    vpand   %ymm5, %ymm0, %ymm12\n    vpshufb %ymm12, %ymm6, %ymm12\n    vpsrlw  $4, %ymm0, %ymm13\n    vpand   %ymm5, %ymm13, %ymm13\n    vpshufb %ymm13, %ymm6, %ymm13\n    vpaddb  %ymm12, %ymm13, %ymm12\n    vpunpckhdq  %ymm1, %ymm12, %ymm13   # ymm13 = ymm12[2],ymm1[2],ymm12[3],ymm1[3],ymm12[6],ymm1[6],ymm12[7],ymm1[7]\n    vpsadbw %ymm1, %ymm13, %ymm13\n    vpunpckldq  %ymm1, %ymm12, %ymm12   # ymm12 = ymm12[0],ymm1[0],ymm12[1],ymm1[1],ymm12[4],ymm1[4],ymm12[5],ymm1[5]\n    vpsadbw %ymm1, %ymm12, %ymm12\n    vpackuswb   %ymm13, %ymm12, %ymm12\n    vpaddd  %ymm2, %ymm0, %ymm13\n    vpaddd  %ymm12, %ymm8, %ymm8\n    vpand   %ymm5, %ymm13, %ymm12\n    vpshufb %ymm12, %ymm6, %ymm12\n    vpsrlw  $4, %ymm13, %ymm13\n    vpand   %ymm5, %ymm13, %ymm13\n    vpshufb %ymm13, %ymm6, %ymm13\n    vpaddb  %ymm12, %ymm13, %ymm12\n    vpunpckhdq  %ymm1, %ymm12, %ymm13   # ymm13 = ymm12[2],ymm1[2],ymm12[3],ymm1[3],ymm12[6],ymm1[6],ymm12[7],ymm1[7]\n    vpsadbw %ymm1, %ymm13, %ymm13\n    vpunpckldq  %ymm1, %ymm12, %ymm12   # ymm12 = ymm12[0],ymm1[0],ymm12[1],ymm1[1],ymm12[4],ymm1[4],ymm12[5],ymm1[5]\n    vpsadbw %ymm1, %ymm12, %ymm12\n    vpackuswb   %ymm13, %ymm12, %ymm12\n    vpaddd  %ymm3, %ymm0, %ymm13\n    vpaddd  %ymm12, %ymm9, %ymm9\n    vpand   %ymm5, %ymm13, %ymm12\n    vpshufb %ymm12, %ymm6, %ymm12\n    vpsrlw  $4, %ymm13, %ymm13\n    vpand   %ymm5, %ymm13, %ymm13\n    vpshufb %ymm13, %ymm6, %ymm13\n    vpaddb  %ymm12, %ymm13, %ymm12\n    vpunpckhdq  %ymm1, %ymm12, %ymm13   # ymm13 = ymm12[2],ymm1[2],ymm12[3],ymm1[3],ymm12[6],ymm1[6],ymm12[7],ymm1[7]\n    vpsadbw %ymm1, %ymm13, %ymm13\n    vpunpckldq  %ymm1, %ymm12, %ymm12   # ymm12 = ymm12[0],ymm1[0],ymm12[1],ymm1[1],ymm12[4],ymm1[4],ymm12[5],ymm1[5]\n    vpsadbw %ymm1, %ymm12, %ymm12\n    vpackuswb   %ymm13, %ymm12, %ymm12\n    vpaddd  %ymm4, %ymm0, %ymm13\n    vpaddd  %ymm12, %ymm10, %ymm10\n    vpand   %ymm5, %ymm13, %ymm12\n    vpshufb %ymm12, %ymm6, %ymm12\n    vpsrlw  $4, %ymm13, %ymm13\n    vpand   %ymm5, %ymm13, %ymm13\n    vpshufb %ymm13, %ymm6, %ymm13\n    vpaddb  %ymm12, %ymm13, %ymm12\n    vpunpckhdq  %ymm1, %ymm12, %ymm13   # ymm13 = ymm12[2],ymm1[2],ymm12[3],ymm1[3],ymm12[6],ymm1[6],ymm12[7],ymm1[7]\n    vpsadbw %ymm1, %ymm13, %ymm13\n    vpunpckldq  %ymm1, %ymm12, %ymm12   # ymm12 = ymm12[0],ymm1[0],ymm12[1],ymm1[1],ymm12[4],ymm1[4],ymm12[5],ymm1[5]\n    vpsadbw %ymm1, %ymm12, %ymm12\n    vpackuswb   %ymm13, %ymm12, %ymm12\n    vpaddd  %ymm12, %ymm11, %ymm11\n    vpaddd  %ymm7, %ymm0, %ymm0\n    addl    $-32, %edx\n    jne .LBB1_3\n\n```\n\nHere is the one produced by Numba:\n\n\n\n```\n.LBB0_8:\n    vpand   %ymm0, %ymm9, %ymm6\n    vpshufb %ymm6, %ymm10, %ymm6\n    vpsrlw  $4, %ymm0, %ymm7\n    vpand   %ymm7, %ymm9, %ymm7\n    vpshufb %ymm7, %ymm10, %ymm7\n    vpaddb  %ymm6, %ymm7, %ymm6\n    vpaddq  -40(%rsp), %ymm0, %ymm7\n    vpsadbw %ymm5, %ymm6, %ymm6\n    vpaddq  %ymm6, %ymm1, %ymm1\n    vpand   %ymm7, %ymm9, %ymm6\n    vpshufb %ymm6, %ymm10, %ymm6\n    vpsrlw  $4, %ymm7, %ymm7\n    vpand   %ymm7, %ymm9, %ymm7\n    vpshufb %ymm7, %ymm10, %ymm7\n    vpaddb  %ymm6, %ymm7, %ymm6\n    vpaddq  -72(%rsp), %ymm0, %ymm7\n    vpsadbw %ymm5, %ymm6, %ymm6\n    vpaddq  %ymm6, %ymm2, %ymm2\n    vpand   %ymm7, %ymm9, %ymm6\n    vpshufb %ymm6, %ymm10, %ymm6\n    vpsrlw  $4, %ymm7, %ymm7\n    vpand   %ymm7, %ymm9, %ymm7\n    vpshufb %ymm7, %ymm10, %ymm7\n    vpaddb  %ymm6, %ymm7, %ymm6\n    vpaddq  %ymm0, %ymm8, %ymm7\n    vpsadbw %ymm5, %ymm6, %ymm6\n    vpaddq  %ymm6, %ymm3, %ymm3\n    vpand   %ymm7, %ymm9, %ymm6\n    vpshufb %ymm6, %ymm10, %ymm6\n    vpsrlw  $4, %ymm7, %ymm7\n    vpand   %ymm7, %ymm9, %ymm7\n    vpshufb %ymm7, %ymm10, %ymm7\n    vpaddb  %ymm6, %ymm7, %ymm6\n    vpsadbw %ymm5, %ymm6, %ymm6\n    vpaddq  %ymm6, %ymm4, %ymm4\n    vpaddq  %ymm0, %ymm11, %ymm6\n    vpand   %ymm6, %ymm9, %ymm7\n    vpshufb %ymm7, %ymm10, %ymm7\n    vpsrlw  $4, %ymm6, %ymm6\n    vpand   %ymm6, %ymm9, %ymm6\n    vpshufb %ymm6, %ymm10, %ymm6\n    vpaddb  %ymm7, %ymm6, %ymm6\n    vpaddq  %ymm0, %ymm12, %ymm7\n    vpsadbw %ymm5, %ymm6, %ymm6\n    vpaddq  %ymm6, %ymm1, %ymm1\n    vpand   %ymm7, %ymm9, %ymm6\n    vpshufb %ymm6, %ymm10, %ymm6\n    vpsrlw  $4, %ymm7, %ymm7\n    vpand   %ymm7, %ymm9, %ymm7\n    vpshufb %ymm7, %ymm10, %ymm7\n    vpaddb  %ymm6, %ymm7, %ymm6\n    vpaddq  %ymm0, %ymm13, %ymm7\n    vpsadbw %ymm5, %ymm6, %ymm6\n    vpaddq  %ymm6, %ymm2, %ymm2\n    vpand   %ymm7, %ymm9, %ymm6\n    vpshufb %ymm6, %ymm10, %ymm6\n    vpsrlw  $4, %ymm7, %ymm7\n    vpand   %ymm7, %ymm9, %ymm7\n    vpshufb %ymm7, %ymm10, %ymm7\n    vpaddb  %ymm6, %ymm7, %ymm6\n    vpaddq  %ymm0, %ymm14, %ymm7\n    vpsadbw %ymm5, %ymm6, %ymm6\n    vpaddq  %ymm6, %ymm3, %ymm3\n    vpand   %ymm7, %ymm9, %ymm6\n    vpshufb %ymm6, %ymm10, %ymm6\n    vpsrlw  $4, %ymm7, %ymm7\n    vpand   %ymm7, %ymm9, %ymm7\n    vpshufb %ymm7, %ymm10, %ymm7\n    vpaddb  %ymm6, %ymm7, %ymm6\n    vpsadbw %ymm5, %ymm6, %ymm6\n    vpaddq  %ymm6, %ymm4, %ymm4\n    vpaddq  %ymm0, %ymm15, %ymm0\n    addq    $-2, %rbx\n    jne .LBB0_8\n\n```\n\n\n\n---\n\n\n## Analysis\n\n\nFirst of all, we can see that the GCC code use the `popcntl` instruction which is very fast, at least for **scalar** operations.\n\n\nClang generate a assembly code using the **AVX-2 SIMD instruction set** on my machine. This is why the program produced by Clang is so fast compared to GCC : it operates on many items in parallel thanks to SIMD instructions.\n\n\nNumba generate a code very similar to Clang. This is not surprising since Numba is based on LLVM-Lite (and so LLVM), while Clang is also based on LLVM. However, there are small differences explaining the performance impact. Indeed, the Numba assembly code operates on twice more items than the Clang counterpart. This can be seen by counting the number of `vpsrlw` instructions (8 VS 4). I do not expect this to make the difference since the Clang loop is already well unrolled and the benefit of unrolling it more is tiny. Actually, this more aggressive unrolling is a side effect. The key difference is that **Numba operates on 64-bit integers while the C code operates on 32-bit integers**! This is why Clang unroll the loop differently and generate different instructions. In fact, the smaller integers causes Clang to generate a sequence of instructions to convert integers of different size which is less efficient. IMHO, this is a side effect impacting the optimizer since operating on smaller items can generally be used generate faster SIMD code. The code produced by LLVM seems sub-optimal in this case : it saturates the port 5 (ie. shuffle\/permute execution unit) on my machine while one can write a code not saturating it (not easy though).\n\n\n\n\n---\n\n\n## Faster C implementation\n\n\nYou can **fix the C++ implementation so to operate 64-bit integers**:\n\n\n\n```\n#include <stdio.h>\n#include <time.h>\n#include <stdint.h>\n\n\/\/ Function to calculate the population count (number of set bits) of an integer using __builtin_popcount\nuint64_t popcount(uint64_t num) {\n    return __builtin_popcountl(num);\n}\n\nint main() {\n    int64_t n;\n    printf(\"Enter the value of n: \");\n    scanf(\"%ld\", &n);\n\n    \/\/ Variables to store start and end times\n    struct timespec start_time, end_time;\n\n    \/\/ Get the current time as the start time\n    clock_gettime(CLOCK_MONOTONIC, &start_time);\n\n    int64_t sum = 0;\n    for (int64_t i = 0; i < n; i++) {\n        sum += popcount(i);\n    }\n\n    \/\/ Get the current time as the end time\n    clock_gettime(CLOCK_MONOTONIC, &end_time);\n\n    \/\/ Calculate the elapsed time in microseconds\n    long long elapsed_time = (end_time.tv_sec - start_time.tv_sec) * 1000000LL +\n                            (end_time.tv_nsec - start_time.tv_nsec) \/ 1000;\n\n    printf(\"Sum of population counts from 0 to %ld-1 is: %ld\\n\", n, sum);\n    printf(\"Elapsed time: %lld microseconds\\n\", elapsed_time);\n\n    return 0;\n}\n\n```\n\nThis produces a program **as fast as Numba** on my machine using Clang (GCC still generate a slow scalar implementation).\n\n\n\n\n---\n\n\n## Notes\n\n\nSIMD versions only make sense if your real-world code is SIMD-friendly, that is if `popcount` can be applied on **multiple contiguous items**. Otherwise, results of the scalar implementation can be drastically different (in fact, the three compilers generate a very close code which I expect to be equally fast).\n\n\nAVX-512 provides the **SIMD instruction `VPOPCNTDQ`** that should clearly outperforms the code generated by LLVM using (only) AVX-2.. Since I do not have AVX-512 on my machine, and AVX-2 does not provide such an instruction, it makes sense of LLVM to produce an assembly code using AVX-2. The AVX-512 instruction can count the number of 1 in 16 x 32-bit integers in parallel while taking about the same number of cycle than its scalar counterpart. To be more precise, the instruction is only available using the instruction set `AVX512VPOPCNTDQ` + `AVX512VL` (which AFAIK is not available on all CPU supporting AVX-512). As of now, this instruction is only available on few x86-64 micro-architectures (eg. Intel Ice-Lake, Intel Sapphire Rapids and AMD Zen4)."}
{"questionId":"beed0f2d82184401832b407ac32ecc51","question":"Credential Manager - How do I create a \"SignInWithGoogle\" credential?\nI'm trying to set up Google's One-Tap but with the new, all-in-one, Credential Manager.\n\n\nHowever, after crawling through the (badly written) documentation, I've come to a halt. Upon \"Signing in with Google\", everything is fine until I get a \"NoCredentialException: no credentials available\", which makes sense.\n\n\nBut then... how do I create a credential?\n\n\nGoogle provides examples to create credentials for both [passwords](https:\/\/developer.android.com\/training\/sign-in\/passkeys#save-password) and [passkeys](https:\/\/developer.android.com\/training\/sign-in\/passkeys#create-passkey) but I can't find any information on creating credentials for the \"Sign In With Google\" button (anywhere on the internet).\n\n\n\n> \n> \"The Sign In With Google Button is supported by Credential Manager with the newest Google ID helper library\"\n> \n> \n> \n\n\n([As stated by Android](https:\/\/developer.android.com\/training\/sign-in\/credential-manager#siwg-button))\n\n\nSo, I use `<CredentialManager>.createCredentialAsync()` because that's what Google used during the examples they provided (and explicitly told to do [here](https:\/\/developer.android.com\/reference\/androidx\/credentials\/exceptions\/NoCredentialException)).\n\n\nHowever, Android's createCredentialAsync requires a `CreateCredentialRequest` and there are only three types that it accepts: \"CreatePasswordRequest\", \"CreatePublicKeyCredentialRequest\" and \"CreateCustomCredentialRequest\".\n\n\nThis is where \"Google's [ID helper library](https:\/\/developers.google.com\/identity\/android-credential-manager\/android\/reference\/com\/google\/android\/libraries\/identity\/googleid\/package-summary.html)\" mentioned in the quote above is supposed to come in handy. The library has the classes\nGetGoogleIdOption and GetSignInWithGoogleOption which are both subclasses of [GetCustomCredentialOption](https:\/\/developer.android.com\/reference\/androidx\/credentials\/GetCustomCredentialOption).\n\n\n**The question** now is how am I supposed to get myself a `CreateCustomCredentialRequest` class (or a subclass of it) for my `<CredentialManager>.createCredentialAsync()` method.\n\n\nGoogle's \"newest ID helper library\" doesn't provide:\n\n\n1. A subclass of CustomCredential & it's Builder for SignInWithGoogle ([it does](https:\/\/developers.google.com\/identity\/android-credential-manager\/android\/reference\/com\/google\/android\/libraries\/identity\/googleid\/GoogleIdTokenCredential) for GoogleIdToken)\n2. A ridiculously long `CreateSignInWithGoogleRequest` class (or a `CreateGoogleIdRequest` class) that's a subclass of the `CreateCustomCredentialRequest` class.\n\n\nTherefore, since I'm stuck on how I'm supposed to get this `CreateCustomCredentialRequest`, I'm not sure how I'm supposed to [\"Integrate Credential Manager with Sign in with Google\"](https:\/\/developer.android.com\/training\/sign-in\/credential-manager) either.\n\n\nBefore I end, I want to mention one last thing. In the [\"Sign up with Google\" section](https:\/\/developer.android.com\/training\/sign-in\/credential-manager#sign-up), it says:\n\n\n\n> \n> If no results are returned after setting setFilterByAuthorizedAccounts to true while instantiating the GetGoogleIdOption request and passing it to GetCredentialsRequest, it indicates that there are no authorized accounts to sign in. At this point, you should set setFilterByAuthorizedAccounts(false) and **call Sign up with Google**.\n> \n> \n> \n\n\nThis doesn't help me because:\n\n\n1. This only references `GetGoogleIdOption` and not `GetSignInWithGoogleOption`.\n2. There's no explanation on how to \"call Sign up with Google\".\n\n\nAfterwards, it says:\n\n\n\n> \n> Once you instantiate the Google sign-up request, launch the authentication flow similarly as mentioned in the Sign in with Google section.\n> \n> \n> \n\n\nIs there supposed to be a `GetGoogleSignUpRequest` class?\n\n\nIs there anything I'm missing? Did I make a stupid mistake somewhere? Any help on this would be great!\n\n\nFor extra context, I've provided the entirety of my code here: <https:\/\/www.online-java.com\/VjQw6cTKig>","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"java"},"answer":"Let me try to clarify a few things and try to answer your questions and issues. From the original post, it seems to me that you're trying to sign in your users using Google ID tokens, please correct me if I am wrong. In that case, you don't need to \"create\" a Google credential. When you try to sign in with a Google account, you rely on the Google accounts that are on the Android device. If there are no Google accounts on the device (or there are, but they need reauthentication, for example if their passwords were changed elsewhere), then the user needs to add a Google account to the device, or reauth the existing accounts first. Now, assuming there are working Google accounts on the device, you can use the Credential Manager APIs (similar to the One Tap APIs) to help users sign-in to your app; the flow is outlined in details [here](https:\/\/developer.android.com\/training\/sign-in\/credential-manager) but I will briefly go through them to make it clear.\n\n\nAfter declaring the dependencies (remember that you'd need to include the dependency on the CredentialManager AND the `com.google.android.libraries.identity.googleid` to be able to use Google ID tokens for your sign-in. Then the flow goes as:\n\n\n1. Build a request:\n\n\n\n```\nGetGoogleIdOption googleIdOption = new GetGoogleIdOption.Builder()\n   .setFilterByAuthorizedAccounts(true)\n   .setServerClientId(WEB_CLIENT_ID)\n   .setNonce(NONCE)\n   .build();\nGetCredentialRequest request = new GetCredentialRequest.Builder()\n  .addCredentialOption(googleIdOption)\n  .build();\n\n```\n2. Make the API call:\n\n\n\n```\ncredentialManager.getCredentialAsync(\n  requireActivity(),\n  request,\n  cancellationSignal,\n  <executor>,\n  new CredentialManagerCallback<GetCredentialResponse, GetCredentialException>() {\n@Override\npublic void onResult(GetCredentialResponse result) {\n  handleSignIn(result);\n}\n\n@Override\npublic void onError(GetCredentialException e) {\n  handleFailure(e);\n}\n\n```\n\n}\n);\n3. Retrieve the credentials that the user selected:\n\n\n\n```\npublic void handleSignIn(GetCredentialResponse result) {\n   Credential credential = result.getCredential();\n\n   if (credential instanceof CustomCredential) {\n     if (GoogleIdTokenCredential.TYPE_GOOGLE_ID_TOKEN_CREDENTIAL.equals(credential.getType())) {\n       try {\n         GoogleIdTokenCredential googleIdTokenCredential = GoogleIdTokenCredential.createFrom(((CustomCredential) credential).getData());\n       } catch (GoogleIdTokenParsingException e) {\n         Log.e(TAG, \"Received an invalid google id token response\", e);\n       }\n    }\n\n```\n\n\nNow a few important things that may help with your app and also user experience:\n\n\n- In step 1 above, you set the `filterByAuthorizedAccounts` to `true`. That means users will see only those accounts that have already been used for your app and have given grants for sharing the usual profileId, email, and such with your app. The reason we insist on making the first call with that setting is the following: if your user has, say, 2 Google accounts on the device and had previously used the first account successfully to sign into your app, in subsequent sessions, you probably want the same user to pick the same Google account to avoid creating a duplicate account by choosing the second account. This can be accomplished by setting that attribute to true: it only shows those accounts that have been used before. If such an account exists and the user selects that, the user basically \"signs-in\" back to your app (see the next bullet point for \"sign-up\").\n- If you get \"No credentials available\" response by following the above steps, you then make a second call but this time, in your request, you set the `filterByAuthorizedAccounts` to `false`. This means: \"show all the Google accounts on the device to the user\". In most cases, it means that this user is a new user for your app, so it amounts to basically \"Signing-up\" with your app.\n- You may get the \"No credentials available\" response again; this would most likely mean that there are no Google accounts on the device (see my earlier comments).\n- **Very importantly**, we strongly recommend that the above API calls be made in your app as soon as the user lands on your app, without any user interaction (i.e without a need for the user to click on a button). This is very different from making these calls if a user taps on the \"Sign-in with Google\" button; there is a different API (exposed through the CredentialManager as well) that I will explain below briefly, that you can call when you want to react to tapping on \"Sign-in with Google\" but the original steps outlined above result in showing a bottomsheet to the user while the proper treatment of clicking on a \"Sign-in with Google\" button should pop-up a regular dialog. The bottomsheet flow can also be called if, for example, users of your app can do certain things in your app without signing-in to your app (say, some free content) and then when they want to access, say, a paid content, you can programmatically call the above APIs to show a bottomsheet before they can move forward.\n- Using the CredentialManager APIs, similar to the older One Tap APIs, you can easily include more types of credentials; for example you can consider adding passkeys as another secure authentication method to your app.\n\n\n**How to use the APIs for \"Sign-in with Google\" button.**\n\n\nTo do this, you can use the following steps:\n\n\n1. You create the same type of request but this time you use [GetSignInWithGoogleOption](https:\/\/developers.google.com\/identity\/android-credential-manager\/android\/reference\/com\/google\/android\/libraries\/identity\/googleid\/GetSignInWithGoogleOption) class to build the options, using its [Builder](https:\/\/developers.google.com\/identity\/android-credential-manager\/android\/reference\/com\/google\/android\/libraries\/identity\/googleid\/GetSignInWithGoogleOption.Builder).\n2. The rest will be identical to the previous case: you call the same API (using the new options) and you extract the returned credential through the same process.\n\n\nThere are a few differences worth noting when using the \"button\" flow:\n\n\n- As was mentioned above, the UI shown to the user is different; instead of a bottomsheet, it will be a regular dialog.\n- It allows your users to add a Google account to their devices if there is none (or if there are some but they don't see the account they are interested in), and also allows your users to re-authenticate their accounts, if needed. These features are missing from the bottomsheet flow intentionally: tapping on a \"Sign-in with Google\" button clearly shows that the intention of the user is to use Google accounts so if there is none, it makes sense to help them create one but in the bottomsheet flow, since that should pop-up automatically by developer when users land on the app (based on our strong recommendation), it is less clear if the user had intended to do so or not.\n- You cannot mix in other types of credentials in your request."}
{"questionId":"5ab67b1f3a8d4e609758c92c28959c26","question":"Efficiently enumerating multinomials with constant sum - R\nLet's say I have an N-side dice with non-uniform probabilities for each side and I throw it M times. Now instead of observing the individual outcomes, we only observe the sum.\n\n\nI have to code the likelihood where I have to sum over the multinomial likelihood components restricted to having that observed sum.\n\n\nIf N=3, M = 2 and the sum is 4, than it is clear that I have to sum over the two cases where one of the throws is 1 and the other 3 plus the case where both are 2.\n\n\nI could also enumerate all possibilities, calculate the sum and restrict the calculation to the combinations with the sum I'm interested in, but clearly that becomes intractable quite quickly with increasing N and M.\n\n\nSo I am looking for an efficient approach to select the constant-sum combinations in R.","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"r"},"answer":"One option is to use `RcppAlgos::compositionsGeneral()` which employs 'efficient algorithms for partitioning numbers under various constraints'.\n\n\n\n```\nlibrary(RcppAlgos)\n\ncompositionsGeneral(3, 2, repetition = TRUE, target = 4)\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    2\n[3,]    3    1\n\n```\n\nAs @ThomasIsCoding has pointed out, this approach can fail with the message:\n\n\n\n```\ncompositionsGeneral(3, 6, repetition = TRUE, target = 10)\n\nError: Currently, there is no composition algorithm for this case.\n Use permuteCount, permuteIter, permuteGeneral, permuteSample, or\n permuteRank instead.\n\n```\n\nSo to deal with this, we can catch errors and fall back on `permuteGeneral()` with constraints in this event:\n\n\n\n```\ncomps <- \\(v, m, x) {\n  tryCatch(\n    compositionsGeneral(v,\n                        m,\n                        repetition = TRUE,\n                        target = x),\n    error = function(e)\n      permuteGeneral(v,\n                     m,\n                     repetition = TRUE,\n                     constraintFun = \"sum\",\n                     comparisonFun = \"==\",\n                     limitConstraints = x\n      )\n  )\n}\n\n\ncomps(3, 6, 10)\n\n      [,1] [,2] [,3] [,4] [,5] [,6]\n [1,]    1    1    1    1    3    3\n [2,]    1    1    1    3    1    3\n [3,]    1    1    1    3    3    1\n [4,]    1    1    3    1    1    3\n [5,]    1    1    3    1    3    1\n...\n[85,]    2    2    1    1    2    2\n[86,]    2    2    1    2    1    2\n[87,]    2    2    1    2    2    1\n[88,]    2    2    2    1    1    2\n[89,]    2    2    2    1    2    1\n[90,]    2    2    2    2    1    1\n\n```\n\nNote that the documentation includes the following about calculating permutations with contraints:\n\n\n\n> \n> Finding all combinations\/permutations with constraints is optimized by\n> organizing them in such a way that when constraintFun is applied, a\n> partially monotonic sequence is produced. Combinations\/permutations\n> are added successively, until a particular combination exceeds the\n> given constraint value for a given constraint\/comparison function\n> combo. After this point, we can safely skip several combinations\n> knowing that they will exceed the given constraint value.\n> \n> \n>"}
{"questionId":"cf045e663d7d4122aef09a4d55029b53","question":"Mapping a List with a Hash argument\nIn the docs for [class List](https:\/\/docs.raku.org\/type\/List#routine_map), it says:\n\n\n\n```\nroutine map\n\nmulti method map(Hash:D \\hash)  \nmulti method map(Iterable:D \\iterable)   \nmulti method map(|c) \nmulti method map(\\SELF: &block;; :$label, :$item)   multi sub map(&code, +values)\nmulti sub map(&code, +values)\n\n```\n\nIn the first multi method clause, I think the signature says that the method takes one argument, which is an instance of the Hash class (Hash:D). The argument will be assigned to a sigil-less variable \\hash (which is a constant), which means you know the hash in the caller cannot be changed by the method.\n\n\nBased on the method name, map, and the hash argument, it seems likely that the hash will map the elements of a list to the corresponding values in the hash. Let's try it:\n\n\n\n```\n[106] > my %hash = %{1 => 'apple', 2 => 'tree', 3 => 'octopus'};\n{1 => apple, 2 => tree, 3 => octopus}\n\n[107] > (1, 2, 3).map(%hash);\nCannot map a List using a Hash\nDid you mean to add a stub ({ ... }) or did you mean to .classify?\n  in block <unit> at <unknown file> line 1\n  in any <main> at \/usr\/local\/bin\/rakudo-moar-2024.01-01-macos-arm64-clang\/bin\/..\/share\/perl6\/runtime\/perl6.moarvm line 1\n  in any <entry> at \/usr\/local\/bin\/rakudo-moar-2024.01-01-macos-arm64-clang\/bin\/..\/share\/perl6\/runtime\/perl6.moarvm line 1\n\n[107] > \n\n```\n\n*Cannot map a List using a Hash*. Whaaa?! How can I call `$list.map(%some_hash)`?","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"perl"},"answer":"This looks like a case where Raku tried to be incredibly helpful and it backfired. The relevant overloads for Raku's `map` are defined [here](https:\/\/github.com\/rakudo\/rakudo\/blob\/3d1820f71b9a2f12bff377bf9336c6cdf8c911bb\/src\/core.c\/Any-iterable-methods.rakumod#L885-L909).\n\n\n\n```\nproto method map(|) is nodal {*}\nmulti method map(Hash:D \\hash) {\n    X::Cannot::Map.new(\n      what       => self.^name,\n      using      => \"a {hash.^name}\",\n      suggestion =>\n\"Did you mean to add a stub (\\{ ... \\}) or did you mean to .classify?\"\n    ).throw;\n}\nmulti method map(Iterable:D \\iterable) {\n    X::Cannot::Map.new(\n      what       => self.^name,\n      using      => \"a {iterable.^name}\",\n      suggestion =>\n\"Did a * (Whatever) get absorbed by a comma, range, series, or list repetition?\nConsider using a block if any of these are necessary for your mapping code.\"\n    ).throw;\n}\nmulti method map(|c) {\n    X::Cannot::Map.new(\n      what       => self.^name,\n      using      => \"'{c.raku.substr(2).chop}'\",\n      suggestion => \"Did a * (Whatever) get absorbed by a list?\"\n    ).throw;\n}\n\n```\n\nThat is, all of the overloads for `map` *except* the one taking a `&code` object just produce specific, helpful error messages like the one you saw. Unfortunately, those overloads got picked up by some sort of (presumably) automated documentation tool and added to the docs, creating more confusion.\n\n\nIt seems to be that these error-only overloads should probably be hidden from the documentation, to prevent exactly this sort of thing from tripping folks up."}
{"questionId":"1794cb3d6cb349c29fde70e09ced62cd","question":"Missing Arabic numbers when formatting date using IntlDateFormatter on PHP 8.2 and newest Alpine Linux 3.19\nI am trying to update my PHP project from PHP 7.4 to 8.2. I have an issue with formatting date using `IntlDateFormatter` class for `ar_AE` locale under PHP 8.2 Alpine 3.19 version. On my previous setup (PHP 7.4 Alpine 3.12) formatting date was returning whole string of Arabic characters, but under 8.2 this is not happening, in output I have Arabic letters, not numbers.\n\n\nDuring work I have found that ICU data has been split (<https:\/\/wiki.alpinelinux.org\/wiki\/Release_Notes_for_Alpine_3.16.0#ICU_data_split>) in Linux Alpine 3.16 so I have added `icu-libs` and `icu-data-full` to 8.2 Dockerfile. This has solved my problem partially, as numbers are still not displayed in `ar_AE` locale.\n\n\nOutput for PHP 7.4:\n\n\n\n```\n\/var\/www $ php test.php\nar_AE: \u0627\u0644\u0633\u0628\u062a\u060c \u0661 \u064a\u0646\u0627\u064a\u0631 \u0662\u0660\u0662\u0662 \u0661\u0662:\u0665\u0669:\u0665\u0669 \u0635 \u062a\u0648\u0642\u064a\u062a \u0648\u0633\u0637 \u0623\u0648\u0631\u0648\u0628\u0627 \u0627\u0644\u0631\u0633\u0645\u064a\nar_QA: \u0627\u0644\u0633\u0628\u062a\u060c \u0661 \u064a\u0646\u0627\u064a\u0631 \u0662\u0660\u0662\u0662 \u0661\u0662:\u0665\u0669:\u0665\u0669 \u0635 \u062a\u0648\u0642\u064a\u062a \u0648\u0633\u0637 \u0623\u0648\u0631\u0648\u0628\u0627 \u0627\u0644\u0631\u0633\u0645\u064a\nar_SA: \u0627\u0644\u0633\u0628\u062a\u060c \u0661 \u064a\u0646\u0627\u064a\u0631 \u0662\u0660\u0662\u0662 \u0645 \u0661\u0662:\u0665\u0669:\u0665\u0669 \u0635 \u062a\u0648\u0642\u064a\u062a \u0648\u0633\u0637 \u0623\u0648\u0631\u0648\u0628\u0627 \u0627\u0644\u0631\u0633\u0645\u064a\n\n```\n\nOutput for PHP 8.2:\n\n\n\n```\n\/var\/www $ php test.php\nar_AE: \u0627\u0644\u0633\u0628\u062a\u060c 1 \u064a\u0646\u0627\u064a\u0631 2022 \u0641\u064a 12:59:59 \u0635 \u062a\u0648\u0642\u064a\u062a \u0648\u0633\u0637 \u0623\u0648\u0631\u0648\u0628\u0627 \u0627\u0644\u0631\u0633\u0645\u064a\nar_QA: \u0627\u0644\u0633\u0628\u062a\u060c \u0661 \u064a\u0646\u0627\u064a\u0631 \u0662\u0660\u0662\u0662 \u0641\u064a \u0661\u0662:\u0665\u0669:\u0665\u0669 \u0635 \u062a\u0648\u0642\u064a\u062a \u0648\u0633\u0637 \u0623\u0648\u0631\u0648\u0628\u0627 \u0627\u0644\u0631\u0633\u0645\u064a\nar_SA: \u0627\u0644\u0633\u0628\u062a\u060c \u0661 \u064a\u0646\u0627\u064a\u0631 \u0662\u0660\u0662\u0662 \u0645 \u0641\u064a \u0661\u0662:\u0665\u0669:\u0665\u0669 \u0635 \u062a\u0648\u0642\u064a\u062a \u0648\u0633\u0637 \u0623\u0648\u0631\u0648\u0628\u0627 \u0627\u0644\u0631\u0633\u0645\u064a\n\n```\n\nSo my question is simple, why on 8.2 PHP Alpine Linux 3.19 I still have non-Arabic numbers in output for `ar_AE` locale? What am I missing? Is there any other dependency that I should have install to achieve that? Maybe that is correct output and I should adjust my phpunit tests?\n\n\nThis is a content of `test.php` file which I used on both Docker images in order to test output of formatting:\n\n\n\n```\n<?php\n\n$formatter = new IntlDateFormatter(\n    'ar_AE',\n    IntlDateFormatter::FULL,\n    IntlDateFormatter::FULL,\n    'Europe\/Berlin'\n);\n\necho 'ar_AE: ' . $formatter->format(new DateTime('2021-12-31 23:59:59'));\necho PHP_EOL;\n\n$formatter = new IntlDateFormatter(\n    'ar_QA',\n    IntlDateFormatter::FULL,\n    IntlDateFormatter::FULL,\n    'Europe\/Berlin'\n);\n\necho 'ar_QA: ' . $formatter->format(new DateTime('2021-12-31 23:59:59'));\necho PHP_EOL;\n\n$formatter = new IntlDateFormatter(\n    'ar_SA',\n    IntlDateFormatter::FULL,\n    IntlDateFormatter::FULL,\n    'Europe\/Berlin'\n);\n\necho 'ar_SA: ' . $formatter->format(new DateTime('2021-12-31 23:59:59'));\necho PHP_EOL;\n\n```\n\nDockerfile to reproduce behaviour under 7.4:\n\n\n\n```\nFROM php:7.4-fpm-alpine3.12\n\nRUN apk update \\\n    && apk --no-cache add \\\n    freetype  \\\n    icu \\\n    icu-dev\n\nRUN docker-php-ext-install intl\n\nUSER www-data\nWORKDIR \/var\/www\n\n```\n\nINTL version after installation:\n\n\n\n```\nInternationalization support => enabled\nICU version => 67.1\nICU Data version => 67.1\nICU TZData version => 2019c\nICU Unicode version => 13.0\n\n```\n\nDockerfile to reproduce behaviour under 8.2:\n\n\n\n```\nFROM php:8.2-fpm-alpine3.19\n\nRUN apk update \\\n    && apk --no-cache add \\\n    freetype  \\\n    icu \\\n    icu-dev \\\n    icu-libs \\\n    icu-data \\\n    icu-data-full\n\nRUN docker-php-ext-install intl\n\nUSER www-data\nWORKDIR \/var\/www\n\n```\n\nINTL version after installation:\n\n\n\n```\nInternationalization support => enabled\nICU version => 74.1\nICU Data version => 74.1\nICU TZData version => 2023c\nICU Unicode version => 15.1\n\n```\n\nI was also installing additional fonts, with no luck:\n\n\n\n```\nfont-arabic-misc \\\nfont-noto-arabic \\\nmusl-locales \\\nfontconfig \\\nttf-dejavu \\\nfont-noto font-noto-extra font-arabic-misc \\\nfont-misc-cyrillic font-mutt-misc font-screen-cyrillic font-winitzki-cyrillic font-cronyx-cyrillic \\\nfont-noto-armenian font-noto-cherokee font-noto-devanagari font-noto-ethiopic font-noto-georgian \\\nfont-noto-hebrew font-noto-lao font-noto-malayalam font-noto-tamil font-noto-thaana font-noto-thai \\\nfreetype ttf-droid ttf-freefont ttf-liberation freetype-dev","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"Considering it works in other locales but not that specific one, I suspect that locale expects latin numbers in dates. This [post on the Apple developer forum](https:\/\/forums.developer.apple.com\/forums\/thread\/715978) confirmed my suspicion:\n\n\n\n> \n> Some regions prefer to use Western digits rather than Arabic-Indic digits, so you'll see this discrepancy for these locales:\n> \n> \n> ar\\_AE ar\\_DZ ar\\_EH ar\\_LY ar\\_MA ar\\_TN\n> \n> \n> \n\n\nIf you really need to use ar\\_AE, though it's not recommended to modify the expected behavior, you can force using Arabic numbers by adding a modifier to the locale:\n\n\n\n> \n> `ar_AE@numbers=arab`\n> \n> \n>"}
{"questionId":"88b54b8bc01341f296373beca2399d76","question":"GCloud Build Failure: ERROR: failed to initialize analyzer, No such object\nMy most recent builds with GCloud all started failing recently and I am not really sure why. I keep getting 404 errors from the build attempting to find an image that doesn't exist.\n\n\nSample error:\n\n\n\n```\nAlready have image (with digest): us.gcr.io\/gae-runtimes\/buildpacks\/google-gae-22\/python\/builder:python_20230925_RC00\n===> ANALYZING\nERROR: failed to initialize analyzer: getting previous image: getting config file for image \"us.gcr.io\/growth-ops-apps\/app-engine-tmp\/app\/default\/ttl-18h:latest\": GET https:\/\/storage.googleapis.com\/us.artifacts.growth-ops-apps.appspot.com\/containers\/images\/sha256:d13213796512322314e6db634cc57318665191dc5437c121f151cb33310c552e?access_token=REDACTED: unexpected status code 404 Not Found: <?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey<\/Code><Message>The specified key does not exist.<\/Message><Details>No such object: us.artifacts.growth-ops-apps.appspot.com\/containers\/images\/sha256:d13213796512322314e6db634cc57318665191dc5437c121f151cb33310c552e<\/Details><\/Error>\n\n```\n\nI can verify that the bucket `us.artifacts.growth-ops-apps.appspot.com` is empty in the console, but I have no idea why. There is no bucket retention policy or anything I can see that is obvious to me.\n\n\nFor reference, I have a single GitHub repository set to trigger builds to 2 separate projects, one for development and one for production. Only the production one is failing in this way.\n\n\nI am using a cloudbuild.yaml file stored in the repository.\n\n\nAny ideas?","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"other"},"answer":"I figured it out! In thinking through one of my own comments, `the build is looking for an artifact that doesn't exist`, I figured there must be some way of forcing a new build. I found the `--no-cache` flag in the [docs](https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/app\/deploy), which forced a brand new build instead of attempting to use information from a non-existent prior build."}
{"questionId":"44c0431870a54a4795a6b96515d95ed9","question":"Why does Data.Dynamic contain a witness instead of a typeclass constraint?\n[`Data.Dynamic`](https:\/\/hackage.haskell.org\/package\/base-4.19.0.0\/docs\/Data-Dynamic.html#t:Dynamic) has the following implementation:\n\n\n\n```\ndata Dynamic where\n    Dynamic :: TypeRep a -> a -> Dynamic\n\n```\n\nIt has occurred to me that the following definition would be equivalent (at least I think):\n\n\n\n```\ndata Dynamic where\n    Dynamic :: Typeable a => a -> Dynamic\n\n```\n\nBecause one can get from `TypeRep a` to a `Typeable` constraint using [`withTypeable`](https:\/\/hackage.haskell.org\/package\/base-4.19.0.0\/docs\/Type-Reflection.html#v:withTypeable), and in the other direction, go from a `Typeable` constraint to `TypeRep a` using [`typeRep`](https:\/\/hackage.haskell.org\/package\/base-4.19.0.0\/docs\/Type-Reflection.html#v:typeRep).\n\n\nI ask this question because I often have created GADTs with typeclass constraints, often to create \"existential\" types, and seeing this type has made me question whether instead of using typeclass constraints I should be adding a \"witness\" field? What should I consider when choosing between these two approaches?\n\n\n\n\n---\n\n\n**Further thoughts...**\n\n\nConsider something like this:\n\n\n\n```\ndata SillyListA m where\n  SillyListA :: Ord a => (a -> m ()) -> [a] -> SillyList m \n\ndata SillyListB m where\n  SillyListB :: (a -> a -> Ordering) -> (a -> m ()) -> [a] -> SillyList m \n\n```\n\nHere it being explicit about the argument instead of just including the typeclass constraint has a practical purpose, because you could have different orderings on the same type, and the second definition allows that without newtype silliness.\n\n\nBut that just seems silly where your type is basically a singleton as is the case in `TypeRep a`.\n\n\nI guess one slight benefit is that perhaps the witness can be made an argument to functions which means you don't have to use type applications to pull out the constructor.\n\n\nSo like with the first definition I can do:\n\n\n\n```\nf (Dynamic tr x) = ...\n\n```\n\nInstead of\n\n\n\n```\nf (Dynamic @a x) = ...\n\n```\n\nBut what I find I do anyway is:\n\n\n\n```\nf (Dynamic @a _ x) = ...\n\n```\n\nBecause the type is really handy to have in scope if `f` has any sub-functions defined that are explicitly typed, and not many functions take `TypeRep a` as an argument, they usually either require a type application or take `Proxy @a`, so I end up needing the type in scope anyway.\n\n\nI have started thinking about this because I have defined the type in my own code (please shout out if I've reinvented the wheel and this exists elsewhere):\n\n\n\n```\ndata DynamicF f where\n  DynamicF :: forall (a :: Type) f. TypeRep a -> f a -> DynamicF f\n\n```\n\nAnd I defined it like this basically copying `Dynamic`, but I'm thinking maybe just:\n\n\n\n```\ndata DynamicF f where\n  DynamicF :: forall (a :: Type). Typeable a => f a -> DynamicF f\n\n```\n\nIs a better definition.","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"haskell"},"answer":"I suspect the answer is mostly just history. `Dynamic` is much older than `ExistentialQuantification` (GHC 6.8.1, according to the manual), which is necessary for both your modern definitions. Before this extension, you could not store a type like `a` in a datatype and you also could not store a constraint like `Typeable a` (even if it didn't mention a stored type like `a`).\n\n\nE.g. inside the GHC 5.04 sources (which, even as a bzip archive, is <5MB!), I find that `Dynamic` is defined as follows\n\n\n\n```\ndata Dynamic = Dynamic TypeRep Obj\ndata Obj = Obj\n-- obviously, neither Dynamic nor Obj is exported!\n\n```\n\nI think it's reasonable to suggest that part of the reason `Dynamic` contains a `TypeRep` now because it's *always* held a `TypeRep`.\n\n\nSince the `Dynamic` data constructor used to be non-exported, the library authors *could* have just changed it to use `Typeable` once it was made public (which happened in `base-4.10`\/GHC 8.2.1) without breaking user code. But there was no strong reason to do this. Actually, there were even two weak reasons to prefer the `TypeRep` version:\n\n\n- Getting `Typeable a` out of a `TypeRep a` is actually black magic, implemented by (essentially) `unsafeCoerce`ing a `Typeable a => r` into a `TypeRep a -> r`. (It is of course still *safe*, and in recent GHC the trick has apparently been codified into its own `withDict` primitive.) `TypeRep a`s are \"normal\" values involving fewer dark arts.\n- When you're packing types up into values and moving them around it's nice to be able to actually *name* them. A `TypeRep a` is a value that you can bind with a name and can be used as a `Proxy`-esque token to represent the type `a`. (Note that many APIs don't require `Proxy a` but rather `proxy a`, where `proxy` is quantified over.) A `Typeable a` vanishes into the background the moment you get it. This is somewhat moot for `Dynamic`, since you are also getting an actual value `x :: a` (so you can do e.g. `let proxyOf :: a -> Proxy a in proxyOf x`), but it's nice to have a `TypeRep` already given. Also note that, at the time `Data.Dynamic` had its constructor exposed, type application patterns did not exist.\n\n\nBoth of these would have been basically just cosmetic considerations at the time and are even less important today.\n\n\nNow, for writing code today, I will point out that type applications\/abstractions still cannot do everything that proxies can do. The ability to name type arguments to lambdas is still missing, and thus there are situations where, for technical reasons, you *must* prefer a proxy-ish API over a type applications API.\n\n\n\n```\nclass Eq a => Structure a where\n    injZ :: Integer -> a\ndata SomeStructure where\n    SomeStructure :: Structure a => SomeStructure\n\n-- purely types-based API...\nwithSomeStructure :: SomeStructure -> (forall a. Structure a => r) -> r\nwithSomeStructure (SomeStructure @a) f = f @a\nx :: SomeStructure -> Bool\nx s = withSomeStructure s _can'tBindaInThisHole\n-- ...doesn't work\n\n-- proxy-based API...\nwithSomeStructure' :: SomeStructure -> (forall a. Structure a => Proxy a -> r) -> r\nwithSomeStructure' (SomeStructure @a) f = f (Proxy @a)\ny :: SomeStructure -> Bool\ny s = withSomeStructure' s (\\a -> injZ 0 == injZ 2 `asProxyTypeOf` a)\n-- ...works!\n\n```\n\nIf you don't anticipate such a situation happening to you, or if you simply don't mind a minor inconsistency in your code, where `withSomeStructure'` gives you a `Proxy` but a hypothetical `withDynamicF` doesn't, then the `Typeable` version of `DynamicF` is fine. I personally would prefer the `TypeRep` version still."}
{"questionId":"5d774022da3c40dca86b7f5dc3018672","question":"Pandas: new column, sampled from list, based on column value\nvalues = [1,2,3,2,3,1]\ncolors = ['r','g','b']\nexpected_output = ['r', 'g', 'b', 'g', 'b', 'r'] # how to create this in pandas?\n\ndf = pd.DataFrame({'values': values})\ndf['colors'] = expected_output\n\n```\n\ni.e. I want to make a new column in my dataframe where the colors are selected based on values in an existing column. I remember doing this in xarray with a vectorised indexing trick, but I can't remember if the same thing is possible in pandas... it feels like it should be a basic indexing task?\n\n\nEdit: The current answers are a nice start, thanks! They take a bit too much advantage of the numerical nature of \"values\" though. I'd rather something generic that would also work if say\n\n\n\n```\nvalues = ['a', 'b', 'c', 'b', 'c', 'a']\n\n```\n\nI guess the \"map\" method probably still works...","questionMetadata":{"Type":"Implementation","Level":"Beginner","Tag":"python"},"answer":"**Code**\n\n\nuse numpy indexing\n\n\n\n```\nimport numpy as np\ndf['colors'] = np.array(colors)[df['values'] - 1]\n\n```\n\ndf\n\n\n\n```\n   values color\n0       1     r\n1       2     g\n2       3     b\n3       2     g\n4       3     b\n5       1     r\n\n```\n\nIf you want to solve this problem using only Pandas, use `map` function. (with @Onyambu comment)\n\n\n\n```\nm = dict(enumerate(colors, 1))\ndf['colors'] = df['values'].map(m)"}
{"questionId":"cf0bff1c75f24989a5877ca8a84b4028","question":"Trying to extract a substring and version number from a filename using bash\nI'm currently trying to extract a substring and version number from a filename using bash.\n\n\nThere are two formats the filenames will be in:\n\n\n\n```\nexample-substring-1.1.0.tgz\nexample-substring-1.1.0-branch-name.tgz\n\n```\n\nFor the first scenario I was able to extract the version number using sed like so:\n\n\n\n```\necho example-substring-1.1.0.tgz | sed \"s\/.*-\\(.*\\)\\.[a-zA-Z0-9]\\{3\\}$\/\\1\/\"\n\n```\n\nHowever this won't work for the second scenario.\n\n\nEventually I would like to create a script that will store the first substring and version in an associative array like below.\n\n\n\n```\nexample_array[\"example-substring\"]=\"1.1.0\"\nexample_array[\"example-substring\"]=\"1.1.0-branch-name\"\n\n```\n\nThis is proving tricky however as I can't seem to find a good way that will work for both scenarios. And for scenarios where the version includes the branch name I can't know before hand how many words the branch name will consist of.\n\n\nI think variable expansion may be the way to go but wasn't able to get it to output what I want.","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"perl"},"answer":"With Perl\n\n\n\n```\necho \"example-substring-1.1.0-branch-name.tgz\" |\n    perl -wne'print join \" \", \/(.+)\\-([0-9]+\\.[0-9]+\\.[0-9]+.*)\\.tgz\/'\n\n```\n\nPrints two words\n\n\n\n```\nexample-substring 1.1.0-branch-name\n\n```\n\nThis is thus its return to the shell script, from which this would be invoked I presume, and then one can form needed structures in the shell script.\u2020 Tested also without the branch name, and with a few other variations of the input string.\n\n\nSince the `example-substring` can contain digits as well (why not?), and so can the branch name (why not?), the regex pattern has no restrictions and both the leading and (possible) trailing parts are matched simply by `.+` and `.*`.\n\n\nBut then we need something more specific for the version number and I've used an assumption that it always consists of *three* numbers separated by dots. I've also assumed the fixed rest of the string, the file extension `.tgz`. These can be relaxed somewhat if needed.\n\n\n\n\n---\n\n\n\u2020 One can directly read a list (key value key value...) into an associative array\n\n\n\n```\n#!\/bin\/bash\n\neval declare -A ver=( $( \n    echo \"example-substring-1.1.0-branch-name.tgz\" | \n    perl -wnE'say join \" \", \/(.+)\\-([0-9]+\\.[0-9]+\\.[0-9]+.*)\\.tgz\/' ))\n\necho ${ver[\"example-substring\"]}\n\n```\n\nOr it may be more suitable to assign to variables first\n\n\n\n```\nstr=\"example-substring-1.1.0-branch-name.tgz\"\n\nread -r str val <<< $( \nperl -wE'say join \" \", $ARGV[0] =~ \/(.+)\\-([0-9]+\\.[0-9]+\\.[0-9]+.+)\\.tgz\/' \n    -- \"$str\" )\n\nver[$str]=$val\n\n```\n\nor even just using positional parameters\n\n\n\n```\nset -- $(\n    perl -wE'say join \" \", $ARGV[0] =~ \/(.+)\\-([0-9]+\\.[0-9]+\\.[0-9]+.+)\\.tgz\/' \n        -- \"$str\" )\n\nver[$1]=$2\n\n```\n\nThere are of course other ways to pass arguments to a Perl script or a command-line program (\"one-liner\"), and other ways to take its output in bash.\n\n\nLet me know if this Perl code needs commentary."}
{"questionId":"27b594cd732843cb879a241d4801f87b","question":"Why is std::nextafter not constant expression?\nWhy code below has no problem with a2 but does not compile for z1?\n\n\n\n```\n#include <cmath>    \/\/ std::nextafter\n#include <limits>   \/\/ std::numeric_limits\n\nint main ()\n{\n    constexpr float a1 {1.f};\n    constexpr float a2 {std::nextafter(a1, std::numeric_limits<float>::max())};\n    constexpr float z0 {0.f};\n    constexpr float z1 {std::nextafter(z0, std::numeric_limits<float>::max())};\n    \n    return 0;\n}\n\n```\n\nCompiled with GCC 13.2\n\n\n\n```\nIn file included from <source>:1:\n\/opt\/compiler-explorer\/gcc-13.2.0\/include\/c++\/13.2.0\/cmath: In function 'int main()':\n<source>:9:39:   in 'constexpr' expansion of 'std::nextafter(((float)z0), std::numeric_limits<float>::max())'\n\/opt\/compiler-explorer\/gcc-13.2.0\/include\/c++\/13.2.0\/cmath:2417:32: error: '__builtin_nextafterf(0.0f, 3.40282347e+38f)' is not a constant expression\n 2417 |   { return __builtin_nextafterf(__x, __y); }\n\n```\n\nSo GCC compiled a2 correctly but is unable to compile z1.\n\n\nNote:\nClang 14.0 and MSVC 19.38 have problems even with a2.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"cpp"},"answer":"libstdc++13.2.0 does not seem to implement [`std::nextafter()`](https:\/\/github.com\/gcc-mirror\/gcc\/blob\/releases\/gcc-13.2.0\/libstdc%2B%2B-v3\/include\/tr1\/cmath#L894-L910) as `constexpr` yet.\n\n\nGCC turns `std::nextafter(a1, std::numeric_limits<float>::max())` into a constant value. This is a lucky exceptional case when compilers can evaluate rare expressions as constexpr even if they are not marked as such.\n\n\nGCC can't turn `std::nextafter(z0, std::numeric_limits<float>::max())` into a constant value, it turns `__builtin_nextafterf()` into the call to the C function `nextafterf()` that can't be constexpr. See <https:\/\/godbolt.org\/z\/v7KMrTdfW>\n\n\nThe code can be simplified to\n\n\n\n```\n#include <cmath>    \/\/ std::nextafter\n#include <limits>   \/\/ std::numeric_limits\n\nint main ()\n{\n    constexpr float a2 {std::nextafter(1.f, std::numeric_limits<float>::max())};\n    constexpr float z1 {std::nextafter(0.f, std::numeric_limits<float>::max())};\n    return 0;\n}\n\n```\n\nwith the similar Assembler result: <https:\/\/godbolt.org\/z\/KE9vGd6j7>"}
{"questionId":"1e4eea467fcf487b9e16497721c634b6","question":"trying to use fl\\_chart of flutter but when i import it i got this kind of error : Error: 'TextScaler' isn't a type\nTime when trying to **import fl\\_chart.dart** it through this error\n\n\n: Error: 'TextScaler' isn't a type.\nbase\\_chart\\_painter.dart:31\nfinal TextScaler textScaler;\n^^^^^^^^^^\n: Error: The getter 'textScaler' isn't defined for the class 'MediaQueryData'.\nline\\_chart\\_renderer.dart:27\n\n\n- 'MediaQueryData' is from 'package:flutter\/src\/widgets\/media\\_query.dart' ('\/C:\/flutter\/packages\/flutter\/lib\/src\/widgets\/media\\_query.dart').\nmedia\\_query.dart:1\nTry correcting the name to the name of an existing getter, or defining a getter or field named 'textScaler'.\n\n\nhow can i solve this error","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"dart"},"answer":"Run `flutter upgrade` in your terminal to upgrade the Flutter SDK. See the release notes of Flutter here <https:\/\/docs.flutter.dev\/release\/release-notes>"}
{"questionId":"6ac934d9d5af40e3b20b7dec8d1d7834","question":"OpenApi java.time.Duration default format\nI am using springdoc to generate my OpenAPI documentation.\n\n\n\n```\npublic class MyDto {\n    ....\n    @JsonFormat(shape = JsonFormat.Shape.STRING)\n    private Duration duration;\n    ....\n}\n\n```\n\nOpenApi shows the definition of `MyDto` as below, the duration gets deserialized to something similar to:\n\n\n\n```\n\"duration\": {\n    \"seconds\": 0,\n    \"nano\": 0,\n    \"negative\": true,\n    \"zero\": true,\n    \"units\": [\n      {\n        \"dateBased\": true,\n        \"timeBased\": true,\n        \"duration\": {\n          \"seconds\": 0,\n          \"nano\": 0,\n          \"negative\": true,\n          \"zero\": true\n        },\n        \"durationEstimated\": true\n      }\n    ]\n  }\n\n```\n\nI want it to be formated as a String without having to add `@Schema(type = \"string\", format = \"duration\")` to each `java.util.Duration` in all of my DTOs on each `Duration` field.\n\n\nI have `jackson-datatype-jsr310` on my classpath and serializing\/deserializing payloads works as expected using [ISO 8601](https:\/\/en.wikipedia.org\/wiki\/ISO_8601) formatted Strings, except for OpenAPI showing wrong format.\n\n\nIs it possible to somehow tell OpenAPI to treat `java.util.Duration` as a String by default?","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"java"},"answer":"I managed to solve this myself. Here is a configuration example if someone else has the same problem:\n\n\n\n```\n@Configuration\nclass SpringDocConfiguration implements InitializingBean\n{\n    @Override\n    public void afterPropertiesSet()\n    {\n        SpringDocUtils.getConfig().replaceWithSchema(Duration.class, new StringSchema().example(\"PT10S\"));\n    }\n}"}
{"questionId":"4f9493b59f714e1eb001c897282731c5","question":"Missing comma in JS array literal - Why does this compile?\nI forgot a comma initializing a 2D array in javascript and, to my surprise, it compiled. E.g.:\n\n\n\n\n\n```\namIMissingSomething = [\n    [1, 2, 3]\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\nconsole.log(amIMissingSomething) \/\/ [undefined, [7, 8, 9]]\n```\n\n\n\n\n\n\nI expected a syntax error to occur. However, after a whole lot of time, I learned this is valid javascript and evaluates to: `[undefined, [7, 8, 9]]`. What's going on here?","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"javascript"},"answer":"It is getting parsed as:\n\n\n\n```\namIMissingSomething = [\n    [1,2,3][4,5,6],\n    [7,8,9]\n]\n\n```\n\nthe comma operator returns the last expression so basically:\n\n\n\n```\namIMissingSomething = [\n    [1,2,3][6],\n    [7,8,9]\n]\n\n```\n\nand since the first array doesn't have 7 elements it evaluates to `undefined"}
{"questionId":"7c8fddaa85f0437e98ed15e3bcf0f9c8","question":"Randomly sampling from a dataframe using variable sample sizes\nI have a dataframe, `lexicon`, with 650 words, and I want to create a series of random wordlists for 5 speakers by randomly selecting words from `lexicon`. I want to do this over a range of 24 months of data collection, with different-sized vocabulary samples taken at each month. The base dataframe where month and vocabulary size are specified is `df1`:\n\n\n\n```\ndf1 <- data.frame(months=rep(1:24, times=5, each=1),\n                  vocab_size=(sample(c(0:25), 120, replace=TRUE)),\n                  Speaker=rep(c(\"A\", \"B\", \"C\", \"D\", \"E\"), times=1, each=24))\n\nlist1 <- split(df1, f=df1$Speaker)\n\n```\n\n`lexicon` looks something like this:\n\n\n\n```\nlexicon <- data.frame(c(\"a\", \"about\", \"above\", \"ain't\", \"all\", \"am\", \"an\", \"and\",  \n                        \"animal\",  \"ankle\", \"ant\" ,\"any\", \"apple\",\"applesauce\", \n                        \"asleep\", \"at\",  \"ate\",  \"aunt\", \"auntie\",  \"aunty's\", \n                        \"awake\", \"away\", \"baa\", \"baby\" , \"baby+doll\", \"bad\" ,\n                        \"ball\", \"balloon\", \"banana\", \"basket\", \"bat\", \"bath\", \n                        \"bathing\", \"bathtub\", \"be\", \"beach\", \"bead\", \"bean\",\n                        \"because\", \"bed\", \"beddy\", \"bee\", \"been\", \"behind\",\n                        \"being\", \"belt\", \"bench\", \"bib\", \"bicycle\", \"big\"))\n\n```\n\nI've been attempting to generate the output I'm after using the following code:\n\n\n\n```\nvocab_data <- lapply(list1, FUN=function(element) {\n  all_vocab <- slice_sample(lexicon, n=element$vocab_size, replace=TRUE)\n})\n\n```\n\nBut I get the following error message\n\n\n\n```\n Error in `slice_sample()`:\n ! `n` must be a constant.\n Caused by error in `element$vocab_size`:\n ! $ operator is invalid for atomic vectors\n\n```\n\nIs there a way of extracting different-sized samples from a dataframe in this way to create a list of vocabulary for each speaker over each month?","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"r"},"answer":"The error occurs because in this case `slice_sample` gets a vector of numbers but it needs just a number *n*. Get around this by using `rowwise`, which works on each row separately and hence `slice_sample` only sees a single number.\n\n\n\n```\nlibrary(dplyr)\n\ndf1 %>% \n  rowwise() %>% \n  mutate(all_vocab = list(slice_sample(lexicon, n=vocab_size))) %>% \n  ungroup()\n# A tibble: 120 \u00d7 4\n   months vocab_size Speaker all_vocab    \n    <int>      <int> <chr>   <list>       \n 1      1          1 A       <df [1 \u00d7 1]> \n 2      2         15 A       <df [15 \u00d7 1]>\n 3      3         18 A       <df [18 \u00d7 1]>\n 4      4         18 A       <df [18 \u00d7 1]>\n 5      5         24 A       <df [24 \u00d7 1]>\n 6      6          4 A       <df [4 \u00d7 1]> \n 7      7          3 A       <df [3 \u00d7 1]> \n 8      8         10 A       <df [10 \u00d7 1]>\n 9      9         24 A       <df [24 \u00d7 1]>\n10     10         19 A       <df [19 \u00d7 1]>\n# \u2139 110 more rows\n# \u2139 Use `print(n = ...)` to see more rows"}
{"questionId":"7afc7a353a2c4cc8ab7001b9f0f942b8","question":"Remove only exact number of repeat matches between two files\nI want to get the remaining difference between two files that have redundant entries.\n\n\n**File1.txt:**\n\n\n\n```\nData1\nData1\nData2\nData2\nData3\nData3\nData3\nData3\nData4\nData5\nData6\nData6\n\n```\n\nand\n\n\n**File2.txt:**\n\n\n\n```\nData1\nData2\nData2\nData3\nData3\nData4\nData5\nData6\n\n```\n\n**Finalfile.txt:**\n\n\n\n```\nData1\nData3\nData3\nData6\n\n```\n\nIn other words: if an entry shows up n times in file 1 and m times in file 2 then, the final file should contain the n-m entries.\nIe: See there are four entries of **Data3** in File1.txt and only two entries in File2.txt, therefore the Finalfile.txt has 2 occurances of **Data3**.\n\n\nI've tried:\n\n\n\n```\ngrep -v -f File1.txt File2.txt > Finalfile.txt\n\n```\n\nbut it give the absolute differences.","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"other"},"answer":"You may use this 2 pass `awk` solution:\n\n\n\n```\nawk '\nNR == FNR {\n   ++fq[$1]\n   next\n}\n{\n   --fq[$1]\n}\nEND {\n   for (s in fq)\n      for (i = 1; i <= fq[s]; ++i)\n         print s\n}' file1 file2\n\nData1\nData3\nData3\nData6"}
{"questionId":"5be88365cb1143489cacc20dc5899617","question":"VBscript operator precedence inconsistency with VB6\nI am running an old VB6.0 program in interpretive mode (i.e. not as a .exe) on Win-7 32-bit.\n\n\nThe program accesses VBScript via an MSScriptControl.ScriptControl (as illustrated in the code block below) in function `calc` which here is called by the test function `tc` which itself can be called from another sub or function or from the immediate pane by typing `tc <expression>`.\nHere `<expression>` is a numeric math expression like `2+3^2*(9-3.5)^3`.\n\n\nWhen the input expression includes a pattern like `-a^b` the results are sometimes surprising to me. (Here `a,b` are placeholders for real numbers - I do not expect `calc` to handle expressions containing variable names.)\n\n\nFrom the rules of operator precedence, based on VB6 behavior I would expect `-a^b --> -(a^b)` but often `-a^b --> +(a^b)`. Spaces inside the expression have no discernible effect, as expected.\n\n\nIn the different `Cases` shown below I label consistent results by \"ok\" and inconsistent results by \"?\".\n\n\nI could address the problem by bracketing every case of `a^b` thus `(a^b)`. But, given I am working with long complicated equations, this will (i) be tedious to implement and (ii) impair the readability of the code.\n\n\nIs this behavior known about? Is there a \"behind-the-scenes\" fix (e.g. a software patch or version upgrade)?\n\n\n\n```\nvb6\n\nPublic Function calc(ipEqtn$)\n    Set objScript = CreateObject(\"MSScriptControl.ScriptControl\")\n    objScript.Language = \"VBScript\"\n    calc = objScript.eval(ipEqtn$)\nEnd Function '... calc\n\nPublic Function tc(n)\n\nSelect Case (n)\n    Case (1):  eqtn$ = \"-(5 ^ 2)         \": VB6 = -(5 ^ 2)              ' --> -25,-25 ok\n    Case (2):  eqtn$ = \"(- 5 ^ 2)        \": VB6 = (-5 ^ 2)              ' -->  25,-25 ?\n    Case (3):  eqtn$ = \"(-5^2)           \": VB6 = (-5 ^ 2)              ' -->  25,-25 ?\n    Case (4):  eqtn$ = \"- 5 ^ 2          \": VB6 = -5 ^ 2                ' -->  25,-25 ?\n    \n    Case (5):  eqtn$ = \"( 5^2 -5^2 )     \": VB6 = (5 ^ 2 - 5 ^ 2)       ' -->  0,0 ok\n    Case (6):  eqtn$ = \"( -5^2 + 5^2 )   \": VB6 = (-5 ^ 2 + 5 ^ 2)      ' --> 50,0 ?\n    Case (7):  eqtn$ = \"  5^2 -5^2       \": VB6 = 5 ^ 2 - 5 ^ 2         ' -->  0,0 ok\n    Case (8):  eqtn$ = \" -5^2 + 5^2      \": VB6 = -5 ^ 2 + 5 ^ 2        ' --> 50,0 ?\n    \n    Case (9):  eqtn$ = \" + 5^2  -5^2     \": VB6 = 5 ^ 2 - 5 ^ 2         ' --> 0,0\n    Case (10): eqtn$ = \" + 5^2 + -5^2    \": VB6 = 5 ^ 2 + -5 ^ 2        ' -->50,0\n    Case (11): eqtn$ = \" + 5^2 + -1*5^2  \": VB6 = 5 ^ 2 + -1 * 5 ^ 2    ' --> 0,0\n    \nEnd Select\nvbs = calc(eqtn$)\nDebug.Print vbs, VB6\n    \nEnd Function '... tc()","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"other"},"answer":"It's worth noting that VBScript and Visual Basic are different languages, one is a scripting language while the other is a fully-fledged programming language.\n\n\nThe Visual Basic Operator Precedence is well documented across the various versions of Visual Basic (VB, VBA, VB.Net etc) - See [What is the operator precedence order in Visual Basic 6.0?](https:\/\/stackoverflow.com\/a\/55215\/692942) and [Operator Precedence in Visual Basic](https:\/\/learn.microsoft.com\/en-us\/dotnet\/visual-basic\/language-reference\/operators\/operator-precedence).\n\n\nWhen you compare this with the [VBScript Operator Precedence](https:\/\/learn.microsoft.com\/en-us\/previous-versions\/windows\/internet-explorer\/ie-developer\/scripting-articles\/6s7zy3d1(v=vs.84)) you see a major difference between the Arithmetic Operation Precedence Order.\n\n\nVBScript favours Negation over Exponentiation, while Visual Basic favours Exponentiation over Negation, which explains why you see different results.\n\n\nThe obvious workaround is to use brackets to be explicit about your intent (which you mention in the original question).\n\n\nWithout brackets\n\n\n\n```\n'VBScript\n-5^2 '-5 * -5 = 25\n'Visual Basic\n-5^2 '-5 * 5 = -25\n\n```\n\nWith brackets, we can force the intent\n\n\n\n```\n'VBScript\n-(5^2) '-(5 * 5) = -25\n'Visual Basic\n-(5^2) '-(5 * 5) = -25"}
{"questionId":"c20c609148be4ca9916d95da4f8a9e8d","question":"Why doesn't gcc 13 display the correct binary represenation?\nWhile answering a question here, I made the following example:\n\n\n\n```\n#include <stdio.h>\n#include <math.h>\n\nint main (void) \n{\n  float_t a = -248.75;\n  printf(\"%f\\n\", a);\n\n  unsigned char* ptr = (unsigned char*)&a;\n  for(size_t i=0; i<sizeof(a); i++)\n  {\n    printf(\"%.2X \", ptr[i]);\n  }\n}\n\n```\n\nOn gcc before version 13 as well as all versions of clang, this gives the expected output (x86 little endian):\n\n\n\n```\n-248.750000\n00 C0 78 C3 \n\n```\n\nHowever, when compiling with gcc 13.1, I get nonsense output:\n\n\n\n```\n-248.750000\n00 00 00 00 \n\n```\n\nFurther examination shows that the culprit is the option `-std=c2x`. If I remove it, the program behaves as expected.\n\n\nCompiler options used: `-std=c2x -pedantic-errors -Wall -Wextra -O3 -lm`. <https:\/\/godbolt.org\/z\/4qbo74eEW>\n\n\nIs this a known bug in gcc?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"c"},"answer":"This is now filed as a GCC bug, cf. <https:\/\/gcc.gnu.org\/PR111884>\n\n\nv11 and v12 seem to work for me.\n\n\n**Edit**: This bug is fixed now in v13.3+"}
{"questionId":"44d4c9fbfe524268b8e74a6c1ccaf829","question":"\u0421ompiler bug? Variable assumed unchanged\nVisual Studio version: **17.7.1 (MSVC 19.37.32822)**  \n\nFreshly created project with default settings and compiler flags.\n\n\nMinimal reproducible example:\n\n\n\n```\n#include <cstdio>\n\n__declspec(noinline) void test2(char** data)\n{\n    \/\/ After moving the pointer:\n    \/\/ data_1 now points to data[1] = 1\n    \/\/ data_0 now points to data[0] = 2\n    *data += 1;\n}\n\n__declspec(noinline) void test(char* data_1)\n{\n    char* data_0 = data_1;\n    test2(&data_1);\n    int len = (int)(data_1 - data_0);\n\n    if (*data_1 & 1)\n    {\n        if (*data_0 & 2)\n            printf(\"good\\n\");\n     }\n}\n\nint main()\n{\n    char data[2];\n    data[0] = 2;\n    data[1] = 1;\n    test(data);\n    return 0;\n}\n\n```\n\n**No line \"good\" is printed in Release| x64 configuration.**  \n\nDebug or Release | x86 produce expected result.\n\n\nI did some experimenting and looked through resulting assembly in order to reduce the original code to this MRE.  \n\nThe root cause of the issue seem to be the compiler's assumption that `data1` remains unchanged in `test2()`. Thus, the line `data_0 = data_1` can be omitted and `data_1` can be used instead of `data_0` in the expression `(*data_0 & 2)`.  \n\nEach line is necessary, including the casting of `(data_1 - data_0)` to int, which probably explains the lack of bug reproduction in the x86 build.\n\n\n## Update 1\n\n\nProblem only occurs in the x64 Release build. Disabling optimizations by changing \/O2 to \/Od or using `#pragma optimize(\"\", off)` \"fixes\" the problem.\n\n\nReproducibility by VS version breakdown:\n\n\n- 17.7.1 - problem\n- 17.7.2 - problem\n- ???\n- 17.7.6 - no problem\n- 17.8 preview 7 - no problem\n\n\nIt's possible that the problem has already been fixed or the conditions for its reproduction have changed.\n\n\n## Update 2\n\n\nOne of the MSVC developers confirmed this problem in a private conversation, I've made this issue to track progress: <https:\/\/developercommunity.visualstudio.com\/t\/Compiler-optimization-bug-in-VS-2022\/10512534>","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"I can confirm that the same behaviour is present here. In addition any attempt to printf values to check the else (fail) conditions results in correct behaviour. Optimiser has been too aggressive. It only fetches \\*data\\_1 and applies both tests to it (assuming that data\\_1==data\\_0 which of course isn't true).\n\n\nAny alteration to print out debugging values seems to result in correct behaviour. I added printf(\"bad data\\_0\") instead to the inner else clause and predictably that got printed out. I prefer to have all paths annotated.\n\n\nFWIW Intel compiler 2023 runs it fine and prints \"good\".\n\n\nSmoking gun - MSC compiler bug excessively aggressive optimisation fails to load \\*data\\_0 and applies both tests to \\*data\\_1. Disassembly is here (with slight additions which don't affect errant behaviour) :\n\n\n--- C:\\Users\\Martin\\source\\repos\\Toy\\_bug1\\Toy\\_bug1.cpp ------------------------\n\n\n\n```\n\/\/ After moving the pointer:\n\/\/ data_1 now points to data[1] = 1\n\/\/ data_0 now points to data[0] = 2\n*data += 1;\n00007FF6D10B1070  inc         qword ptr [rcx]  \n}\n00007FF6D10B1073  ret  \n[snip]\n__declspec(noinline) void test(char* data_1)\n{\n00007FF6D10B1080  mov         qword ptr [rsp+8],rcx  \n00007FF6D10B1085  sub         rsp,28h  \nchar* data_0 = data_1;\ntest2(&data_1);\n00007FF6D10B1089  lea         rcx,[data_1]  \n00007FF6D10B108E  call        test2 (07FF6D10B1070h)  \nint len = (int)(data_1 - data_0);\n\nif (*data_1 & 1)\n00007FF6D10B1093  mov         rax,qword ptr [data_1]  \n00007FF6D10B1098  movzx       ecx,byte ptr [rax]  \n00007FF6D10B109B  test        cl,1  \n00007FF6D10B109E  je          test+3Eh (07FF6D10B10BEh)  \n{\n    if (*data_0 & 2)\n00007FF6D10B10A0  test        cl,2  \/\/ this test is incorrect!\n        printf(\"good\\n\");\n    else\n        printf(\"bad d0\");\n}\n00007FF6D10B10A3  lea         rax,[string \"bad d0\" (07FF6D10B2258h)]  \n00007FF6D10B10AA  lea         rcx,[string \"good\\n\" (07FF6D10B2250h)]  \n00007FF6D10B10B1  cmove       rcx,rax  \n}\n\n```\n\nI remain a little mystified about the significance of the crucial line in the MRE which is optimised out of existence but is essential for the fault to show!\nNamely:\n\n\nint len = (int)(data\\_1 - data\\_0);\n\n\nWithout this line present the MSC compiler gets it right in x64 release. Here is the disassembly of the correct code generated in that case:\n\n\n\n```\n\/\/    int len = (int)(data_1 - data_0);\n\nif (*data_1 & 1)\n00007FF6D6D11096  mov         rax,qword ptr [data_1]  \n00007FF6D6D1109B  test        byte ptr [rax],1  \n00007FF6D6D1109E  je          test+3Eh (07FF6D6D110BEh)  \n{\n     if (*data_0 & 2)\n00007FF6D6D110A0  test        byte ptr [rdx],2  \n\/\/ rax  is data_1\n\/\/ rdx  is data_0\n\n```\n\nMoral of story is beware of doing things that confuse optimising compilers!"}
{"questionId":"d61e603721bb4980b4c0406f5b4b6a8c","question":"Why do R external pointers' \"unusual copying semantics\" mean they should not be used stand-alone?\nSection 5.13, [External pointers and weak references](https:\/\/cran.r-project.org\/doc\/manuals\/R-exts.html#External-pointers-and-weak-references), of Writing R Extensions states:\n\n\n\n> \n> External pointer SEXPs are intended to handle references to C\n> structures such as `handles', and are used for this purpose in package\n> RODBC for example. They are unusual in their copying semantics in that\n> when an R object is copied, the external pointer object is not\n> duplicated. (For this reason external pointers should only be used as\n> part of an object with normal semantics, for example an attribute or\n> an element of a list.)\n> \n> \n> \n\n\nWhat is meant here by \"external pointer object\", the external pointer itself or the memory that the external pointer points to? Why would the unusual copying semantics mean that external pointers should only be used as part of an object with normal semantics?\n\n\nTo clarify, my R package is a wrapper around a C library, Baz. The Baz library provides a C structure, `Foo`, which is used by Baz as a sort of internal working space. Baz provides C functions `Foo* baz_allocate_foo()` and `void baz_free_foo(Foo*)` to allocate and free `Foo` structures, which it does outside of R memory management.\n\n\nIn my R package, I want to use external pointers to store the addresses of these allocated `Foo` structures. Part of my R package's C++ code (using Rcpp for interfacing) looks like this:\n\n\n\n```\n\/\/ baz.h is the Baz C library header; contains the definition of struct Foo\n#include <R.h>\n#include <Rinternals.h>\nextern \"C\" {\n#include <baz.h>\n}\n\n\/\/ For use as the external pointer's tag\n#define FOO_CODE 0xF00C0DE\n\n\/\/ Finalizer for garbage collection of external pointers to Foo\nvoid finalize_foo(SEXP x)\n{\n    baz_free_foo(reinterpret_cast<Foo*>(R_ExternalPtrAddr(x)));\n    R_ClearExternalPtr(x);\n}\n\n\/\/ Exported function for users of my R package\n\/\/ [[Rcpp::export]]\nSEXP get_foo()\n{\n    SEXP tag = PROTECT(Rf_ScalarInteger(FOO_CODE));\n    SEXP x = PROTECT(R_MakeExternalPtr(baz_allocate_foo(), tag, R_NilValue));\n    R_RegisterCFinalizerEx(x, finalize_foo, TRUE);\n    UNPROTECT(2);\n    return x;\n}\n\n```\n\nIn R code, the user does things like this:\n\n\n\n```\nmyfoo = bazwrap::get_foo()\nbazwrap::say_hello(myfoo, \"Alice\")\nbazwrap::say_goodbye(myfoo, \"Bob\")\n\n```\n\nand the intention is that the memory pointed to by `myfoo` gets freed either when `myfoo` is garbage collected or before R exits.\n\n\nSo here I am using an external pointer completely on its own, not as part of a list or as an attribute of an object with \"normal semantics\" as advised by Writing R Extensions. I haven't found any issues with this, even when doing e.g.\n\n\n\n```\nlibrary(bazwrap)\nmyfoo1 = get_foo()\nmyfoo2 = myfoo1\nrm(myfoo2)\ngc() # as expected, this does not trigger the finalizer as myfoo1 is still around\nsay_hello(myfoo1, \"Alice\") # doesn't crash...\n\n```\n\nThis leads me to my questions at the top of the post.","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"r"},"answer":"Here, \"unusual copying semantics\" just means that `duplicate1(s, .)` in C [1] returns `s` and not a copy of `s`. Such semantics are employed for external pointers and 9 other types; indeed, in [`duplicate.c`](https:\/\/svn.r-project.org\/R\/trunk\/src\/main\/duplicate.c) we see that `duplicate1` does something like:\n\n\n\n```\nswitch (TYPEOF(s)) {\ncase NILSXP:\ncase SYMSXP:\ncase ENVSXP:\ncase SPECIALSXP:\ncase BUILTINSXP:\ncase EXTPTRSXP:\ncase BCODESXP:\ncase WEAKREFSXP:\ncase CHARSXP:\ncase PROMSXP:\n    return s;\n\n```\n\nThe answer to your main question has everything to do with setting of attributes. For `s` of one of the above 10 types, `attr(s, name) <- value` either is an error (`NILSXP`, `SYMSXP`, `CHARSXP`) or sets an attribute on `s` and never a copy of `s`. Whether `s` is referenced makes no difference; a copy is never made because `duplicate1` is a no-op.\n\n\nWith that in mind, we can understand attributes on external pointers (`EXTPTRSXP`) by looking at attributes on environments (`ENVSXP`), which are an exact analogy with the convenience of an R level API:\n\n\n\n```\n> e1 <- e2 <- new.env()\n> attr(e2, \"a\")\nNULL\n> attr(e1, \"a\") <- 0\n> attr(e2, \"a\")\n[1] 0\n\n```\n\nThat is, setting an attribute on an environment affects all other references to it. The same can be said for external pointers. Hence the advice in the [Writing R Extensions manual](https:\/\/stat.ethz.ch\/R-manual\/R-patched\/doc\/manual\/R-exts.html#External-pointers-and-weak-references) and in [Luke Tierney's original notes](https:\/\/homepage.divms.uiowa.edu\/%7Eluke\/R\/simpleref.html) is to always place external pointers in a list or pairlist, which `duplicate1` does copy:\n\n\n\n```\n> e1 <- e2 <- list(new.env())\n> attr(e2, \"a\")\nNULL\n> attr(e1, \"a\") <- 0\n> attr(e2, \"a\")\nNULL\n\n```\n\nIf you are wondering why it is not an error to set attributes on external pointers, in light of their unusual copying semantics, then don't forget that classed external pointers must have a `class` attribute. I imagine that R-core wanted to allow classed external pointers, even if its advice is to instead use classed lists or pairlists containing unclassed external pointers.\n\n\n\n\n---\n\n\n1. API functions `duplicate` and `shallow_duplicate` are just wrappers for `duplicate1`."}
{"questionId":"84b9387091e3439b9a7dc2bbce13e0e9","question":"MUI DateCalendar With Luxon Adapter Start Week On Sunday\nRecently, Luxon added support for [localized week information](https:\/\/github.com\/moment\/luxon\/pull\/1454). I updated my version of Luxon to the latest release that contains this update, `3.4.4` but when opening up the calendar, the week still starts on Monday instead of Sunday.\n\n\nI would assume that the `LuxonAdapter` would pick up the changes and use the localized week data but it doesn't seem to be any different.\n\n\nMy `package.json` has the latest versions of both libraries:\n\n\n\n```\n\"@mui\/x-date-pickers\": \"^7.0.0-alpha.0\",\n\"luxon\": \"^3.4.4\",\n\n```\n\nThen, in my `App.tsx` I add the `LocalizationProvider` with the `LuxonAdapter`:\n\n\n\n```\nimport { LocalizationProvider } from \"@mui\/x-date-pickers\";\nimport { AdapterLuxon } from \"@mui\/x-date-pickers\/AdapterLuxon\";\n\n<LocalizationProvider dateAdapter={AdapterLuxon}>\n    <Outlet \/>\n<\/LocalizationProvider>\n\n```\n\nLastly, the `DateCalendar` component:\n\n\n\n```\nimport { DateTime } from \"luxon\";\nimport { DateCalendar } from \"@mui\/x-date-pickers\";\n\nconst [calendarValue, setCalendarValue] = useState<DateTime | null>(DateTime.now());\n\n<DateCalendar\n    value={calendarValue}\n    onChange={setCalendarValue}\n    views={[\"day\"]}\n    disablePast\n    sx={{ m: 0 }}\n\/>","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"typescript"},"answer":"I see you currently have:\n\n\n\n```\nClient Application\n\u2502\n\u251c\u2500 package.json\n\u2502  \u251c\u2500 \"@mui\/x-date-pickers\": \"^7.0.0-alpha.0\"\n\u2502  \u2514\u2500 \"luxon\": \"^3.4.4\"\n\u2502\n\u251c\u2500 App.tsx\n\u2502  \u251c\u2500 LocalizationProvider (with LuxonAdapter)\n\u2502  \u2514\u2500 Outlet\n\u2502\n\u2514\u2500 DateCalendar Component\n   \u251c\u2500 DateTime from \"luxon\"\n   \u2514\u2500 DateCalendar from \"@mui\/x-date-pickers\"\n\n```\n\nVerify first that [Luxon](https:\/\/github.com\/moment\/luxon) is configured with the correct locale that starts the week on Sunday. If your locale is set to one where the week traditionally starts on Monday, Luxon will follow that setting.\n\n\n\n```\nimport { Settings } from 'luxon';\n\n\/\/ Set the locale globally for Luxon\nSettings.defaultLocale = 'your-locale'; \/\/ Replace 'your-locale' with the appropriate locale code\n\n```\n\nIf the LuxonAdapter does not automatically pick the start of the week from Luxon settings, you might need to configure it explicitly. But I do not see a way to do this directly in [`@mui\/x-date-pickers`](https:\/\/mui.com\/x\/react-date-pickers\/date-picker\/).\n\n\n[`mui\/material-ui` issue 30591](https:\/\/github.com\/mui\/material-ui\/issues\/30591#issuecomment-1758587765) proposes, on a similar issue referenced by [`moment\/luxon` issue 1447](https:\/\/github.com\/moment\/luxon\/issues\/1447) (the very issue your [`moment\/luxon` PR 1454](https:\/\/github.com\/moment\/luxon\/pull\/1454) fixes)\n\n\n\n```\nfunction adapterLuxonFactory(weekStartsOn: number) {\n  class Adapter extends AdapterLuxon {\n    private weekStartsOn = weekStartsOn;\n\n    \/** Controls the header of the calendar month view. *\/\n    public getWeekdays = () => {\n      const weekdays = Info.weekdaysFormat(\"narrow\", { locale: this.locale });\n      const start = weekdays.slice(0, this.weekStartsOn - 1);\n      const end = weekdays.slice(this.weekStartsOn - 1);\n      return [...end, ...start];\n    };\n\n    \/** Controls the day buttons of the calendar month view. *\/\n    getWeekArray = (date: DateTime) => {\n      const startOfMonth = date.startOf(\"month\");\n      const endOfMonth = date.endOf(\"month\");\n      const firstDayOfMonth = startOfMonth.weekday;\n\n      const startOfWeek = startOfMonth.minus({\n        days: (firstDayOfMonth - this.weekStartsOn + 7) % 7,\n      });\n\n      const endOfWeek = endOfMonth\n        .plus({ days: 6 - endOfMonth.weekday + this.weekStartsOn })\n        .endOf(\"day\");\n\n      const { days } = endOfWeek.diff(startOfWeek, \"days\").toObject();\n\n      const weeks: DateTime[][] = [];\n      new Array<number>(Math.round(days ?? 0))\n        .fill(0)\n        .map((_, i) => i)\n        .map((day) => startOfWeek.plus({ days: day }))\n        .forEach((v, i) => {\n          if (i === 0 || (i % 7 === 0 && i > 6)) {\n            weeks.push([v]);\n            return;\n          }\n\n          weeks[weeks.length - 1].push(v);\n        });\n\n      return weeks;\n    };\n  }\n\n  return Adapter;\n}\n\nexport function LocalizationProvider({\n  children,\n}: {\n  children?: React.ReactNode;\n}): JSX.Element {\n  const { weekStartsOn } = useYourOwnDatabase();\n\n  return (\n    <MuiLocalizationProvider dateAdapter={adapterLuxonFactory(weekStartsOn)}>\n      {children}\n    <\/MuiLocalizationProvider>\n  );\n}\n\n```\n\nThe custom adapter factory function `adapterLuxonFactory` takes a `weekStartsOn` parameter (number representing the day the week starts on, with `1` for Monday, `2` for Tuesday, up to `7` for Sunday). That function is used in the `LocalizationProvider` to make sure the entire calendar component respects the specified start of the week.\n\n\nTo integrate this solution into your current setup, you would replace, for testing, the standard `LuxonAdapter` with this custom adapter, by modifying the `LocalizationProvider` in your `App.tsx` to use this custom adapter.  \n\nMake sure you provide the correct `weekStartsOn` value when initializing the adapter. For starting the week on Sunday, this would be `7`.\n\n\n\n\n---\n\n\nI also see [`mui\/mui-x` issue 9984](https:\/\/github.com\/mui\/mui-x\/issues\/9984#issuecomment-1787547736), which says:\n\n\n\n> \n> Since 2021, I've been using the workaround noted in [`mui\/material-ui-pickers` issue 1270](https:\/\/github.com\/mui\/material-ui-pickers\/issues\/1270) to force Luxon to use Sunday as the first day of the week and just noticed that it no longer works.\n> \n> \n> \n> ```\n> import {Info} from 'luxon';\n> import LuxonUtils from '@date-io\/luxon';\n> \n> class CustomLuxonUtils extends LuxonUtils {\n>   getWeekdays() {\n>     \/\/ need to copy the existing, and use Info to preserve localization\n>     const days = [...Info.weekdaysFormat('narrow', this.locale)];\n>     \/\/ remove Sun from end of list and move to start of list\n>     days.unshift(days.pop());\n>     return days;\n>   }\n> \n>   getWeekArray(date) {\n>     const endDate = date\n>       .endOf('month')\n>       \/\/ if a month ends on sunday, luxon will consider it already the end of the week\n>       \/\/ but we need to get the _entire_ next week to properly lay that out\n>       \/\/ so we add one more day to cover that before getting the end of the week\n>       .plus({days: 1})\n>       .endOf('week');\n>     const startDate = date\n>       .startOf('month')\n>       .startOf('week')\n>       \/\/ must subtract 1, because startOf('week') will be Mon, but we want weeks to start on Sun\n>       \/\/ this is the basis for every day in a our calendar\n>       .minus({days: 1});\n> \n>     const {days} = endDate.diff(startDate, 'days').toObject();\n> \n>     const weeks = [];\n>     new Array(Math.round(days))\n>       .fill(0)\n>       .map((_, i) => i)\n>       .map(day => startDate.plus({days: day}))\n>       .forEach((v, i) => {\n>         if (i === 0 || (i % 7 === 0 && i > 6)) {\n>           weeks.push([v]);\n>           return;\n>         }\n> \n>         weeks[weeks.length - 1].push(v);\n>       });\n> \n>     \/\/ a consequence of all this shifting back\/forth 1 day is that you might end up with a week\n>     \/\/ where all the days are actually in the previous or next month.\n>     \/\/ this happens when the first day of the month is Sunday (Dec 2019 or Mar 2020 are examples)\n>     \/\/ or the last day of the month is Sunday (May 2020 or Jan 2021 is one example)\n>     \/\/ so we're only including weeks where ANY day is in the correct month to handle that\n>     return weeks.filter(w => w.some(d => d.month === date.month));\n>   }\n> }\n> \n> ```\n> \n> I was able to fix it by adding an override for the startOfWeek method as follows:\n> \n> \n> \n> ```\n> startOfWeek = (value: DateTime) => {\n>     return value.startOf('week').minus({ days: 1 });\n> }\n> \n> ```\n> \n> \n\n\nThis is discussed in [`mui\/mui-x` issue 10805](https:\/\/github.com\/mui\/mui-x\/issues\/10805): \"`[pickers]` Support changing start of week day on AdapterLuxon\"\n\n\n\n> \n> The clean fix will be merged in the v7 alpha branch.\n> \n> \n> \n\n\nThis is [`mui\/mui-x` PR 10964](https:\/\/github.com\/mui\/mui-x\/pull\/10964)"}
{"questionId":"f6fdd693d94c469e9cd3174f3a8243cd","question":"How to control the size of a class to be a multiple of the size of a member?\nI have the following class,\n\n\n\n```\nstruct employee {\n    std::string name;\n    short salary;\n    std::size_t age;\n};\n\n```\n\nJust as an example, in Linux amd64, the size of the struct is 48 bytes, and the size of std::string is 32, that is, not a multiple.\n\n\nNow, I need, in a cross-platform way, for `employee` to have a size that is a multiple of the size of `std::string` (first member).\n\n\n(Cross-platform could mean, for example, both Linux amd64 and Apple ARM.)\n\n\nThat is, `sizeof(employee) % sizeof(std::string) == 0`.\n\n\nI tried controlling the padding using `alignas` for the whole class or the members, but the requirement to be a power of 2 is too restrictive, it seems.\n\n\nThen I tried to add a `char` array at the end.\nStill, I had two problems, first, what is the exact size of the array in different platforms at compile-time, and second not adding another member that can screw up the nice aggregate initialization of the class.\n\n\nFor the first, I do this:\n\n\n\n```\nstruct employee_dummy {\n    std::string name;\n    short salary;\n    std::size_t age;\n};\n\nstruct employee {\n    std::string name;\n    short salary;\n    std::size_t age;\n    char padding[(sizeof(employee_dummy)\/sizeof(std::string)+1)*sizeof(std::string) - sizeof(employee_dummy)];\n};\n\n```\n\nNote the ugly dummy class, and I don't even know if the logic is correct.\n\n\nFor the second problem, I don't have a solution.\nI could do this, but then I would need to add a constructor, the class would not be an aggregate, etc.\n\n\n\n```\nstruct employee {\n    std::string name;\n    short salary;\n    std::size_t age;\n private:\n    char padding[(sizeof(employee_dummy)\/sizeof(std::string)+1)*sizeof(std::string) - sizeof(employee_dummy)];\n};\n\n```\n\n**How can I control the size of the struct with standard or non-standard mechanisms and keep the class as an aggregate?**\n\n\nHere is a link to play with this problem empirically: <https:\/\/cppinsights.io\/s\/f2fb5239>\n\n\n\n\n---\n\n\nNOTE ADDED:\n\n\nI realized that, if the technique to add padding is correct, the calculation is even more difficult because the dummy class might be already adding padding, so I have to take into account the offset of the last element instead.\n\n\nIn this example I want `data` to be a multiple of the first member (`std::complex`):\n\n\n\n```\nstruct dummy {\n    std::complex<double> a;\n    double b;\n    std::int64_t b2;\n    int c;\n};\n\nstruct data {\n    std::complex<double> a;\n    double b;\n    std::int64_t b2;\n    int c;\n    char padding[ ((offsetof(dummy, c) + sizeof(c)) \/ sizeof(std::complex<double>) + 1)* sizeof(std::complex<double>) - (offsetof(dummy, c) + sizeof(c)) ];\n};\n\n```\n\nNote the formula is even worse now.","questionMetadata":{"Type":"Implementation","Level":"Advanced","Tag":"cpp"},"answer":"Here is a standard-compliant, no ifs or buts, version.\n\n\n\n```\ntemplate <template<std::size_t> class tmpl, std::size_t need_multiple_of>\nstruct adjust_padding\n{\n    template <std::size_t n>\n    static constexpr std::size_t padding_size()\n    {\n        if constexpr (sizeof(tmpl<n>) % need_multiple_of == 0) return n;\n        else return padding_size<n+1>();\n    }\n\n    using type = tmpl<padding_size<0>()>;\n};\n\n```\n\nUse it like this:\n\n\n\n```\ntemplate <std::size_t K>\nstruct need_strided\n{\n    double x;\n    const char pad[K];\n};\n\ntemplate <>\nstruct need_strided<0>\n{\n    double x;\n};\n\nusing strided = adjust_padding<need_strided, 47>::type;\n\n```\n\nNow `strided` has a size that is a multiple of 47 (and of course is aligned correctly). On my computer it is 376.\n\n\nYou can make `employee` a template in this fashion:\n\n\n\n```\ntemplate <std::size_t K>\nstruct employee { ...\n\n```\n\nor make it a member of a template (instead of `double x`):\n\n\n\n```\ntemplate <std::size_t K>\nstruct employee_wrapper { \n   employee e;\n   \n\n```\n\nand then use `employee_wrapper` as a vector element. But provide a specialization for 0 either way.\n\n\nYou can try using `std::array` instead of a C-style array and avoid providing a specialization for 0, but it may or may not get optimized out when the size is 0. `[[no_unique_address]]` (C++20) may help here.\n\n\nNote, something like `adjust_padding<need_strided, 117>::type` may overflow the default constexpr depth of your compiler."}
{"questionId":"e747e5c452d14d10b02cb2d2de17d949","question":"SBMainWorkspace - The request was denied by service delegate\nI\u2019m encountering a bug when launching an app on the simulator which is: The request was denied by service delegate (SBMainWorkspace).\n\n\nI know that there are some topic about this error, but any solution works for me.\n\u2028This is what I have already try:\n\n\n- Clean Derived data + project\n- Reset simulator\n- Check empty environment variables (for all target + pod)\n- Update Xcode\n- Reinstall Xcode\n- Restart the Mac (multiple time)\n- Delete project & re clone it\n- Check for Mach-O Type build settings is set to executable\n- Try on different simulators (iPhone 15, 15 Pro & 15 Plus)\n- Try on different iOS versions (16, 17 & 17.0.1)\n- Desintegrate + reinstall pod\n- Kill simulators, restart Xcode\n- Many of the things mentioned above at the same time\n\n\nNote that this bug only occurs on one specific project. I can run any other project on simulator normally.\nThe same project run also normally on other Mac.\n\n\nHere\u2019s my configuration:\nXcode 15.0.1 (15A507)\nMacOS Sonoma 14.0\nMac mini M2\n\n\nHere\u2019s the full error log:\n\n\n\n```\nSimulator device returned an error for the requested operation.\nDomain: FBSOpenApplicationServiceErrorDomain\nCode: 1\nFailure Reason: The request was denied by service delegate (SBMainWorkspace).\nUser Info: {\n    BSErrorCodeDescription = RequestDenied;\n    FBSOpenApplicationRequestID = 0x9ca3;\n    IDERunOperationFailingWorker = IDELaunchiPhoneSimulatorLauncher;\n    SimCallingSelector = \"launchApplicationWithID:options:pid:error:\";\n}\n--\nThe request to open \"com.---------\" failed.\nDomain: FBSOpenApplicationServiceErrorDomain\nCode: 1\nFailure Reason: The request was denied by service delegate (SBMainWorkspace).\nUser Info: {\n    BSErrorCodeDescription = RequestDenied;\n    FBSOpenApplicationRequestID = 0x9ca3;\n}\n--\nThe operation couldn\u2019t be completed. The process failed to launch.\nDomain: FBProcessExit\nCode: 64\nFailure Reason: The process failed to launch.\nUser Info: {\n    BSErrorCodeDescription = \"launch-failed\";\n}\n--\nThe operation couldn\u2019t be completed. Launch failed.\nDomain: RBSRequestErrorDomain\nCode: 5\nFailure Reason: Launch failed.\n--\nLaunchd job spawn failed\nDomain: NSPOSIXErrorDomain\nCode: 111\n--\n\nEvent Metadata: com.apple.dt.IDERunOperationWorkerFinished : {\n    \"device_model\" = \"iPhone16,1\";\n    \"device_osBuild\" = \"17.0.1 (21A342)\";\n    \"device_platform\" = \"com.apple.platform.iphonesimulator\";\n    \"dvt_coredevice_version\" = \"348.1\";\n    \"dvt_mobiledevice_version\" = \"1643.2.4\";\n    \"launchSession_schemeCommand\" = Run;\n    \"launchSession_state\" = 1;\n    \"launchSession_targetArch\" = arm64;\n    \"operation_duration_ms\" = 11035;\n    \"operation_errorCode\" = 1;\n    \"operation_errorDomain\" = FBSOpenApplicationServiceErrorDomain;\n    \"operation_errorWorker\" = IDELaunchiPhoneSimulatorLauncher;\n    \"operation_name\" = IDERunOperationWorkerGroup;\n    \"param_debugger_attachToExtensions\" = 0;\n    \"param_debugger_attachToXPC\" = 1;\n    \"param_debugger_type\" = 3;\n    \"param_destination_isProxy\" = 0;\n    \"param_destination_platform\" = \"com.apple.platform.iphonesimulator\";\n    \"param_diag_MainThreadChecker_stopOnIssue\" = 0;\n    \"param_diag_MallocStackLogging_enableDuringAttach\" = 0;\n    \"param_diag_MallocStackLogging_enableForXPC\" = 1;\n    \"param_diag_allowLocationSimulation\" = 1;\n    \"param_diag_checker_tpc_enable\" = 1;\n    \"param_diag_gpu_frameCapture_enable\" = 0;\n    \"param_diag_gpu_shaderValidation_enable\" = 0;\n    \"param_diag_gpu_validation_enable\" = 0;\n    \"param_diag_memoryGraphOnResourceException\" = 0;\n    \"param_diag_queueDebugging_enable\" = 1;\n    \"param_diag_runtimeProfile_generate\" = 0;\n    \"param_diag_sanitizer_asan_enable\" = 0;\n    \"param_diag_sanitizer_tsan_enable\" = 0;\n    \"param_diag_sanitizer_tsan_stopOnIssue\" = 0;\n    \"param_diag_sanitizer_ubsan_stopOnIssue\" = 0;\n    \"param_diag_showNonLocalizedStrings\" = 0;\n    \"param_diag_viewDebugging_enabled\" = 1;\n    \"param_diag_viewDebugging_insertDylibOnLaunch\" = 1;\n    \"param_install_style\" = 0;\n    \"param_launcher_UID\" = 2;\n    \"param_launcher_allowDeviceSensorReplayData\" = 0;\n    \"param_launcher_kind\" = 0;\n    \"param_launcher_style\" = 0;\n    \"param_launcher_substyle\" = 0;\n    \"param_runnable_appExtensionHostRunMode\" = 0;\n    \"param_runnable_productType\" = \"com.apple.product-type.application\";\n    \"param_structuredConsoleMode\" = 1;\n    \"param_testing_launchedForTesting\" = 0;\n    \"param_testing_suppressSimulatorApp\" = 0;\n    \"param_testing_usingCLI\" = 0;\n    \"sdk_canonicalName\" = \"iphonesimulator17.0\";\n    \"sdk_osVersion\" = \"17.0\";\n    \"sdk_variant\" = iphonesimulator;\n}\n--\n\n```\n\n**Update**\n\n\nI have try with another Xcode version (14.3.1) without success, the same error occur. Work perfectly on my personal phone..\n\n\nI investigate log with this command:\n\n\n\n```\nxcrun simctl spawn booted log show --info --debug --predicate 'processImagePath contains \"CoreSimulatorBridge\"'\n\n```\n\nI found a lot of strange log which said \"Bootstraping failed\" but it's to deep for me to understand. I'll give you more log details if you ask for.","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"other"},"answer":"I have been trying the above steps and formatting my complete system again for the past 24 hours.  \n\nWhat finally worked for me was the step I was trying to avoid.  \n\nInstall rosetta running this command on the Terminal:  \n\n`softwareupdate --install-rosetta"}
{"questionId":"b44f9acf6d9f4347b3e113e054148a4f","question":"How to enable dark mode for CefSharp web browser in .NET?\n## SCENARIO\n\n\nI'm very new to [CefSharp](https:\/\/cefsharp.github.io\/), in fact, its the first time that I'm using it. And I'm using it under a net core 6.0 winforms application, but this same question and scenario applies under .NET Framework.\n\n\nI'm running Windows 10 with OS dark mode enabled, in case of that matters.\n\n\nThis is the method that I'm calling to initialize **CefSharp** environment:\n\n\n\n```\npublic bool InitializeCefSharp()\n{\n    if (Cef.IsInitialized)\n    {\n        throw new InvalidOperationException(\"CefSharp is already initialized.\");\n    }\n\n    string cachePath = $\"{System.Windows.Forms.Application.StartupPath}\\\\cache\\\\CefSharp\";\n\n    var settings = new CefSettings\n    {\n        CachePath = cachePath,\n        RootCachePath = cachePath,\n        CommandLineArgsDisabled = false,\n        IgnoreCertificateErrors = true,\n        Locale = \"en-US\",\n        PersistSessionCookies = true,\n        PersistUserPreferences = false,\n        WindowlessRenderingEnabled = false\n    };\n\n    bool success = Cef.Initialize(settings, performDependencyCheck: true);\n    return success;\n}\n\n```\n\n*Note: Although that's C# code, I'm actually using VB.NET and the [Startup application event](https:\/\/learn.microsoft.com\/en-us\/dotnet\/api\/microsoft.visualbasic.applicationservices.windowsformsapplicationbase.startup) to initialize **CefSharp**.*\n\n\n\n\n---\n\n\n## QUESTION\n\n\n**How to enable dark mode for the initialized **CefSharp** browser?.**\n\n\nMy intention is that when loading a web page in the CEF-based browser, it will automatically be colored with a dark theme.\n\n\nOr at least, and as a last resort, only a solution for websites that checks whether the web browser is using dark or light mode and then the website automatically adapts its colors.\n\n\n\n\n---\n\n\n## RESEARCH\n\n\n*Please take into account that, as I said, I'm very new to **Cefsharp** and **CEF** APIs in general.*\n\n\n## About SetAutoDarkModeOverrideAsync method\n\n\nI've found [this interesting method named **SetAutoDarkModeOverrideAsync**](https:\/\/cefsharp.github.io\/api\/96.0.x\/html\/M_CefSharp_DevTools_Emulation_EmulationClient_SetAutoDarkModeOverrideAsync.htm) whose description says:\n\n\n\n> \n> Automatically render all web contents using a dark theme.\n> \n> \n> \n\n\nThis may be a solution, but I was unable to find an example about how to use it.\n\n\n## Using Command-Line Args\n\n\n[This answer](https:\/\/superuser.com\/a\/1642278) shows different command-line arguments to enable dark mode for CEF-based? browser. Let's take for example these two:\n\n\n- `--enable-features=WebContentsForceDark`\n- `--enable-features=WebContentsForceDark:inversion_method\/hsl_based\/image_behavior\/none\/text_lightness_threshold\/256\/background_lightness_threshold\/0`\n\n\nWell, I've tried to use those command-line arguments in this way (in my custom `InitializeCefSharp` method):\n\n\n\n```\nsettings.CefCommandLineArgs.Add(\"--enable-features=WebContentsForceDark\");\n\n```\n\nand:\n\n\n\n```\nsettings.CefCommandLineArgs.Add(\"--enable-features=WebContentsForceDark:inversion_method\/hsl_based\/image_behavior\/none\/text_lightness_threshold\/256\/background_lightness_threshold\/0\");\n\n```\n\nBut it takes no effect.\n\n\n## Using web browser Extensions (LoadExtensionFromDirectory method)\n\n\nAfter I have given up trying command-line arguments, I thought of a different methodology: use a third party extension like [Darkreader](https:\/\/github.com\/darkreader\/darkreader\/releases\/latest)\n\n\nSo I've downloaded the zip file, extracted it to a folder, and I used the [LoadExtensionFromDirectory](https:\/\/cefsharp.github.io\/api\/85.3.x\/html\/M_CefSharp_RequestContextExtensions_LoadExtensionsFromDirectory.htm) method to load the extension on the browser:\n\n\n\n```\nstring extensionDirectory = @\"C:\\darkreader-chrome\";\nvar extensionHandler = new ExtensionHandler();\nChromiumWebBrowser1.GetBrowserHost().RequestContext.LoadExtensionFromDirectory(extensionDirectory, extensionHandler);\n\n```\n\nBut it literally breaks the web browser instance; it does not load\/navigate to any webpage, it does not responds.\n\n\nIt just does not work, or at least not in the way I've tried it.\n\n\n## Using web browser Extensions (LoadExtension method)\n\n\nIf I use the [LoadExtension](https:\/\/cefsharp.github.io\/api\/79.1.x\/html\/M_CefSharp_IRequestContext_LoadExtension.htm) method in this way:\n\n\n\n```\nstring extensionDirectory = @\"C:\\darkreader-chrome\";\nvar extensionHandler = new ExtensionHandler();\nChromiumWebBrowser1.GetBrowserHost().RequestContext.LoadExtension(extensionDirectory, null, extensionHandler);\n\n```\n\nI get the same results: the web browser brokes.\n\n\n## Digging into web browser extensions usage\n\n\nI've also tried using the set of command-line args shown in [this question](https:\/\/stackoverflow.com\/q\/69143230\/1248295) to follow his same steps and maximize extension loading compatibility, without success.\n\n\nAnd in the comments section of that question there is a useful hyperlink that points to [this article](https:\/\/dev.to\/dotnetbrowser\/chrome-extensions-in-net-web-view-controls-11lm) which mentions important things like these:\n\n\n\n> \n> CefSharp's implementation is minimal, and most extensions don't work.\n> \n> \n> CefSharp supports the extensions, but most of them will not work. This\n> is because extensions need special browser APIs, of which there are\n> over 70. And CEF has only 4\u2014a small set required for a built-in PDF\n> viewer.\n> \n> \n> \n\n\nSo it seems to me that using extensions like **DarkReader** to enable dark mode is not a viable solution.\n\n\nHowever, I've also found [this answer](https:\/\/superuser.com\/a\/1588285) confirming that the mentioned command-line argument and the **DarkReader** extension both works for a chrome-based browser. But I was unable to make work any of these workarounds in **CefSharp**.\n\n\n## About `prefers-color-scheme`\n\n\nI've found [this answer](https:\/\/github.com\/cefsharp\/CefSharp\/discussions\/3955#discussioncomment-1944133) about **CefSharp** that mentions `prefers-color-scheme` with a [documentation article from Mozilla](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/@media\/prefers-color-scheme).\n\n\nHonestly, I know absolutely nothing about how to use or modify that CSS and where to use it, so I asked **ChatGPT** for help about `prefers-color-scheme` using **CefSharp**, and the answer I got, which is useless to me, was to use a code like this to set a custom attribute in the loaded web page document:\n\n\n\n```\nstring script = \"document.documentElement.setAttribute('data-theme', 'dark');\";\nbrowser.ExecuteScriptAsync(script);\n\n```\n\n...Yes, **ChatGPT** has fooled me once again, answering something that seems to have no relation to what I just asked him.\n\n\nAnd I have not enough experience in CSS to go deep into that possible solution using `prefers-color-scheme`.\n\n\n## Changing the background color\n\n\nThe [BrowserSettings.BackgroundColor](https:\/\/cefsharp.github.io\/api\/87.1.x\/html\/P_CefSharp_BrowserSettings_BackgroundColor.htm) \/ `CefSettings.BackgroundColor` property describes how to change the background color of the **CefSharp** web browser control, but as expected it only takes effect when there is no page loaded in the web browser.\n\n\n#### And about using a custom dark color scheme\n\n\nAnyway, my intention is not to create a custom dark color scheme or arbitrarily set the background of all loaded pages to a dark color since that would be too conflicting depending on which pages and their text colors and their colors for other html elements. So I think it would be too complicated to make a custom dark color scheme that is perfectly compatible with all kind of web pages.\n\n\nI am convinced that there must be a more optimal and \"safe\" or compatible solution (than creating a custom dark scheme color), such as that command line argument based on the HSL color inversion method that I mentioned at the beginning of this Research section.\n\n\n\n\n---\n\n\n## Final words\n\n\nI have taken a lot of time and effort to do my best trying to find a solution to this problem being my first time using **CefSharp**, with very little knowledge of CEF and Chromium (and Html \/ CSS) in general from my side, but to carry out something that It should be as simple and easy to do as setting dark or light mode in the damn **CefSharp** browser. It seemed very complicated to me, and it shouldn't be.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"csharp"},"answer":"As stated in the [issue you linked](https:\/\/github.com\/cefsharp\/CefSharp\/discussions\/3955#discussioncomment-1944133), CEF is able to detect the OS setting itself. In my case, I had to uncomment the compatibility entry for Windows 10 in my *app.manifest* file:\n\n\n\n```\n<compatibility xmlns=\"urn:schemas-microsoft-com:compatibility.v1\">\n  <application>\n    <!-- ... -->\n\n    <!-- Windows 10 -->\n    <supportedOS Id=\"{8e0f7a12-bfb3-4fe8-b9a5-48fd50a15a9a}\" \/>\n\n  <\/application>\n<\/compatibility>\n\n```\n\nAfter setting this, a minimal Windows forms application with a single `ChromiumWebBrowser` control automatically detected the system's color setting.\n\n\n\n\n---\n\n\nHowever, you can also *force* dark mode (i.e., [`prefers-color-scheme: dark`](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/@media\/prefers-color-scheme)) in your CEF browser control via\n\n\n\n```\nbrowser.GetDevToolsClient().Emulation.SetAutoDarkModeOverrideAsync(true);\n\n```\n\nThis retrieves the browser's [`DevToolsClient`](https:\/\/cefsharp.github.io\/api\/86.0.x\/html\/T_CefSharp_DevTools_DevToolsClient.htm) and the corresponding device emulation feature, which allows to configure numerous settings, including forcing dark mode.\n\n\n\n\n---\n\n\nI have tested both approaches with the following minimal HTML page (stored at `path\/to\/test.html` and then loaded via `file:\/\/\/path\/to\/test.html` in the browser):\n\n\n\n```\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>Test page<\/title>\n    <style>\n        body {\n            background-color: white;\n            color: black;\n        }\n\n        @media (prefers-color-scheme: dark) {\n          body {\n            background-color: black;\n            color: white;\n          }\n        }\n    <\/style>\n  <\/head>\n  <body>\n    <h1>Dark mode test<\/h1>\n  <\/body>\n<\/html>"}
{"questionId":"5e2ca667cbb74c6f9327fa74ee328a0a","question":"ERR\\_ECH\\_FALLBACK\\_CERTIFICATE\\_INVALID error on Chrome (117.0.5938.132) on Windows\n**Problem**\n\n\nUntil about a week ago, my clients site on a sub domain (app.xxx.com) worked fine and we haven't pushed anything major between now and then, but now on Chrome (117.0.5938.132) on Windows it started returning this error:\n\n\n\n```\nThis site can't be reached\nThe website at XXX might be temporarily down or it may have been moved permanently to a new web address.\nERR_ECH_FALLBACK_CERTIFICATE_INVALID\n\n```\n\nTLD seems to be working fine. Cert is issued by LetsEncrypt.\n\n\n**Testing**\n\n\nI then tested this on the latest Chrome on MacOs an Linux and they both worked fine. I also tested this on other browsers like Firefox and Safari and they also worked fine. I also issued a new LetsEncrypt cert, but this made no difference.\n\n\n**Codebase, server environment and other oddities**\n\n\nSometimes the site will resolve for a minute or two and then stops working? We have a staging version of the app on another subdomain (staging.xxx.com) that does work. We use docker containers on the same server so they should be the same environment. The code differences between the two are fairly minimal, and shouldn't effect this I would have thought?\n\n\n**Help**\n\n\nI've pushed this out to our web hosting company to try and resolve, but figured I should also post it out to see if others have any experience with this or a way to resolve it as there isn't much useful (to me at least) information out there that I have found yet. For now I'm telling our customers to use another browser, but this isn't a long term solution anyone is happy with.\n\n\nDoes anyone know anything more about this or how to resolve it?\n\n\n***EDIT - Found the issue***\n\n\nOk, so I can now confirm it's to do with `Encrypted Client Hello` (ECH), also referred to as `Secure SNI`: <https:\/\/www.ghacks.net\/2022\/11\/25\/google-chrome-canary-gets-experimental-encrypted-client-hello-ech-support\/>\n\n\nIf I turn this flag (chrome:\/\/flags\/#encrypted-client-hello - and restart your browser afterwards) on in Chrome Linux, it now throws that error.\n\n\nTurning it explicitly off in Windows and the site works - so this is the issue.\n\n\nMore information about ECH being enforced in Cloudflare and how it works: <https:\/\/blog.cloudflare.com\/announcing-encrypted-client-hello\/>\n\n\nHow to fix this is probably on whoever is dealing with your sites ECH (eg: Cloudflare) - but please do post if anyone has any other useful information.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"I found a bit better of a solution for my case I think (and others on CF free plans).\n\n\nIf you are not on a free plan - you can disable ECH as noted in the question.\n\n\nOtherwise, you can try disabling TLS 1.3 in Cloudflare settings.\n\n\nFor me the issue was as follows:\n\n\n- I have a domain on Cloudflare using SSL on a free account\n- Apparently, Cloudflare is now enforcing ECH in free accounts (<https:\/\/cmp.onl\/tn9e>)\n- I have some subdomains that are NOT using Cloudflare proxy\/SSL\n- Those subdomains encounter issues, presumably because my browser see's cloudflare's ECH record and presumes that all subdomains are using ECH.\n\n\nLong-term, I may need to figure out a better solution (maybe get my server supporting ECH?), but for now:\n\n\nDisabling TLS 1.3 seems to be a viable solution - at least for me."}
{"questionId":"4846862e2cc4446387ffb8fc10f17aae","question":"Split dataframe into multiple dataframes by grouping columns\nI have a dataframe of expression data where gene are rows and columns are samples. I also have a dataframe containing metadata for each sample in the expression dataframe. In reality my expr dataframe has 30,000+ rows and 100+ columns. However, below is an example with smaller data.\n\n\n\n```\nexpr <- data.frame(sample1 = c(1,2,2,0,0), \n                   sample2 = c(5,2,4,4,0), \n                   sample3 = c(1,2,1,0,1), \n                   sample4 = c(6,5,6,6,7), \n                   sample5 = c(0,0,0,1,1))\nrownames(expr) <- paste0(\"gene\",1:5)\nmeta <- data.frame(sample = paste0(\"sample\",1:5),\n                   treatment = c(\"control\",\"control\",\n                                 \"treatment1\", \n                                 \"treatment2\", \"treatment2\"))\n\n```\n\nI want to find the mean for each gene per treatment. From the examples I've seen with split() or group\\_by() people group based on a column already present in the data.frame. However, I have a separate dataframe (meta) that classifies the grouping for the columns in another dataframe (expr).\n\n\nI would like my output to be a dataframe with genes as rows, treatment as columns, and values as the mean.\n\n\n\n```\n#        control   treatment1   treatment2\n#  gene1  mean        mean         mean\n#  gene2  mean        mean         mean","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"r"},"answer":"Something like this. It's not entirely clear what you want to group by in the last step, but you can adjust that easily.\n\n\n\n```\nlibrary(dplyr)\nlibrary(tidyr)\n\nexpr |>\n  mutate(gene = row.names(expr)) |>\n  pivot_longer(-gene, names_to = \"sample\") |>\n  left_join(meta, by = \"sample\") |>\n  summarize(mean = mean(value), .by = c(gene, treatment)) |> \n  pivot_wider(names_from = treatment, values_from = mean)\n# # A tibble: 5 \u00d7 4\n#   gene  control treatment1 treatment2\n#   <chr>   <dbl>      <dbl>      <dbl>\n# 1 gene1       3          1        3  \n# 2 gene2       2          2        2.5\n# 3 gene3       3          1        3  \n# 4 gene4       2          0        3.5\n# 5 gene5       0          1        4  "}
{"questionId":"50eb1fcf65eb4229970dbbbbf532d05c","question":"Why does invoking a virtual method in constructor and binding a virtual method then calling it later yield different results?\nThis is my code snippet:\n\n\n\n```\nclass Base {\n  public:\n\n  Base() {\n    foo();\n    bind();\n  }\n\n  virtual void foo() {\n    std::cout << \"base foo\\n\";\n  }\n\n  void bind() {\n    fn = std::bind(&Base::foo, this);\n  };\n\n  std::function<void()> fn;\n};\n\nclass Derived : public Base {\n  public:\n\n  void foo() override {\n    std::cout << \"derived foo\\n\";\n  }\n\n  void bind()  {\n  }\n\n  int val;    \n};\n\nint main() {\n  Base* p = new Derived();\n  p->fn();\n}\n\n```\n\nThe output is:\n\n\n\n```\nbase foo\nderived foo\n\n```\n\n`foo()` prints `base foo`, because at this point, the vtable still points to `Base::foo`, according to answers under [this question](https:\/\/stackoverflow.com\/questions\/30258639\/when-is-it-safe-to-call-this-in-constructor-and-destructor).\n\n\nDuring the construction of `Base`, the object is not yet of `Derived` class. So, when calling `std::bind()` the `this` pointer is still a pointer to the `Base` class, and the argument pointer is passed in the constructor body of `Base`, so why does `p->fn` call `foo` in the `Derived` class?\n\n\nMy compiler is Apple clang version 14.0.3","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"cpp"},"answer":"> \n> when called std::bind is still a pointer to Base class, and the argument pointer is passed in constructor body of Base, why p->fn called foo in derived class?\n> \n> \n> \n\n\nOnce your `Derived` constructor is called, every pointer to the `Base` is **now a pointer to `Derived`**. The vtable accessed from that location is now a Derived vtable\n\n\nThat's the \"dynamic\" part of dynamic dispatch."}
{"questionId":"c93d7afcbf5d4970b3ace2b68c6db6b2","question":"ENOENT: no such file or directory .. NO BUILD\\_ID\n> \n> next start\n> \n> \n> \n\n\nDebugger attached.\n[Error: ENOENT: no such file or directory, open 'C:\\Janani\\ticket-app.next\\BUILD\\_ID'] {\nerrno: -4058,\ncode: 'ENOENT',\nsyscall: 'open',\npath: 'C:\\Janani\\ticket-app\\.next\\BUILD\\_ID'\n}\nWaiting for the debugger to disconnect...\n\n\nthis error is repeating again and its happening for every project... i dont know the BUILD\\_ID. can you help me in this error","questionMetadata":{"Type":"Debugging","Level":"Beginner","Tag":"javascript"},"answer":"You ran `next start` without first running `next build"}
{"questionId":"c60eabb3f44e4a568684e4aa31c57c7d","question":"How to declare a constructor of template class as friend with clang? (compiles with g++, not clang++)\nI have a template class with a private constructor which is to be friends of every typed instance of the class. The following compiles under g++ 11.4.0 but fails under clang++ version 14.0.0-1ubuntu1.1\n\n\n\n```\ntemplate <typename T>\nclass foo {\n    foo(T){}\n    template <typename U> friend foo<U>::foo(U);\n\n    public:\n    foo(){}\n};\n\nint main() {\n    foo<int> a{};\n}\n\n```\n\nclang gives the error\n\n\n\n```\nmain.cpp:4:34: error: missing 'typename' prior to dependent type name 'foo<U>::foo'\n    template <typename U> friend foo<U>::foo(U);\n                                 ^~~~~~~~~~~\n                                 typename\nmain.cpp:4:46: error: friends can only be classes or functions\n    template <typename U> friend foo<U>::foo(U);\n\n```\n\nI don't think `foo<U>::foo` is a dependent type name, but at any rate adding typename as suggested results in the error\n\n\n\n```\nmain.cpp:4:55: error: friends can only be classes or functions\n    template <typename U> friend typename foo<U>::foo(U);","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"cpp"},"answer":"This appears to be a compiler bug in clang. If we look at the related case where the class template constructor being befriended belongs to a different non-template class:\n\n\n\n```\ntemplate<class T>\nstruct X {\n    X(T);\n};\nclass C {\n    template<class U> friend X<U>::X(U);\n    C();\n};\ntemplate<class T> X<T>::X(T) { C c; }\nX<int> x(1);\n\n```\n\nThis gives a clue to the behavior of clang; which accepts with a warning:\n\n\n\n> \n> warning: dependent nested name specifier 'X::' for friend class declaration is not supported; turning off access control for 'C' [-Wunsupported-friend]\n> \n> \n> \n\n\nSo, clang is missing support in this area and doesn't realize it (to at least be able to warn) in the case where the constructor being befriended belongs to a class template.\n\n\nThe simplest workaround is probably to follow the suggestion at <https:\/\/gcc.gnu.org\/bugzilla\/show_bug.cgi?id=42328#c4> and befriend the whole class:\n\n\n\n```\n    template <typename U> friend class foo;"}
{"questionId":"fd5f3267573c461081f66751e9c3c40a","question":"Is this bitwise conversion safe?\nI have a situation where I need to pack 16 bits into a 64-bit number and later read them back as a signed integer in the range [ -32768, 32768 ). The method I have chosen for this is to compute the number as a signed 16-bit int, immediately cast it to an unsigned 16-bit int, and then upcast this to a 64-bit unsigned int before performing the proper bit shift to get the critical 16 bits into the proper place.\n\n\nHere is the pseudo-code to create the bit-packed arrangement:\n\n\n\n```\nGiven int x, y such that x - y >= -32768 and y - x < 32768;\nconst int MASK_POS = 45;\nconst unsigned short int u_s = x - y;\nunsigned long long int ull_s = u_s;\null_s <<= MASK_POS;\n\n```\n\nHere is the pseudo-code to extract the difference in the original numbers:\n\n\n\n```\nGiven unsigned long long int ull_s with 16 bits encoding a signed integer in the 46th through 61st bits;\nconst unsigned short int u_s = ((ulls >> MASK_POS) & 0xffff);\nconst short int s_s = u_s;\nconst int difference_x_and_y = s_s;\n\n```\n\nThis seems to me like a reasonable way to package a signed integer and extract it. I'm wary of platform-specific behavior when performing bit shifts on negative integers, but I think that converting to the unsigned form of the same number of bits prior to upgrading the number of overall bits in the number, and in reverse extracting the unsigned integer of the desired bit length before converting to a signed integer of equal size, will be safe.\n\n\n(In case anyone is curious, there will be a LOT going on in the other 48 bits of this 64-bit unsigned integer that stuff ends up in--from the high three bits to the low 31 and the middle 14, everything has been parsed out. I can certainly write some unit tests to ensure that this behavior holds on whatever architecture, but if anyone can see a flaw now that's better to know in advance.)","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"cpp"},"answer":"What you're doing is perfectly fine. Since C++20, signed integers are required to have two's complement representation, and all signed\/unsigned conversions are well-defined, and equivalent to `std::bit_cast`. Even before that, any implementation you care about would behave in this way.\n\n\nHowever, it would probably be better if you used fixed-width types like `std::uint16_t` since your code heavily relies on a specific width.\n\n\n\n```\nstruct quad {\n    std::int16_t x, y, z, w;\n};\n\ninline std::uint64_t pack(quad q) {\n    \/\/ Two-step conversion to std::uint16_t -> std::uint64_t\n    \/\/ to avoid a sign extension when going directly to std::uint64_t.\n    \/\/ Alternatively, mask each operand with 0xffff.\n    return std::uint64_t{std::uint16_t(q.x)} <<  0\n         | std::uint64_t{std::uint16_t(q.y)} << 16\n         | std::uint64_t{std::uint16_t(q.z)} << 32\n         | std::uint64_t{std::uint16_t(q.w)} << 48;\n    \/\/ alternatively, if you don't care about relying on\n    \/\/ platform endianness ...\n    return std::bit_cast<std::uint64_t>(q); \/\/ note: only works if quad is unpadded\n}\n\ninline quad unpack(std::uint64_t x) {\n    \/\/ just let implicit conversions do their thing\n    return { x >> 0, x >> 16, x >> 32, x >> 48 };\n    \/\/ once again, alternatively ...\n    return std::bit_cast<quad>(x);\n}\n\n```\n\nYou can pack integers like this, but it begs the question why you couldn't just use a `struct` like `quad` directly.\nNo sane compiler is going to add padding to `quad`, and you could make sure of it with\n\n\n\n```\nstatic_assert(sizeof(quad) == sizeof(std::uint64_t));\n\n```\n\nThe compiler also isn't allowed to reorder the members of `quad`, so for all intents and purposes, you could just bundle integers up in `quad` instead of packing them into an integer."}
{"questionId":"64be7af0fb754f9ab37262fcc54b1784","question":"seleniumbase (undetected Chrome driver): how to set request header?\nI am using seleniumbase with Driver(uc=True), which works well for my specific scraping use case (and appears to be the only driver that consistently remains undetected for me).\n\n\nIt is fine for everything that doesn't need specific header settings.\n\n\nFor one particular type of scrape I need to set the Request Header (Accept -> application\/json).\n\n\nThis works fine, and consistently, done manually in Chrome via the Requestly extension, but I cannot work out how to put it in place for seleniumbase undetected Chrome.\n\n\nI tried using execute\\_cdp\\_cmd with Network.setExtraHTTPHeaders (with Network.enable first): this ran without error but the request appeared to ignore it. (I was, tbh, unconvinced that the uc=True support was handling this functionality properly, since it doesn't appear to have full Chromium driver capabilities.)\n\n\nRequestly has a selenium Python mechanism, but this has its own driver and I cannot see how it would integrate with seleniumbase undetected Chrome.\n\n\nThe built-in seleniumbase wire=True support won't coexist with uc=True, as far as I can see.\n\n\nselenium-requests has an option to piggyback on an existing driver, but this is (to be honest) beyond my embryonic Python skills (though it does feel like this might be the answer if I knew how to put it in place).\n\n\nMy scraping requires initial login, so I can't really swap from one driver to another in the course of the scraping session.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"My code fragments from second effective solution derived from now deleted bountied answer (the .v2 was the piece I had not seen previously and which I think is what made it work):\n\n\n\n```\n...\nfrom seleniumwire import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom seleniumwire.undetected_chromedriver.v2 import Chrome, ChromeOptions\n...\nchrome_options = ChromeOptions()\ndriver = Chrome(seleniumwire_options={'options': chrome_options})\ndriver.header_overrides = {\n    'Accept': 'application\/json',\n}\n..."}
{"questionId":"4289786bb32d450bb5b108e2613d02b5","question":"Perl's \"Package::->method()\" (colon-colon-arrow) notation\nI was reading the [documentation for the `attributes` module](https:\/\/perldoc.perl.org\/attributes) and came across a method call notation I've never seen:\n\n\n\n```\nuse attributes ();\nattributes::->import(__PACKAGE__, \\$x, 'Bent');\n\n```\n\nThe doc did not provide an explanation for this syntax.\n\n\nOn investigation, it seems `Package::->method()` is equivalent to `Package->method()` and `'Package'->method()`.\n\n\nIt also seems equivalent to `Package::method('Package')`, provided the method is not inherited.\n\n\n**Question 0:** Is there any practical reason for using `Package::->method()` notation?\n\n\n\n\n---\n\n\nEdit: I found if you have a constant with the same name as a package, `Package::->method()` calls the package method, whereas `Package->method()` calls the method on the constant.\n\n\n\n```\nuse constant Foo => bless {}, 'Bar'; # or just => 'Bar'\n\nprint Foo::->method(); # prints 'Foo method'\nprint 'Foo'->method(); # prints 'Foo method'\nprint Foo->method();   # prints 'Bar method'\n\npackage Foo;\nsub method { 'Foo method' }\npackage Bar;\nsub method { 'Bar method' }\n\n```\n\nStrangely, with `use warnings`, this script will give a \"Bareword 'Foo::' refers to nonexistent package\" warning but *will execute it anyway*, which just confuses me more.\n\n\nEven more strangely, adding the line `Foo::method();` before the `Foo::->method()` line *gets rid of the \"Bareword\" warning*.\n\n\n**Question 1:** Can someone please explain what's going on?\n\n\nFor posterity, this is Perl v5.38.0.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"perl"},"answer":"Package::->method` is a (very slightly) safer way of writing `Package->method`.\n\n\n\n\n---\n\n\nThis is answered by the [documentation](https:\/\/perldoc.perl.org\/perlobj#Invoking-Class-Methods). [How does Perl parse unquoted bare words?](https:\/\/stackoverflow.com\/q\/58263206\/589924) is also relevant.\n\n\n`Foo::` is equivalent to `\"Foo\"`, except it also checks if namespace `Foo` exists.\n\n\n`Foo` also means `\"Foo\"`, but only if it doesn't mean anything else. It's exempt from strictures when before `->`, but there's otherwise nothing special about `Foo` here. So if you have a sub called `Foo`, and it will call that sub. If you have a constant named `Foo`, it will be used.\n\n\nIn summary,\n\n\n- `Foo::->method` always means `\"Foo\"->method`.\n- `Foo->method` usually means `\"Foo\"->method`.\n- `Foo->method` could also mean `Foo()->method`.\n\n\nFrom that, we derive the following reasons for using `Foo::`:\n\n\n- By using `Foo::`, you avoid calling a sub by accident. (This includes constants.)\n\n\nThe chances of this happening are incredibly small in practice. And it's something that would be found in testing, one hopes. But given the rarity of occurrences, it could be confusing were it to ever happen.\n- By using `Foo::`, you signal that it's not a sub call.\n\n\nBut that's already the presumption when you see something of the form `Foo->method`. It's more important to signal when it *is* a sub call (for example, by using `Foo()->method`).\n- By using `Foo::`, you get better diagnostics if you forget to load the module.\n\n\nYou get\n\n\n\n```\nBareword \"Foo::\" refers to nonexistent package at ...\nCan't locate object method \"method\" via package \"Foo\" (perhaps you forgot to load \"Foo\"?)` at ...\n\n```\n\ninstead of\n\n\n\n```\nCan't locate object method \"method\" via package \"Foo\" (perhaps you forgot to load \"Foo\"?)` at ...\n\n```\n\nDid I say better? That's just noisier to me."}
{"questionId":"3e98490e3f3141a683ecb76e0366a059","question":"Difference between numpy power and \\*\\* for certain values\nI have a numpy array where the entries in `f**2` differ from `f[i]**2`, but only for some specific value.\n\n\n\n```\nimport numpy as np\nnp.set_printoptions(precision = 16)\n\nf = np.array([ -40709.6555510835, -40708.6555510835, -33467.081758611654, -27653.379955714125])\n\nf2 = f**2\n# f2 = np.power(f,2)\n\nprint(\"outside loop\", np.abs(f[1]**2 - f2[1]), np.abs(1.0 - f2[1] \/ f[1]**2), f.dtype, f[1].dtype, f2.dtype, f2[1].dtype)\n\nfor i, val in enumerate(f):        \n    print(\"inside loop\", i, np.abs(val**2 - f2[i]), np.abs(1.0 - f2[i] \/ val**2), val.dtype, f2.dtype, f2[i].dtype)\n\n```\n\nProduces output:\n\n\n\n```\noutside loop 2.384185791015625e-07 2.220446049250313e-16 float64 float64 float64 float64\ninside loop 0 0.0 0.0 float64 float64 float64\ninside loop 1 2.384185791015625e-07 2.220446049250313e-16 float64 float64 float64\ninside loop 2 0.0 0.0 float64 float64 float64\ninside loop 3 0.0 0.0 float64 float64 float64\n\n```\n\nI do note that this is a relative error on the order of epsilon.\n\n\nThis issue goes away when using `np.power` instead of `**` in the definition of `f2`.\nEven so, why is `f[i]**2` not the same as the `i`th value of `f**2` (even if only for certain values in `f`).\n\n\nI'm using python 3.10.6 and the latest numpy 1.26.4.\n\n\n## Edit:\n\n\nThe fundamental issue is captured in:\n\n\n\n```\nimport numpy as np\nf = np.array([-40708.6555510835])\nprint((f[0])**2 - (f**2)[0])\n\n```\n\nwhich displays a value of\n\n\n\n```\n-2.384185791015625e-07\n\n```\n\nI would like to know why that specific number has this specific issue. If you'd like confirmation, or to try different values for `f`, see this [demo](https:\/\/ato.pxeger.com\/run?1=m72soLIkIz9vwYKlpSVpuhY3SzNzC_KLShTySnMLKhUSixXyCrjyCvSKU0viC4oy80ryC0oy8_OKNQqKUpMzi4FMBVsFQzNNrjQgDVSXWFSUWKkRrWtiYG5goWdmCgSGBhbGprGaXGDtGhpp0QaxmlpaRgq6ChppQFoTxIdYDnUDzC0A).","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"python"},"answer":"The results are different because `f**2` calls `numpy.square`, while `f[0]**2` and `numpy.power(f, 2)` call `numpy.power`.\n\n\n\n\n---\n\n\n`numpy.ndarray.__pow__` is written in C. [It looks like this](https:\/\/github.com\/numpy\/numpy\/blob\/v1.26.4\/numpy\/core\/src\/multiarray\/number.c#L579-L595):\n\n\n\n```\nstatic PyObject *\narray_power(PyObject *a1, PyObject *o2, PyObject *modulo)\n{\n    PyObject *value = NULL;\n\n    if (modulo != Py_None) {\n        \/* modular exponentiation is not implemented (gh-8804) *\/\n        Py_INCREF(Py_NotImplemented);\n        return Py_NotImplemented;\n    }\n\n    BINOP_GIVE_UP_IF_NEEDED(a1, o2, nb_power, array_power);\n    if (fast_scalar_power(a1, o2, 0, &value) != 0) {\n        value = PyArray_GenericBinaryFunction(a1, o2, n_ops.power);\n    }\n    return value;\n}\n\n```\n\nThe `value = PyArray_GenericBinaryFunction(a1, o2, n_ops.power);` is a Python function call to the `numpy.power` ufunc object, but first, it tries a `fast_scalar_power` function. This function tries to optimize exponentiation with common scalar powers, such as 2.\n\n\nFor the `f**2` operation, `fast_scalar_power` detects the exponent of 2, and [delegates](https:\/\/github.com\/numpy\/numpy\/blob\/v1.26.4\/numpy\/core\/src\/multiarray\/number.c#L531-L533) the operation to `numpy.square`:\n\n\n\n```\nelse if (exponent ==  2.0) {\n    fastop = n_ops.square;\n}\n\n```\n\nFor `numpy.power(f, 2)`, this is of course a direct call to `numpy.power`. `numpy.power` doesn't go through `fast_scalar_power`, and doesn't have any special handling for an exponent of 2. (Depending on what underlying power implementation it hits, that implementation might still have special handling for 2, though.)\n\n\nFor scalars, I believe `numpy.float64.__pow__` [actually just calls](https:\/\/github.com\/numpy\/numpy\/blob\/v1.26.4\/numpy\/core\/src\/multiarray\/scalartypes.c.src#L119) `array_power`:\n\n\n\n```\nstatic PyObject *\ngentype_power(PyObject *m1, PyObject *m2, PyObject *modulo)\n{\n    if (modulo != Py_None) {\n        \/* modular exponentiation is not implemented (gh-8804) *\/\n        Py_INCREF(Py_NotImplemented);\n        return Py_NotImplemented;\n    }\n\n    BINOP_GIVE_UP_IF_NEEDED(m1, m2, nb_power, gentype_power);\n    return PyArray_Type.tp_as_number->nb_power(m1, m2, Py_None);\n}\n\n```\n\nso it hits `fast_scalar_power`, but [one of the first checks](https:\/\/github.com\/numpy\/numpy\/blob\/v1.26.4\/numpy\/core\/src\/multiarray\/number.c#L513C1-L513C29) in `fast_scalar_power` is\n\n\n\n```\nif (PyArray_Check(o1) &&\n\n```\n\nAn instance of `numpy.float64` does not pass `PyArray_Check`, which checks for objects whose type is exactly `numpy.ndarray`. Thus, the scalar goes through the general `numpy.power` code path."}
{"questionId":"a0b0bb2fe1c8442788dc3bc0e755896c","question":"Existence or not of long\\_double\\_t (standard C: C23)\nThe new standard C (ISO\/IEC 9899:2023, aka C23),  \n\nin the Annex H, several macros and types are defined,  \n\nrelative to real and complex arithmetic,  \n\nsubject to the standard ISO\/IEC 60559\n(floating-point arithmetic).\n\n\nAccording to subsection H.1, there are three  \n\nimplementation-defined macros to test conformance  \n\nwith the definitions of Annex H.\n\n\n\n```\n__STDC_IEC_60559_TYPES__  \n__STDC_IEC_60559_BFP__  \n__STDC_IEC_60559_DFP__  \n\n```\n\nIn addition, to \"activate\" the associated macros and types,  \n\nthe user has to define the macro:\n\n\n\n```\n__STDC_WANT_IEC_60559_TYPES_EXT__  \n\n```\n\nThe wording of the standard is clear in several details:\n\n\n- If `__STDC_IEC_60559_BFP__` is defined by the compiler, it implies that `_FloatN`, `_FloatN_t` and other `_FloatN*` types are defined.\n- If `__STDC_IEC_60559_DFP__` is defined by the compiler, it implies that `_DecimalN`, `_DecimalN_t` and other `_DecimalN*` types are defined.\n- In either case, the macro `__STDC_IEC_60559_TYPES__` is defined.\n- If the macro `__STDC_IEC_60559_TYPES__` is defined, it means that at least one of the macros `__STDC_IEC_60559_BFP__` or `__STDC_IEC_60559_DFP__` (or both) is defined.\n\n\nIn subsection H.11, the type `long_double_t` is added to `<math.h>`.\n\n\nHowever, I cannot deduce under which conditions one can ensure the existence of `long_double_t` in a given implementation.\n\n\n**QUESTION:**\n\n\nIf `__STDC_IEC_60559_TYPES__` is defined,\nis it enough to be sure that `long_double_t` is also defined in `<math.h>`?\n\n\nMy doubt comes from the following particular case that could arise:\n\n\n- `__STDC_IEC_60559_TYPES__` and `__STDC_IEC_60559_DFP__` are defined, but `__STDC_IEC_60559_BFP__` is not defined.\n\n\nIn this case, decimal types are defined.  \n\nHowever, `long_double_t` is not a decimal type.\n\n\n**SUB-QUESTION:**  \n\nIs the macro `__STDC_IEC_60559_BFP__` required to exist in order to ensure existence of `long_double_t`?\n\n\n**COMMENTS:**\n\n\nAs @John Bollinger explains in his answer,  \n\nthe existence of `*BFP__` and\/or `*DFP__` macros  \n\nonly ensures that `_FloatN` and `_DecimalN` exist\nfor certain *small* values of `N`.  \n\n(I was aware of that detail, but my explanation was not the best.)\n\n\n[Link to a standard draft of C23](https:\/\/www.open-std.org\/jtc1\/sc22\/wg14\/www\/docs\/n3096.pdf)","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"c"},"answer":"**TL;DR: the minimum basis for relying on a conforming implementation to provide `long_double_t` is that it defines macro `__STDC_IEC_60559_TYPES__` to `202311L`.**\n\n\nHowever, *the translation unit* must also define `__STDC_WANT_IEC_60559_TYPES_EXT__` (to anything) before including math.h for the first time for that definition to actually be exposed. (C23 H.8\/1)\n\n\n\n> \n> The wording of the standard is clear in several details:\n> \n> \n> \n\n\nApparently not so clear.\n\n\nAs a preliminary matter, the spec says\n\n\n\n> \n> An implementation that defines `__STDC_IEC_60559_TYPES__` to `202311L`\n> shall conform to the specifications in this annex.\n> \n> \n> \n\n\nThis must be understood to relieve implementations from conforming to anything in the annex if they do not define `__STDC_IEC_60559_TYPES__` to that specific value (even if they define it to some other value). This is an intentional forward-compatibility measure.\n\n\nThe spec goes on to say:\n\n\n\n> \n> An implementation may define `__STDC_IEC_60559_TYPES__` only if it\n> defines `__STDC_IEC_60559_BFP__` [...] or defines `__STDC_IEC_60559_DFP__`\n> \n> \n> \n\n\nNote well that per the previous, neither that provision nor anything else in the annex applies unless `__STDC_IEC_60559_TYPES__` is defined to exactly `202311L`. Otherwise, all bets are off as far as Annex H (of C23) is concerned. For example, an implementation that defined `__STDC_IEC_60559_TYPES__` to nothing, and did not define either `__STDC_IEC_60559_BFP__` or `__STDC_IEC_60559_DFP__`, would not for that reason fail to conform.\n\n\n\n> \n> - If `__STDC_IEC_60559_BFP__` is defined by the compiler, it implies that `_FloatN`, `_FloatN_t` and other `_FloatN*` types are defined.\n> \n> \n> \n\n\nNot exactly. The spec does not associate any requirements with an implementation defining `__STDC_IEC_60559_BFP__` alone. In addition to the general gating by definition of `__STDC_IEC_60559_TYPES__`, discussed above, the spec *explicitly* conditions some of the particular provisions in this area on the latter macro being defined.\n\n\nWhen all those conditions are met, a conforming implementation provides types `_Float32` and `_Float64`, and those are equivalent to `float` and `double`, respectively. Also, under those circumstances, if `long double` corresponds to one of the ISO 60559 binary exchange formats for some width *N* greater than 64, then the implementation provides the corresponding `_FloatN` type as an equivalent to `long double`. Other `_FloatN` types are allowed, but not required, and the set of them is implementation-defined. (H.2.1\/4)\n\n\nA conforming implementation that binds itself to the provisions of this annex provides `_FloatN_t` in its math.h for each `_FloatN` type it provides.\n\n\n\n> \n> - If `__STDC_IEC_60559_DFP__` is defined by the compiler, it implies that `_DecimalN`, `_DecimalN_t` and other `_DecimalN*` types are defined.\n> \n> \n> \n\n\nAgain, implementations are not bound to anything in the annex unless they define `__STDC_IEC_60559_TYPES__` appropriately. In that case, `__STDC_IEC_60559_DFP__` being defined implies that `_Decimal32`, `_Decimal64`, and `_Decimal128` are provided. Additional `_DecimalN` types are explicitly allowed, and that the set of such additional types is implementation defined.\n\n\nWhen the provisions of the annex apply at all, math.h will provide a `_DecimalN_t` corresponding to each `_DecimalN` that the implementation defines.\n\n\n\n> \n> - In either case, the macro `__STDC_IEC_60559_TYPES__` is defined.\n> \n> \n> \n\n\nNo, there is no such requirement. The logic is the other way around. As already discussed, none of Annex H applies unless `__STDC_IEC_60559_TYPES__` is defined appropriately. If it is, then at least one of the other two macros must also be defined, but one of those being defined is not a basis for concluding anything about `__STDC_IEC_60559_TYPES__`.\n\n\n\n> \n> - If the macro `__STDC_IEC_60559_TYPES__` is defined, it means that at least one of the macros `__STDC_IEC_60559_BFP__` or\n> `__STDC_IEC_60559_DFP__` (or both) is defined.\n> \n> \n> \n\n\nMore specifically, this applies only if `__STDC_IEC_60559_TYPES__` is defined to exactly `202311L`.\n\n\n\n\n---\n\n\n\n> \n> If `__STDC_IEC_60559_TYPES__` is defined, is it enough to be sure that\n> `long_double_t` is also defined in <math.h>?\n> \n> \n> \n\n\nMacro `__STDC_IEC_60559_TYPES__` being defined to exactly `202311L` is enough to be sure that `long_double_t` is available from the implementation. The annex does not place any additional conditions on that. The program does need to opt in, however. C23 H.8\/1:\n\n\n\n> \n> The identifiers added to library headers by this annex are defined or declared by their respective\n> headers only if the macro `__STDC_WANT_IEC_60559_TYPES_EXT__` is defined (by the user) at the\n> point in the code where the appropriate header is first included.\n> \n> \n> \n\n\nYou wrote:\n\n\n\n> \n> My doubt comes from the following particular case that could arise:\n> \n> \n> - `__STDC_IEC_60559_TYPES__` and `__STDC_IEC_60559_DFP__` are defined, but `__STDC_IEC_60559_BFP__` is not defined.\n> \n> \n> In this case, decimal types are defined.\n> \n> \n> \n\n\nTrue.\n\n\n\n> \n> However, `long_double_t` is not a decimal type.\n> \n> \n> \n\n\nWho says? C23 6.2.6.1\/1:\n\n\n\n> \n> The representations of all types are unspecified except as stated in this subclause.\n> \n> \n> \n\n\nThat subclause does not define the representation of any FP types.\n\n\nThere is no requirement that either `long double` or `long_double_t` be a binary type. There seems not even to be a requirement that those two types be equivalent, though I would expect that as a quality of implementation matter.\n\n\nBut there is no problem even if `long_double_t` is a binary type. `__STDC_IEC_60559_BFP__` not being defined does not imply that the implementation does not provide any binary FP types. It means only that the implementation declines to promise that it conforms to all the provisions of Annex H with regard to binary FP types.\n\n\n\n> \n> Is the macro `__STDC_IEC_60559_BFP__` required to exist in order to ensure existence of `long_double_t`?\n> \n> \n> \n\n\nNo. H.11\/6 specifies that `long_double_t` is available from math.h, subject to program opt-in as discussed above, and that is not otherwise conditioned on anything other than the same `__STDC_IEC_60559_TYPES__` feature test macro on which *everything* in the annex is conditioned.\nThereore, if a conforming implementation defines `__STDC_IEC_60559_TYPES__` to `202311L` then its math.h defines `long_double_t` to those programs that opt in."}
{"questionId":"b9f087fb10784aa2b6233ba056ddfe77","question":"Calculate timestamps for data with known frequency and missing data\nI have data as follows, where data of type \"S\" contains a timestamp, and I need to assign timestamps to \"D\" lines.\n\n\n\n```\n   type  timestamp               count\n   <chr> <dttm>                  <int>\n 1 $     NA                         NA\n 2 D     NA                        229\n 3 M     NA                         NA\n 4 D     NA                        230\n 5 D     NA                        231\n 6 D     NA                        232\n 7 D     NA                        233\n 8 D     NA                        234\n 9 D     NA                        235\n10 D     NA                        236\n11 D     NA                        237\n12 D     NA                        238\n13 D     NA                        239\n14 S     2024-01-24 16:11:11.000    NA\n15 D     NA                        241\n16 D     NA                        242\n17 D     NA                        243\n18 D     NA                        126\n19 D     NA                        127\n20 S     2024-01-24 16:13:29.000    NA\n21 D     NA                        128\n\n```\n\n\"Count\" is a 1 byte iterator that goes from 0-255 and repeats. Missing counts indicate missing data lines. Data lines are sent at 16Hz, so each count iteration represents 1\/16 sec. I'm trying to assign the correct timestamps using the counts of the D lines to get the nearest S line timestamp and calculate the timestamp by the difference in count between the current D line and the D line immediately following an S line.\nTypically the S lines are every second, but I picked this subset to show some of the issues with the data, mainly the gap of 2:18 at line 17.\n\n\nI've figured out in a way that works, but is incredibly slow (4ms\/row, need to process ~1M lines of data per day for files that span many days). The real data is in files with lines in several formats (ick), and the times and counts in this example are parsed out of that. This is beginning to sound like a adventofcode problem, but sadly, this system is real.\n\n\nIf you'd like to check out my slow solution or see more complete data, it's in this file in the repo: <https:\/\/github.com\/blongworth\/mlabtools\/blob\/main\/R\/time_alignment.R> The data above are simplified, so the method in the repo won't work on the reprex data without modification. There are tests, but not a set for how the result from this Reprex should look yet.\n\n\nAny ideas for how to do this efficiently? I'll likely have to go to data.tables eventually, but as long as I have a start on more efficient logic, I think I can get there.\n\n\nHere's dput output for the test df above:\n\n\n\n```\nstructure(list(type = c(\"$\", \"D\", \"M\", \"D\", \"D\", \"D\", \"D\", \"D\", \n\"D\", \"D\", \"D\", \"D\", \"D\", \"S\", \"D\", \"D\", \"D\", \"D\", \"D\", \"S\", \"D\"\n), timestamp = structure(c(NA, NA, NA, NA, NA, NA, NA, NA, NA, \nNA, NA, NA, NA, 1706130671, NA, NA, NA, NA, NA, 1706130809, NA\n), tzone = \"America\/New_York\", class = c(\"POSIXct\", \"POSIXt\")), \n    count = c(NA, 229L, NA, 230L, 231L, 232L, 233L, 234L, 235L, \n    236L, 237L, 238L, 239L, NA, 241L, 242L, 243L, 126L, 127L, \n    NA, 128L)), row.names = c(NA, -21L), class = c(\"tbl_df\", \n\"tbl\", \"data.frame\"))\n\n```\n\nHere's the example data with the expected output:\n\n\n\n```\n   type  timestamp               count\n   <chr> <dttm>                  <int>\n 1 $     NA                         NA\n 2 D     2024-01-24 16:11:10.250   229\n 3 M     NA                         NA\n 4 D     2024-01-24 16:11:10.312   230\n 5 D     2024-01-24 16:11:10.375   231\n 6 D     2024-01-24 16:11:10.437   232\n 7 D     2024-01-24 16:11:10.500   233\n 8 D     2024-01-24 16:11:10.562   234\n 9 D     2024-01-24 16:11:10.625   235\n10 D     2024-01-24 16:11:10.687   236\n11 D     2024-01-24 16:11:10.750   237\n12 D     2024-01-24 16:11:10.812   238\n13 D     2024-01-24 16:11:10.875   239\n14 S     2024-01-24 16:11:11.000    NA\n15 D     2024-01-24 16:11:11.000   241\n16 D     2024-01-24 16:11:11.062   242\n17 D     2024-01-24 16:11:11.125   243\n18 D     2024-01-24 16:13:28.875   126\n19 D     2024-01-24 16:13:28.937   127\n20 S     2024-01-24 16:13:29.000    NA\n21 D     2024-01-24 16:13:29.000   128","questionMetadata":{"Type":"Optimization","Level":"Intermediate","Tag":"r"},"answer":"Here's a shot that goes through some timestamp-gymnastics.\n\n\n\n```\nlibrary(dplyr)\n# library(tidyr) # fill\ndf |>\n  mutate(count2 = count, nexttime = timestamp, prevtime = timestamp) |>\n  tidyr::fill(count2, .direction = \"updown\") |>\n  mutate(\n    count2 = count2 + 256*cumsum(c(FALSE, diff(count2) < 0)),\n    nextind = if_else(is.na(timestamp), count2[NA], count2),\n    prevind = nextind\n  ) |>\n  tidyr::fill(prevtime, prevind, .direction = \"down\") |>\n  tidyr::fill(nexttime, nextind, .direction = \"up\") |>\n  mutate(\n    newtimestamp = case_when(\n      !is.na(timestamp) ~ timestamp,\n      is.na(prevtime) | abs(count2 - nextind) < abs(count2 - prevind) ~\n        nexttime + (count2 - nextind)\/16,\n      TRUE ~\n        prevtime + (count2 - prevind)\/16\n    )\n  ) |>\n  select(names(df), newtimestamp)\n# # A tibble: 21 \u00d7 4\n#    type  timestamp               count newtimestamp           \n#    <chr> <dttm>                  <int> <dttm>                 \n#  1 $     NA                         NA 2024-01-24 16:11:10.250\n#  2 D     NA                        229 2024-01-24 16:11:10.250\n#  3 M     NA                         NA 2024-01-24 16:11:10.312\n#  4 D     NA                        230 2024-01-24 16:11:10.312\n#  5 D     NA                        231 2024-01-24 16:11:10.375\n#  6 D     NA                        232 2024-01-24 16:11:10.437\n#  7 D     NA                        233 2024-01-24 16:11:10.500\n#  8 D     NA                        234 2024-01-24 16:11:10.562\n#  9 D     NA                        235 2024-01-24 16:11:10.625\n# 10 D     NA                        236 2024-01-24 16:11:10.687\n# 11 D     NA                        237 2024-01-24 16:11:10.750\n# 12 D     NA                        238 2024-01-24 16:11:10.812\n# 13 D     NA                        239 2024-01-24 16:11:10.875\n# 14 S     2024-01-24 16:11:11.000    NA 2024-01-24 16:11:11.000\n# 15 D     NA                        241 2024-01-24 16:11:11.000\n# 16 D     NA                        242 2024-01-24 16:11:11.062\n# 17 D     NA                        243 2024-01-24 16:11:11.125\n# 18 D     NA                        126 2024-01-24 16:13:28.875\n# 19 D     NA                        127 2024-01-24 16:13:28.937\n# 20 S     2024-01-24 16:13:29.000    NA 2024-01-24 16:13:29.000\n# 21 D     NA                        128 2024-01-24 16:13:29.000\n\n```\n\nNotes:\n\n\n- `count2` is just `count` fully interpolated for `NA`s\n- The use of `nexttime`\/`prevtime` is to carry-forward and carry-backward `timestamp` until there is another non-`NA` timestamp, I choose which to use in the `case_when`;\n- The `nextind`\/`prevind` are used to subtract from `count2` so that I can account for 1\/16th seconds.\n- The `case_when` is really where most of the logic works, determine if the original `timestamp` should be retained, or `(count2-nextind)\/16` (or `prevind`) 1\/16ths seconds from the `nexttime` (`prevtime`).\n\n\n\n\n---\n\n\nA `data.table` solution looks fairly similar. Using R-4.2 or newer, we can use `|> _[]` formatting:\n\n\n\n```\nlibrary(data.table)\nout <- as.data.table(df) |>\n  _[, count2 := nafill(nafill(count, type = \"nocb\"), type = \"locf\") ] |>\n  _[, count2 := count2 + 256*cumsum(c(FALSE, diff(count2) < 0)) ] |>\n  _[, nextind := fifelse(is.na(timestamp), count2[NA], count2) ] |>\n  _[, prevind := nextind ] |>\n  _[, c(\"prevtime\", \"prevind\") := lapply(.SD, nafill, type = \"locf\"), .SDcols = c(\"timestamp\", \"prevind\")] |>\n  _[, c(\"nexttime\", \"nextind\") := lapply(.SD, nafill, type = \"nocb\"), .SDcols = c(\"timestamp\", \"nextind\")] |>\n  _[, newtimestamp := fcase(\n    !is.na(timestamp), timestamp,\n    is.na(prevtime) | abs(count2 - nextind) < abs(count2 - prevind), nexttime + (count2 - nextind)\/16,\n    rep(TRUE, .N), prevtime + (count2 - prevind)\/16) ] |>\n  _[, .SD, .SDcols = c(names(df), \"newtimestamp\")]\n\n```\n\nIf on R before 4.2, we can use `data.table`'s `][`-piping.\n\n\n\n```\nDT <- as.data.table(df) # setDT(df) is canonical, avoiding that here for side-effect\nDT[, count2 := nafill(nafill(count, type = \"nocb\"), type = \"locf\")\n   ][, count2 := count2 + 256*cumsum(c(FALSE, diff(count2) < 0))\n   ][, nextind := fifelse(is.na(timestamp), count2[NA], count2)\n   ][, prevind := nextind\n   ][, c(\"prevtime\", \"prevind\") := lapply(.SD, nafill, type = \"locf\"), .SDcols = c(\"timestamp\", \"prevind\")\n   ][, c(\"nexttime\", \"nextind\") := lapply(.SD, nafill, type = \"nocb\"), .SDcols = c(\"timestamp\", \"nextind\")\n   ][, newtimestamp := fcase(\n     !is.na(timestamp), timestamp,\n     is.na(prevtime) | abs(count2 - nextind) < abs(count2 - prevind), nexttime + (count2 - nextind)\/16,\n     rep(TRUE, .N), prevtime + (count2 - prevind)\/16)\n   ][, .SD, .SDcols = c(names(df), \"newtimestamp\")]\n\n\n```\n\nI prefer `tidyr::fill`'s `.direction=\"updown\"`, it reduces the call stack and is easier to read in pipes like this."}
{"questionId":"e75da9db3a664d09bab3745a0bac5744","question":"Can you dereference a temporary array?\nConsider the following code:\n\n\n\n```\nvoid foo() {\n    int arr[1];\n    *arr; \/\/ OK\n\n    using T = int[1];\n    *T{}; \/\/ OK for Clang and MSVC\n          \/\/ GCC error: taking address of temporary array\n}\n\n```\n\n*See [live code at Compiler Explorer](https:\/\/godbolt.org\/z\/r3WzcdMq8)*.\n\n\nMy intuition is that `*T{}` should result in array-to-pointer conversion, and the indirection is well-formed.\nHowever, I'm not entirely sure about this.\n\n\nIs GCC right, or is this a bug? Is it intentional, to prevent developers from making mistakes? After all, you don't usually dereference arrays. Is this documented anywhere?\n\n\n\n\n\n| Disclaimer |\n| --- |\n| [CWG Issue 2548](https:\/\/cplusplus.github.io\/CWG\/issues\/2548) has confirmed that **\"indirection through an array prvalue is also not valid today\"**. The answer by @StoryTeller is wrong, and misinterprets the meaning of [destination type](https:\/\/timsong-cpp.github.io\/cppwp\/n4868\/dcl.init.general#def:destination_type) by assuming that this also applies to `*T{}`, but this expression is not initialization of a pointer. |\n\n\n\n*More discussion at editorial issue [EDIT 6555](https:\/\/github.com\/cplusplus\/draft\/issues\/6555)*","questionMetadata":{"Type":"Debugging","Level":"Advanced","Tag":"cpp"},"answer":"GCC is right because indirection can only be applied to pointers. [[expr.unary.op]\/1](https:\/\/timsong-cpp.github.io\/cppwp\/n4868\/expr.unary.op#1):\n\n\n\n> \n> The unary `*` operator performs indirection: the expression to which it is applied **shall be a pointer to an object type, or a pointer to a function type** and the result [...]\n> \n> \n> \n\n\n*Note: [CWG 1642](https:\/\/cplusplus.github.io\/CWG\/issues\/1642) and [EDIT 3945](https:\/\/github.com\/cplusplus\/draft\/pull\/3945) makes indirection apply to a **prvalue** operand.*\n\n\n`*arr` is OK because `arr` is an lvalue, and [[basic.lval]\/6](https:\/\/timsong-cpp.github.io\/cppwp\/n4868\/basic.lval#6):\n\n\n\n> \n> Whenever a glvalue appears as an operand of an operator that expects a prvalue for that operand, the lvalue-to-rvalue, array-to-pointer, or function-to-pointer standard conversions are applied to convert the expression to a prvalue.\n> \n> \n> \n\n\nArray-to-pointer conversion ([[conv.array]](https:\/\/timsong-cpp.github.io\/cppwp\/n4868\/conv.array)) is applied to`arr`.\n\n\n`T{}` is already a prvalue (of array type), and there is no wording which necessitates array-to-pointer conversion here, so `*T{}` is not OK."}
{"questionId":"002f7b974c4245c2bae5a33d07758a43","question":"Transversal visibility of Bash's exported functions\nI ran an experiment to see if a Bash's exported function was still visible with a different shell in-between the processes chain.\n\n\nTo my surprise the exported function is sometimes still visible.\n\n\nI am looking for an explanation of how and why a Bash's specific exported function is maintained within the environment through different shells, whereas it is not a POSIX standard environment variable.\n\n\nSomehow I have a clue that the exported function declaration is stored as a string within a (standard ?) environment variable. But how is it invisible as an environment variable to incompatible shells, and how is it maintained within the environment?\n\n\nHere is my experiment code:\n\n\n\n```\n#!\/usr\/bin\/env bash\n\nLANG=C\n\n# Declare function and export it\nfn(){ printf 'Hello World\\n';}\nexport -f fn\n\n# Standard usage, call exported function within a child bash\nbash -c fn\n\n# Available shells for tests\navailable_shells=()\nfor sh in ash bash dash ksh tcsh zsh\ndo\n  if shell_path=$(command -v \"$sh\" 2> \/dev\/null)\n  then\n    available_shells+=(\"$shell_path\")\n    real_shell_path=$(realpath \"$shell_path\")\n    shell_version=$(\n      read -r pkg < <(apt-file search \"${real_shell_path#\/usr}\" | awk -F: '{print $1}')\n      dpkg -s \"$pkg\" | awk '\/Version:\/{print $2}'\n    )\n    printf 'Found: %s\\tversion: %s\\n' \"$shell_path\" \"$shell_version\"\n  fi\ndone\n\n# Test transversal visibility of exported fn through different shells\nfor sh in \"${available_shells[@]}\"; do\n  printf \"Testing %s -c 'bash -c fn':\\\\n\" \"$sh\"\n  \"$sh\" -c 'bash -c fn'\n  printf \\\\n\ndone\n\n```\n\nThe result on my system reveal that exported `fn` transversal visibility is maintained through all tested shells except `ash` and `dash`:\n\n\n\n```\nHello World\nFound: \/usr\/bin\/ash     version: 0.5.12-2ubuntu1\nFound: \/usr\/bin\/bash   version: 5.2.15-2ubuntu1\nFound: \/usr\/bin\/dash   version: 0.5.12-2ubuntu1\nFound: \/usr\/bin\/ksh     version: 1.0.4-3\nFound: \/usr\/bin\/tcsh    version: 6.24.07-1\nFound: \/usr\/bin\/zsh     version: 5.9-4\nTesting \/usr\/bin\/ash -c 'bash -c fn':\nbash: line 1: fn: command not found\n\nTesting \/usr\/bin\/bash -c 'bash -c fn':\nHello World\n\nTesting \/usr\/bin\/dash -c 'bash -c fn':\nbash: line 1: fn: command not found\n\nTesting \/usr\/bin\/ksh -c 'bash -c fn':\nHello World\n\nTesting \/usr\/bin\/tcsh -c 'bash -c fn':\nHello World\n\nTesting \/usr\/bin\/zsh -c 'bash -c fn':\nHello World","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"other"},"answer":"The environment can really only contain strings, so when you use `export -f` in bash, it actually just creates a regular environment variable (with a weird name) and sets it to the text of the function's definition. For example:\n\n\n\n```\n$ fn(){ printf 'Hello World\\n';}\n$ export -f fn\n$ printenv BASH_FUNC_fn%%\n() {  printf 'Hello World\\n'\n}\n\n```\n\nDuring bash initialization, it scans the environment for variables with names like this, and converts any that it finds into functions.\n\n\nSince this is just a regular environment variable (other than its name), it'll persist through various other programs (including shells), unless the programs do something to remove them from the environment. It appears that both dash and ash do this (though I don't have ash handy to test):\n\n\n\n```\n$ dash -c 'printenv BASH_FUNC_fn%% || echo \"not found\"'\nnot found\n\n```\n\nYou can see the same effect with other weirdly-named environment variables:\n\n\n\n```\n$ env test%=\"still here\" bash -c 'printenv test% || echo \"not found\"'\nstill here\n$ env test%=\"still here\" ksh -c 'printenv test% || echo \"not found\"'\nstill here\n$ env test%=\"still here\" zsh -c 'printenv test% || echo \"not found\"'\nstill here\n$ env test%=\"still here\" dash -c 'printenv test% || echo \"not found\"'\nnot found"}
{"questionId":"c93f647076454a669f86594102197d75","question":"How does Perl apparently avoid catastrophic backtracking?\nI am using Perl 5.34.1 and Python 3.10.12.\n\n\nThe following script takes 16 seconds in Python:\n\n\n\n```\nimport re\npatt = re.compile(r\"W(X|Y+)+Z\")\nprint(patt.search(\"WYYYYYYYYYYYYYYYYYYYYYYYYYYYYA\"))\n\n```\n\nBut takes almost no time at all in Perl:\n\n\n\n```\nuse v5.34;\nmy $patt = qr\/W(X|Y+)+Z\/;\nprint(\"WYYYYYYYYYYYYYYYYYYYYYYYYYYYYA\" =~ $patt);\n\n```\n\nThis is an example of catastrophic backtracking provided by <https:\/\/medium.com\/bigpanda-engineering\/catastrophic-backtracking-the-dark-side-of-regular-expressions-80cab9c443f6>.\n\n\nI would expect that *any* backtracking regex engine would suffer in this case, but apparently the Perl regex engine does not. How does it avoid the catastrophic backtracking in this example? Is it actually backtracking \"catastrophically\" internally, but orders of magnitude faster than the Python engine? Is the some fast-path optimization it uses to prevent fail early before completing the full backtracking sequence?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"perl"},"answer":"Because of optimizations. It realizes a `Z` is needed, but it doesn't find one.\n\n\n\n```\n$ perl -Mre=debug -e'\n    use v5.34;\n    my $patt = qr\/W(X|Y+)+Z\/;\n    print(\"WYYYYYYYYYYYYYYYYYYYYYYYYYYYYA\" =~ $patt);\n'\nCompiling REx \"W(X|Y+)+Z\"\nFinal program:\n   1: EXACT <W> (3)\n   3: CURLYX<1:1>[0]{1,INFTY} (21)\n   7:   OPEN1 (9)\n   9:     BRANCH (buf:1\/1) (13)\n  11:       EXACT <X> (18)\n  13:     BRANCH (buf:1\/1) (FAIL)\n  15:       PLUS (18)\n  16:         EXACT <Y> (0)\n  18:   CLOSE1 (20)\n  20: WHILEM[1\/1] (0)\n  21: NOTHING (22)\n  22: EXACT <Z> (24)\n  24: END (0)\nanchored \"W\" at 0..0 floating \"Z\" at 2..9223372036854775807 (checking floating) minlen 3\nMatching REx \"W(X|Y+)+Z\" against \"WYYYYYYYYYYYYYYYYYYYYYYYYYYYYA\"\nIntuit: trying to determine minimum start position...\n  doing 'check' fbm scan, [2..30] gave -1\n  Did not find floating substr \"Z\"...\nMatch rejected by optimizer\nFreeing REx: \"W(X|Y+)+Z\"\n\n```\n\n\n\n---\n\n\nThat said, using `qr\/W(X|Y+)+(Z|!)\/` bypasses that optimization.\n\n\n\n```\n$ perl -Mre=debug -e'\n    use v5.34;\n    my $patt = qr\/W(X|Y+)+Z\/;\n    print(\"WYYYYYYYYYYYYYYYYYYYYYYYYYYYYA\" =~ $patt);\n' 2>&1 | wc -l\n22\n\n$ perl -Mre=debug -e'\n    use v5.34;\n    my $patt = qr\/W(X|Y+)+(Z|!)\/;\n    print(\"WYYYYYYYYYYYYYYYYYYYYYYYYYYYYA\" =~ $patt);\n' 2>&1 | wc -l\n2971\n\n```\n\nStill, the difference is less than 1 ms on my machine.\n\n\n\n```\n$ time perl -e'\n    use v5.34;\n    my $patt = qr\/W(X|Y+)+Z\/;\n    print(\"WYYYYYYYYYYYYYYYYYYYYYYYYYYYYA\" =~ $patt);\n' 2>&1 | wc -l\n0\n\nreal    0m0.002s\nuser    0m0.002s\nsys     0m0.000s\n\n$ time perl -e'\n    use v5.34;\n    my $patt = qr\/W(X|Y+)+(Z|!)\/;\n    print(\"WYYYYYYYYYYYYYYYYYYYYYYYYYYYYA\" =~ $patt);\n' 2>&1 | wc -l\n0\n\nreal    0m0.002s\nuser    0m0.002s\nsys     0m0.000s\n\n```\n\nObviously, there are other optimizations. `-Mre=debug` with this version includes this message:\n\n\n\n```\nWHILEM: Detected a super-linear match, switching on caching...\n\n```\n\nYou could try looking if Yves \"demerphq\" Orton has done any talks on this."}
{"questionId":"b1fa81c6ef10499896196a4b6a145d69","question":"Does a restrict-qualified pointer parameter of a function allow optimization of the caller functions?\nA restrict-qualified pointer parameter of a function is an effective way to allow compilers to optimize **that function**:\n\n\n\n```\nint f(const int *restrict p) {\n  int n=*p;\n  printf(\"Debug\\n\");\n  return *p==n;\n}\n\n```\n\nHere Clang 17 with -O3 will just return 1 without re-examining `*p`, and without performing the comparison.\nWithout `restrict`, `*p` has to be re-examined and the comparison has to be performed. This is because `printf` is an external function, and the compiler doesn't know if it might modify `*p`. Indeed, if `p` happens to be pointing into the file position indicator of `stdout`, `printf` is actually going to modify it.\n\n\nSo, as a general principle, if you want maximum optimization for **the function**, and your function ever calls an external function, it is not meaningless to declare all your parameters `restrict`. Of course, this places a severe restriction on callers, because it requires them to promise that they will not call your function with pointers aliased into unexpected places.\n\n\nMy question goes further than this observation. I ask if `restrict` permits optimization of **the callers of your function** too?\n\n\n\n```\nint f(const int *restrict p);\nint caller(int *p) {\n  int n=*p;\n  f(p);\n  return *p==n;\n}\n\n```\n\nIt would seem to me that this guarantees to the compiler that `f` will not modify `*p`, thus it can omit re-examining `*p` and the comparison, and can safely return 1. Neither Clang nor GCC do this optimization.\n\n\nI know it's OK that they don't optimize the caller. I am asking whether the are permitted to optimize. (This is primarily a question about the semantics of my program, not about performance.)","questionMetadata":{"Type":"Conceptual","Level":"Advanced","Tag":"c"},"answer":"In this code:\n\n\n\n```\nint f(const int *restrict p);\nint caller(int *p) {\n  int n=*p;\n  f(p);\n  return *p==n;\n}\n\n```\n\nthe compiler cannot conclude that `*p` does not change during execution of `f` because the code below shows a definition of `f` and a call to `caller` in which the behavior is defined (does not violate the `restrict` requirements) yet the value of `*p` changes during execution of `f`:\n\n\n\n```\nextern int m[2];\n\nint f(const int * restrict p)\n{\n    m[0] = 3;\n    return p[1];\n}\n\nvoid bar(void)\n{\n    caller(m);\n}\n\n```\n\nWhen `f` is called in this case, `p` points to `m[0]`, so `*p` is `m[0]`. Since `f` changes `m[0]`, the value of `*p` changes during execution of `f`, but this does not violate the `restrict` requirements because the `restrict` definition only imposes requirements if `p` is used, directly or indirectly, to access the object. C 2018 6.7.3.1 4 says:\n\n\n\n> \n> During each execution of `B`, let `L` be any lvalue that has `&L` based on `P`. If `L` is used to access the value of the object `X` that it designates, and `X` is also modified (by any means), then the following requirements apply:\u2026\n> \n> \n> \n\n\nIn the above `f`, `m[0]` is accessed only through the lvalue `m[0]` and not through any lvalue whose address is based on `p`. So the prerequisite condition for `restrict` never occurs for `m[0]`, so it imposes no constraints on whether or how `m[0]` is modified."}
{"questionId":"0d28fe7e9c57452096cf3d351c1a3c9d","question":"Shadcn Dialog inside of Dropdown closes automatically\nI use shadcn in my next.js 13 project. I want to have a dropdown with the option to edit or delete an entry. When the user clicks on \"delete\" a dialog should pop up and ask them for a confirmation. However, the dialog only shows for about 0.5 seconds before it closes together with the dropdown. How can I prevent that from happening?\n\n\nHere is the example on codesandbox: [Codesandbox](https:\/\/codesandbox.io\/p\/sandbox\/shadcn-playground-3gs3v6?file=%2Fsrc%2Flib%2Futils.js%3A1%2C1)\n\n\nThis is the code:\n\n\n\n```\n    <DropdownMenu>\n      <DropdownMenuTrigger>\n        <p>Trigger<\/p>\n      <\/DropdownMenuTrigger>\n      <DropdownMenuContent>\n        <Dialog>\n          <DropdownMenuLabel>Edit Entry<\/DropdownMenuLabel>\n          <DropdownMenuSeparator \/>\n          <DropdownMenuItem\n            onClick={() => conosle.log(\"Navigate to edit page\")}\n          >\n            Edit\n          <\/DropdownMenuItem>\n          <DialogTrigger>\n            <DropdownMenuItem>Delete<\/DropdownMenuItem>\n          <\/DialogTrigger>\n          <DialogContent>\n            <DialogHeader>\n              <DialogTitle>Are you sure?<\/DialogTitle>\n              <DialogDescription>\n                Do you want to delete the entry? Deleting this entry cannot be\n                undone.\n              <\/DialogDescription>\n            <\/DialogHeader>\n            <DialogFooter>\n              <DialogClose asChild>\n                <Button variant=\"outline\">Cancel<\/Button>\n              <\/DialogClose>\n              <Button>Delete<\/Button>\n            <\/DialogFooter>\n          <\/DialogContent>\n        <\/Dialog>\n      <\/DropdownMenuContent>\n    <\/DropdownMenu>","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"javascript"},"answer":"## What is the problem?\n\n\nWhen you click any `<DropdownMenuItem \/>`, It will trigger the action (onClick) and close (unmount) the `<DropdownMenuContent \/>` which includes the `<DialogContent \/>` so it'll be unmounted with it.\n\n\n## Solutions\n\n\n### 1. Move the `<DialogContent \/>` outside of the `<DropdownMenuContent \/>`\n\n\n\n```\n\/\/ ...\nexport default function App() {\n  return (\n    <Dialog> {\/* \ud83d\udd34 The dialog provider outside of the DropdownMenuContent *\/}\n      <DropdownMenu>\n        <DropdownMenuTrigger>\n          <p>Trigger<\/p>\n        <\/DropdownMenuTrigger>\n        <DropdownMenuContent>\n          <DropdownMenuItem>\n            <DialogTrigger>\n              Open Popup\n            <\/DialogTrigger>\n          <\/DropdownMenuItem>\n        <\/DropdownMenuContent>\n      <\/DropdownMenu>\n      {\/* \ud83d\udd34 DialogContent ouside of DropdownMenuContent *\/}\n      <DialogContent>\n        <DialogHeader>\n          <DialogTitle>Are you sure?<\/DialogTitle>\n          <DialogDescription>\n            Do you want to delete the entry? Deleting this entry cannot be\n            undone.\n          <\/DialogDescription>\n        <\/DialogHeader>\n        <DialogFooter>\n          <DialogClose asChild>\n            <Button variant=\"outline\">Cancel<\/Button>\n          <\/DialogClose>\n          <Button>Delete<\/Button>\n        <\/DialogFooter>\n      <\/DialogContent>\n    <\/Dialog>\n  );\n}\n\n```\n\nThis solution works well if you have a single item triggers dialog. But what if you have multiple dialogs?\n\n\n### 2. Multiple dialogs\n\n\nMove your dialog outside the `<DropdownMenuContent \/>`, create a state for each one:\n\n\n\n```\nconst [isEditDialogOpen, setIsEditDialogOpen] = useState(false)\nconst [isDeleteDialogOpen, setIsDeleteDialogOpen] = useState(false)\n\n```\n\nthen remove any `<DialogTrigger \/>`, add onClick instead\n\n\n\n```\n<DropdownMenuItem onClick={() => setIsEditDialogOpen(true)}>Edit<\/DropdownMenuItem>\n<DropdownMenuItem onClick={() => setIsDeleteDialogOpen(true)}>Delete<\/DropdownMenuItem>\n\n```\n\nIn your dialog add\n\n\n\n```\n<Dialog open={isEditDialogOpen || isDeleteDialogOpen} \n        onOpenChange={isEditDialogOpen ? \n            setIsEditDialogOpen : setIsDeleteDialogOpen}>\n...\n<\/Dialog>\n\n```\n\nIf you don't want to make it controlled and can render two trigger buttons, you can render two separate dialogs:\n\n\n\n```\n<Dialog>\n  <DialogTrigger>Edit Post<\/DialogTrigger>\n  <DialogContent>\n     Content \n  <\/DialogContent>\n<\/Dialog>\n\n<Dialog>\n  <DialogTrigger>Edit Post<\/DialogTrigger>\n  <DialogContent>\n    Content \n  <\/DialogContent>\n<\/Dialog>"}
{"questionId":"7694f5341f254182b8f66e3593bd2a99","question":"RuntimeIdentifier is not recognized using .NET 8\nI would like to create a new WinUI 3 desktop application using .NET 8. After I initialze the project in Visual Studio, the project is on .NET 6. I change the .csproj file's TargetFramework from `net6.0-windows10.0.19041.0` to `net8.0-windows10.0.19041.0`, but this error pops up. How can I solve this issuewith .NET 8?\n\n\nThe errors:\n\n\n\n```\nNETSDK1083: The specified RuntimeIdentifier \"win10-x86\" is not recognized\nNETSDK1083: The specified RuntimeIdentifier \"win10-x64\" is not recognized\nNETSDK1083: The specified RuntimeIdentifier \"win10-arm64\" is not recognized\n\n```\n\nMy `.csproj` file:\n\n\n\n```\n<PropertyGroup>\n  <OutputType>WinExe<\/OutputType>\n  <TargetFramework>net8.0-windows10.0.19041.0<\/TargetFramework>\n  <TargetPlatformMinVersion>10.0.17763.0<\/TargetPlatformMinVersion>\n  <RootNamespace>SudokuEngineApp<\/RootNamespace>\n  <ApplicationManifest>app.manifest<\/ApplicationManifest>\n  <Platforms>x86;x64;ARM64<\/Platforms>\n  <RuntimeIdentifiers>win10-x86;win10-x64;win10-arm64<\/RuntimeIdentifiers>\n  <PublishProfile>win10-$(Platform).pubxml<\/PublishProfile>\n  <UseWinUI>true<\/UseWinUI>\n  <EnableMsixTooling>true<\/EnableMsixTooling>\n<\/PropertyGroup>\n\n```\n\nI have tried with .NET 7 and it worked:\n\n\n`<TargetFramework>net7.0-windows10.0.19041.0<\/TargetFramework>`\n\n\nWith .NET 8 it doesn't.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"csharp"},"answer":"Well, after a bit of searching, I found the solution, that works for me.\n\n\nThe [official Microsoft documentation](https:\/\/learn.microsoft.com\/en-us\/dotnet\/core\/compatibility\/sdk\/8.0\/rid-graph) says:\n\n\n\n> \n> Use portable RIDs, for example, `linux-<arch>`, `linux-musl-<arch>`, `osx-<arch>`, and `win-<arch>`.\n> \n> \n> \n\n\nSo I changed the `RuntimeIdentifiers` and the `PublishProfile`.\n\n\nThe \"10\" had to be removed. Like `win10-x64` to `win-x64`.\n\n\nMy modified `.csproj` is the following:\n\n\n\n```\n<PropertyGroup>\n  <OutputType>WinExe<\/OutputType>\n  <TargetFramework>net8.0-windows10.0.19041.0<\/TargetFramework>\n  <TargetPlatformMinVersion>10.0.17763.0<\/TargetPlatformMinVersion>\n  <RootNamespace>SudokuEngineApp<\/RootNamespace>\n  <ApplicationManifest>app.manifest<\/ApplicationManifest>\n  <Platforms>x86;x64;ARM64<\/Platforms>\n  <RuntimeIdentifiers>win-x86;win-x64;win-arm64<\/RuntimeIdentifiers>\n  <PublishProfile>win-$(Platform).pubxml<\/PublishProfile>\n  <UseWinUI>true<\/UseWinUI>\n  <EnableMsixTooling>true<\/EnableMsixTooling>\n<\/PropertyGroup>"}
{"questionId":"1b4d247cb72940ca825b6862d2861eca","question":"Sum two std::array of non-default-constructible types\nIf I want to write a function summing two `std::array`, I would do something like:\n\n\n\n```\ntemplate<class T, std::size_t N>\nauto sum(const std::array<T, N>& a, const std::array<T, N>& b)\n{\n    std::array< decltype(a[0] + b[0]), N> result; \/\/the underlying type is not necessarily T\n    for (std::size_t i = 0; i < N; ++i)\n    {\n        result[i] = a[i] + b[i];\n    }\n    return result;\n} \n\n```\n\nHow would I write this function in case `decltype(a[0] + b[0])` is not default-constructible?\n\n\nHere is an example on compiler explorer: <https:\/\/godbolt.org\/z\/qndGfhehM>\n\n\nIdeally, I want to directly create and initialize the `std::array`, but I failed to write a function for any `N`.\n\n\nI would like to rely only on a C++ standard.\n\n\nI guess the implementation can be achieved with an intermediate `std::vector`, but it would require copies.\n\n\nI tried to write a recursive template function such as:\n\n\n\n```\ntemplate<class T, std::size_t N>\nauto operator+(const std::array<T, N>& a, const std::array<T, N>& b)\n{\n    if constexpr (N > 1)\n    {\n        return {a[0] + b[0], ???};\n    }\n    else\n    {\n        return {a[0] + b[0]};\n    }\n}\n\n```\n\nbut I don't know what to write instead of the *???*.\n\n\nI am aware that `std::index_sequence` exists, but I don't know how to use it in this case.","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"cpp"},"answer":"Something along these lines:\n\n\n\n```\ntemplate <typename T, std::size_t N, std::size_t... Is>\nauto sum_helper(const std::array<T, N>& a, const std::array<T, N>& b,\n                std::index_sequence<Is...>) {\n    return std::array<decltype(a[0] + b[0]), N>{(a[Is] + b[Is])...};\n}\n\ntemplate<class T, std::size_t N>\nauto sum(const std::array<T, N>& a, const std::array<T, N>& b)\n{\n    return sum_helper(a, b, std::make_index_sequence<N>{});\n} \n\n```\n\n[Demo](https:\/\/godbolt.org\/z\/cch5KGKc4)"}
{"questionId":"ecdb48c7953f4ae2b4a72b50a2c53c61","question":"How to log errors with `log\/slog\nThe official docs show how to use the new structured logging package but seem to omit how to log errors.\n\n\n<https:\/\/pkg.go.dev\/log\/slog>\n\n\n\n```\npackage main\n\nimport (\n    \"fmt\"\n    \"log\/slog\"\n    \"os\"\n)\n\nfunc demoFunction() error {\n    return fmt.Errorf(\"oh no: %v\", 123)\n}\n\nfunc main() {\n    logger := slog.New(slog.NewJSONHandler(os.Stdout, nil))\n    slog.SetDefault(logger)\n\n    slog.Info(\"info demo\", \"count\", 3)\n    slog.Warn(\"warn demo\", slog.String(\"somekey\", \"somevalue\"))\n    slog.Error(\"error demo\", slog.Int(\"someintkey\", 123))\n    err := demoFunction()\n    if err != nil {\n        \/\/ Here I'm logging the error as a string, but I presume there is a better way\n        \/\/ possibly that will log stack trace info as well.\n        slog.Error(\"the demo function got an error.\", slog.String(\"error\", err.Error()))\n    }\n}","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"other"},"answer":"Someone [opened up a proposal](https:\/\/github.com\/golang\/go\/issues\/63547) and closed it. Thought is it ends up being unnecessary syntax sugar.\n\n\nIt appears [some people](https:\/\/github.com\/golang\/go\/issues\/59364) have decided to wrap the [slog.Any](https:\/\/pkg.go.dev\/log\/slog#Any) call\n\n\n\n```\nfunc ErrAttr(err error) slog.Attr {\n    return slog.Any(\"error\", err)\n}"}
{"questionId":"eabed9139f094fb59149c8036d4302a9","question":"Is it possible to predefine the initial size of a Str?\nIn some languages it is possible for the developer to specify an initial size for a string. Has Raku something similar? I have a string where I'll be concatenating text a few thousand times. The final size can be calculated before creating the string. I would like to test if I can save a few 100ms with this.","questionMetadata":{"Type":"Optimization","Level":"Intermediate","Tag":"perl"},"answer":"Short answer: No.\n\n\nLonger answer: when you concatenate strings in Rakudo on the MoarVM backend, you don't actually create a concatenated string: internally, you have an object that consists of the separate pieces of strings.\n\n\nThere is only one case where the string actually gets concatenated: and that's when you use it as a haystack in a regular expression. And I believe there's a maximum number of \"pieces\" of string that, when reached, will cause actual concatenation in memory.\n\n\nIf you're looking at optimization: then don't concatenate each time, but push to a native `str` array, and when done, then `.join` that:\n\n\n\n```\nmy str @parts;\nwhile $todo {\n    @parts.push: \"foo\";\n}\n@parts.join\n\n```\n\nThis is generally faster because a. you're using native strings, and b. all of the concatenation logic (which is actually not as simple as you think because e.g. a diacritic codepoint can be at the start of a part, and that needs to possibly be joined with the last codepoint of the previous part) can be done in the VM without needing to switch between HLL ops and C-code of the VM."}
{"questionId":"361c5c1e60674d1983235e0664cc538d","question":"Xcode 15 replaced Symbol navigator with Bookmark Navigator. What's the way to access class hierarchy?\nI recently updated to Xcode 15 and observed that Symbol Navigator has been replaced with Bookmark Navigator.\n\n\nI've been trying to locate it without success.\n\n\nWith the Symbol Navigator apparently gone, is there a method to view the class hierarchy and a comprehensive list of all project and linked framework symbols in a single location?","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"other"},"answer":"There are now 3 menu items in the find menu (click right on any symbol in an editor). They are Find Ancestor Types, Find Descendent Types, and Find Conforming Types. These will bring up Symbol Browser. Make sure you set the search scope just below the search field."}
{"questionId":"05607e189fa749b4800b5d94b3b4a5a5","question":"nestjs swc Error: Cannot find module 'path-to-project\/src\/app.module'\nWhen I installed the swc, I get this Error: Cannot find module 'path-to-project\/src\/app.module'\n\n\nI'm using:\n\n\n- nodejs: 18.17.1\n- nestjs: 10.1.16\n- @swc\/cli: 0.1.62\n- @swc\/core: 1.3.81\n\n\n\"**dist\/main.js**\"\n\n\n\n```\n\"use strict\";\nObject.defineProperty(exports, \"__esModule\", {\n  value: true,\n});\nconst _cors = \/*#__PURE__*\/ _interop_require_default(require(\"@fastify\/cors\"));\nconst _csrfprotection = \/*#__PURE__*\/ _interop_require_default(\n  require(\"@fastify\/csrf-protection\"),\n);\nconst _helmet = \/*#__PURE__*\/ _interop_require_default(\n  require(\"@fastify\/helmet\"),\n);\nconst _common = require(\"@nestjs\/common\");\nconst _config = require(\"@nestjs\/config\");\nconst _core = require(\"@nestjs\/core\");\nconst _platformfastify = require(\"@nestjs\/platform-fastify\");\nconst _swagger = require(\"@nestjs\/swagger\");\nconst _appmodule = require(\"path-to-projects\/src\/app.module\");\nfunction _interop_require_default(obj) {\n  return obj && obj.__esModule\n    ? obj\n    : {\n        default: obj,\n      };\n}\nasync function bootstrap() {\n  const app = await _core.NestFactory.create(\n    _appmodule.AppModule,\n    new _platformfastify.FastifyAdapter(),\n  );\n  app.useGlobalPipes(\n    new _common.ValidationPipe({\n      transform: true,\n      whitelist: true,\n    }),\n  );\n  const config = app.get(_config.ConfigService);\n  {\n    const options = new _swagger.DocumentBuilder()\n      .setTitle(config.get(\"API_TITLE\"))\n      .setDescription(config.get(\"API_DESCRIPTION\"))\n      .setVersion(config.get(\"API_VERSION\"))\n      .build();\n    const document = _swagger.SwaggerModule.createDocument(app, options);\n    _swagger.SwaggerModule.setup(\"docs\", app, document);\n  }\n  await app.register(_cors.default);\n  await app.register(_csrfprotection.default);\n  await app.register(_helmet.default);\n  await app.listen(config.get(\"API_PORT\"), \"0.0.0.0\");\n  console.log(`\ud83d\ude80 Application is running on: ${await app.getUrl()}`);\n}\nbootstrap();\n\n```\n\n\"**src\/main.ts**\"\n\n\n\n```\nimport fastifyCors from \"@fastify\/cors\";\nimport fastifyCsrf from \"@fastify\/csrf-protection\";\nimport fastifyHelmet from \"@fastify\/helmet\";\nimport { ValidationPipe } from \"@nestjs\/common\";\nimport { ConfigService } from \"@nestjs\/config\";\nimport { NestFactory } from \"@nestjs\/core\";\nimport {\n  FastifyAdapter,\n  NestFastifyApplication,\n} from \"@nestjs\/platform-fastify\";\nimport { DocumentBuilder, SwaggerModule } from \"@nestjs\/swagger\";\nimport { AppModule } from \".\/app.module\";\n\nasync function bootstrap() {\n  const app = await NestFactory.create<NestFastifyApplication>(\n    AppModule,\n    new FastifyAdapter(),\n  );\n  app.useGlobalPipes(new ValidationPipe({ transform: true, whitelist: true }));\n  const config = app.get(ConfigService);\n\n  {\n    const options = new DocumentBuilder()\n      .setTitle(config.get(\"API_TITLE\"))\n      .setDescription(config.get(\"API_DESCRIPTION\"))\n      .setVersion(config.get(\"API_VERSION\"))\n      .build();\n    const document = SwaggerModule.createDocument(app, options);\n    SwaggerModule.setup(\"docs\", app, document);\n  }\n\n  await app.register(fastifyCors);\n  await app.register(fastifyCsrf);\n  await app.register(fastifyHelmet);\n  await app.listen(config.get(\"API_PORT\"), \"0.0.0.0\");\n  console.log(`\ud83d\ude80 Application is running on: ${await app.getUrl()}`);\n}\nbootstrap();\n\n```\n\n\"**package.json**\"\n\n\n\n```\n{\n  \"name\": \"test\",\n  \"version\": \"0.0.1\",\n  \"description\": \"\",\n  \"author\": \"\",\n  \"private\": true,\n  \"license\": \"UNLICENSED\",\n  \"scripts\": {\n    \"prebuild\": \"rimraf dist\",\n    \"build\": \"nest build\",\n    \"format\": \"prettier --write \\\"src\/**\/*.ts\\\" \\\"test\/**\/*.ts\\\"\",\n    \"start\": \"nest start\",\n    \"start:dev\": \"nest start --watch\",\n    \"start:debug\": \"nest start --debug --watch\",\n    \"start:prod\": \"node dist\/main\",\n    \"lint\": \"eslint \\\"{src,apps,libs,test}\/**\/*.ts\\\" --fix\",\n    \"test\": \"jest\",\n    \"test:watch\": \"jest --watch\",\n    \"test:cov\": \"jest --coverage\",\n    \"test:debug\": \"node --inspect-brk -r tsconfig-paths\/register -r ts-node\/register node_modules\/.bin\/jest --runInBand\",\n    \"test:e2e\": \"jest --config .\/test\/jest-e2e.json\"\n  },\n  \"dependencies\": {\n    \"@fastify\/cors\": \"^8.3.0\",\n    \"@fastify\/csrf-protection\": \"^6.3.0\",\n    \"@fastify\/helmet\": \"^11.0.0\",\n    \"@fastify\/static\": \"^6.10.2\",\n    \"@nestjs\/common\": \"^10.0.0\",\n    \"@nestjs\/config\": \"^3.0.1\",\n    \"@nestjs\/core\": \"^10.0.0\",\n    \"@nestjs\/platform-express\": \"^10.0.0\",\n    \"@nestjs\/platform-fastify\": \"^10.2.4\",\n    \"@nestjs\/swagger\": \"^7.1.10\",\n    \"@prisma\/client\": \"^5.2.0\",\n    \"bcrypt\": \"^5.1.1\",\n    \"class-transformer\": \"^0.5.1\",\n    \"class-validator\": \"^0.14.0\",\n    \"reflect-metadata\": \"^0.1.13\",\n    \"rimraf\": \"^5.0.1\",\n    \"rxjs\": \"^7.8.1\"\n  },\n  \"devDependencies\": {\n    \"@nestjs\/cli\": \"^10.0.0\",\n    \"@nestjs\/schematics\": \"^10.0.0\",\n    \"@nestjs\/testing\": \"^10.0.0\",\n    \"@swc\/cli\": \"^0.1.62\",\n    \"@swc\/core\": \"^1.3.81\",\n    \"@types\/bcrypt\": \"^5.0.0\",\n    \"@types\/express\": \"^4.17.17\",\n    \"@types\/jest\": \"^29.5.2\",\n    \"@types\/node\": \"^20.3.1\",\n    \"@types\/supertest\": \"^2.0.12\",\n    \"@typescript-eslint\/eslint-plugin\": \"^6.0.0\",\n    \"@typescript-eslint\/parser\": \"^6.0.0\",\n    \"eslint\": \"^8.42.0\",\n    \"eslint-config-prettier\": \"^9.0.0\",\n    \"eslint-plugin-prettier\": \"^5.0.0\",\n    \"jest\": \"^29.5.0\",\n    \"prettier\": \"^3.0.0\",\n    \"prisma\": \"^5.2.0\",\n    \"source-map-support\": \"^0.5.21\",\n    \"supertest\": \"^6.3.3\",\n    \"ts-jest\": \"^29.1.0\",\n    \"ts-loader\": \"^9.4.3\",\n    \"ts-node\": \"^10.9.1\",\n    \"tsconfig-paths\": \"^4.2.0\",\n    \"typescript\": \"^5.1.3\"\n  },\n  \"jest\": {\n    \"moduleFileExtensions\": [\n      \"js\",\n      \"json\",\n      \"ts\"\n    ],\n    \"rootDir\": \"src\",\n    \"testRegex\": \".*\\\\.spec\\\\.ts$\",\n    \"transform\": {\n      \"^.+\\\\.(t|j)s$\": \"ts-jest\"\n    },\n    \"collectCoverageFrom\": [\n      \"**\/*.(t|j)s\"\n    ],\n    \"coverageDirectory\": \"..\/coverage\",\n    \"testEnvironment\": \"node\"\n  },\n  \"prisma\": {\n    \"schema\": \"src\/database\/prisma\/schema.prisma\"\n  }\n}\n\n```\n\n\"**tsconfig.json**\"\n\n\n\n```\n{\n  \"compilerOptions\": {\n    \"module\": \"commonjs\",\n    \"declaration\": true,\n    \"removeComments\": true,\n    \"emitDecoratorMetadata\": true,\n    \"experimentalDecorators\": true,\n    \"allowSyntheticDefaultImports\": true,\n    \"target\": \"ES2021\",\n    \"sourceMap\": true,\n    \"outDir\": \".\/dist\",\n    \"baseUrl\": \".\/\",\n    \"incremental\": true,\n    \"skipLibCheck\": true,\n    \"strictNullChecks\": false,\n    \"noImplicitAny\": false,\n    \"strictBindCallApply\": false,\n    \"forceConsistentCasingInFileNames\": false,\n    \"noFallthroughCasesInSwitch\": false\n  }\n}\n\n```\n\n\"**tsconfig.build.json**\"\n\n\n\n```\n{\n  \"extends\": \".\/tsconfig.json\",\n  \"exclude\": [\"node_modules\", \"test\", \"dist\", \"**\/*spec.ts\"]\n}\n\n```\n\n\"**nest-cli.json**\"\n\n\n\n```\n{\n  \"$schema\": \"https:\/\/json.schemastore.org\/nest-cli\",\n  \"collection\": \"@nestjs\/schematics\",\n  \"sourceRoot\": \"src\",\n  \"compilerOptions\": {\n    \"deleteOutDir\": true,\n    \"builder\": \"swc\",\n    \"typeCheck\": true\n  }\n}\n\n```\n\n[structure](https:\/\/i.stack.imgur.com\/4Z80B.jpg)\n\n\nI tried to change the base Url, create .swcrc","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"typescript"},"answer":"Resolved similar issue by downgrading version to `@swc\/core@1.3.78` (`@swc\/cli` stayed latest, 0.1.62)."}
{"questionId":"94fc8654c13448daa9caab0d13a6359c","question":"Call of overloaded  is ambiguous with inconvertible types\nRelated question:\n\n\n- [Calling an explicit constructor with a braced-init list: ambiguous or not?](https:\/\/stackoverflow.com\/questions\/34622076\/calling-an-explicit-constructor-with-a-braced-init-list-ambiguous-or-not)\n\n\n\n\n---\n\n\nThe question is similar to [1](https:\/\/stackoverflow.com\/questions\/14587436\/call-of-overloaded-brace-enclosed-initializer-list-is-ambiguous-how-to-deal-w) and [2](https:\/\/stackoverflow.com\/questions\/14587436\/call-of-overloaded-brace-enclosed-initializer-list-is-ambiguous-how-to-deal-w), but not the same.\n\n\n\n\n---\n\n\n\n```\n#include <type_traits>\n#include <vector>\n\nstruct A {\n    A();\n};\nstatic_assert(std::is_convertible_v<double, A> == false);\nstatic_assert(std::is_convertible_v<A, double> == false);\n\nvoid func(std::vector<double> values);\nvoid func(std::vector<A> as);\n\nint main() {\n    func({ 4.2 });\n}\n\n```\n\nApparently, `double` and `A` cannot be implicitly converted to each other. So I think `void func(std::vector<double>)` should be called.\n\n\nBut the results for different compilers are different:\n<https:\/\/godbolt.org\/z\/c1hW47f4c>\n\n\nGCC fails to compile with:\n\n\n\n```\n<source>: In function 'int main()':\n<source>:14:9: error: call of overloaded 'func(<brace-enclosed initializer list>)' is ambiguous\n   14 |     func({ 4.2 });\n      |     ~~~~^~~~~~~~~\n<source>:10:6: note: candidate: 'void func(std::vector<double>)'\n   10 | void func(std::vector<double> values);\n      |      ^~~~\n<source>:11:6: note: candidate: 'void func(std::vector<A>)'\n   11 | void func(std::vector<A> as);\n      |      ^~~~\nCompiler returned: 1\n\n```\n\n(VC15 (VS 2017) rejects the example too.)\n\n\n\n\n---\n\n\nWhich one is right? Why?","questionMetadata":{"Type":"Debugging","Level":"Advanced","Tag":"cpp"},"answer":"**tldr;**\n\n\nThe program is **ill-formed** because during the overload resolution of the call `func({4.2})` for the second overload `func(std::vector<A> as)` the explicit ctor of `std::vector` that takes a `size_t` is choosen which isn't allowed in copy-list-initialization.\n\n\n#### Language-Lawyer Explanation\n\n\n##### Step 1\n\n\nFirst let us consider the overload resolution for the call `func({4.2})` against the first overload `func(std::vector<double>)`.\n\n\nNote note that `func({ 4.2 })` is [copy-initialization](https:\/\/eel.is\/c++draft\/dcl.init.general#14):\n\n\n\n> \n> **The initialization that occurs in the** = form of a brace-or-equal-initializer or condition ([stmt.select]), **as well as in argument passing**, function return, throwing an exception ([except.throw]), handling an exception ([except.handle]), and aggregate member initialization other than by a designated-initializer-clause ([dcl.init.aggr]), is called copy-initialization.\n> \n> \n> \n\n\nNow we move onto [dcl.init.general#16](https:\/\/eel.is\/c++draft\/dcl.init.general#16) to see that this will use **list initialization**:\n\n\n\n> \n> The semantics of initializers are as follows.\n> The destination type is the type of the object or reference being initialized and the source type is the type of the initializer expression.\n> If the initializer is not a single (possibly parenthesized) expression, the source type is not defined.\n> \n> \n> - **If the initializer is a (non-parenthesized) braced-init-list** or is = braced-init-list, **the object or reference is list-initialized ([dcl.init.list])**.\n> \n> \n> \n\n\nSo from [dcl.init.list](https:\/\/eel.is\/c++draft\/dcl.init.list) we also see that this is **copy-list-initialization**:\n\n\n\n> \n> List-initialization is initialization of an object or reference from a braced-init-list.\n> Such an initializer is called an initializer list, and the comma-separated initializer-clauses of the initializer-list or designated-initializer-clauses of the designated-initializer-list are called the elements of the initializer list.\n> An initializer list may be empty.\n> List-initialization can occur in direct-initialization or copy-initialization contexts; list-initialization in a direct-initialization context is called direct-list-initialization and **list-initialization in a copy-initialization context is called copy-list-initialization.** Direct-initialization that is not list-initialization is called direct-non-list-initialization.\n> \n> \n> \n\n\nFinally we move onto [dcl.init.list#3](https:\/\/eel.is\/c++draft\/dcl.init.list#3):\n\n\n\n> \n> List-initialization of an object or reference of type T is defined as follows:\n> \n> \n> - Otherwise, **if T is a class type, constructors are considered.\n> The applicable constructors are enumerated and the best one is chosen through overload resolution** ([over.match], [over.match.list]).\n> If a narrowing conversion (see below) is required to convert any of the arguments, the program is ill-formed.\n> \n> \n> \n\n\nThis means that overload resolution is done with `std::vector`'s for the argument `{4.2}` and the best one will be choosen.\n\n\nSo we move onto [over.match.list](https:\/\/eel.is\/c++draft\/over.match.list):\n\n\n\n> \n> **When objects of non-aggregate class type T are list-initialized** such that [dcl.init.list] specifies that overload resolution is performed according to the rules in this subclause or when forming a list-initialization sequence according to [over.ics.list], overload resolution selects the constructor in two phases:\n> \n> \n> - If the initializer list is not empty or T has no default constructor, **overload resolution is first performed where the candidate functions are the initializer-list constructors ([dcl.init.list]) of the class T** and the argument list consists of the initializer list as a single argument.\n> - Otherwise, or **if no viable initializer-list constructor is found, overload resolution is performed again**, where the candidate functions are all the constructors of the class T and the argument list consists of the elements of the initializer list.\n> \n> \n> \n\n\nNote that since an initializer list ctor was found, so overload resolution won't be performed again. This in turn means that the initializer list ctor is the choosen option when matching `func({4.2})` against the first overload `func(std::vector<double>)`\n\n\n##### Step 2\n\n\nNow we see how `func({4.2})` matches against the second overload `func(std::vector<A>)`.\n\n\nIn this case, almost all the steps are same(as in the last case) except that this time the initializer list ctor `std::vector(std::initializer_list<A>)` is **not viable** and so the statement **if no viable initializer-list constructor is found, overload resolution is performed again** is satisfied and so\n\n\n\n> \n> Otherwise, or if no viable initializer-list constructor is found, **overload resolution is performed again, where the candidate functions are all the constructors of the class T** and the argument list consists of the elements of the initializer list.\n> \n> \n> \n\n\nThis means that this time, the `std::size_t` argument ctor of `std::vector` will be choosen. But note that this ctor of `std::vector` is `explicit` and we have:\n\n\n\n> \n> **In copy-list-initialization, if an explicit constructor is chosen, the initialization is ill-formed.**\n> \n> \n> \n\n\nThus the selection of `size_t` argument ctor of `std::vector` makes the program **ill-formed**."}
{"questionId":"873774e343534354b411b98fa8e9f7e7","question":"You're importing a component that imports react-dom\/server\nI am having the following error when I use the react-email package in my next.js project the problem is caused by the Tailwind component so when I comment it out it works but since I want to apply some classes to style my email any way I can get around this would be appreciated, thanks in advance!\n\n\n\n```\nFailed to compile\n.\/node_modules\\@react-email\\tailwind\\dist\\index.mjs\nReactServerComponentsError:\n\nYou're importing a component that imports react-dom\/server. To fix it, render or return the content directly as a Server Component instead for perf and security.\nLearn more: https:\/\/nextjs.org\/docs\/getting-started\/react-essentials\n\nMaybe one of these should be marked as a client entry \"use client\":\n  .\/node_modules\\@react-email\\tailwind\\dist\\index.mjs\n  .\/email\\contact-form-email.tsx\n  .\/actions\\sendEmail.ts\n\n\n```\n\ncontact-form-email.tsx\n\n\n\n```\nimport React from 'react'\nimport {\n    Html,Body,Head,Heading,Hr,Container,Preview,Section,Text\n} from '@react-email\/components';\n\/\/ import {Tailwind} from '@react-email\/tailwind'\n\ntype ContactFormEmailProps = {\n    message: string;\n    email: string;\n}\n\nexport default function ContactFormEmail({message,email}: ContactFormEmailProps) {\n  return (\n    <Html>\n    <Head \/>\n    <Preview>New message from your portfolio website<\/Preview>\n    {\/* <Tailwind> *\/}\n        <Body className=\"bg-gray-100 text-black\">\n            <Container>\n                <Section className=\"bg-white borderBlack my-10 px-10 py-4 rounded-md\">\n                    <Heading className=\"leading-tight\">You received the following message from the contact form.<\/Heading>\n                    <Text>{message}<\/Text>\n                    <Hr \/>\n                    <Text>The sender's email is: {email}<\/Text>\n                <\/Section>\n            <\/Container>\n        <\/Body>\n    {\/* <\/Tailwind> *\/}\n    <\/Html>\n  )\n}","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"javascript"},"answer":"Just mark them as external packages.\n\n\n\n```\nconst nextConfig = {\n    ...,\n    experimental: {\n        ...,\n        serverComponentsExternalPackages: [\n            '@react-email\/components',\n            '@react-email\/render',\n            '@react-email\/tailwind'\n        ]\n    }\n};\n\n```\n\n<https:\/\/nextjs.org\/docs\/app\/api-reference\/next-config-js\/serverComponentsExternalPackages>"}
{"questionId":"14acfc508a194ff485338ffebc7ac439","question":"How to debug cro installation with zef\nI'd like to try some things with `raku` and `cro` but can't install it with:\n\n\n\n```\nzef install cro\n\n```\n\nI'm getting an error when it tries to install the module `YAMLish`. Running the installation with `--debug` gives:\n\n\n\n```\n[YAMLish] Extracting with plugin Zef::Service::Shell::tar aborted.\nsomething went wrong extracting C:\\Users\\zb\\AppData\\Local\\Temp\/.zef\/1698391208.16200\\1698391222.16200.367.3485459071357\\YAMLish%3Aver%3C0.0.6%3E%3Aauth%3Cgithub%3ALeont%3E.tar.gz\nto C:\\Users\\zb\\AppData\\Local\\Temp\/.zef\/1698391208.16200\\YAMLish%3Aver%3C0.0.6%3E%3Aauth%3Cgithub%3ALeont%3E.tar.gz\nwith Zef::Service::Shell::git<3508240998224>,Zef::Service::FetchPath<3508225362272>,Zef::Service::Shell::tar<3508225362240>,Zef::Service::Shell::unzip<3508225361216>\n\n```\n\nThe archive in question is present and was partly extracted to the target directory. If I were to guess, it seems to have choked on `yamlish-master\/test-suite\/meta` which contains a lot of [symbolic links](https:\/\/github.com\/Leont\/yamlish\/tree\/master\/test-suite\/meta) that point to non-existant files?\n\n\nHow can I resolve this or debug further? I'm running Rakudo v2023.10 on Windows 10.0.19045.3570.","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"perl"},"answer":"*Update: The [issue was fixed](https:\/\/github.com\/Leont\/yamlish\/issues\/37#issuecomment-1783625641) a few hours later by YAMLish's maintainer* \ud83d\ude42\n\n\n\n\n---\n\n\nWith [help from ugexe](https:\/\/github.com\/ugexe\/zef\/issues\/533) it became clear this is indeed caused by failure to create the symbolic links in question and that a workaround is to install directly from the [`YAMLish` github repo](https:\/\/github.com\/Leont\/yamlish):\n\n\n\n```\nzef install https:\/\/github.com\/Leont\/yamlish.git\n\n```\n\nDigging a bit deeper, I've found that creating symlinks in Windows (e.g. [`mklink`](https:\/\/learn.microsoft.com\/en-us\/windows-server\/administration\/windows-commands\/mklink)) is only allowed\n\n\n- with administrative privileges or\n- in \"Developer Mode\" (added 2016 in [Windows 10 build 14972](https:\/\/blogs.windows.com\/windowsdeveloper\/2016\/12\/02\/symlinks-windows-10\/))\n\n\nGoing either route allows installing the `YAMLish` module with just:\n\n\n\n```\nzef install YAMLish"}
{"questionId":"955d43aab5524f1a81af6f1637c9a6c5","question":"Grade build issue with the latest version of androidx room libraries version 2.6.0\nThe latest release of libraries will make by grade build error.\n\n\nI have to roll back the following libraries to get rid of these errors below:\nlibraries with versions that work w\/o errors:\n\n\n\n```\n    implementation (\"androidx.room:room-runtime:2.5.2\")\n    implementation(\"androidx.room:room-ktx:2.5.2\")\n\n```\n\n\n```\n annotationProcessor(\"androidx.room:room-compiler:2.5.2\")\n    \/\/\/kapt('androidx.room:room-compiler:2.5.2')\n    ksp (\"androidx.room:room-compiler:2.5.2\")\n    testImplementation(\"androidx.room:room-testing:2.5.2\")\n\n```\n\nI got these errors before I roll back these libraries:\n\n\n\n```\nerror message: java.lang.NoSuchMethodError: 'kotlin.sequences.Sequence com.google.devtools.ksp.processing.Resolver.getPackagesWithAnnotation(java.lang.String)'\n    at androidx.room.compiler.processing.ksp.KspRoundEnv.getElementsAnnotatedWith(KspRoundEnv.kt:107)\n    at androidx.room.compiler.processing.CommonProcessorDelegate.processRound(XBasicAnnotationProcessor.kt:100)\n    at androidx.room.compiler.processing.ksp.KspBasicAnnotationProcessor.process(KspBasicAnnotationProcessor.kt:62)\n    at com.google.devtools.ksp.AbstractKotlinSymbolProcessingExtension$doAnalysis$6$1.invoke(KotlinSymbolProcessingExtension.kt:291)\n    at com.google.devtools.ksp.AbstractKotlinSymbolProcessingExtension$doAnalysis$6$1.invoke(KotlinSymbolProcessingExtension.kt:289)\n    at com.google.devtools.ksp.AbstractKotlinSymbolProcessingExtension.handleException(KotlinSymbolProcessingExtension.kt:394)\n    at com.google.devtools.ksp.AbstractKotlinSymbolProcessingExtension.doAnalysis(KotlinSymbolProcessingExtension.kt:289)\n    at org.jetbrains.kotlin.cli.jvm.compiler.TopDownAnalyzerFacadeForJVM.analyzeFilesWithJavaIntegration(TopDownAnalyzerFacadeForJVM.kt:123)\n    at org.jetbrains.kotlin.cli.jvm.compiler.TopDownAnalyzerFacadeForJVM.analyzeFilesWithJavaIntegration$default(TopDownAnalyzerFacadeForJVM.kt:99)\n    at org.jetbrains.kotlin.cli.jvm.compiler.KotlinToJVMBytecodeCompiler$analyze$1.invoke(KotlinToJVMBytecodeCompiler.kt:257)\n    at org.jetbrains.kotlin.cli.jvm.compiler.KotlinToJVMBytecodeCompiler$analyze$1.invoke(KotlinToJVMBytecodeCompiler.kt:42)\n    at org.jetbrains.kotlin.cli.common.messages.AnalyzerWithCompilerReport.analyzeAndReport(AnalyzerWithCompilerReport.kt:115)\n    at org.jetbrains.kotlin.cli.jvm.compiler.KotlinToJVMBytecodeCompiler.analyze(KotlinToJVMBytecodeCompiler.kt:248)\n    at org.jetbrains.kotlin.cli.jvm.compiler.KotlinToJVMBytecodeCompiler.compileModules$cli(KotlinToJVMBytecodeCompiler.kt:88)\n    at org.jetbrains.kotlin.cli.jvm.compiler.KotlinToJVMBytecodeCompiler.compileModules$cli$default(KotlinToJVMBytecodeCompiler.kt:47)\n    at org.jetbrains.kotlin.cli.jvm.K2JVMCompiler.doExecute(K2JVMCompiler.kt:168)\n    at org.jetbrains.kotlin.cli.jvm.K2JVMCompiler.doExecute(K2JVMCompiler.kt:53)\n    at org.jetbrains.kotlin.cli.common.CLICompiler.execImpl(CLICompiler.kt:100)\n    at org.jetbrains.kotlin.cli.common.CLICompiler.execImpl(CLICompiler.kt:46)\n    at org.jetbrains.kotlin.cli.common.CLITool.exec(CLITool.kt:101)\n    at org.jetbrains.kotlin.daemon.CompileServiceImpl.compile(CompileServiceImpl.kt:1486)\n    at java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n    at java.base\/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.base\/java.lang.reflect.Method.invoke(Method.java:568)\n    at java.rmi\/sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:360)\n    at java.rmi\/sun.rmi.transport.Transport$1.run(Transport.java:200)\n    at java.rmi\/sun.rmi.transport.Transport$1.run(Transport.java:197)\n    at java.base\/java.security.AccessController.doPrivileged(AccessController.java:712)\n    at java.rmi\/sun.rmi.transport.Transport.serviceCall(Transport.java:196)\n    at java.rmi\/sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:587)\n    at java.rmi\/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:828)\n    at java.rmi\/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:705)\n    at java.base\/java.security.AccessController.doPrivileged(AccessController.java:399)\n    at java.rmi\/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:704)\n    at java.base\/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.base\/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.base\/java.lang.Thread.run(Thread.java:833)","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"kotlin"},"answer":"**Update:**\n\n\nYou have to update your ksp version and then you can update your room version:\n\n\n\n```\nplugins {\n     id 'com.google.devtools.ksp' version '1.9.0-1.0.12' apply false\n}\n\n```\n\n**Previous answer:**\n\n\nThis version is not compatible with `ksp` currently. `for now` you can change it to `kapt`:\n\n\n\n```\nimplementation 'androidx.room:room-ktx:2.6.0'\nimplementation 'androidx.room:room-runtime:2.6.0'\nkapt 'androidx.room:room-compiler:2.6.0'\n\n```\n\nalso you need to use kapt schemaLocation config:\n\n\n\n```\ndefaultConfig {\n\n        javaCompileOptions {\n            annotationProcessorOptions {\n                arguments += [\"room.schemaLocation\": \"$projectDir\/schemas\".toString()]\n            }\n        }\n    }"}
{"questionId":"acee92a19f6a459b882775cc57d99202","question":"How to fix react-native error after updating expo to 50.0\nWhen I run `npm run ios` in terminal, I get the below error\n\n\n\n> \n> error: node\\_modules\/expo-router\/entry.js: [BABEL]: expo-router\/babel is deprecated in favor of babel-preset-expo in SDK 50\n> \n> \n> \n\n\nI tried to install babel-preset-expo, but it was already installed","questionMetadata":{"Type":"Version","Level":"Beginner","Tag":"javascript"},"answer":"Removed the expo-router\/babel from babel.config.js file and it works."}
{"questionId":"bcc958a98883416e80c2bde1d03976b6","question":"CLIENT\\_ID missing from GoogleService-Info.plist with Firebase on IOS in a flutter app\nI'm trying to configure Firebase on a flutter mobile app.\nI have two flavors (prod and preprod) in my app, so I created two Firebase apps, one for each flavor\nfor the prod flavor everything works fine.\nHowever for the preprod flavor on IOS I get this error :\n\n\n\n```\n[VERBOSE-2:dart_vm_initializer.cc(41)] Unhandled Exception: [core\/duplicate-app] A Firebase App named \"[DEFAULT]\" already exists\n#0      MethodChannelFirebase.initializeApp (package:firebase_core_platform_interface\/src\/method_channel\/method_channel_firebase.dart:134:11)\n<asynchronous suspension>\n#1      Firebase.initializeApp (package:firebase_core\/src\/firebase.dart:43:31)\n<asynchronous suspension>\n#2      start (package:hexatrip_mobile_app\/main_preprod.dart:24:3)\n\n```\n\nThe only difference I see between both flavors is that the GoogleService-Info.plist that I download for the preprod Firebase app does not contains those keys : CLIENT\\_ID and REVERSED\\_CLIENT\\_ID\nI suspect that the absence of those keys is responsible for the error but I can't find why they would be missing.\n\n\nThank you.","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"dart"},"answer":"for all those who had this problem\nif you downloaded GoogleServices-Info.plist file and it doesn't contain CLIENT\\_ID and REVERSE\\_CLIENT\\_ID\n\n\nSolution:\n\n\nJust Go to Authentication section in firebase -> Sign-in method -> Add new provider then select Google and enable it .\n\n\n[sample\\_firebase\\_auth](https:\/\/i.stack.imgur.com\/ZethL.png)\n\n\nnow download the updated GoogleService-Info.plist file now it had the CLIENT\\_ID and REVERSE\\_CLIENT\\_ID then replace the old one with updated one.\nThanks"}
{"questionId":"7be68e5a904f49c0bd802da47e3b3eeb","question":"Setting up formatters in Neovim with mason & lsp-zero\nI am using lsp-zero & mason to install LSPs, formatters and linters. However, I am not sure how the formatters work and how can I configure them.\n\n\nFor example, the formatter for `yml` files seems not to be working even though that I've installed `yamlls` and `yamlfmt`. On the other hand, Golang's formatter seems to be working just fine, when I save a `*.go` file, it will be automatically formatted.\n\n\nHow would you setup the YAML formatter in this case? Here is a snippet of my configuration:\n\n\n**lsp.lua**\n\n\n\n```\nlocal lsp = require(\"lsp-zero\")\n\nlsp.preset(\"recommended\")\n\n-- Fix Undefined global 'vim'\nlsp.nvim_workspace()\n\nlocal cmp = require('cmp')\nlocal cmp_select = {behavior = cmp.SelectBehavior.Select}\nlocal cmp_mappings = lsp.defaults.cmp_mappings({\n  ['<C-p>'] = cmp.mapping.select_prev_item(cmp_select),\n  ['<C-n>'] = cmp.mapping.select_next_item(cmp_select),\n  ['<CR>'] = cmp.mapping.confirm({ select = true }),\n  [\"<C-Space>\"] = cmp.mapping.complete(),\n})\n\ncmp_mappings['<Tab>'] = nil\ncmp_mappings['<S-Tab>'] = nil\n\nlsp.setup_nvim_cmp({\n  mapping = cmp_mappings\n})\n\nlsp.set_preferences({\n    suggest_lsp_servers = false,\n    sign_icons = {\n        error = 'E',\n        warn = 'W',\n        hint = 'H',\n        info = 'I'\n    }\n})\n\nlsp.on_attach(function(client, bufnr)\n  local opts = {buffer = bufnr, remap = false}\n\n  vim.keymap.set(\"n\", \"gd\", function() vim.lsp.buf.definition() end, opts)\n  vim.keymap.set(\"n\", \"K\", function() vim.lsp.buf.hover() end, opts)\n  vim.keymap.set(\"n\", \"<leader>vws\", function() vim.lsp.buf.workspace_symbol() end, opts)\n  vim.keymap.set(\"n\", \"<leader>vd\", function() vim.diagnostic.open_float() end, opts)\n  vim.keymap.set(\"n\", \"[d\", function() vim.diagnostic.goto_next() end, opts)\n  vim.keymap.set(\"n\", \"]d\", function() vim.diagnostic.goto_prev() end, opts)\n  vim.keymap.set(\"n\", \"<leader>vca\", function() vim.lsp.buf.code_action() end, opts)\n  vim.keymap.set(\"n\", \"<leader>vrr\", function() vim.lsp.buf.references() end, opts)\n  vim.keymap.set(\"n\", \"<leader>vrn\", function() vim.lsp.buf.rename() end, opts)\n  vim.keymap.set(\"i\", \"<C-h>\", function() vim.lsp.buf.signature_help() end, opts)\nend)\n\nvim.diagnostic.config({\n    virtual_text = true\n})\n\n```\n\n**mason.lua**\n\n\n\n```\nrequire('mason-tool-installer').setup {\n  ensure_installed = {\n    'golangci-lint',\n    'bash-language-server',\n    'lua-language-server',\n    'vim-language-server',\n    'gopls',\n    'stylua',\n    'shellcheck',\n    'sqlfmt',\n    'editorconfig-checker',\n    'gofumpt',\n    'golines',\n    'gomodifytags',\n    'gotests',\n    'goimports',\n    'impl',\n    'json-to-struct',\n    'jq',\n    'misspell',\n    'revive',\n    'shellcheck',\n    'shfmt',\n    'staticcheck',\n    'vint',\n    'yamllint',\n    'yamlfmt',\n    'yamlls',\n    'hadolint',\n    'dockerls',\n    'diagnosticls',\n    'sqlls',\n    'terraformls',\n    'delve'\n  }\n}","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"other"},"answer":"Mason is a package manager that allows you to *manage* packages. Some packages will work out of the box, others require manual set up and\/or calling the required functionality via commands---formatters are one example of this.\n\n\nOnce you've installed the formatter via Mason, you can set up a Neovim auto-command that calls the formatter on particular files with desired arguments. For example, for `yamlfmt`, you can create a file (e.g. `~\/.config\/nvim\/lua\/autocmds.lua`) and add the following code:\n\n\n\n```\n-- Create group to assign commands\n-- \"clear = true\" must be set to prevent loading an\n-- auto-command repeatedly every time a file is resourced\nlocal autocmd_group = vim.api.nvim_create_augroup(\"Custom auto-commands\", { clear = true })\n\nvim.api.nvim_create_autocmd({ \"BufWritePost\" }, {\n    pattern = { \"*.yaml\", \"*.yml\" },\n    desc = \"Auto-format YAML files after saving\",\n    callback = function()\n        local fileName = vim.api.nvim_buf_get_name(0)\n        vim.cmd(\":!yamlfmt \" .. fileName)\n    end,\n    group = autocmd_group,\n})\n\n```\n\nThen, ensure this file (`autocmds.lua`) is sourced every time you start Neovim by adding `require(\"autocmds\")` to your `~\/.config\/nvim\/init.lua` file.\n\n\nNow, every time you write a buffer (`BufWritePost`) (i.e. save a file) ending with a `.yaml` or `.yml` extension, the callback will be made which gets the absolute path of the file loaded in the current buffer (`nvim_buf_get_name(0)`), and then executes a non-interactive terminal command that calls the YAML formatter on that file.\n\n\nTo suppress the command's output, you add `silent` to the non-interactive terminal command:\n\n\n\n```\nvim.cmd(\":silent !yamlfmt \" .. fileName)\n\n```\n\nYou can also add multiple terminal commands to the callback. For example, when editing my Python files, I call the following formatters, each of which handles a specific part of code:\n\n\n\n```\nvim.api.nvim_create_autocmd({ \"BufWritePost\" }, {\n    pattern = { \"*.py\" },\n    desc = \"Auto-format Python files after saving\",\n    callback = function()\n        local fileName = vim.api.nvim_buf_get_name(0)\n        vim.cmd(\":silent !black --preview -q \" .. fileName)\n        vim.cmd(\":silent !isort --profile black --float-to-top -q \" .. fileName)\n        vim.cmd(\":silent !docformatter --in-place --black \" .. fileName)\n    end,\n    group = autocmd_group,\n})\n\n```\n\nDon't forget to add each auto-command to an auto-command group that has the `clear` flag set to avoid repeatedly loading a command every time a file is resourced. You can read more about auto-commands via the manual `:h autocmd`."}
{"questionId":"ccbde95f25be43f08a45423c5c001801","question":"Kotlin Multiplatform error :shared:linkDebugFrameworkIosSimulatorArm64\nI setup a KMP project but I can't start the iOS Simulator. Android works perfectly. When trying to run the iOS the following gradle task fails:\n\n\n\n```\nThe \/Applications\/Xcode.app\/Contents\/Developer\/Toolchains\/XcodeDefault.xctoolchain\/usr\/bin\/ld command returned non-zero exit code: 1.\noutput:\nld: unknown options: -ios_simulator_version_min -sdk_version \n\n```\n\nI didn't do any changes to the project after creating it.","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"kotlin"},"answer":"You're probably using Xcode 15 and an older version of Kotlin. If on Xcode 15, bump Kotlin to 1.9.10. Alternatively, install Xcode 14.3.1."}
{"questionId":"78c1eb7340c345c1b8c88edab4b9bf71","question":"Nullability of an array in Eclipse\nsuggest the following code in Eclipse 4.30:\n\n\n\n```\n    public void doSomething(@NonNull SomeType[] pArray) {\n       \/\/ Whatever\n    }\n\n```\n\nIf I enable checks for nullability (Preferences\/Java Compiler\/\"Errors\/Warnings\"\/Null analysis\/Enable annotation based null-analysis), then I notice the following:\n\n\nThe Compiler takes the parameter type as \"an array of SomeType elements, all of which are non-null\", or in other words an array of @NonNull SomeType. (If you don't believe me, change SomeType to byte, and notice the error message, because byte is a primitive type, so @NonNull byte makes no sense.)\n\n\nWhich is all very well. However, I would like to tell the compiler, that the array itself is non-null. Is there any possibility to express this? (Let's say @NonNull (SomeType[]) pArray.)\n\n\nThanks!","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"java"},"answer":"Sure, of course. There are 2 layers of nullity to this story. In fact, with, say, an `Object[][]`, there are *3* different nullity.\n\n\nIt could be a nullable reference to an array, which contains definitely-not-null arrays of definitely-not-null objects.\n\n\nIt could be a definitely-not-null reference to an array, which contains nullable refs to arrays of definitely-not-null objects.\n\n\nAnd so on.\n\n\nThe syntax to specify at each level is like this:\n\n\n\n```\n@Nullable String @NonNull [] x\n\n```\n\nThat means: a definitely-not-null reference to an array of nullable strings.\n\n\nEach `[]` can be prefixed with annotations, applying to that 'layer'.\n\n\nThe same '3 layers' situation applies to a list of a list of objects, but the syntax is a lot more obvious there:\n\n\n\n```\n@NonNull List<@Nullable String>\n\n```\n\nIs the equivalent.\n\n\n**BIG CAVEAT**: This requires the annotation to marked as applying to `TYPE_USE`. Many nullity annotations were created in the java 1.5 days which did not have `TYPE_USE`; only `PARAMETER` and `METHOD` and `FIELD`. Many nullity annotations, as a consequence, still only target P\/M\/F instead of T\\_U. For these, what you want simply isn't possible. Update to a better nullity annotation system, or embrace how they work.\n\n\nSome major annotation libraries that work on the basis of `TYPE_USE`:\n\n\n\n\n\n| Library | Annotation class |\n| --- | --- |\n| [*Eclipse Java development tools (JDT)*](https:\/\/eclipse.dev\/jdt\/) | [`org.eclipse.jdt.annotation.NonNull`](https:\/\/help.eclipse.org\/latest\/index.jsp?topic=\/org.eclipse.jdt.doc.isv\/reference\/api\/org\/eclipse\/jdt\/annotation\/NonNull.html) |\n| [*Checker Framework*](https:\/\/checkerframework.org\/) | [`org.checkerframework.checker.nullness.qual.NonNull`](https:\/\/checkerframework.org\/api\/org\/checkerframework\/checker\/nullness\/qual\/NonNull.html) and [`PolyNull`](https:\/\/checkerframework.org\/api\/org\/checkerframework\/checker\/nullness\/qual\/PolyNull.html) |\n| [*JSpecify*](https:\/\/jspecify.dev\/) | [`org.jspecify.annotations.Nullable`](https:\/\/jspecify.dev\/docs\/api\/org\/jspecify\/annotations\/Nullable.html) |"}
{"questionId":"f9b95b21d0f54859a9ffc841980f78c7","question":"VSCode server unexpected \"Missing GLIBC >= 2.28\"\nI'm connecting to a server via ssh using VSCode.  \n\nThe server has Ubuntu 18.04, and until yesterday night, it was working fine, however since this morning, I get the following error (checking the logs in `~\/.vscode-server`):\n\n\n\n> \n> Warning: Missing GLIBC >= 2.28! from \/lib\/x86\\_64-linux-gnu\/libc-2.27.so\n> Error: Missing required dependencies. Please refer to our FAQ <https:\/\/aka.ms\/vscode-remote\/faq\/old-linux> for additional information.\n> \n> \n> \n\n\nI checked and the library indeed is an older version, however it's strange to me that until yesterday it worked\n\n\nHas VSCode updated the server extension on the server in the background silently or what? Anybody with this problem?","questionMetadata":{"Type":"Version","Level":"Intermediate","Tag":"other"},"answer":"There was an update released last night that drops support of older glibc versions. The official workaround is to downgrade to VSCode 1.85, as described [here](https:\/\/github.com\/microsoft\/vscode\/issues\/203967#issuecomment-1923440629)."}
{"questionId":"0f61e539601844d490c7bd7d2bb118d1","question":"Why is the address of a constexpr local variable not a constant expression?\nThe following constexpr function fails to compile:\n\n\n\n```\nconstexpr void fnc()\n{\n    constexpr int i = 5;\n    constexpr auto ptr = &i;\n}\n\n```\n\nWhy can't `ptr` be constexpr, considering that all the evaluations take place in a constexpr function and it works without declaring `ptr` constexpr?","questionMetadata":{"Type":"Debugging","Level":"Intermediate","Tag":"cpp"},"answer":"The [error message from clang](https:\/\/godbolt.org\/z\/8ae8rbMvM) is clear enough:\n\n\n\n> \n> address of non-static constexpr variable 'i' may differ on each invocation of the enclosing function; add 'static' to give it a constant address\n> \n> \n>"}
{"questionId":"75d278b934c84e6a88293c8bbabdf682","question":"std::filesystem's parent\\_path() is wrong for file in current directory?\nThe traditional Unix shell utility, `dirname`, finds the name of the directory that contains the given file. This is useful if you want to find other sister files in the same directory. Here are a few examples:\n\n\n\n```\n$ dirname \/some\/dir\/filename\n\/some\/dir\n$ dirname dirname\/filename  \ndirname\n$ dirname filename\n.\n\n```\n\nThe new C++ std::filesystem's `parent_path()` gets this last one wrong (or so it appears):\n\n\n\n```\n#include <filesystem>\n#include <iostream>\nint main() {\n    std::cout << \"parent of filename: \" <<\n        std::filesystem::path(\"filename\").parent_path() << '\\n';\n}\n\n```\n\noutputs:\n\n\n\n```\nparent of filename: \"\"\n\n```\n\nReturning an empty string instead of \".\" is not only different from the traditional `dirname`, it's also wrong when used in code which wants to tack on a \"\/...\" on this parent to create the name of a sister files - this results in the wrong \"\/sister\" instead of expected \".\/sister\".\n\n\nIs this a bug in std::filesystem, or deliberate behavior? Is the reason for this behavior documented somewhere?","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"cpp"},"answer":"I won't argue with you whether this is correct or not, but if you always use [`std::filesystem::operator\/`](https:\/\/en.cppreference.com\/w\/cpp\/filesystem\/path\/operator_slash) instead of doing string manipulation, I don't see how you will run into the issue you mention in the question.\n\n\nIn particular, `std::filesystem::path(\"filename\").parent_path()\/std::filesystem::path(\"filename\")` gives you just `\"filename\"`, not `\"\/filename\"`. I believe this is how the API is supposed to be used."}
{"questionId":"5877ffde7967422aa37e3ee0580782b0","question":"Is a function template accepting const char(&)[N] more specialized than function template accepting const T&?\nI've defined two versions of a function template called `compare`:\n\n\n\n```\n#include <cstring>\n\nusing namespace std;\n\n\/\/ fist version\ntemplate <size_t N, size_t M>\nint compare(const char (&a)[N], const char (&b)[M]) {\n    return strcmp(a, b);\n}\n\n\/\/ second version\ntemplate <typename T>\nint compare(const T &a, const T &b) {\n    if (a < b) return -1;\n    if (b < a) return 1;\n    return 0;\n}\n\nint main() {\n    const char *p1 = \"dog\", *p2 = \"cat\";\n    compare(p1, p2); \/\/ call second version\n    compare(\"dog\", \"cat\"); \/\/ call second version?\n\n    return 0;\n}\n\n```\n\nIn the book *CPP Primer (5th edition)*, which uses c++11std, the authors say `compare(p1, p2)` will call the second version template, because there is no way to convert a pointer to a reference to an array. The `compare(\"dog\", \"cat\")` will call the second version of `compare` because it is more specialized than the first version.\n\n\nHowever, when I run this code, I get a compiler error:\n\n\n\n> \n> call of overloaded \u2018compare(const char [4], const char [4])\u2019 is ambiguous\n> \n> \n> \n\n\nThen, I changed `compare(\"dog\", \"cat\")` to `compare(\"dog\", \"cats\")`, and it can call the second version template with no problem.\n\n\nWhy does `a` and `b` having the same length cause the ambiguity?","questionMetadata":{"Type":"Debugging","Level":"Advanced","Tag":"cpp"},"answer":"**tldr;** This is [CWG 2160](https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/cwg_active.html#2160) which points out that the current wording does not states\/specify if conflicting deduced types during *partial ordering* should be invalid or ignored(valid). MSVC seems to ignore the conflicts due to the call `compare(\"dog\", \"cat\");` and accept the program while GCC and Clang seem to reject the conflicting deduced types during partial ordering and reject the program.\n\n\n\n\n---\n\n\n#### Case 1: for the call compare(p1, p2);\n\n\nVersion 1 is specifically written for `const char` arrays. For all other types, the second version is the viable option. Thus for the first call `compare(p1, p2)` the second version `compare(const T &a, const T &b)` should be selected as the first version `compare(const char (&a)[N], const char (&b)[M])` is **not even viable**.\n\n\n\n\n---\n\n\n#### Case 2: for the call compare(\"dog\", \"cat\");\n\n\nNow coming to the second call `compare(\"dog\", \"cat\")`. Here during *partial ordering*, when matching the transformed template of version 1 against original template version 2 **conflicting** types are deduced for `T`. Now the [current wording doesn't specify if this conflicting behavior should be ignored or accepted](https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/cwg_active.html#2160). MSVC seems to ignore the conflicting deduced types while GCC and Clang seem to reject it. This is still an [open issue](https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/cwg_active.html#2160). If conflicting types are ignored then this matching succeeds while if conflicting types are not to be ignored then this matching fails. So there are two possibilities in this first case.\n\n\nDoing the same process again, i.e., trying to match the transformed type of second template with the original template version 1 fails.\n\n\n\n\n---\n\n\nEssentially, there are *two possibilities* depending on whether conflicting types are to be ignored or not during partial ordering:\n\n\n- If they are to be ignored, then version 1 is more specialized and MSVC is right in accepting the program.\n- If they are not to be ignored, then neither is more specialized and the program will be ill-formed for the call `compare(\"dog\", \"cat\")`.\n\n\n\n\n---\n\n\n[GCC rejects call to more specialized `const char` array version with string literal.](https:\/\/gcc.gnu.org\/bugzilla\/show_bug.cgi?id=113110)"}
{"questionId":"19ea53dc3bc14d08b60a0d1549bed23f","question":"Why is `Deref` not implemented on `Cell`?\nCell is just an in-line-storage\/wrapper type that controls mutability\/access semantics to the inner type T and provides interior mutability functionality. From the [rust documentation](https:\/\/doc.rust-lang.org\/std\/cell\/struct.Cell.html#trait-implementations) `Deref` is NOT implemented for the `Cell<T>` type. Why is this ? An instance of `Cell<T>` is always in a valid state(the memory is always valid and initialized). This means that when implementing `Deref` on `Cell<T>` it cannot possibly panic which is one of the requirements for implementing `Deref`.\n\n\nProviding a `Deref` impl for `Cell<T>` would have been convenient.","questionMetadata":{"Type":"Conceptual","Level":"Intermediate","Tag":"rust"},"answer":"But it would be unsound!\n\n\nThe premise of `Cell` is that it *never* gives out references to its contents (indeed, that's what its brother, `*Ref*Cell` does), so all read and write operations on it are atomic1.\n\n\nAs part of its implementation, `Cell` has a `set` method with the following signature:\n\n\n\n```\nfn set(&self, val: T)\n\n```\n\n...which changes the value of `&Cell<T>`, i.e. the value of the cell behind a shared reference2. That's how interior mutability works.\n\n\nNow imagine you could create a reference to the contents of the `Cell`:\n\n\n\n```\nlet cell = Cell::new(1);\n\nlet one: &i32 = &*cell;\n\n```\n\n...and someone else called the `set` method:\n\n\n\n```\ncell.set(2);\n\n```\n\nNow we have mutated the data pointed to by a shared reference (`&T`), which is UB3!\n\n\n\n```\nprintln!(\"one = {}\", *one); \/\/ one = 2 !!!\n\n```\n\n[Playground link](https:\/\/play.rust-lang.org\/?version=stable&mode=debug&edition=2021&gist=eb95841fd7d7a63557dc53d229999ebd)\n\n\n1: not in the \"threadsafe\" sense  \n\n2: it can do this soundly by using an `UnsafeCell`: a compiler builtin that says \"trust me compiler, I can enforce borrowing rules, so you don't have to\"  \n\n3: *outside* of an `UnsafeCell` (`&T` doesn't know about the `Cell`)"}
{"questionId":"0a716d901f9a45c986a9a7ad9b32cfdf","question":"How to replace Counter to use numpy code only\nI have this code:\n\n\n\n```\nfrom collections import Counter\nimport numpy as np\n\ndef make_data(N):\n    np.random.seed(40)\n    g = np.random.randint(-3, 4, (N, N))\n    return g\n\n\nN = 100\ng = make_data(N)\nn = g.shape[0]\n\nsum_dist = Counter()\nfor i in range(n):\n    for j in range(n):\n        dist = i**2 + j**2\n        sum_dist[dist] += g[i, j]\n\nsorted_dists = sorted(sum_dist.keys())\nfor i in range(1, len(sorted_dists)):\n    sum_dist[sorted_dists[i]] += sum_dist[sorted_dists[i-1]]\n\n# print(sum_dist)\nprint(max(sum_dist, key=sum_dist.get))\n\n```\n\nThe output is 7921.\n\n\nI want to convert it into numpy only code and get rid of Counter. How can I do that?","questionMetadata":{"Type":"Implementation","Level":"Intermediate","Tag":"python"},"answer":"Can you just make `sum_dist` into an array, since you know its maximum index?\n\n\n\n```\nsum_dist = np.zeros(2 * N * N, dtype=int)\nfor i in range(n):\n    for j in range(n):\n        dist = i**2 + j**2\n        sum_dist[dist] += g[i, j]\n\nprint(np.argmax(np.cumsum(sum_dist)))"}
{"questionId":"5fb24ccf0812480480bf186c8ee17bdd","question":"How can you enable a class template member function only if a template argument was provided?\nIs it possible to have a class with an optional template parameter that can be called like this?:\n\n\n\n```\n#include <iostream>\n\ntemplate <typename T = void>\nclass A final\n{\npublic:\n    \/\/ This class can be called only when T exists.\n    void f()\n    {\n        printf(\"%d\\n\", member);\n    }\n\n    \/\/ This method can be called only when T is missing.\n    void g()\n    {\n        printf(\"No template parameter\\n\");\n    }\npublic:\n    T member;\n};\n\nint main()\n{\n\n    A<int> a1;\n    A a2;\n    a1.f(); \/\/ should be valid\n    a1.g(); \/\/ should be invalid, cannot compile\n    a2.f(); \/\/ should be invalid, cannot compile\n    a2.g(); \/\/ should be valid\n\n    return 0;\n}\n\n```\n\nIf yes, what are the std functions that should be used?","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"cpp"},"answer":"You might use the \"old\" way with specialization:\n\n\n\n```\ntemplate <typename T = void>\nclass A final \/\/ generic case, T != void\n{\npublic:\n    void f() { std::cout << member << std::endl; }\n\npublic:\n    T member;\n};\n\ntemplate <>\nclass A<void> final\n{\npublic:\n    void g() { printf(\"No template parameter\\n\"); } \/\/ actually `void` and not \"No\".\n};\n\n```\n\nBy turning your class into variadic, to handle absent parameter as empty pack, instead of void, you might do:\n\n\n\n```\ntemplate <typename... Ts>\nclass A final\n{\nstatic_assert(sizeof...(Ts) < 2);\npublic:\n    void f() requires (sizeof...(Ts) == 1) { std::cout << std::get<0>(member) << std::endl; }\n    void g() requires (sizeof...(Ts) == 0) { printf(\"No template parameter\\n\"); }\n\npublic:\n    std::tuple<Ts...> member;\n};"}
{"questionId":"d8f2f0bd978b4623bc32446721bc780d","question":"E is not a legal path since it is not a concrete type (Scala 3)\nI'm trying to migrate a library from Scala 2.13 to Scala 3, but the existing code does not compile.\n\n\nHere is a snippet\n\n\n\n```\ntrait Identity\ntrait Authenticator\n\ntrait Env {\n  type I <: Identity\n  type A <: Authenticator\n}\n\ntrait IdentityService[T <: Identity]\ntrait AuthenticatorService[T <: Authenticator]\n\ntrait Environment[E <: Env] {\n  def identityService[T <: Identity]: IdentityService[E#I]\n  def authenticatorService: AuthenticatorService[E#A]\n}\n\n```\n\nThe Scala 3 compiler fails with:\n\n\n\n```\nerror] 14 |  def identityService[T <: Identity]: IdentityService[E#I]\n[error]    |                                                      ^\n[error]    |                                         E is not a legal path\n[error]    |                                         since it is not a concrete type\n[error] -- Error: \n[error] 15 |  def authenticatorService: AuthenticatorService[E#A]\n[error]    |                                                 ^\n[error]    |                                         E is not a legal path\n[error]    |                                         since it is not a concrete type\n[error] two errors found\n\n```\n\nYou can try directly at <https:\/\/scastie.scala-lang.org\/GuqSqC9yQS6uMiw9wyKdQg>","questionMetadata":{"Type":"Version","Level":"Advanced","Tag":"other"},"answer":"You can use [match types](https:\/\/docs.scala-lang.org\/scala3\/reference\/new-types\/match-types.html) (and [Aux-pattern](https:\/\/stackoverflow.com\/questions\/51131067\/when-are-dependent-types-needed-in-shapeless) for technical reasons, namely refined types `Env { type I = i }` can't be type patterns)\n\n\n\n```\ntrait Env:\n  type I <: Identity\n  type A <: Authenticator\nobject Env:\n  type AuxI[_I <: Identity] = Env { type I = _I }\n  type AuxA[_A <: Authenticator] = Env { type A = _A }\n\ntrait IdentityService[T <: Identity]\ntrait AuthenticatorService[T <: Authenticator]\n\n\/\/ match types\ntype I[E <: Env] <: Identity = E match\n  case Env.AuxI[i] => i \/\/ lower case is significant\n\ntype A[E <: Env] <: Authenticator = E match\n  case Env.AuxA[a] => a\n\ntrait Environment[E <: Env]:\n  def identityService[T <: Identity]: IdentityService[I[E]]\n  def authenticatorService: AuthenticatorService[A[E]]\n\n```\n\n[What does Dotty offer to replace type projections?](https:\/\/stackoverflow.com\/questions\/50043630\/what-does-dotty-offer-to-replace-type-projections)\n\n\n[In Scala 3, how to replace General Type Projection that has been dropped?](https:\/\/stackoverflow.com\/questions\/75227132\/in-scala-3-how-to-replace-general-type-projection-that-has-been-droppe)\n\n\n<https:\/\/users.scala-lang.org\/t\/converting-code-using-simple-type-projections-to-dotty\/6516>\n\n\nAlternatively you can use type classes."}

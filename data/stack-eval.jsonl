{"questionId":"032f2981d80f4ca7bf068fd024246cd1","question":"How do I print an integer in binary with leading zeros?\nI'm doing some bit twiddling and I'd like to print all the bits in my u16.\n\n\n\n```\nlet flags = 0b0000000000101100u16;\nprintln!(\"flags: {:#b}\", flags);\n\n```\n\nThis prints `flags: 0b101100`.\n\n\nHow do I make it print `flags: 0b0000000000101100`?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"rust"},"answer":"I have come to prefer these patterns:\n\n\n\n```\nprintln!(\"{:08b}\", 1i8);\nprintln!(\"{:016b}\", 12000u16);\nprintln!(\"{:032b}\", 624485u32);\n\n```\n\nit will print:\n\n\n\n```\n00000001\n0010111011100000\n00000000000010011000011101100101\n\n```\n\n"}
{"questionId":"be1dc66d8057452690bf559595b2da0c","question":"Android Studio could not find or load main class of \"Scratch\" file\n**Update**: Below problem is remained until `Android Studio 3.4.2`.\n\n\n\n\n---\n\n\nI use `Android Studio 3.3` and created a `scratch` file (`File > new > Scratch File > Java`). The IDE created `scratch.java` in `~\/.AndroidStudio3.3\/config\/scratches`:\n\n\n\n```\nclass Scratch {\n    public static void main(String[] args) {\n\n    }\n}\n\n```\n\nBut when I run that scratch file ( by pressing green button near to the class name or `main` method), I get this error:\n\n\n\n```\nError: Could not find or load main class Scratch\n\n```\n\nI think that IDE does not make `Scratch.class` and so `Java` cannot find it. Is there a way for solving this problem and running scratch files from IDE (without using `Terminal`)?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"**Update**\n\n\nI just noticed that this issue was fixed in Android Studio 3.5!\n\n\n\n\n---\n\n\nThe only way I got this to work (in Android Studio 3.4.1) is this **very dirty** approach:\n\n\n1. Go to `Run` > `Edit Configurations` > `Scratch`\n2. In `Before launch` at the bottom add (+) `Run External tool`\n3. In External Tools popup press `+` and fill in the following `Tool Settings`\n\n\n*Name*: `Compile Scratch`\n\n\n*Program*: `javac`\n\n\n*Arguments*: `$FileName$`\n\n\n*Working Directory*: your scratch file directory\n4. Press OK\n5. Again in External Tools popup press `+` and fill in the following `Tool Settings`\n\n\n*Name*: `Run Scratch`\n\n\n*Program*: `java`\n\n\n*Arguments*: `$FileClass$`\n\n\n*Working Directory*: your scratch file directory\n6. Press OK and Apply in Run\/Debug Configurations\n\n\nWeirdly this works only if you use the green arrows beside the class\/main declaration. If you use the Run button in the menu this also fails. \n\n\nYou will still get the not found error in the `Scratch` tab of the run window. But two additional tabs will pop up (one for each external tool). In the `Run Scratch` tab you will get your output.\n\n\nI personally don't like this, But it's the best workaround I came up with.\n\n\n"}
{"questionId":"de9215f8bab046d3aaa5ab5909cdb491","question":"In Java, when should we use private instance methods in interfaces?\n\n> \n> As of Java 9, methods in an interface can be private. A private method\n> can be static or an instance method. Since private methods can only be\n> used in the methods of the interface itself, their use is limited to\n> being helper methods for the other methods of the interface.\n> \n> \n> **Cay S. Horstmann, Core Java Volume I - Fundamentals**\n> \n> \n> \n\n\nI get that we can put the common functionality in the private methods and not make it accessible to public. But we can have two kind of private methods here:\n\n\n1. `private`\n2. `private static`\n\n\nUsing `private static` methods is understandable, but when should we use `private` methods? We are not dealing with instances here as this is an interface, so why creating `private` methods is allowed? Don't we need only `private static` methods?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"java"},"answer":"OK, another attempt at actually answering OP's questions. When you need to call another non-static method on the interface from a private method, the private method cannot be static. For example, there would be a compilation error if the private method below was static:\n\n\n\n```\npublic interface InterfaceWithMethods {\n    public default void doSomething() {\n        doSomethingCommon();\n    }\n\n    public default void doSomethingElse() {\n        doSomethingCommon();\n    }\n\n    public void actuallyDoSomething();\n\n    private void doSomethingCommon() {\n        System.out.println(\"Do something first.\");\n        actuallyDoSomething();\n    }\n}\n\n```\n\n"}
{"questionId":"f1166897db13412394c8eae8a93c72e9","question":"VSCode keeps opening powershell on start even though Git Bash is set as default\nI have set my integrated default terminal in VSCode as Git Bash but whenever I launch VSCode, powershell pops up instead of bash. It does not, however, change the default terminal setting but it is irritating.\nI have tried removing the profile of powershell from settings but that seems to do nothing.\nWhy is this happening?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"This worked for me:\n\n\n\n```\n{\n    \"terminal.integrated.defaultProfile.windows\": \"Git Bash\",\n    \"terminal.integrated.profiles.windows\": {\n        \"PowerShell\": null,\n        \"Git Bash\": {\n            \"source\": \"Git Bash\"\n        },\n        \"Windows PowerShell\": null\n    },\n    \"terminal.integrated.automationShell.windows\": \"Git Bash\",\n    \"powershell.enableProfileLoading\": false,\n    \"powershell.integratedConsole.showOnStartup\": false\n}\n\n```\n\n"}
{"questionId":"bf00e03c52854e5dade37b2292ade5d9","question":"Switching to the system Perl using perlbrew\nFirst, some background.\n\n\n`perlbrew` is a tool to assist with the installation of Perl into a non-standard directory (usually under your home directory).\n\n\nIt also helps you control which Perl installation is used when executing `perl` in an interactive shell. Switching between installations is done using `perlbrew use` and `perlbrew switch`. `perlbrew use` only affects the current shell, while `perlbrew switch` is more permanent.\n\n\n\n```\n$ perl -V:version             |  $ perl -V:version\nversion='5.20.0';             |  version='5.20.0';\n                              |\n$ perlbrew use 5.18.2t        |  $ perlbrew switch 5.18.2t\n                              |\n$ perl -V:version             |  $ perl -V:version\nversion='5.18.2';             |  version='5.18.2';\n                              |\n$ bash -ic 'perl -V:version'  |  $ bash -ic 'perl -V:version'\nversion='5.20.0';             |  version='5.18.2';\n\n```\n\n`perlbrew off` is used to revert to using the system Perl, but it's temporary like `perlbrew use`. Is there a way to revert to the system Perl with the permanency of `perlbrew switch`?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"perl"},"answer":"You can use the following command\n\n\n\n```\nperlbrew switch-off\n\n```\n\n"}
{"questionId":"35491d3d9c7c4c739835580ef546d247","question":"Idea behind \"[...] makes pointer from integer without a cast\"\nI always wondered why warnings `passing argument 1 from of 'foo' makes pointer from integer without a cast` and alike are only warnings and not errors.\n\n\nActually these warnings are almost always errors.\n\n\nDoes somebody know what's the idea behind this? \n\n\n- Is it mostly to allow prehistoric code to be compiled without errors?\n- Or just to comply to the standard? Then latter maybe needs some fixing.\n\n\nExample:\n\n\n\n```\nint foo(int *bar)\n{\n  *bar = 42;\n}\n\nvoid bar()\n{\n  int n = 0;\n  foo(n);      \/\/ this is obviously an error\n  ...\n}\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"\n> \n> Does somebody know what's the idea behind this?\n> \n> \n> - Is it mostly to allow prehistoric code to be compiled without errors?\n> - Or just to comply to the standard? Then latter maybe needs some fixing.\n> \n> \n> \n\n\nIt is to comply with the standard in the sense that the standard requires conforming implementations to diagnose such issues, as @R.. describes in his answer. Implementations are *not* required to reject programs on account of such issues, however. As for why some compilers instead accept such programs, that would need to be evaluated on a per-implementation basis, but this quotation from the first edition of K&R may shed a bit of light:\n\n\n\n> \n> **5.6 Pointers are not Integers**\n> \n> \n> You may notice in older C programs a rather cavalier attitude toward\n>  copying pointers. It has generally been true that on most machines a\n>  pointer may be assigned to an integer and back again; no scaling or\n>  conversion takes place, and no bits are lost. Regrettably, this has\n>  led to the taking of liberties with routines that return pointers\n>  which are then merely passed to other routines -- the requisite\n>  pointer declarations are often left out.\n> \n> \n> \n\n\n(Kernighan & Ritchie, *The C Programming Language*, 1st ed., 1978)\n\n\nNotice in the first place that this long predates even C89. I'm a bit amused today that the authors were *then* talking about \"older\" C programs. But note too that even at that time, the C language as defined by K&R did not formally permit implicit conversion between pointers and integers (though it did permit casting between them).\n\n\nNevertheless, there were programs that relied on implicit conversion anyway, apparently because it happened to work on the targeted implementations. It was attractive, by some people's standards at the time, in conjunction with primordial C's implicit typing rules. One could let a variable or function intended to return or store a pointer default to type `int` by omitting its declaration altogether, and as long as it was interpreted as a pointer wherever it ultimately was used, everything usually happened to work as intended.\n\n\nI'm inclined to guess that everything continuing to work as intended, thereby supporting backwards compatibility, was a consideration for compiler developers in continuing to accept implicit conversions, so that's \"allow[ing] prehistoric code to be compiled.\" I note, however, that these days code with implicit conversions of this kind are much less likely to work as intended than they used to be, for many machines these days have 64-bit pointers but only 32-bit `int`s.\n\n\n"}
{"questionId":"526433947a884bf3a37c7868e82eff13","question":"jetpack compose java.lang.IllegalStateException: Start\/end imbalance\nthis piece of code cause this crash:\n\n\nim using compose version 1.0.0-alpha06\n\n\n\n> \n> java.lang.IllegalStateException: Start\/end imbalance \u00a0\u00a0at androidx.compose.runtime.Composer.finalizeCompose(Composer.kt:2369) \u00a0\u00a0at androidx.compose.runtime.Composer.endRoot(Composer.kt:575) \u00a0\u00a0at androidx.compose.runtime.Composer.composeInitial(Composer.kt:2054) \u00a0\u00a0at androidx.compose.runtime.Recomposer.composeInitial$runtime\\_release(Recomposer.kt:276) \u00a0\u00a0at androidx.compose.runtime.CompositionImpl.setContent(Composition.kt:110) \u00a0\u00a0at androidx.compose.ui.platform.WrappedComposition$setContent$1.invoke(Wrapper.kt:234) \u00a0\u00a0at androidx.compose.ui.platform.WrappedComposition$setContent$1.invoke(Wrapper.kt:-1) \u00a0\u00a0at androidx.compose.ui.platform.AndroidComposeView.onAttachedToWindow(AndroidComposeView.kt:627) \u00a0\u00a0at android.view.View.dispatchAttachedToWindow(View.java:20479) \u00a0\u00a0at android.view.ViewGroup.dispatchAttachedToWindow(ViewGroup.java:3489) \u00a0\u00a0at android.view.ViewGroup.dispatchAttachedToWindow(ViewGroup.java:3496) \u00a0\u00a0at android.view.ViewGroup.dispatchAttachedToWindow(ViewGroup.java:3496) \u00a0\u00a0at android.view.ViewGroup.dispatchAttachedToWindow(ViewGroup.java:3496) \u00a0\u00a0at android.view.ViewGroup.dispatchAttachedToWindow(ViewGroup.java:3496) \u00a0\u00a0at android.view.AttachInfo\\_Accessor.setAttachInfo(AttachInfo\\_Accessor.java:44)\n> \n> \n> \n\n\ncan someone help me? thanks\n\n\n\n```\n@Composable\n@Preview\nprivate fun Image1() {\n    Box(modifier = Modifier.fillMaxWidth().wrapContentHeight()) {\n        Image(\n                asset = imageResource(id = R.mipmap.fit_static_image_1),\n                contentScale = ContentScale.FillWidth,\n        )\n        Column(Modifier.padding(start = 16.dp, end = 16.dp).align(Alignment.CenterStart), horizontalAlignment = Alignment.Start) {\n            Text(\n                    color = getColor(id = R.color.white),\n                    fontWeight = FontWeight.Bold,\n                    fontSize = TextUnit.Sp(18),\n                    text = dicString(id = R.string.fit_static_image_1_title),\n                    textAlign = TextAlign.Start\n            )\n            Text(\n                    text = dicString(id = R.string.fit_static_image_1_description),\n                    color = getColor(id = R.color.white),\n                    fontSize = TextUnit.Sp(14),\n                    modifier = Modifier.padding(top = 4.dp),\n                    textAlign = TextAlign.Start\n            )\n        }\n    }\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"kotlin"},"answer":"i was using **remember { }** wrong, i was trying to use it in **dicString** function to remember obtained string. That caused the issue. I fixed that by adding the resource id to the remember function as parameter. **remember( id ) { }**\n\n\n"}
{"questionId":"86c595ac1a514e7b9d7bf6a420844d2f","question":"Conda dependencies do not install on local package build\nI am building a Python package using `conda-build`. Right now, my structure looks like this:\n\n\n\n```\n- my_recipe\/\n    - meta.yaml\n    - build.sh\n\n```\n\nAnd my `meta.yaml` reads thusly:\n\n\n\n```\npackage:\n  name: my_pkg\nversion: \"0.2.0\"\n\nsource:\n  path: ..\/my_pkg\n\nrequirements:\n  build:\n    - python\n    - setuptools\n  run:\n    - python\n    - pandas\n    - numpy\n    - plotly\n    - matplotlib\n    - pyqtgraph\n    - pyopengl\n    - gdal\n    - scipy\n    - scikit-image\n\n```\n\nThe package itself builds correctly when I run\n\n\n`conda-build my_recipe\/`\n\n\nand it installs successfully when I run\n\n\n`conda install -n my_env --use-local ~\/miniconda3\/envs\/my_env\/conda-bld\/linux-64\/my_pkg-0.2.0-py36_0.tar.bz2`\n\n\nHowever, none of the dependencies listed under `run` seem to install along with the package. For example, when I import the package in Python it says that `pandas` could not be found.\n\n\nAre my dependencies listed in the correct location? Do I also need to list the dependencies in `setup.py`? The documentation is not very clear on where this information should be.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"As commented by @darthbith, using the `--use-local` flag with the package name,\n\n\n\n```\nconda install -n my_env --use-local my_pkg\n\n```\n\nworks as intended. Using a path to a tarball directly triggers Conda to install without dependencies.\n\n\n"}
{"questionId":"5447ef504d5246399a1d298ba868bc48","question":"Add a method to existing C++ class in other file\nIs it possible in C++ to extend a class(add a method) in a different source file without editing the original source file where the class is written?\n\n\nIn obj-c it is possible by writing another `@interface AbcClass (ExtCategory) ... @end`\n\n\nI got compile-time error(s) when I tried something like this:\n\n\n\n```\n\/\/Abc.h\nclass Abc {            \/\/This class is from a 3rd party library....\n                       \/\/  ...I don't want to edit its source file.\n    void methodOne();\n    void methodTwo();\n\n}\n\n\n\/\/Abc+Ext.h\nclass Abc {       \/\/ ERROR: Redefinition of 'Abc'\n    void methodAdded();\n}\n\n```\n\nMy target is to retain the 'Abc' name and add methods to it. A specific class in a 3rd party library that I used lacks some methods and I want to add those methods but I am keeping the source file unedited.\n\n\nIs there a way to do this? I am new in writing C++ codes. I am familiar with some of its syntax but don't know much.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c++"},"answer":"i found out that `c++` is better at doing this than `obj-c`.\n\n\ni tried the following and it works great!\n\n\nthe key is, enclose all of your classes in a namespace and then extend your target classes there with the same class name.\n\n\n\n```\n\/\/Abc.h\nnamespace LibraryA {\n    class Abc {            \/\/This class is from a 3rd party library....\n                           \/\/  ...I don't want to edit its source file.\n        void methodOne();\n        void methodTwo();\n\n    }\n}\n\n\/\/Abc+Ext.hpp\nnamespace MyProj {\n    class Abc : public LibraryA::Abc {\n        using Base = LibraryA::Abc;   \/\/desc: this is to easily access the original class...\n                                      \/\/   ...by using code: Abc::Base::someOrigMethod();\n        using Base::Base;             \/\/desc: inherit all constructors.\n        \n    protected:\n        \/\/---added members:\n        int memberAdded;\n        \n    public:\n        \/\/---added methods:\n        void methodAdded();\n        \n        \/\/---modified virtual member funcs from original class.\n        void origMethod_A() override;\n        \n    }\n}\n\n\/\/Abc+Ext.cpp\nnamespace MyProj {\n    void Abc::origMethod_A() {\n        \/\/...some code here...\n        Base::origMethod_A();    \/\/desc: you can still call the orignal method\n        \/\/...some code here...\n    }\n}\n\n\/\/SomeSourceFile_ThatUses_Abc.cpp\nnamespace MyProj {      \/\/IMPT NOTE: you really need to enclose your...\n                        \/\/   ...project specific code to a namespace so you can...\n                        \/\/   ...use the version of class Abc you extended.\n                        \n                        \n    void SomeClass::SampleFunc(){\n        Abc objX;                   \/\/create MyProj::Abc object.\n        objX.methodAdded();         \/\/calls MyProj::Abc::methodAdded();\n        objX.origMethod_A();        \/\/calls MyProj::Abc::origMethod_A();\n        \n        Abc::Base objY;             \/\/create Library::Abc object.\n        \/\/objY.methodAdded();       \/\/method not existing.\n        objY.origMethod_A();        \/\/calls Library::Abc::origMethod_A();\n        \n        \/\/...some code here...\n    }\n    \n}\n\n\/\/SomeModule.cpp\nnamespace OtherNamespace {\n    void SomeOtherClass::SampleOtherFunc(){\n        Abc objZ;                   \/\/create Library::Abc object.\n        \/\/objZ.methodAdded();       \/\/method not existing.\n        objZ.origMethod_A();        \/\/calls LibraryA::Abc::origMethod_A();\n    }\n    \n}\n\n```\n\nyou can even extend `class Abc` differently within other module namespaces.  \n\n\n\n\n```\n\/\/MyLib_ModuleA_Classes.hpp\nnamespace MyLib_ModuleA {\n    class Abc : public LibraryA::Abc {\n        \/\/...add extensions here...\n        void addedMethod_X();\n        void origMethod_A() override;    \/\/own overriden behavior specific to this ModuleA only.\n    }\n    \n}\n\n\/\/MyLib_ModuleB_Classes.hpp\nnamespace MyLib_ModuleB {\n    class Abc : public LibraryA::Abc {\n        \/\/...add extensions here...\n        void addedMethod_Y();\n        void origMethod_A() override;    \/\/own overriden behavior specific to this ModuleB only.\n    }\n    \n}\n\n```\n\nif in case `class Abc` is in global namespace, though i haven't tried it yet, i think you can just replace `LibaryA::Abc` to `::Abc`.\n\n\nsorry for the very late answer i've been doing this approach for around 4 years now and it's structure is very well useful.\ni tried this in `c++14` but i think this is still doable in `c++11`. now i used `c++17` and it compiles fine. i'm planning to convert to `c++20`\nwhen the compilers i used already completed `c++20` features.\n\n\n"}
{"questionId":"ad8195b11ab24122837e00c9ed607cd6","question":"How to Ignore Duplicate Key Errors Safely Using insert\\_many\nI need to ignore duplicate inserts when using insert\\_many with pymongo, where the duplicates are based on the index. I've seen this question asked on stackoverflow, but I haven't seen a useful answer.\n\n\nHere's my code snippet:\n\n\n\n```\ntry:\n    results = mongo_connection[db][collection].insert_many(documents, ordered=False, bypass_document_validation=True)\nexcept pymongo.errors.BulkWriteError as e:\n    logger.error(e)\n\n```\n\nI would like the insert\\_many to ignore duplicates and not throw an exception (which fills up my error logs). Alternatively, is there a separate exception handler I could use, so that I can just ignore the errors. I miss \"w=0\"...\n\n\nThanks\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"The correct solution is to use a WriteConcern with w=0 and ordered=False:\n\n\n\n```\nimport pymongo\nfrom pymongo.write_concern import WriteConcern\n\n\nmongodb_connection[db][collection].with_options(write_concern=WriteConcern(w=0)).insert_many(messages, ordered=False)\n\n```\n\n"}
{"questionId":"3a526d4a8b8945f5ab165aaf5469fdcf","question":"Secure and effective way for waiting for asynchronous task\nIn the system, I have an object - let's call it `TaskProcessor`. It holds queue of tasks, which are executed by some pool of threads (`ExecutorService` + `PriorityBlockingQueue`)\nThe result of each task is saved in the database under some unique identifier.\n\n\nThe user, who knows this unique identifier, may check the result of this task. The result could be in the database, but also the task could still wait in the queue for execution. In that case, `UserThread` should wait until the task will be finished.\n\n\nAdditionally, the following assumptions are valid:\n\n\n- **Someone else could enqueue** the task to `TaskProcessor` and some random `UserThread` can access the result if he knows the unique identifier.\n- `UserThread` and `TaskProcess` are in the same app. `TaskProcessor` contains a pool of threads, and `UserThread` is simply servlet Thread.\n- `UserThread` should be blocked when asking for the result, and the result is not completed yet. `UserThread` should be unblocked immediately after `TaskProcessor` complete task (or tasks) grouped by a unique identifier\n\n\nMy first attempt (the naive one), was to check the result in the loop and sleep for some time:\n\n\n\n```\n\/\/ UserThread\nwhile(!checkResultIsInDatabase(uniqueIdentifier))\n  sleep(someTime)\n\n```\n\nBut I don't like it. First of all, I am wasting database connections. Moreover, if the task would be finished right after sleep, then the user will wait even if the result just appeared.\n\n\nNext attempt was based on wait\/notify:\n\n\n\n```\n\/\/UserThread \nwhile (!checkResultIsInDatabase())\n  taskProcessor.wait()\n\n\/\/TaskProcessor\n... some complicated calculations\nthis.notifyAll()\n\n```\n\nBut I don't like it either. If more `UserThreads` will use `TaskProcessor`, then they will be wakened up unnecessarily every time some task would be completed and moreover - they will make unnecessary database calls.\n\n\nThe last attempt was based on something which I called `waitingRoom`:\n\n\n\n```\n\/\/UserThread\nObject mutex = new Object();\ntaskProcessor.addToWaitingRoom(uniqueIdentifier, mutex)\nwhile (!checkResultIsInDatabase())\n  mutex.wait()\n\n\/\/TaskProcessor\n... Some complicated calculations\nif (uniqueIdentifierExistInWaitingRoom(taskUniqueIdentifier))\n  getMutexFromWaitingRoom(taskUniqueIdentifier).notify()\n\n```\n\nBut it seems to be not secure. Between database check and `wait()`, the task could be completed (`notify()` wouldn't be effective because `UserThread` didn't invoke `wait()` yet), which may end up with deadlock.\n\n\nIt seems, that I should synchronize it somewhere. But I am afraid that it will be not effective.\nIs there a way to correct any of my attempts, to make them secure and effective? Or maybe there is some other, better way to do this?\n\n\n","questionMetadata":{"type":"implementation","level":"advanced","tag":"java"},"answer":"I believe replacing of `mutex` with `CountDownLatch` in `waitingRoom` approach prevents deadlock.\n\n\n\n```\nCountDownLatch latch = new CountDownLatch(1)\ntaskProcessor.addToWaitingRoom(uniqueIdentifier, latch)\nwhile (!checkResultIsInDatabase())\n  \/\/ consider timed version\n  latch.await()\n\n\/\/TaskProcessor\n... Some complicated calculations\nif (uniqueIdentifierExistInWaitingRoom(taskUniqueIdentifier))\n  getLatchFromWaitingRoom(taskUniqueIdentifier).countDown()\n\n```\n\n"}
{"questionId":"79fbe846b3f449609c9baa07697a9c2e","question":"Count the number of arguments in a lambda\nI need to know the exact number of arguments that a lambda has. I do not care for their types, I just need a count.\n\n\n\n```\nauto lambda0 = [&]() { ... };\nauto lambda1 = [&](int32_t a) { ... };\nauto lambda2 = [&](int32_t a, auto b) { ... };\n\nlambda_details<decltype(lambda0)>::argument_count; \/\/ Equals 0\nlambda_details<decltype(lambda1)>::argument_count; \/\/ Equals 1\nlambda_details<decltype(lambda2)>::argument_count; \/\/ Equals 2\n\n```\n\nDetecting variadic lambdas would also be nice so that I can deal with that edge case as well.\n\n\n\n```\nauto lambda_variadic = [&](auto... args){ ... };\n\nlambda_details<decltype(lambda_variadic)>::is_variadic; \/\/ Equals true\n\n```\n\nHow can I get this information?\n\n\n","questionMetadata":{"type":"implementation","level":"advanced","tag":"c++"},"answer":"I have solved it using a modified version of @yuri kilochek's answer.\n\n\nInstead of starting from 50 arguments and counting down, we start at zero and count up. When we get a match we know the minimum amount of arguments required to call the lambda. We then keep searching up until a sane maximum to see if there is a maximum amount of arguments (this can happen when you have default arguments). \n\n\nIf the argument count limit is reached, we assume the lambda to be variadic.\n\n\nThis implementation reduces the amount of template instantiations for non variadic lambdas significantly. It also gives us the minimum amount of arguments for all lambdas, and the maximum amount of arguments for any non-variadic lambdas.\n\n\nAgain, big thanks to Yuri Kilochek for laying the foundation for this elegant solution. Check his answer for more details about the implementation.\n\n\n\n```\nstruct any_argument\n{\n    template <typename T>\n    operator T && () const;\n};\n\ntemplate <typename Lambda, typename Is, typename = void>\nstruct can_accept_impl : std::false_type\n{};\n\ntemplate <typename Lambda, std::size_t ...Is>\nstruct can_accept_impl <Lambda, std::index_sequence<Is...>, decltype(std::declval<Lambda>()(((void)Is, any_argument{})...), void())> : std::true_type\n{};\n\ntemplate <typename Lambda, std::size_t N>\nstruct can_accept : can_accept_impl<Lambda, std::make_index_sequence<N>>\n{};\n\ntemplate <typename Lambda, std::size_t N, size_t Max, typename = void>\nstruct lambda_details_maximum\n{\n    static constexpr size_t maximum_argument_count = N - 1;\n    static constexpr bool is_variadic = false;\n};\n\ntemplate <typename Lambda, std::size_t N, size_t Max>\nstruct lambda_details_maximum<Lambda, N, Max, std::enable_if_t<can_accept<Lambda, N>::value && (N <= Max)>> : lambda_details_maximum<Lambda, N + 1, Max>\n{};\n\ntemplate <typename Lambda, std::size_t N, size_t Max>\nstruct lambda_details_maximum<Lambda, N, Max, std::enable_if_t<can_accept<Lambda, N>::value && (N > Max)>>\n{\n    static constexpr bool is_variadic = true;\n};\n\ntemplate <typename Lambda, std::size_t N, size_t Max, typename = void>\nstruct lambda_details_minimum : lambda_details_minimum<Lambda, N + 1, Max>\n{\n    static_assert(N <= Max, \"Argument limit reached\");\n};\n\ntemplate <typename Lambda, std::size_t N, size_t Max>\nstruct lambda_details_minimum<Lambda, N, Max, std::enable_if_t<can_accept<Lambda, N>::value>> : lambda_details_maximum<Lambda, N, Max>\n{\n    static constexpr size_t minimum_argument_count = N;\n};\n\ntemplate <typename Lambda, size_t Max = 50>\nstruct lambda_details : lambda_details_minimum<Lambda, 0, Max>\n{};\n\n```\n\nAnother important thing to note is that `any_argument` doesn't automatically play nice with operators. You will have to overload every single one if you want it to work with `auto` arguments that are operated upon (e.g. `[](auto a) { return a * 2; }`). It will end up looking more like this:\n\n\n\n```\nstruct any_argument\n{\n    template <typename T> operator T && () const;\n\n    any_argument& operator ++();\n    any_argument& operator ++(int);\n    any_argument& operator --();\n    any_argument& operator --(int);\n\n    template <typename T> friend any_argument operator + (const any_argument&, const T&);\n    template <typename T> friend any_argument operator + (const T&, const any_argument&);\n    template <typename T> friend any_argument operator - (const any_argument&, const T&);\n    template <typename T> friend any_argument operator - (const T&, const any_argument&);\n    template <typename T> friend any_argument operator * (const any_argument&, const T&);\n    template <typename T> friend any_argument operator * (const T&, const any_argument&);\n    template <typename T> friend any_argument operator \/ (const any_argument&, const T&);\n    template <typename T> friend any_argument operator \/ (const T&, const any_argument&);\n\n    \/\/ And every other operator in existence\n};\n\n```\n\n"}
{"questionId":"aa29f59a0a734540840e65fa81abfce8","question":"Python range to list\nI am trying to convert a range to list.\n\n\n\n```\nnums = []\nfor x in range (9000, 9004):\n    nums.append(x)\n    print nums\n\n```\n\noutput\n\n\n\n```\n[9000]\n[9000, 9001]\n[9000, 9001, 9002]\n[9000, 9001, 9002, 9003]\n\n```\n\nI just need something like \n\n\n\n```\n [9000, 9001, 9002, 9003]\n\n```\n\nHow do I get just the requred list ? \n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"python"},"answer":"Since you are taking the print statement under the for loop, so just placed the print statement out of the loop.\n\n\n\n```\nnums = []\nfor x in range (9000, 9004):\n    nums.append(x)\nprint (nums)\n\n```\n\n"}
{"questionId":"e2d7fb4ed7e24ce6acd88eda6a072e22","question":"Sort Dict by Values in Python 3.6+\nI was looking for a method to sort a dictionary in Python with its values, after a few attempts, is what it comes:\n\n\n\n```\na = {<populated dict...>}\na = {v: k for k, v in a.items()}\na = {v: k for k, v in sorted(a.items())}\n\n```\n\nThis code seems to work, but I think it's poor for performance, is there a better way?\n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"python"},"answer":"By default, the dictionary is sorted based on keys, but the sorted function takes a function as a parameter using which you can alter the behaviour for\nprogram.\n\n\n\n```\nd={'a':6,'b':4,'k':3}\nprint(sorted(d)) \n\nsorted_by_values= sorted(d,key=lambda x:d[x])\nprint(sorted_by_values)\n\n```\n\n"}
{"questionId":"328b0d7a29f540f5bbd72432f6e4ed7b","question":"How can I invert a MelSpectrogram with torchaudio and get an audio waveform?\nI have a `MelSpectrogram` generated from:\n\n\n\n```\neval_seq_specgram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_fft=256)(eval_audio_data).transpose(1, 2)\n\n```\n\nSo `eval_seq_specgram` now has a `size` of `torch.Size([1, 128, 499])`, where 499 is the number of timesteps and 128 is the `n_mels`.\n\n\nI'm trying to invert it, so I'm trying to use `GriffinLim`, but before doing that, I think I need to invert the `melscale`, so I have:\n\n\n\n```\ninverse_mel_pred = torchaudio.transforms.InverseMelScale(sample_rate=sample_rate, n_stft=256)(eval_seq_specgram)\n\n```\n\n`inverse_mel_pred` has a `size` of `torch.Size([1, 256, 499])`\n\n\nThen I'm trying to use `GriffinLim`:\n\n\n\n```\npred_audio = torchaudio.transforms.GriffinLim(n_fft=256)(inverse_mel_pred)\n\n```\n\nbut I get an error:\n\n\n\n```\nTraceback (most recent call last):\n  File \"evaluate_spect.py\", line 63, in <module>\n    main()\n  File \"evaluate_spect.py\", line 51, in main\n    pred_audio = torchaudio.transforms.GriffinLim(n_fft=256)(inverse_mel_pred)\n  File \"\/home\/shamoon\/.local\/share\/virtualenvs\/speech-reconstruction-7HMT9fTW\/lib\/python3.8\/site-packages\/torch\/nn\/modules\/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"\/home\/shamoon\/.local\/share\/virtualenvs\/speech-reconstruction-7HMT9fTW\/lib\/python3.8\/site-packages\/torchaudio\/transforms.py\", line 169, in forward\n    return F.griffinlim(specgram, self.window, self.n_fft, self.hop_length, self.win_length, self.power,\n  File \"\/home\/shamoon\/.local\/share\/virtualenvs\/speech-reconstruction-7HMT9fTW\/lib\/python3.8\/site-packages\/torchaudio\/functional.py\", line 179, in griffinlim\n    inverse = torch.istft(specgram * angles,\nRuntimeError: The size of tensor a (256) must match the size of tensor b (129) at non-singleton dimension 1\n\n```\n\nNot sure what I'm doing wrong or how to resolve this.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"Just for history, full code:\n\n\n\n```\nimport torch\nimport torchaudio\nimport IPython\n\nwaveform, sample_rate = torchaudio.load(\"wavs\/LJ030-0196.wav\", normalize=True)\n\nn_fft = 256\n\nn_stft = int((n_fft\/\/2) + 1)\ntransofrm = torchaudio.transforms.MelSpectrogram(sample_rate, n_fft=n_fft)\ninvers_transform = torchaudio.transforms.InverseMelScale(sample_rate=sample_rate, n_stft=n_stft)\ngrifflim_transform = torchaudio.transforms.GriffinLim(n_fft=n_fft)\n\nmel_specgram = transofrm(waveform)\ninverse_waveform = invers_transform(mel_specgram)\npseudo_waveform = grifflim_transform(inverse_waveform)\n\n```\n\nAnd\n\n\n\n```\nIPython.display.Audio(waveform.numpy(), rate=sample_rate)\n\n```\n\n\n```\nIPython.display.Audio(pseudo_waveform.numpy(), rate=sample_rate)\n\n```\n\n"}
{"questionId":"840c7c3be9a24bc597db9ce4dd790905","question":"Enum parameter for powershell cmdlet\nI'm writing a cmdlet (script) on powershell and I wanted to use eunm as one of the parameters. But I don't know where to put enum definition so that it would be visible for cmdlet parameters declaration.\n\n\nFor example, I have a parameters definition of the script like this\n\n\n\n```\n[cmdletbinding()]\nparam(\n    [Parameter(Mandatory=$True)]\n    [string]$Level\n)\n\n```\n\nand an enum like this\n\n\n\n```\nenum LevelEnum { NC = 1; NML = 2; CS = 3 }\n\n```\n\nI can't replace `[string]` with `[LevelEnum]` in the parameter definition because script will fail to locate enum definition. And I can't put definition before `cmdletbinding`, it's not allowed.\nI know how to do it if that would've been a function, I know it can be solved using `ValidateSet`, but I need to have integer values correstonding to enum options.\n\n\n\n```\n[ValidateSet('NC','NML','CS')]\n\n```\n\nBut the question is, can I do the same for a cmdlet?\n\n\n\n\n---\n\n\nThanks to everyone.\nI ended up with a combination of defferent answers.\n\n\n\n```\n[cmdletbinding()]\nparam(\n    [Parameter(Mandatory=$True)]\n    [ValidateSet('NC','NML','CS')]\n    [string]$Level\n)\n# Convert level from string to enum\nenum PatchLevel { NC = 1; NML = 2; CS = 3 }\n[PatchLevel]$l = $Level\n\n# Use the numeric value\nWrite-Host $l.value__\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"If this script needs to accept a custom enum, it means that you will be calling it from some other place, where enum definition already exists. And now you're trying to add the same definition again in a script. It would be good idea to push it out to a module as per @Alex\\_P suggestion so the definition resides in one place, but the downside is that `Import-Module` or `#Requires` won't import it, thus the need for `using module` clause.\n\n\nBut if you're willing to accept simpler and less secure solution, you could take advantage of the fact that any enum you define is derived from System.Enum. `[System.Enum]$Level` will accept only and all enums and if it's not LevelEnum the script will break, but still it filters most possible mistakes before the script execution and gives some information about parameter type.\n\n\n"}
{"questionId":"f066a2b4502247329db3834a41a94b00","question":"How to map a TIMESTAMP column to a ZonedDateTime JPA entity property?\nI'm using Spring data **jpa** and **mariadb** latest version, and **MariaDB 10.3.16**\n\n\n\n```\n+--- org.springframework.boot:spring-boot-starter-data-jpa -> 2.1.5.RELEASE\n...\n|    +--- org.springframework.boot:spring-boot-starter-jdbc:2.1.5.RELEASE\n...\n|    +--- org.hibernate:hibernate-core:5.3.10.Final\n\n```\n\nThis is my Entity:\n\n\n\n```\n@Entity\n@Data\n@Table\n@NoArgsConstructor\n@AllArgsConstructor\npublic class Note {\n    @Id\n    @GeneratedValue(strategy = GenerationType.SEQUENCE)\n    private Integer id;\n\n    @Column\n    private String gsn;\n\n    @Column\n    @Enumerated(EnumType.STRING)\n    private NoteType type;\n\n    @Column\n    private String text;\n\n    @Column\n    private ZonedDateTime scheduleDt;\n\n    @Column\n    @CreationTimestamp\n    private Instant createDt;\n\n    @Column\n    @UpdateTimestamp\n    private ZonedDateTime updateDt;\n}\n\n```\n\nWhen I persist my entity, Hibernate tries to save `ZonedDateTime` member as **DATETIME** column. But I want to use **TIMESTAMP** column instead of **DATETIME** column.\n\n\nThis is create DDL, what I see from log.\n\n\n\n```\ncreate table `note` (`id` integer not null, `create_dt` datetime,\n    `gsn` varchar(255), `schedule_dt` datetime, `text` varchar(255),\n    `type` varchar(255), `update_dt` datetime, primary key (`id`)) \n  engine=MyISAM\n\n```\n\nHere `create_dt`, `schedule_dt`, `update_dt` is created as `datetime` column type, what is not I wanted. (I don't like MyISAM, too).\n\n\nHow can I fix it?\n\n\n\n\n---\n\n\nAdded because comment cannot express ddl.\n\n\nWhen I use columnDefinition attribute, generated ddl is ...\n\n\n\n```\ncreate table `note` (`id` integer not null, `create_dt` datetime,\n    `gsn` varchar(255), `schedule_dt` datetime, `text` varchar(255),\n    `type` varchar(255), `update_dt` `TIMESTAMP`, primary key (`id`)) \n\n```\n\nengine=MyISAM\n\n\nThere is unrequired '`' around TIMESTAMP.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"You can define the column type using the @Column annotation:\n\n\n\n```\n@Column(columnDefinition=\"TIMESTAMP\")  \n@UpdateTimestamp\nprivate ZonedDateTime updateDt;\n\n```\n\n"}
{"questionId":"82be8e5e74a7458cbed2664263d3b0d6","question":"PHP str\\_replace(): Passing null to parameter #3 ($subject) of type array|string is deprecated\nI'm getting this error while running my app:\n\n\n\n> \n> PHP str\\_replace(): Passing null to parameter #3 ($subject) of type\n> array|string is deprecated\n> \n> \n> \n\n\nI am Using CodeIgniter Version : V4.1.8 PHP Version : 8.1.2, full error output below:\n\n\n\n```\n{\"data\":[\n    [\"omron\",\"<span class=\\\"label label-success\\\">Active<\\\/span>\",\n    \"<button type=\\\"button\\\" class=\\\"btn btn-default\\\" onclick=\\\"editBrand(4)\\\" data-toggle=\\\"modal\\\" data-target=\\\"#editBrandModal\\\"><i class=\\\"fa fa-pencil\\\"><\\\/i><\\\/button> <button type=\\\"button\\\" class=\\\"btn btn-default\\\" onclick=\\\"removeBrand(4)\\\" data-toggle=\\\"modal\\\" data-target=\\\"#removeBrandModal\\\"><i class=\\\"fa fa-trash\\\"><\\\/i><\\\/button>\\n\\t\\t\\t\\t\"]\n    ]\n}\n<div style=\"border:1px solid #990000;padding-left:20px;margin:0 0 10px 0;\">\n\n<h4>A PHP Error was encountered<\/h4>\n\n<p>Severity: 8192<\/p>\n<p>Message:  str_replace(): Passing null to parameter #3 ($subject) of type array|string is deprecated<\/p>\n<p>Filename: core\/Output.php<\/p>\n<p>Line Number: 457<\/p>\n\n\n    <p>Backtrace:<\/p>\n\n```\n\nUPDATE: Code\n\n\n\n```\nif ($this->parse_exec_vars === TRUE) \n{ \n    $memory = round(memory_get_usage() \/ 1024 \/ 1024, 2).'MB'; \n    \/\/ below is line 457\n    $output = str_replace(array('{elapsed_time}', '{memory_usage}'), array($elapsed, $memory), $output); \n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"This error message can show up if you have not loaded any views. Codeigniter's internal output buffer is never initialized thus being `null`. The output buffer is the third parameter to `str_replace()`. There might be other ways to trigger this error message.\n\n\nYou probably want to load a valid view, at some point.\n\n\nPHP 7 and lower just would ignore the missing parameter, while PHP 8+ displays the warning. It may also vary with your environment \/ debug settings.\n\n\n"}
{"questionId":"12ce39d7e02247bea97403635529dcf9","question":"Golang reports \"context deadline exceeded\" with MongoDB\nI wrote an update function, but multiple executions will give the error `context deadline exceeded`.\n\n\nMy function:\n\n\n\n```\nfunc Update(link string, m bson.M) {\n    configInfo := config.Config()\n\n    \/\/ client := GetInstance().client\n    \/\/ ctx := GetInstance().ctx\n\n    client, _ := mongo.NewClient(options.Client().ApplyURI(\"mongodb:\/\/localhost:27017\"))\nctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n    defer cancel()\n    err := client.Connect(ctx)\n    if err != nil {\n        fmt.Print(\"connect error!\")\n        fmt.Println(err)\n    }\n    db := client.Database(\"test\")\n    lianjia := db.Collection(\"test\")\n    _, err = lianjia.UpdateOne(ctx, bson.M{\"Link\": link}, bson.M{\"$set\": m})\n    if err != nil {\n        fmt.Print(\"update error!\")\n        fmt.Println(err)\n    }\n}\n\n```\n\nThe output:\n\n\n\n```\nupdate error!context deadline exceeded\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"go"},"answer":"**Change** `mongodb:\/\/localhost:27017` **to** `mongodb:\/\/127.0.0.1:27017\/`\n\n\n"}
{"questionId":"251c8995ffad46a1addcbccec50260b5","question":"Scala doobie fragment with generic type parameter\nI am trying to abstract inserting objects of different types into sql tables of similar structure. Here's what I'm trying to do:\n\n\n\n```\nclass TableAccess[A : Meta](table: String) {\n  def insert(key: String, a: A): ConnectionIO[Unit] = {\n    (fr\"insert into \" ++ Fragment.const(table) ++ fr\" values ($key, $a);\").update.run.map(_ => ())\n  }\n}\n\n```\n\nBut I get this compile error:\n\n\n\n```\n[error] diverging implicit expansion for type doobie.util.param.Param[A]\n[error] starting with method fromMeta in object Param\n[error]     (fr\"insert into \" ++ Fragment.const(table) ++ fr\" values ($key, $a);\").update.run.map(_ => ())\n\n```\n\nAll I can find in the documentation is:\n\n\n\n> \n> doobie allows you to interpolate values of any type (and options\n>  thereof) with an Meta instance, which includes...\n> \n> \n> \n\n\nBut it seems that is not enough in this case; what's the right typeclass\/imports\/conversions I need?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"I'll go ahead an answer my own question, almost a year later. I never fully understood what was happening, and I have since updated to a newer version of doobie, so I am not sure how relevant this is. But now the documentation contains this clue:\n\n\n\n> \n> Note: it is important to understand that Meta exists only to introduce\n>  Get\/Put pairs into implicit scope. You should never demand Meta as\n>  evidence in user code: instead demand Get, Put, or both.\n> \n> \n> \n> ```\n> def foo[A: Meta](...)     \/\/ don't do this\n> def foo[A: Get: Put](...) \/\/ ok\n> \n> ```\n> \n> \n\n\nAnd indeed, between that change and the new version, this now compiles just fine for me:\n\n\n\n```\nclass TableAccess[A: Get: Put](table: String) {\n\n```\n\n"}
{"questionId":"5a2c0180914f4524b58ec2f8634e6991","question":"Query exhausted resources at this scale factor\nI was running SQL query on Amazon Athena. And I got the following error couple of times: \n\n\nQuery exhausted resources at this scale factor\n\n\nThis query ran against the \"test1\" database, unless qualified by the query. Please post the error message on our forum or contact customer support with Query Id: \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"Without seeing the query it's hard to say for sure what the problem is, but it's very likely that it is due to an internal issue in Athena that has to do with sorting of large intermediary result sets.\n\n\nThe version of Presto that Athena uses does not have support for sorting datasets that are too big to fit in memory. It used to be the same for aggregations too, but that has been fixed by the Athena team.\n\n\nThe issue most often occurs when you have very wide tables, i.e. many columns, or columns with a lot of data. Each individual row can represent a big chunk of memory, and if a node runs out of memory while trying to sort its chunk the query aborts with the \"query exhausted resources at this scale factor\" error.\n\n\nIf this matches your situation the only way around is unfortunately to limit the number of columns, or eliminate the sorting. Sometimes you can rearrange the query to do the sorting at a different stage to make the memory pressure on the sorting stage lower.\n\n\n"}
{"questionId":"d1fa40a7089946d09574682ffc907e7a","question":"org.h2.jdbc.JdbcSQLSyntaxErrorException after H2 version upgrade\nI recently upgraded h2 version from 1.4.200 to 2.0.206. Some of the queries that used to work in the older version are not working properly after the upgrade.\n\n\n\n```\nCREATE TABLE SOMETABLE (\n  ID INT(11) NOT NULL AUTO_INCREMENT,\n  SOURCE_ID VARCHAR(255) NOT NULL,\n  MESSAGE VARCHAR(255) NOT NULL,\n  PRIMARY KEY (`ID`)\n);\n\n```\n\n\n```\nCREATE TABLE IF NOT EXISTS SOMEOTHERTABLE (\n    ID VARCHAR(255) NOT NULL,\n    NAME VARCHAR(255) NOT NULL,\n    CREATED_TIME TIMESTAMP NOT NULL,\n    LAST_MODIFIED TIMESTAMP NOT NULL,\n    HAS_FILE BOOLEAN(1) NOT NULL,\n    PRIMARY KEY (ID)\n);\n\n```\n\nFor both these, I get similar errors\n\n\n\n```\norg.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement \"  CREATE TABLE SOMETABLE ( ID INT([*]11) NOT NULL AUTO_INCREMENT, SOURCE_ID VARCHAR(255) NOT NULL, MESSAGE VARCHAR(255) NOT NULL, PRIMARY KEY (`ID`) )\"; expected \"ARRAY, INVISIBLE, VISIBLE, NOT, NULL, AS, DEFAULT, GENERATED, ON, NOT, NULL, AUTO_INCREMENT, DEFAULT, NULL_TO_DEFAULT, SEQUENCE, SELECTIVITY, COMMENT, CONSTRAINT, COMMENT, PRIMARY, UNIQUE, NOT, NULL, CHECK, REFERENCES, AUTO_INCREMENT, ., )\";\n\n```\n\n\n```\norg.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement \"  CREATE TABLE IF NOT EXISTS SOMEOTHERTABLE ( ID VARCHAR(255) NOT NULL, NAME VARCHAR(255) NOT NULL, CREATED_TIME TIMESTAMP NOT NULL, LAST_MODIFIED TIMESTAMP NOT NULL, HAS_FILE BOOLEAN([*]1) NOT NULL, PRIMARY KEY (ID) )\"; expected \"ARRAY, INVISIBLE, VISIBLE, NOT, NULL, AS, DEFAULT, GENERATED, ON, NOT, NULL, AUTO_INCREMENT, DEFAULT, NULL_TO_DEFAULT, SEQUENCE, SELECTIVITY, COMMENT, CONSTRAINT, COMMENT, PRIMARY, UNIQUE, NOT, NULL, CHECK, REFERENCES, AUTO_INCREMENT, ., )\";\n\n```\n\nIt seems that in both these cases, having `INT(11)` and `BOOLEAN(1)` is the issue. Are those not allowed anymore in the new version? If so, how should I change those? Any help regarding this is appreciated.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"Why do you have such definitions? Documentation of H2 1.4.200 doesn't allow any parameters for these data types.\n\n\n`INT(11)` is allowed only in MySQL and MariaDB compatibility modes, but the specified precision is ignored by H2. This definition is rejected in all other compatibility modes in H2 2.0, you need to use `INT` or `INTEGER`.\n\n\n`BOOLEAN(1)` is not allowed at all, if it worked in 1.4.200, it was a bug in the parser. You need to use `BOOLEAN`.\n\n\n`AUTO_INCREMENT` clause also should normally be used only in MySQL and MariaDB compatibility modes, but it works in Regular mode too. The proper clause is `GENERATED BY DEFAULT AS IDENTITY` and explicit `NOT NULL` constraint isn't required for primary key and identity columns, you can remove it. Constraints also should normally be specified after all other clauses, `NOT NULL` before identity options is actually accepted by H2, but this wrong order of clauses isn't documented and isn't supported.\n\n\n"}
{"questionId":"3204d4c1aef84ae7b16d92ee20fac7b7","question":"Passing environment object between non-view classes in SwiftUI\nI understand that `EnvironmentObject` property wrapper can be used to pass around objects to views. I have a session object which I am passing around to my *views*. Now I have a requirement to pass this into one of my *model* classes (i.e., non-view). Ideally, this model (receiving the session object) is instantiated as a `StateObject`.\n\n\n\n```\nstruct CreditDetailsView: View {\n  @EnvironmentObject var session: Session\n  @StateObject var transactionsModel = TransactionsModel(token: session.token)\n\n```\n\nThe code above will not work (understandably) because:\n\n\n\n```\ncannot use instance member 'session' within property initializer; property initializers run before 'self' is available\n\n```\n\nAny suggestions on how I can pass in the session into `TransactionsModel`?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"swift"},"answer":"Try initializing the StateObject in an `.onAppear()` prop to a child view, like this:\n\n\n\n```\nstruct CreditDetailsView: View {\n  @EnvironmentObject var session: Session\n  @StateObject var transactionsModel: TransactionModel?\n  \n  var body: some View {\n    SomeChildView()\n      .onAppear(perform: {\n        transactionModel = TransactionModel(token: session.token)\n      })\n  }\n}\n\n```\n\nThis way, the variable is initialized when the view renders on the screen. It doesn't matter much which child view you add the `onAppear` prop to, as long as it is rendered as soon as the parent does.\n\n\n"}
{"questionId":"00b3d633ba404e898588cac218476cc5","question":"Find first non-null values for multiple columns\nI'm attempting to get the first non-null value in a set of many columns. I'm aware that I could accomplish this using a sub-query per column. In the name of performance, which really does count in this scenario, I'd like to do this in a single pass. \n\n\nTake the following example data:\n\n\n\n```\ncol1     col2     col3     sortCol\n====================================\nNULL     4        8        1\n1        NULL     0        2\n5        7        NULL     3\n\n```\n\nMy dream query would find the first non-null value in each of the data columns, sorted on the `sortCol`.\n\n\nFor example, when selecting the magical aggregate of the first three columns, sorted by the `sortCol` descending.\n\n\n\n```\ncol1     col2     col3\n========================\n5        7         0\n\n```\n\nOr when sorting ascending:\n\n\n\n```\ncol1     col2     col3\n========================\n1        4         8\n\n```\n\nDoes anyone know a solution?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"sql"},"answer":"# Using `first_value()`\n\n\n`first_value(col)` can be used with `and OVER (ORDER BY CASE WHEN col IS NOT NULL THEN sortcol ELSE maxvalue END)`. `ELSE maxvalue` is required because SQL Server sorts nulls first)\n\n\n\n```\nCREATE TABLE foo(a int, b int, c int, sortCol int);\nINSERT INTO foo VALUES\n    (null, 4, 8, 1),\n    (1, null, 0, 2),\n    (5, 7, null, 3);\n\n```\n\nNow you can see what we have to do to force nulls to sort after the `sortcol`. To do `desc` you have to make sure they have a negative value. \n\n\n\n```\nSELECT TOP(1)\n     first_value(a) OVER (ORDER BY CASE WHEN a IS NOT NULL THEN sortcol ELSE 2^31-1 END) AS a,\n     first_value(b) OVER (ORDER BY CASE WHEN b IS NOT NULL THEN sortcol ELSE 2^31-1 END) AS b,\n     first_value(c) OVER (ORDER BY CASE WHEN c IS NOT NULL THEN sortcol ELSE 2^31-1 END) AS c\nFROM foo;\n\n```\n\n# PostgreSQL\n\n\nPostgreSQL is slightly simpler,\n\n\n\n```\nCREATE TABLE foo(a,b,c,sortCol)\nAS VALUES\n  (null, 4, 8, 1),\n  (1, null, 0, 2),\n  (5, 7, null, 3);\n\nSELECT\n     first_value(a) OVER (ORDER BY CASE WHEN a IS NOT NULL THEN sortcol END) AS a,\n     first_value(b) OVER (ORDER BY CASE WHEN b IS NOT NULL THEN sortcol END) AS b,\n     first_value(c) OVER (ORDER BY CASE WHEN c IS NOT NULL THEN sortcol END) AS c\nFROM foo\nFETCH FIRST ROW ONLY;\n\n```\n\nI believe all of this goes away when RDBMS start to adopt `IGNORE NULLS`. Then it'll just be `first_value(a IGNORE NULLS)`.\n\n\n"}
{"questionId":"ff817e31546843eeaa00e74dfaa34aff","question":"Presto: Cast timestamp w\/TZ to plain timestamp WITHOUT converting to UTC\nThis query in Presto:\n\n\n\n```\nselect *, \n  cast(ts_w_tz as timestamp) as ts, \n  cast(substr(cast(ts_w_tz as varchar), 1, 23) as timestamp) as local_ts_workaround \nfrom (select timestamp '2018-02-06 23:00:00.000 Australia\/Melbourne' as ts_w_tz);\n\n```\n\nReturns:\n\n\n\n```\n                   ts_w_tz                   |           ts            |   local_ts_workaround   \n---------------------------------------------+-------------------------+-------------------------\n 2018-02-06 23:00:00.000 Australia\/Melbourne | 2018-02-06 12:00:00.000 | 2018-02-06 23:00:00.000\n\n```\n\nAs you can see, the act of casting the timestamp with timezone to a timestamp has resulted in the timestamp being converted back to UTC time (eg `ts`). IMO the correct behaviour should be to return the 'wall reading' of the timestamp, as per `local_ts_workaround`.\n\n\nI realise there are many posts about how Presto's handling of this is wrong and doesn't conform to the SQL standard, and that there is a fix in the works. But in the meantime this is a major pain since the effect is that there appears to be no built in way to get a localised timestamp withOUT timezone (as per `local_ts_workaround`).\n\n\nObviously, I have the string conversion workaround for now, but this seems horrible. I am wondering if anyone has a better workaround or can point out something that I have missed?\n\n\nThanks.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"It seems like there's no great solution, but building off of the previous answer, I like this a little better... see the `date_format_workaround` column:\n\n\n\n```\nselect *,\n  cast(from_iso8601_timestamp(date_format(ts_w_tz, '%Y-%m-%dT%H:%i:%s')) as timestamp) as date_format_workaround,\n  cast(ts_w_tz as timestamp) as ts,\n  cast(substr(cast(ts_w_tz as varchar), 1, 23) as timestamp) as local_ts_workaround\nfrom (select timestamp '2018-02-06 23:00:00.000 Australia\/Melbourne' as ts_w_tz);\n\n```\n\n"}
{"questionId":"eebbe310775140f2a865dd6bb07b9bce","question":"DELPHI Where can I add common library path in 10.3 Community version\nI have just installed 10.3 Rio Community version and are trying to add mORMot-libraries to it.\n\n\nBut I can't find where I shall add it.\n\n\nIn my XE6-version it's in the Tools-menu but in 10.3 I can't find it.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"delphi"},"answer":"search path is in Project->Options->Delphi Compiler->Search Path \n\n\nattention: this options are available only after project creation\n\n\n"}
{"questionId":"608c4fd53cfc448a9ef8e488f58a0c82","question":"How to find intersection of values that meet multiple conditions in R?\nAssume I have the following data frame:\n\n\n\n```\ndf <- data.frame(year=c(2010,2011,2012,2010,2011,2010,2011,2012), company = c(\"a\",\"a\",\"a\",\"b\",\"b\",\"c\",\"c\",\"c\"))\n\n  year company\n1 2010       a\n2 2011       a\n3 2012       a\n4 2010       b\n5 2011       b\n6 2010       c\n7 2011       c\n8 2012       c\n\n\n```\n\nI want to find the companies that are present in all three years. One cumbersome approach would be:\n\n\n\n```\nlibrary(dplyr)\n\ncompanies_2010 <- df %>% filter(year==2010) %>% select(company)\ncompanies_2011 <- df %>% filter(year==2011) %>% select(company)\ncompanies_2012 <- df %>% filter(year==2012) %>% select(company)\n\ncompanies <- intersect(companies_2010, companies_2011) %>% intersect(., companies_2012)\n\n  company\n1       a\n2       c\n\n```\n\nIs there any more elegant way to do this?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"r"},"answer":"This won't work *in general* to compute arbitrary intersections, but (\u00bf I think ?) does what you specified above:\n\n\n\n```\n(df \n   %>% group_by(company)\n   %>% filter(all(2010:2012 %in% year))\n   %>% select(company)\n   %>% distinct()\n)\n\n```\n\n"}
{"questionId":"9bac546378a34483a4dac5991a97bcd4","question":"How to implement generic interfaces?\nI've just seen Go has incorporated generics in its latest release, and I'm trying to create a small project to understand how it works. I don't seem to figure out how it works apart from very simple functions being now generic. I'd like to be able to do things like this:\n\n\n\n```\ntype Dao[RT any] interface {\n    FindOne(id string) *RT\n}\n\ntype MyDao struct {\n}\n\ntype ReturnType struct {\n    id int\n}\n\nfunc (m *MyDao) FindOne(id string) *ReturnType {\n    panic(\"implement me\")\n}\n\n\/\/ how should this look like?\n\nfunc NewMyDao() *Dao[ReturnType] {\n    return &MyDao[ReturnType]{}\n}\n\n```\n\nIs that even possible? I don't seem to be implementing the interface that way, and I've tried many combinations of the same.\n\n\nIs there a way to implement a generic interface? If not, is the alternative only to return the `interface{}` type?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"go"},"answer":"The type `*MyDao` implements the interface `Dao[ReturnType]`. Thus, the function should look like:\n\n\n\n```\nfunc NewMyDao() Dao[ReturnType] {\n    return &MyDao{}\n}\n\n```\n\nNote that the return type is an instance of the generic interface, and the returned value is simply an instance of the `*MyDao` type.\n\n\n"}
{"questionId":"02fc6e7bd1004ed7917972344cbb664d","question":"Conda install takes forever (stuck as SAT solver)\nHow can I fix problem with conda installer, which takes forever solving huge SAT problems (why do we need to solve them?):\n\n\n\n```\nDEBUG conda.common.logic:_run_sat(734): Invoking SAT with clause count: 9661561\nDEBUG conda.common.logic:_run_sat(734): Invoking SAT with clause count: 5164645\nDEBUG conda.common.logic:_run_sat(734): Invoking SAT with clause count: 2751948\nDEBUG conda.common.logic:_run_sat(734): Invoking SAT with clause count: 1518175\nDEBUG conda.common.logic:_run_sat(734): Invoking SAT with clause count: 964848\nDEBUG conda.common.logic:_run_sat(734): Invoking SAT with clause count: 1249154\nDEBUG conda.common.logic:_run_sat(734): Invoking SAT with clause count: 1105581\n\n```\n\nWaited more then half of a hour, what else can I do rather then reinstall this?\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"python"},"answer":"Try:\n\n\n\n```\nconda update -n base -c defaults conda\n\n```\n\n"}
{"questionId":"530b0a59c6e24906ab83a9d5f89ad1b8","question":"Detecting if a macro argument is a typename\nWithin C11\/gnuC11 is it possible to write a macro that returns an integer constant expression of value 1 or 0 respectively if the macro argument is or isn't a type name or at least a macro can distinguish between integer constant expressions and typenames (i.e., if can detect the argument isn't one of these, it can assume it is the other)?\n\n\n\n```\n#define IS_TYPENAME(X) \/*???*\/ \n_Static_assert( IS_TYPENAME(int), \"\" );\n_Static_assert( !IS_TYPENAME(42), \"\" );\n\n```\n\nMotivation:\n\n\nMy motivation was to wrap `_Aligna`s with a macro that would simply do nothing if the suggested alignment (either type or integer expression) was less than the current one (normal `_Alignas` with a smaller alignment results in an error) and so I wanted to also accept either a typename or an integer expr, but now I'm thinking simply requiring an integer expr (which you can always get from a typename by applying `_Alignof`) will be the simpler\/clearer way to go. \n\n\n","questionMetadata":{"type":"implementation","level":"advanced","tag":"c"},"answer":"In order to do this, you will need to check if the parameter is of type integer, and you need to check if it is a type or an expression. \n\n\n\n\n---\n\n\n**Checking if a macro parameter, that may be a type or an expression, is of integer type:**\n\n\nThis can be done with `_Generic`. A `_Generic` expression cannot contain two types that are identical, so it should suffice if you compare against all the stdint.h types only. Since these will alias with the default integer types, but not collide with each other (like for example `int` and `long` might).\n\n\nNow `_Generic` doesn't accept a type as operand, so you have to tweak the input to always become an expression.\n\n\nThe trick that I invented just now, is to use the ambiguity between the parenthesis operator and the cast operator, and at the same time use the ambiguity between unary + and binary + operators.\n\n\nGiven `(x)+0`. \n\n\n- If `x` is a type, `()` becomes the cast operator and `+0` is the unary addition operator applied to an integer constant.\n- If `x` is an expression, it will get parenthesized and then `+` is the binary addition operator.\n\n\nSo you can do:\n\n\n\n```\n#define IS_INT(x) _Generic((x)+0, \\\n  uint8_t:  1, int8_t:  1,        \\\n  uint16_t: 1, int16_t: 1,        \\\n  uint32_t: 1, int32_t: 1,        \\\n  uint64_t: 1, int64_t: 1,        \\\n  default: 0)\n\n```\n\nThis will work for all integer, character and float types, as well as pointers. It will not work on struct\/union types (compiler error). It will not work with `void*` and probably not with `NULL` (compiler error, can't do pointer arithmetic).\n\n\n\n\n---\n\n\n**Checking if a macro parameter, that may be a type or an expression, is an expression:**\n\n\nThis can also be done using the same trick as above, use the ambiguity between different operators. For example:\n\n\n\n```\n#define IS_EXPR(x) (!!(x) + !(x) + 1 == 2)\n\n```\n\n- If `x` is a non-zero integer constant expression, we get `1 + 0 + 1 = 2`.\n- If `x` is a zero integer constant expression, we get `0 + 1 + 1 = 2`.\n- If `x` is a type, we get `!!(int)+!(int)+1` which equals `0`. Both + are unary.\n\n\nThis doesn't make a difference between float and integers though, so we need to combine this trick with the `IS_INT` macro.\n\n\n\n\n---\n\n\n**Solution:**\n\n\n\n```\n#define IS_INTCONSTEXPR(x) ( IS_INT(x) && IS_EXPR(x) )\n\n```\n\nComplete example with test cases, printing 1 if integer constant expression, otherwise 0:\n\n\n\n```\n#include <stdint.h>\n#include <stdio.h>\n\n#define IS_INT(x) _Generic((x)+0, \\\n  uint8_t:  1, int8_t:  1,        \\\n  uint16_t: 1, int16_t: 1,        \\\n  uint32_t: 1, int32_t: 1,        \\\n  uint64_t: 1, int64_t: 1,        \\\n  default: 0)\n\n#define IS_EXPR(x) (!!(x) + !(x) + 1 == 2)\n\n#define IS_INTCONSTEXPR(x) ( IS_INT(x) && IS_EXPR(x) )\n\n\n#define test(arg) printf(\"%d %s\\n\", IS_INTCONSTEXPR(arg),(#arg))\n\nint main (void)\n{\n  test(42);\n  test(sizeof(int));\n  test(1+1);\n  test(int);\n  test(unsigned int);\n  test(42.0);\n  test(double);\n  test(uint32_t);\n  test(uint32_t*);\n  test(_Bool);\n\n  _Static_assert( !IS_INTCONSTEXPR(int), \"\" ); \/\/ OK, passed\n  _Static_assert( IS_INTCONSTEXPR(42), \"\" );   \/\/ OK, passed\n\n  return 0;\n}\n\n```\n\nOutput:\n\n\n\n```\n1 42\n1 sizeof(int)\n1 1+1\n0 int\n0 unsigned int\n0 42.0\n0 double\n0 uint32_t\n0 uint32_t*\n0 _Bool\n\n```\n\n"}
{"questionId":"606e2800845a4dbc82fd44f7bcb084a7","question":"How exactly does the ?: operator work in C?\nI have a question, how the compiler operate on the following code:\n\n\n\n```\n#include<stdio.h>\n\nint main(void)\n{\n  int b=12, c=11;\n  int d = (b == c++) ? (c+1) : (c-1);\n  printf(\"d = %i\\n\", d);\n}\n\n```\n\nI am not sure why the result is \u200d\u200d\u200d`d = 11`.\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"c"},"answer":"Translated to a regular if-statement your code would look like this:\n\n\n\n```\nint b=12, c=11;\nint d;\n\nif (b == c++)\n   d = c+1;\nelse\n   d = c-1;\n\n```\n\nThe clue here is that c is incremented **after** the condition is checked. So you enter the `else` state but c already has the value 12 there.\n\n\n"}
{"questionId":"84d20aa3cf9445a0a43cb5ecb36084e1","question":"Algorithmic way to combine different contact number and emails for same contact\nI have the following tibble,\n\n\n\n```\ncontact <- tribble(\n  ~name, ~phone, ~email,\n  'John', 123, 'john_abc@gmail.com',\n  'John', 456, 'john_abc@gmail.com',\n  'John', 456, 'john_xyz@gmail.com',\n  'John', 789, 'john_pqr@gmail.com'\n)\n\n```\n\nI'd like to combine the phone numbers and emails if phone or email are the same, the desired output is the following,\n\n\n\n```\ncontact_combined <- tribble(\n  ~name, ~phone, ~email,\n  'John', '123;456', 'john_abc@gmail.com;john_xyz@gmail.com',\n  'John', '789', 'john_pqr@gmail.com'\n)\n\n```\n\nI've tried grouping it first by name and phone and then by name and emails but it's not giving me the expected results. I'm stuck on finding an algorithmic way to solve this problem, could someone please give me an advice?\n\n\nNote: The collapsing of the values in a column is not the question here. It's about selecting the records for the collapsing.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"r"},"answer":"here is s `data.table` approach\n\n\n\n```\nsetDT(contact)\n# set keys\nsetkey(contact, name, phone, email)\n# self join on each unique key, filter and summarise on the fly \nans <- contact[contact, c(\"phone2\", \"email2\") := {\n  temp <- contact[ name == i.name & \n                     (phone %in% contact[name == i.name & email == i.email, ]$phone | \n                        email %in% contact[name == i.name & phone == i.phone, ]$email), ]\n  email_temp <- paste0(unique(temp$email), collapse = \";\")\n  phone_temp <- paste0(unique(temp$phone), collapse = \";\")\n  list(phone_temp, email_temp)\n}, by = .EACHI]\n# final step\nunique(ans, by = c(\"name\", \"phone2\", \"email2\"))[, .(name, phone = phone2, email = email2)]\n#    name   phone                                 email\n# 1: John 123;456 john_abc@gmail.com;john_xyz@gmail.com\n# 2: John     789                    john_pqr@gmail.com\n\n```\n\n**explanation**\n\n\n\n```\n# so, for the first row, the variable 'temp' is calculated as follows\ncontact[ name == 'John' &\n          (phone %in% contact[name == 'John' & email == 'john_abc@gmail.com', ]$phone | \n           email %in% contact[name == 'John' & phone == 123, ]$email), ]\n#    name phone              email\n# 1: John   123 john_abc@gmail.com\n# 2: John   456 john_abc@gmail.com\n# 3: John   456 john_xyz@gmail.com\n\n# then, put the unique emails together in a string using\n#     email_temp <- paste0(unique(temp$email), collapse = \";\")\n# and do the same for the phones using \n#     phone_temp <- paste0(unique(temp$phone), collapse = \";\")\n\n# and return there two strings to the columns \"phone2\" ans \"email2\"\n\n#repeat for each unique key-combination (.EACHI)\n\n```\n\n"}
{"questionId":"a7756c887a564e368b666a5843dadfe2","question":"Merge arrays to make a new array in perl\nWhat is the way to merge two arrays (column-wise) into a new composite array in perl? \n\n\n@array1\n\n\n\n```\ncar\nscooter\ntruck\n\n```\n\n@array2\n\n\n\n```\nfour\ntwo\nsix\n\n```\n\nI tried using following:\n\n\n\n```\nmy @merged = (@array1, @array2); print @merged;\n\n```\n\nBut it merges both arrays in one column as follows:\n\n\n\n```\ncar\nscooter\ntruck\nfour\ntwo\nsix\n\n```\n\nBut what I want is as follows:\n\n\n\n```\n@merged[0] @merged[1] \ncar             four\nscooter         two\ntruck           six\n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"perl"},"answer":"\n```\nmy @merged;\nfor (0 .. $#array1) {\n    push(@merged, ([$array1[$_],$array2[$_]]));\n}\n\n```\n\nor \n\n\n\n```\nmy @merged;\npush @merged, map {[$array1[$_], $array2[$_]]} 0 .. $#array1;\n\n```\n\n"}
{"questionId":"84a9760296794cd38850f5ddaa8b6cb5","question":"Screenshot robot only captures a black screen on Debian\nI am creating a screen capture using `java.awt.Robot` under Linux with OpenJDK 11. The result on Linux is a whole black image. The same code works on Ubuntu and Windows (using another file path of course).\n\n\nAny clue?\n\n\n\n```\npublic void captureScreen() throws AWTException {\n    Robot robot = new Robot(GraphicsEnvironment.getLocalGraphicsEnvironment().getDefaultScreenDevice());\n    BufferedImage screen = robot.createScreenCapture(new Rectangle(getDefaultToolkit().getScreenSize()));\n    try {\n        ImageIO.write(screen, \"jpg\", new File(\"\/tmp\/screenshot.jpg\"));\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n}\n\n```\n\n\n> \n> **UPDATE:**\n>  The the cause of the problem lies in the combination of OpenJDK and Wayland.\n>  With Oracle JDK\/JRE (13.0.1) everything works fine.\n> \n> \n> \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"If you are using wayland instead of XOrg this may be causing the problem as it is less stable with Java interfaces for graphics operations.\n\n\nEdit: This bug has now been fixed (see OP)\n\n\n"}
{"questionId":"437c1ce3283346c28e57bc1d0149e20c","question":"Add custom property to ThemeData in Flutter\nI need to change `color` of a widget based on theme. I have separate `ThemeData` for light and dark theme. Now is it possible to add a custom property to `ThemeData` so that I can change the color of the widget based on theme and using that custom property?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"dart"},"answer":"Instead of adding custom property, we can extend `ThemeData` by `extension` function. For example, if we need a custom property for color, we can add `extension` function on `ColorScheme`. `Color` dependencies are now moved to `Themedata`.\n\n\n\n```\n\/\/ checking brightness to support dynamic themeing\nextension CustomColorSchemeX on ColorScheme {\n  Color get smallBoxColor1 =>\n      brightness == Brightness.light ? Colors.blue : Colors.grey[400];\n}\n\n```\n\nAnd then access that property through `Theme.of(context)...`\n\n\n\n```\n                        Container(\n                          decoration: BoxDecoration(\n                            border: Border.all(\n                                color: Theme.of(context)\n                                    .colorScheme\n                                    .smallBoxColor1),\n                        ),\n\n```\n\n"}
{"questionId":"1aeed3edf6d4471f984bc5d92c908ec0","question":"Flink: No operators defined in streaming topology. Cannot execute\nI am trying to setup a very basic flink job. When I try to run, get the following error:\n\n\n\n```\nCaused by: java.lang.IllegalStateException: No operators defined in streaming topology. Cannot execute.\n    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:1535)\n    at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:53)\n    at org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.scala:654)\n    at com.test.flink.jobs.TestJobRunnable$.run(TestJob.scala:223)\n\n```\n\nThe error is caused by the code below:\n\n\n\n```\nval streamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment\nval messageStream = streamExecutionEnvironment.addSource(kafkaConsumer)\nmessageStream.keyBy(_ => \"S\")\n\nstreamExecutionEnvironment.execute(\"Test Job\")\n\n```\n\nThe error goes away when I add a `print()` call to the end of the stream:\n\n\n\n```\nval streamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment\nval messageStream = streamExecutionEnvironment.addSource(kafkaConsumer)\nmessageStream.keyBy(_ => \"S\")\nmessageStream.print()\n\nstreamExecutionEnvironment.execute(\"Test Job\")\n\n```\n\nI'm confused as to why `print()` solves this issue. Is the idea that a streaming topology does not process any of its operators until a sink is introduced? Is `print()` acting as a sink here? Any help would be appreciated. Thanks.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"scala"},"answer":"In programming language theory, **lazy evaluation**, or call-by-need is an evaluation strategy which delays the evaluation of an expression until its value is needed and which also avoids repeated evaluations. The opposite of lazy evaluation is **eager evaluation**, sometimes known as strict evaluation. \nThe benefits of lazy evaluation include:\n\n\n- The ability to define control flow (structures) as abstractions\ninstead of primitives.\n- The ability to define potentially infinite data structures. This\nallows for more straightforward implementation of some algorithms.\n- Performance increases by avoiding needless calculations, and avoiding\nerror conditions when evaluating compound expressions.\n\n\nLazy evaluation can lead to reduction in memory footprint, since values are created when needed. However, lazy evaluation is difficult to combine with imperative features such as exception handling and input\/output, because the order of operations becomes indeterminate.\n\n\nGenerally, Flink divided operations into two class: **transformations** operations and **sink** operations. As you guess, Flink transformations are lazy, meaning that they are not executed until a sink operation is invoked.\n\n\n\n> \n> Flink programs are regular programs that implement transformations on\n>  distributed collections (e.g., filtering, mapping, updating state,\n>  joining, grouping, defining windows, aggregating). Collections are\n>  initially created from sources (e.g., by reading from files, Kafka\n>  topics, or from local, in-memory collections). Results are returned\n>  via sinks, which may, for example, write the data to (distributed)\n>  files, or to standard output (for example, the command line terminal).\n> \n> \n> \n\n\n"}
{"questionId":"69872ab640364e32abbb22595c44dc87","question":"static inline vs inline static\nI have noticed that both work, what is the correct way to use inline here?\n\n\n\n```\nstatic inline int getAreaIndex()\n\n```\n\nOR\n\n\n\n```\ninline static int getAreaIndex()\n\n```\n\nPlus, getAreaIndex contains a large loop. sometimes I call it only one and sometimes I call it through a loop, should I inline it? (it's 10 line tall)\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"From the C standard (6.7 Declarations)\n\n\n\n```\ndeclaration:\n    declaration-specifiers init-declarator-listopt ;\n    static_assert-declaration\n\ndeclaration-specifiers:\n    storage-class-specifier declaration-specifiersopt\n    type-specifier declaration-specifiersopt\n    type-qualifier declaration-specifiersopt\n    function-specifier declaration-specifiersopt\n    alignment-specifier declaration-specifiersopt\n\n```\n\nIt means that you may specify declaration specifiers in any order. \n\n\nSo for example all shown below function declarations declare the same one function.\n\n\n\n```\n#include <stdio.h>\n\nstatic inline int getAreaIndex( void );\ninline static int getAreaIndex( void );\nint static inline getAreaIndex( void );\nstatic int inline getAreaIndex( void );\ninline int static getAreaIndex( void )\n{\n    return  0;\n}\n\n\nint main(void) \n{\n    return 0;\n}\n\n```\n\nAs for the inline function specifier then according to the C Standard (6.7.4 Function specifiers)\n\n\n\n> \n> 6 A function declared with an inline function specifier is an inline\n>  function. Making a \u2217function an inline function suggests that calls to\n>  the function be as fast as possible.138)**The extent to which such\n>  suggestions are effective is implementation-defined.**\n> \n> \n> \n\n\nand there is a footnote\n\n\n\n> \n> 139) For example, **an implementation might never perform inline\n>  substitution**, or might only perform inline substitutions to calls in\n>  the scope of an inline declaration\n> \n> \n> \n\n\nPay attention to that you should specify as the function parameter `void`. Otherwise the compiler will decide that the number and types of arguments are deduced from a function call.\n\n\n"}
{"questionId":"3b1ac5fbf7a64c3ba6f94d900c356321","question":"Filter a dictionary of lists\nI have a dictionary of the form:\n\n\n\n```\n{\"level\": [1, 2, 3],\n \"conf\": [-1, 1, 2],\n \"text\": [\"here\", \"hel\", \"llo\"]}\n\n```\n\nI want to filter the lists to remove every item at index `i` where an index in the value `\"conf\"` is not >0.\n\n\nSo for the above `dict`, the output should be this:\n\n\n\n```\n{\"level\": [2, 3],\n \"conf\": [1, 2],\n \"text\": [\"hel\", \"llo\"]}\n\n```\n\nAs the first value of `conf` was not > 0.\n\n\nI have tried something like this:\n\n\n\n```\nnew_dict = {i: [a for a in j if a >= min_conf] for i, j in my_dict.items()}\n\n```\n\nBut that would work just for one key.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"I solved it with this:\n\n\n\n```\nfrom typing import Dict, List, Any, Set\n\nd = {\"level\":[1,2,3], \"conf\":[-1,1,2], \"text\":[\"-1\", \"hel\", \"llo\"]}\n\n# First, we create a set that stores the indices which should be kept.\n# I chose a set instead of a list because it has a O(1) lookup time.\n# We only want to keep the items on indices where the value in d[\"conf\"] is greater than 0\nfiltered_indexes = {i for i, value in enumerate(d.get('conf', [])) if value > 0}\n\ndef filter_dictionary(d: Dict[str, List[Any]], filtered_indexes: Set[int]) -> Dict[str, List[Any]]:\n    filtered_dictionary = d.copy()  # We'll return a modified copy of the original dictionary\n    for key, list_values in d.items():\n        # In the next line the actual filtering for each key\/value pair takes place. \n        # The original lists get overwritten with the filtered lists.\n        filtered_dictionary[key] = [value for i, value in enumerate(list_values) if i in filtered_indexes]\n    return filtered_dictionary\n\nprint(filter_dictionary(d, filtered_indexes))\n\n```\n\nOutput:\n\n\n\n```\n{'level': [2, 3], 'conf': [1, 2], 'text': ['hel', 'llo']}\n\n```\n\n"}
{"questionId":"1ea39bb6d9334d6a8b6f1dbdcd913930","question":"Is there a way to have a Swift script use multiple files\nI am trying to write a script with Swift (not an Xcode project). To be clear, the first line of my file is\n\n\n\n```\n #!\/usr\/bin\/swift\n\n```\n\nAnd I am just calling it from the command-line. \n\n\nHowever, I can't figure out how to have that script use code that is in another .swift file. It doesn't pick it up from the same directory and there is no way to `import` that I can see.\n\n\nIs this supported?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"swift"},"answer":"There's a better way!\n\n\n\n```\n#!\/usr\/bin\/swift -frontend -interpret -enable-source-import -I.\n\nimport other_file  \/\/ this imports other_file.swift in the same folder\n\nfuncFromOtherFile()\n\n```\n\nif you want to import files from `ExampleFolder` it would be like:\n\n\n\n```\n#!\/usr\/bin\/swift -frontend -interpret -enable-source-import -I.\/ExampleFolder\n\nimport other_file  \/\/ this imports .\/ExampleFolder\/other_file.swift\n\nfuncFromOtherFile()\n\n```\n\n"}
{"questionId":"2cf166148bb348018bedcc5c809bc5cb","question":"How can I detect zero denominator when reading Ratios?\nI want to read a Ratio from a String, but I don't want my program to crash when the denominator is zero. How can I detect a zero denominator and avoid an error? Just using `readMaybe` doesn't work:\n\n\n\n```\nPrelude Text.Read> readMaybe \"1 % 0\" :: Maybe Rational\nJust *** Exception: Ratio has zero denominator\n\n```\n\nI created this far from perfect solution:\n\n\n\n```\nreadMaybeRational :: String -> Maybe Rational\nreadMaybeRational s =\n  case ((readMaybe $ drop 1 $ dropWhile (\/='%') s) :: Maybe Int)\n    of Just 0 -> Nothing\n       _ -> readMaybe s\n\n```\n\nBut I don't know how to handle a nested Ratio nicely:\n\n\n\n```\n\"Just (1 % 0)\"\n\n```\n\nIf I could override Ratio's Read instance, I could get readMaybe to return Nothing when the denominator is zero:\n\n\n\n```\ninstance (Integral a, Read a) => Read (Ratio a) where\n  readPrec =\n    parens\n    ( prec ratioPrec\n      ( do x <- step readPrec\n           expectP (L.Symbol \"%\")\n           y <- step readPrec\n           -- is y 0? If so, do something here\n           return (x % y)\n      )\n    )\n\n```\n\nBut I'm pretty sure I can't do that.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"haskell"},"answer":"I think your best solution is a `newtype` wrapper around `Ratio`, like this:\n\n\n\n```\nimport Control.Monad\nimport GHC.Read\nimport GHC.Real\nimport qualified Text.Read.Lex as L\nimport Text.ParserCombinators.ReadPrec\n\nnewtype SaneReadRatio a = SaneReadRatio (Ratio a)\ntype SaneReadRational = SaneReadRatio Integer\n\ninstance (Integral a, Read a) => Read (SaneReadRatio a) where\n  readPrec =\n    parens\n    ( prec ratioPrec\n      ( do x <- step readPrec\n           expectP (L.Symbol \"%\")\n           y <- step readPrec\n           guard (y \/= 0)\n           return (SaneReadRatio (x % y))\n      )\n    )\n\n  readListPrec = readListPrecDefault\n  readList     = readListDefault\n\n```\n\nUse it by reading in your data with `SaneReadRational` in place of `Rational`, then using `coerce` from `Data.Coerce` on the result, which will change it back to the underlying `Rational` no matter how deeply it's buried inside your type.\n\n\n"}
{"questionId":"3329af6e1341467da3f3e096a2f19c61","question":"Docker exec linux terminal create alias\nI have a running and detached container. I want to create a command alias there before attaching to that container.\n\n\nWhen I am attached to the container and I type:\n\n\n\n```\nalias bar='foo'\n\n```\n\nan alias is created, and might be checked by:\n\n\n\n```\nalias\n\n```\n\ncommand.\n\n\nbut if I want to do the same by **docker exec** command ie this way\n\n\n\n```\ndocker exec -it <container-name> \/bin\/bash -c \"alias bar='foo'\"\n\n```\n\nit does not work, probably because when I'm attached to the container and type into its terminal\n\n\n\n```\n\/bin\/bash -c \"alias bar='foo'\"\n\n```\n\nit does not work as well.\n\n\nDo you know how to modify `alias bar='foo'` so it works together with `docker exec` command applied to a detached container?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"The `alias` built-in creates an alias in the current shell. Aliases, like environment variables, are not persisted, only loaded. You need to update your .bashrc or whatever inside the container to have the desired alias so that it can be loaded on each start of bash.\n\n\n"}
{"questionId":"ae636df09cf843a48cc1c0bf44c95d14","question":"How to manage memory in agent-based modeling with R\nI am building an agent-based model with R but I have memory issues by trying to use large objects. In particular, 8 3D arrays are created at initialization and at each time step each 3D array is filled by different functions. \n\n\nFor the moment, the ABM runs over 1825 days and 2500 individuals are simulated to move across the landscape. There are 1000 cells in the landscapes. With this configuration, I don't have memory issues.\n\n\nAt initialization,\n\n\n- 1 3D array is like:\n\n\n\n```\nh <- array(NA, dim=c(1825, 48, 2500),\n           dimnames=list(NULL, NULL, as.character(seq(1, 2500, 1))))\n           ## 3th dimension = individual ID\n\n```\n- 1 3D array is like:\n\n\n\n```\np <- array(NA, dim=c(1825, 38, 1000),\n           dimnames=list(NULL, NULL, as.character(seq(1, 1000, 1))))\n           ## 3th dimension = cell ID\n\n```\n- 6 3D arrays are like:\n\n\n\n```\nt <- array(NA, dim=c(1825, 41, 2500),\n           dimnames=list(NULL, NULL, as.character(seq(1, 2500, 1))))\n           ## 3th dimension = individual ID\n\n```\n\n\nThe arrays contain character\/string data types.\n\n\nIdeally, I would like to increase the number of individuals and\/or number of patches, but this is impossible due to memory issues. It seems that there are some tools available like `bigmemory`, `gc` to manage memory. Are these tools efficient? I\u2019m a beginner in programming and I don\u2019t have experience in managing memory and high performance computing. Any advice is greatly appreciated, thanks for your time.\n\n\n\n> \n> sessionInfo()\n>  R version 3.5.3 (2019-03-11)\n>  Platform: x86\\_64-w64-mingw32\/x64 (64-bit)\n>  Running under: Windows 7 x64 (build 7601) Service Pack 1\n> \n> \n> \n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"r"},"answer":"From my understanding `bigmemory` just works on matrices and not multi-dimensional arrays, but you could save a multidimensional array as a list of matrices.\n\n\n`gc` is just the garbace collector and you don't really have to call it, since it will be called automatically, but the manual also states:\n\n\n\n> \n> It can be useful to call gc after a large object has been removed, as\n>  this may prompt R to return memory to the operating system.\n> \n> \n> \n\n\nI think the most useful package for you're task would be `ff`.\nHere's a short example to illustrate the strength of the package `ff`, which **stores data on disk** and almost doesn't affect memory.\n\n\n**Initialization arrays with base-R:**\n\n\n\n```\np <- array(NA, dim=c(1825, 38, 1000),\n           dimnames=list(NULL, NULL, as.character(seq(1, 1000, 1))))\n\nformat(object.size(p), units=\"Mb\")\n\n```\n\n\n> \n> \"264.6 Mb\"\n> \n> \n> \n\n\nSo in total, your initial arrays would take almost up to 5GB memory already, which will get you in trouble with heavy computation.\n\n\n\n\n---\n\n\n**Initialization arrays with ff:**\n\n\n\n```\nlibrary(ff)\nmyArr <- ff(NA, dim=c(1825, 38, 1000), \n            dimnames=list(NULL, NULL, as.character(seq(1, 1000, 1))),\n            filename=\"arr.ffd\", vmode=\"logical\", overwrite = T)\n\nformat(object.size(myArr), units=\"Mb\")\n\n```\n\n\n> \n> [1] \"0.1 Mb\"\n> \n> \n> \n\n\n\n\n---\n\n\n**Test for equality:**\n\n\n\n```\neuqals <- list()\nfor (i in 1:dim(p)[1]) {\n  euqals[[i]] <-  all.equal(p[i,,],\n                            myArr[i,,])\n}\nall(unlist(euqals))\n\n```\n\n\n> \n> [1] TRUE\n> \n> \n> \n\n\n"}
{"questionId":"d6009e70961444a1ae062aeb248636f8","question":"Adding multiple products to productlist for queryProductDetailsAsync in android billing 5.0.0\nIn the old android billing implementation you would build an sku list to query products:\n\n\n\n```\nList<String> skuList = new ArrayList<>();\n        skuList.add(SKU_POTION);\n        skuList.add(SKU_SWORD);\n        skuList.add(SKU_BOW);\n        SkuDetailsParams.Builder params = SkuDetailsParams.newBuilder();\n        params.setSkusList(skuList).setType(BillingClient.SkuType.INAPP);\n\n```\n\nThe new billing implementation is more involved, and appears to limit you to adding just one product to a query list:\n\n\n\n```\nImmutableList<QueryProductDetailsParams.Product> productList = ImmutableList.from(QueryProductDetailsParams.Product.newBuilder()\n                    .setProductId(SKU_POTION)\n                    .setProductType(BillingClient.ProductType.INAPP)\n                    .build());\n    \n            QueryProductDetailsParams params = QueryProductDetailsParams.newBuilder()\n                    .setProductList(productList)\n                    .build();\n    \n            billingClient.queryProductDetailsAsync(\n            params,\n            new ProductDetailsResponseListener() {\n                public void onProductDetailsResponse(BillingResult billingResult, List<ProductDetails> productDetailsList) {\n                    if (billingResult.getResponseCode() == BillingClient.BillingResponseCode.OK && productDetailsList != null) {\n                        for (ProductDetails skuDetails : productDetailsList) {                    \n                            mProductDetailsMap.put(skuDetails.getProductId(), skuDetails);                           \n                        }\n                    }\n                   \n                }\n            }\n    );\n\n```\n\nIt makes you build the productList for the productDetailsList for the mProductDetailsMap that's needed to start the purchase flow:\n\n\n\n```\npuchasestring=SKU_POTION;\ninitiatePurchaseFlow(mProductDetailsMap.get(puchasestring));\n\n```\n\nHow would I add multiple products to the productList that begins the implementation? I don't want to have to repeat the entire code segment for each item to add to the mProductDetailsMap, which is the Primitive Pete method I'm using for now.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"For multiple products:\n\n\n\n```\nImmutableList<QueryProductDetailsParams.Product> productList = ImmutableList.from(\nQueryProductDetailsParams.Product.newBuilder()\n                    .setProductId(SKU_POTION)\n                    .setProductType(BillingClient.ProductType.INAPP)\n                    .build(),\nQueryProductDetailsParams.Product.newBuilder()\n                    .setProductId(SKU_SWORD)\n                    .setProductType(BillingClient.ProductType.INAPP)\n                    .build(),\nQueryProductDetailsParams.Product.newBuilder()\n                    .setProductId(SKU_BOW)\n                    .setProductType(BillingClient.ProductType.INAPP)\n                    .build());\n\n```\n\n"}
{"questionId":"b27db62de33c42838abcb503d71a45d7","question":"Bash: Wrap Long Lines Inside the Same Column\nI would like to create a table with two columns. The first column contains the key name and the second column contains the value. The value can be text that usually results in multiple lines in the terminal.\n\n\nWith `printf` or `column` I can easily get the following output:\n\n\n\n```\n<----- Terminal Length ------>\nkey1     This is the value for\nkey1 with a very long text. \n...\n\n```\n\nBut I would like the value to be displayed in the same column like this:\n\n\n\n```\n<----- Terminal Length ------>\nkey1     This is the value for\n         key1 with a very long\n         text.\n...\n\n```\n\nHow can I wrap a long line inside the same column?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"Actually, the util-linux 'column' command can do it. It is very versatile.\n\n\n\n```\n#!\/bin\/bash\n\ncat <<- EOF | column --separator '|' \\\n                     --table \\\n                     --output-width 30 \\\n                     --table-noheadings \\\n                     --table-columns C1,C2 \\\n                     --table-wrap C2\nkey1|Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nkey2|blahhhhhhhhhhhhhhhhhhhhhhhhhhhhzzz\nEOF\n\n```\n\nThis gives :\n\n\n\n```\nkey1  Lorem ipsum dolor sit am\n      et, consectetur adipisci\n      ng elit, sed do eiusmod\n      tempor incididunt ut lab\n      ore et dolore magna aliq\n      ua.\nkey2  blahhhhhhhhhhhhhhhhhhhhh\n      hhhhhhhzzz\n\n```\n\n`--output-width` : give desired size (can use 'tput cols' as described above)\n\n\n`--table-columns C1,C2` : give names to columns to be used with other options\n\n\n`--table-wrap C2` : wrap column 2\n\n\nColumn version : \n\n\n\n```\n# column -V\ncolumn from util-linux 2.33.1\n\n```\n\n"}
{"questionId":"f0e38235350d4182a59a6a06dbc23b46","question":"PowerShell Delete File If Exists\nCould you help me with a powershell script?\nI want to check if multiple files exist, if they exist then delete the files.\nThan provide information if the file has been deleted or information if the file does not exist.\n\n\nI have found the script below, it only works with 1 file, and it doesn't give an message if the file doesn't exist.\nCan you help me adjust this? I would like to delete file c:\\temp\\1.txt, c:\\temp\\2.txt, c:\\temp\\3.txt if they exist.\nIf these do not exist, a message that they do not exist. Powershell should not throw an error or stop if a file doesn't exist.\n\n\n\n```\n$FileName = \"C:\\Test\\1.txt\"\nif (Test-Path $FileName) {\n   Remove-Item $FileName -verbose\n}\n\n```\n\nThanks for the help!\nTom\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"bash\/shell"},"answer":"Step 1: You want multiple files. You can do that two ways:\n\n\n\n```\n$files = \"C:\\file1.txt\",\"C:\\file2.txt\",\"C:\\file3.txt\"\n\n```\n\nThat would work, is cumbersome. Easier? Have all files in one .csv list, import that. Remember, the first row is not read, since its consider the header:\n\n\n\n```\n$files = Import-Csv \"C:\\yourcsv.csv\"\n\n```\n\nAlright Step 2: now you got your files, now we want to cycle them:\n\n\n\n```\nForeach ($file in $files) {\nIf (Test-Path $file) {\nRemove-Item $file -verbose | Add-Content C:\\mylog.txt }\nelse { Write-Host \"$file not found\" }}\n\n```\n\nForeach loops take each individual \"entry\" in one variable and do whatever you want with them.\nThat should do what you want.\n\n\n"}
{"questionId":"b2392fb934c046db9b90267c1f7b2e81","question":"Error : Trying to access array offset on value of type int\nI have a problem with the error message \"Trying to access array offset on the value of type int\" I use PHP version 7.4, as I see on the web :\n\n\nArray-style access of non-arrays\n\n\nbool, int, float or resource as an array (such as $null[\"key\"]) will now generate a notice.\n\n\nCode is:\n\n\n\n```\n <?php\n        foreach($gdata_worksheets As $key => $value ){\n            \/\/$key=\"1361298261\";\n        ?>\n            <option value=\"<?php echo strToHex($key); ?>\"<?php echo $key == $gdata->worksheet_id ? ' selected=\"selected\"' : ''?>><?php echo htmlentities($value, ENT_QUOTES, 'UTF-8');?><\/option>\n            \n\n\n\nfunction strToHex($string){\n\n$hex = '';\nfor ($i=0; $i<strlen($string); $i++){\n    $ord = ord($string[$i]);\n    $hexCode = dechex($ord);\n    $hex .= substr('0'.$hexCode, -2);\n}\nreturn strToUpper($hex);\n\n```\n\n}\n\n\nHow solve this, any idea?\n\n\nRegards\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"`$key` is probably not a string, you can use `gettype()` to check.\n\n\nYou can access to number digits with `substr()` :\n\n\n\n```\nfor ($i=0; $i<strlen($string); $i++){\n    $ord = ord(substr($string, $i, 1));\n\n```\n\nIf you prefer use array-access you must cast `$string` to `(string)` :\n\n\n\n```\nfunction strToHex($string){\n    $string = (string)$string;\n\n```\n\nA final propal could be :\n\n\n\n```\nfunction strToHex($string)\n{\n    $result = '';\n    $n = strlen($string);\n    for ($i = 0; $i < $n; $i++) {\n        $c = substr($string, $i, 1);\n        $c = ord($c);\n        $result .= sprintf('%02X', $c);\n    }\n    return $result;\n}\n\necho strToHex(1234); \/\/ 31323334\necho strToHex('Yop!'); \/\/ 596F7021\n\n```\n\n"}
{"questionId":"3dd5c961e4074a95ad89ece56bb6ddd5","question":"Get only file names from s3 bucket folder\nI have a s3 bucket named 'Sample\\_Bucket' in which there is a folder called 'Sample\\_Folder'. I need to get only the names of all the files in the folder 'Sample\\_Folder'.\n\n\nI am using the following code to do so - \n\n\n\n```\nimport boto3\ns3 = boto3.resource('s3', region_name='us-east-1', verify=False)\n    bucket = s3.Bucket('Sample_Bucket')\n    for files in bucket.objects.filter(Prefix='Sample_Folder):\n        print(files)\n\n```\n\nThe variable files contain object variables which has the filename as key.\n\n\n\n```\ns3.ObjectSummary(bucket_name='Sample-Bucket', key='Sample_Folder\/Sample_File.txt')\n\n```\n\nBut I need only the filename.\nHow do I extract that? Or is there any other way to do it?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"You should use list\\_object\\_v2 which gives you the list from the defined prefix used.\n\n\n\n```\n... snippet ...\n\nfilenames = []\n\ndef get_filenames(s3):\n    result = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n    for item in result['Contents']:\n        files = item['Key']\n        print(\"file: \", files)\n        filenames.append(files)   #optional if you have more filefolders to got through.\n    return filenames\n\nget_filenames(my_bucketfolder)\n\n```\n\n"}
{"questionId":"4a93a27a48d5458595f7a96dc38b85e8","question":"Is ti possible to `map` an enum to Binding in SwiftUI?\nWorking with `MVVM` in SwifUI. My aim is to have an `enum` state property in the `ViewModel` so the `View` could adjust it self according to the state property. States could be: `idle`, `busy`, `done` and `error`. On `done` I want to navigate to another screen using `NavigationLink`, however the problem is that it is expecting a `Binding<Bool>` and I could not figure out a way to map my enum state to bool.\n\n\nHere is the simplified code:\n\n\n\n```\nstruct LoginView: View {\n\n    @ObservedObject private var viewModel: LoginViewModel\n\n    @ViewBuilder\n    var body: some View {\n        ...\n        \/\/ success state\n        NavigationLink(destination: HomeFactory().make(), isActive: self.$viewModel.state \/* <---- some sort of mapping should come here *\/){ EmptyView() }\n        ...\n    }\n}\n\n```\n\nHope that I am missing something really basic and it could be easily achieved in an elegant way.\n\n\n**EDIT:**\n\n\nSeems like it should be possible with the next method:  \n\n`NavigationLink(destination: HomeFactory().make(), tag: .done, selection: self.$viewModel.viewState, label: { EmptyView() })`\n\n\nHowever I get an error and I can't figure out what is wrong: `Cannot convert value of type 'Binding<ViewState>' to expected argument type 'Binding<_?>'`\n\n\nHere is the code:\n\n\n\n```\nfinal class LoginViewModel: ObservableObject {\n  @Published var viewState: ViewState = .idle\n  func begin() {\n    ..\n    self.viewState = .done\n    ..\n  }\n}\n\nstruct LoginView: View {\n  @ObservedObject private var viewModel: LoginViewModel\n\n  @ViewBuilder\n  var body: some View {\n    ..\n    NavigationLink(destination: HomeFactory().make(), tag: .done, selection: self.$viewModel.viewState, label: { EmptyView() })\n    ..\n  }\n\n```\n\n**UPDATE:**\n\n\nI was very close. The `ViewState` in the vm should be optional:  \n\n`@Published var viewState: ViewState? = .idle`\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"swift"},"answer":"There isn't an elegant way to map it in the view.  \n\nHowever, in your `LoginViewModel` you can have an @Published variable that gets set when the state is updated.\n\n\nHere is an example:\n\n\n\n```\nclass LoginViewModel: ObservableObject {\n    @Published var shouldNavigate = false  \n\n    var state: State = .idle {\n        didSet {\n            self.shouldNavigate = state == .done\n        }\n    }\n}\n\n```\n\nThen change your `NavigationLink` to:\n\n\n\n```\nNavigationLink(destination: HomeFactory().make(), isActive: self.$viewModel.shouldNavigate){ EmptyView() }\n\n```\n\n**EDIT:**  \n\nYou can navigate based on state, or some other enum using a NavigationLink like this: \n\n\n\n```\nNavigationLink(destination: HomeFactory().make(), tag: State.done, selection: self.$state){ EmptyView() }\n\n```\n\nAnd update your vm state definition to:  \n\n`@Published var state: State = .idle`\n\n\n"}
{"questionId":"b23791a038f24820a6dc953489c31f10","question":"Java - get the quotient and remainder in the same step?\nIt seems that in order to find both the quotient and remainder of a division in Java, one has to do:\n\n\n\n```\nint a = ...\nint b = ...\n\nint quotient = a \/ b;\nint remainder = a % b;\n\n```\n\nIs there a way to write this so that the quotient and remainder are found in a single step (one division operation)? Or does Java already automatically optimize this code so that they are?\n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"java"},"answer":"The natural behaviour of all architectures is for the divide instructions to supply the quotient and remainder in separate registers (for binary) or storage areas (for packed decimal as found on the IBM zSeries). The only high level language that I know of that does the same is COBOL. It does always seem wasteful having to repeat the divide instruction again to get the remainder.\n\n\n"}
{"questionId":"0a3fd6d7c4f0496abd52c2bb3673a039","question":"Is pointer arithmetic on allocated storage allowed since C++20?\nIn the C++20 standard, it is said that array types are *implicit lifetime type*.\n\n\nDoes it mean that an array to a non implicit lifetime type can be implicitly created? The implicit creation of such an array would not cause creation of the array's elements?\n\n\nConsider this case:\n\n\n\n```\n\/\/implicit creation of an array of std::string \n\/\/but not the std::string elements:\nvoid * ptr = operator new(sizeof (std::string) * 10);\n\/\/use launder to get a \"pointer to object\" (which object?)\nstd::string * sptr = std::launder(static_cast<std::string*>(ptr));\n\/\/pointer arithmetic on not created array elements well defined?\nnew (sptr+1) std::string(\"second element\");\n\n```\n\nIs this code not UB any more since C++20?\n\n\n\n\n---\n\n\nMaybe this way is better?\n\n\n\n```\n\/\/implicit creation of an array of std::string \n\/\/but not the std::string elements:\nvoid * ptr = operator new(sizeof (std::string) * 10);\n\/\/use launder to get a \"pointer to object\" (actually not necessary)\nstd::string (* sptr)[10] = std::launder(static_cast<std::string(*)[10]>(ptr));\n\/\/pointer arithmetic on an array is well defined\nnew (*sptr+1) std::string(\"second element\");\n\n```\n\n\n\n---\n\n\n**TC Answer + Comments conclusion:**\n\n\n1. Array elements are not created but the array is created\n2. The use of `launder` in the first example cause UB, and is\nnot necessary in the second example.\n\n\nThe right code is:\n\n\n\n```\n    \/\/implicit creation of an array of std::string \n    \/\/but not the std::string elements:\n    void * ptr = operator new(sizeof (std::string) * 10);\n    \/\/the pointer already points to the implicitly created object\n    \/\/so casting is enough \n    std::string (* sptr)[10] = static_cast<std::string(*)[10]>(ptr);\n    \/\/pointer arithmetic on an array is well defined\n    new (*sptr+1) std::string(\"second element\");\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c++"},"answer":"\n> \n> Does it means that an array to a non implicit lifetime type can be implicitly created?\n> \n> \n> \n\n\nYes.\n\n\n\n> \n> The implicit creation of such an array would not cause creation of the array's elements?\n> \n> \n> \n\n\nYes.\n\n\nThis is what makes `std::vector` implementable in ordinary C++.\n\n\n"}
{"questionId":"0d4b3921c6f243b3879d698a3cc0021b","question":"Which Python dunder\/magic methods do you need to implement to correctly proxy an object?\nI'm trying to create an object proxy.\nAttribute\/property lookup can be done by simply implementing the `__getattribute__`, `__setattr__` and `__delattr__` methods.\nHowever, other functionalities like `len(x), x[], bool(x)` require other dunder methods like `__len__, __getitem__, __bool__` to be implemented. If you don't implement these on the proxy class, but the object you're proxying supports them, your proxy will be incomplete and cause runtime errors.\n\n\nI would therefore like to have a comprehensive list of all the things I need to implement, but I couldn't find any reliable list online.\n\n\nHere's 97 unique dunder method names of them I got from the `typing` and `builtins` modules.\nI know what a lot of them do, but there are some that I have no clue about. It will be a pain to implement all or most of them for my proxy class, so I would be glad if there is a workaround.\n\n\n\n```\n__abs__\n__add__\n__aenter__\n__aexit__\n__aiter__\n__and__\n__anext__\n__await__\n__bool__\n__bytes__\n__call__\n__class__\n__cmp__\n__complex__\n__contains__\n__delattr__\n__delete__\n__delitem__\n__delslice__\n__dir__\n__div__\n__divmod__\n__enter__\n__eq__\n__exit__\n__float__\n__floordiv__\n__format__\n__fspath__\n__ge__\n__get__\n__getattribute__\n__getitem__\n__getnewargs__\n__getslice__\n__gt__\n__hash__\n__iadd__\n__iand__\n__import__\n__imul__\n__index__\n__init__\n__init_subclass__\n__instancecheck__\n__int__\n__invert__\n__ior__\n__isub__\n__iter__\n__ixor__\n__le__\n__len__\n__lshift__\n__lt__\n__mod__\n__mul__\n__ne__\n__neg__\n__new__\n__next__\n__nonzero__\n__or__\n__pos__\n__pow__\n__prepare__\n__radd__\n__rand__\n__rdiv__\n__rdivmod__\n__reduce__\n__reduce_ex__\n__repr__\n__reversed__\n__rfloordiv__\n__rlshift__\n__rmod__\n__rmul__\n__ror__\n__round__\n__rpow__\n__rrshift__\n__rshift__\n__rsub__\n__rtruediv__\n__rxor__\n__set__\n__setattr__\n__setitem__\n__setslice__\n__sizeof__\n__str__\n__sub__\n__subclasscheck__\n__subclasses__\n__truediv__\n__xor__\n\n```\n\n","questionMetadata":{"type":"implementation","level":"advanced","tag":"python"},"answer":"To proxy an object, you only need to implement the dunder methods that the object has, so in the simplest world, you wouldn't need to do anything special to proxy them that you're not already doing to proxy the object's other attributes.\n\n\nHowever, the wrinkle is that dunder methods are looked up on the class, not on the object, so while for example `Foo().bar` will look up `bar` on the instance before falling back to the class if the instance has no `bar` attribute, `Foo() + 5` will look up `__add__` on the class `Foo`, completely ignoring the instance. That is, if the instance does have an instance attribute named `__add__`, then `Foo() + 5` still won't use that instance attribute.\n\n\nSo to proxy those dunder methods, they need to be proxied at the class level, not the instance level.\n\n\n\n```\nfrom functools import wraps\n\ndef proxy_function(name, f):\n    @wraps(f)\n    def proxied_f(*args, **kwargs):\n        print('Proxying function:', name)\n        return f(*args, **kwargs)\n    return proxied_f\n\ndef proxy_object(obj):\n    class Proxy:\n        def __getattr__(self, name):\n            print('Proxying getattr:', name)\n            return getattr(obj, name)\n        def __hasattr__(self, name):\n            print('Proxying hasattr:', name)\n            return hasattr(obj, name)\n        def __setattr__(self, name, value):\n            print('Proxying setattr:', name, '=', repr(value))\n            setattr(obj, name, value)\n        def __delattr__(self, name):\n            print('Proxying delattr:', name)\n            delattr(obj, name)\n    \n    for name, f in obj.__class__.__dict__.items():\n        # don't try to overwrite __class__, __getattr__, etc.\n        if callable(f) and name not in Proxy.__dict__:\n            f = proxy_function(name, f)\n            setattr(Proxy, name, f)\n    \n    return Proxy()\n\n```\n\nUsage:\n\n\n\n```\n>>> class Foo:\n...     def __add__(self, other):\n...         return 'Adding with ' + repr(other)\n... \n>>> foo = Foo()\n>>> proxy_foo = proxy_object(foo)\n>>> foo + 5\n'Adding with 5'\n>>> proxy_foo + 5\nProxying function: __add__\n'Adding with 5'\n\n```\n\n"}
{"questionId":"ae2c2c3442b04b0799fd9a370c877013","question":"Split String Into Array and Append Prev Value\nI have this string:\n\n\n\n> \n> var\/log\/file.log\n> \n> \n> \n\n\nI eventually want to end up with an array looking like this:\n\n\n\n```\nArray => [\n    '1' => 'var',\n    '2' => 'var\/log',\n    '3' => 'var\/log\/file.log'\n]\n\n```\n\nI currently have this:\n\n\n\n```\n<?php\n    $string = 'var\/log\/file.log';\n    $array = explode('\/', $string);\n    $output = [\n        1 => $array[0],\n        2 => $array[0]. '\/' .$array[1],\n        3 => $array[0]. '\/' .$array[1]. '\/' .$array[2]\n    ];\n\n    echo '<pre>'. print_r($output, 1) .'<\/pre>';\n\n```\n\nThis feels really counter-intuitive and I'm not sure if there's already something built into PHP that can take care of this.\n\n\nHow do I build an array using appending previous value?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"php"},"answer":"This solution takes the approach of starting with your input path, and then removing a path one by one, adding the remaining input to an array at each step. Then, we reverse the array as a final step to generate the output you want.\n\n\n\n```\n$input = \"var\/log\/file.log\";\n$array = [];\nwhile (preg_match(\"\/\\\/\/i\", $input)) {\n    array_push($array, $input);\n    $input = preg_replace(\"\/\\\/[^\\\/]+$\/\", \"\", $input);\n    echo $input;\n}\narray_push($array, $input);\n$array = array_reverse($array);\nprint_r($array);\n\nArray\n(\n    [0] => var\n    [1] => var\/log\n    [2] => var\/log\/file.log\n)\n\n```\n\nThe above call to `preg_replace` strips off the final path of the input string, including the forward slash. This is repeated until there is only one final path component left. Then, we add that last component to the same array.\n\n\n"}
{"questionId":"acfda63c98c44c5d85333ef1b727d4d7","question":"How to execute large amount of sql queries asynchronous and in threads\n**Problem:** I have huge amount of sql queries (around 10k-20k) and I want to run them asynchronous in 50 (or more) threads. \n\n\nI wrote a powershell script for this job, but it is very slow (It took about 20 hours to execute all). **Desired result is 3-4 hours max.**\n\n\n**Question:** How can I optimize this powershell script? Should I reconsider and use another technology like `python` or `c#`?\n\n\nI think it's powershell issue, because when I check with `whoisactive` the queries are executing fast. Creating, exiting and unloading jobs takes a lot of time, because for each thread is created separate PS instances.\n\n\n**My code:**\n\n\n\n```\n$NumberOfParallerThreads = 50;\n\n\n$Arr_AllQueries = @('Exec [mystoredproc] @param1=1, @param2=2',\n                    'Exec [mystoredproc] @param1=11, @param2=22',\n                    'Exec [mystoredproc] @param1=111, @param2=222')\n\n#Creating the batches\n$counter = [pscustomobject] @{ Value = 0 };\n$Batches_AllQueries = $Arr_AllQueries | Group-Object -Property { \n    [math]::Floor($counter.Value++ \/ $NumberOfParallerThreads) \n};\n\nforEach ($item in $Batches_AllQueries) {\n    $tmpBatch = $item.Group;\n\n    $tmpBatch | % {\n\n        $ScriptBlock = {\n            # accept the loop variable across the job-context barrier\n            param($query) \n            # Execute a command\n\n            Try \n            {\n                Write-Host \"[processing '$query']\"\n                $objConnection = New-Object System.Data.SqlClient.SqlConnection;\n                $objConnection.ConnectionString = 'Data Source=...';\n\n                $ObjCmd = New-Object System.Data.SqlClient.SqlCommand;\n                $ObjCmd.CommandText = $query;\n                $ObjCmd.Connection = $objConnection;\n                $ObjCmd.CommandTimeout = 0;\n\n                $objAdapter = New-Object System.Data.SqlClient.SqlDataAdapter;\n                $objAdapter.SelectCommand = $ObjCmd;\n                $objDataTable = New-Object System.Data.DataTable;\n                $objAdapter.Fill($objDataTable)  | Out-Null;\n\n                $objConnection.Close();\n                $objConnection = $null;\n            } \n            Catch \n            { \n                $ErrorMessage = $_.Exception.Message\n                $FailedItem = $_.Exception.ItemName\n                Write-Host \"[Error processing: $($query)]\" -BackgroundColor Red;\n                Write-Host $ErrorMessage \n            }\n\n        }\n\n        # pass the loop variable across the job-context barrier\n        Start-Job $ScriptBlock -ArgumentList $_ | Out-Null\n    }\n\n    # Wait for all to complete\n    While (Get-Job -State \"Running\") { Start-Sleep 2 }\n\n    # Display output from all jobs\n    Get-Job | Receive-Job | Out-Null\n\n    # Cleanup\n    Remove-Job *\n\n}\n\n```\n\n***UPDATE*:**\n\n\n**Resources:** The DB server is on a remote machine with: \n\n\n- 24GB RAM,\n- 8 cores,\n- 500GB Storage,\n- SQL Server 2016\n\n\nWe want to use the maximum cpu power.\n\n\n**Framework limitation:** The only limitation is ***not*** to use SQL Server to execute the queries. The requests should come from outside source like: Powershell, C#, Python, etc. \n\n\n","questionMetadata":{"type":"optimization","level":"advanced","tag":"sql"},"answer":"RunspacePool is the way to go here, try this:\n\n\n\n```\n$AllQueries = @( ... )\n$MaxThreads = 5\n\n# Each thread keeps its own connection but shares the query queue\n$ScriptBlock = {\n    Param($WorkQueue)\n\n    $objConnection = New-Object System.Data.SqlClient.SqlConnection\n    $objConnection.ConnectionString = 'Data Source=...'\n\n    $objCmd = New-Object System.Data.SqlClient.SqlCommand\n    $objCmd.Connection = $objConnection\n    $objCmd.CommandTimeout = 0\n\n    $query = \"\"\n\n    while ($WorkQueue.TryDequeue([ref]$query)) {\n        $objCmd.CommandText = $query\n        $objAdapter = New-Object System.Data.SqlClient.SqlDataAdapter $objCmd\n        $objDataTable = New-Object System.Data.DataTable\n        $objAdapter.Fill($objDataTable) | Out-Null\n    }\n\n    $objConnection.Close()\n\n}\n\n# create a pool\n$pool = [RunspaceFactory]::CreateRunspacePool(1, $MaxThreads)\n$pool.ApartmentState  = 'STA'\n$pool.Open()\n\n# convert the query array into a concurrent queue\n$workQueue = New-Object System.Collections.Concurrent.ConcurrentQueue[object]\n$AllQueries | % { $workQueue.Enqueue($_) }\n\n$threads = @()\n\n# Create each powershell thread and add them to the pool\n1..$MaxThreads | % {\n    $ps = [powershell]::Create()\n    $ps.RunspacePool = $pool\n    $ps.AddScript($ScriptBlock) | Out-Null\n    $ps.AddParameter('WorkQueue', $workQueue) | Out-Null\n    $threads += [pscustomobject]@{\n        Ps = $ps\n        Handle = $null\n    }\n}\n\n# Start all the threads\n$threads | % { $_.Handle = $_.Ps.BeginInvoke() }\n\n# Wait for all the threads to complete - errors will still set the IsCompleted flag\nwhile ($threads | ? { !$_.Handle.IsCompleted }) {\n    Start-Sleep -Seconds 1\n}\n\n# Get any results and display an errors\n$threads | % {\n    $_.Ps.EndInvoke($_.Handle) | Write-Output\n    if ($_.Ps.HadErrors) {\n        $_.Ps.Streams.Error.ReadAll() | Write-Error\n    }\n}\n\n```\n\nUnlike powershell jobs, a RunspacePools can share resources. So there is one concurrent queue of all the queries, and each thread keeps its own connection to the database.\n\n\nAs others have said though - unless you're stress testing your database, you're probably better off reorganising the queries into bulk inserts.\n\n\n"}
{"questionId":"dc462d4d39854a62a9734a84e27fc707","question":"How to get the number of parameters of a run-time determined callable?\n*NOTE: By virtue of writing this quesiton, I've already figured out that I was being overly enthousiastic about using a new language feature. The far cleaner solution was using a Strategy Pattern instead... still, I'm curious if there's a proper way to go about this problem.*\n\n\nTL;DR: Can you reflect on a generic Callable in PHP without resorting to manually typechecking all kinds of callable?\n\n\nIn PHP 5.4 we've got a new typehint: callable. This seems like a lot of fun. I thought I'd make use of this through the following:\n\n\n\n```\n<?php\n    public function setCredentialTreatment(callable $credentialTreatment) {\n       \/\/ Verify $credentialTreatment can be used (ie: accepts 2 params)\n       ... magic here ...\n    }\n?>\n\n```\n\nSo far my line of thought has been to do a series of type-checks on the callable, and inferring from that which Reflection\\* class to use:\n\n\n\n```\n<?php\nif(is_array($callable)) {\n    $reflector = new ReflectionMethod($callable[0], $callable[1]);\n} elseif(is_string($callable)) {\n    $reflector = new ReflectionFunction($callable);\n} elseif(is_a($callable, 'Closure') || is_callable($callable, '__invoke')) {\n    $objReflector = new ReflectionObject($callable);\n    $reflector    = $objReflector->getMethod('__invoke');\n}\n\n\/\/ Array of ReflectionParameters. Yay!\n$parameters = $reflector->getParameters();\n\/\/ Inspect parameters. Throw invalidArgumentException if not valid.\n?>\n\n```\n\nNow, to me, this feels overly complicated. Am I missing some kind of shortcut way to achieving what I'm trying to do here? Any insight would be welcomed :)\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"TL;DR I don't think so. You need to check for all callable types if you want a generic solution.\n\n\nThe following function can be used to get a `ReflectionFunctionAbstract` instance for any generic callable in PHP:\n\n\n\n```\nfunction reflectionOf(callable $callable): ReflectionFunctionAbstract\n{\n    if ($callable instanceof Closure) {\n        return new ReflectionFunction($callable);\n    }\n    if (is_string($callable)) {\n        $pcs = explode('::', $callable);\n        return count($pcs) > 1 ? new ReflectionMethod($pcs[0], $pcs[1]) : new ReflectionFunction($callable);\n    }\n    if (!is_array($callable)) {\n        $callable = [$callable, '__invoke'];\n    }\n    return new ReflectionMethod($callable[0], $callable[1]);\n}\n\n```\n\nThen it is possible to get the number of parameters as follows:\n\n\n\n```\nreflectionOf($func)->getNumberOfParameters();\n\n```\n\nHope this helps to someone.\nThis answer might be late to the party, but none of the other solutions provide a full coverage for generic callables.\n\n\n"}
{"questionId":"296ba77c01354917b51d6ed664aa5b06","question":"Prevent missing fields in struct initialization\nConsider this example. Let's say I have this object which is ubiquitous throughout my codebase:\n\n\n\n```\ntype Person struct {\n    Name string\n    Age  int\n    [some other fields]\n}\n\n```\n\nSomewhere deep in the codebase, I also have some code that creates a new `Person` struct. Maybe it's something like the following utility function (note that this is just an example of some function that creates a `Person`-- the point of my question is not to ask about the copy function specifically):\n\n\n\n```\nfunc copyPerson(origPerson Person) *Person {\n    copy := Person{\n        Name: origPerson.Name,\n        Age:  origPerson.Age,\n        [some other fields]\n    }\n    return &copy\n}\n\n```\n\nAnother developer comes along and adds a new field `Gender` to the `Person` struct. However, because the `copyPerson` function is in a distant piece of code they forget to update `copyPerson`. Since golang doesn't throw any warning or error if you omit a parameter when creating a struct, the code will compile and appear to work fine; the only difference is that the `copyPerson` method will now fail to copy over the `Gender` struct, and the result of `copyPerson` will have `Gender` replaced with a nil value (e.g. the empty string). \n\n\nWhat is the best way to prevent this from happening? Is there a way to ask golang to enforce no missing parameters in a specific struct initialization? Is there a linter that can detect this type of potential error?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"go"},"answer":"First of all, your `copyPerson()` function does not live up to its name. It copies *some* fields of a `Person`, but not (necessarily) all. It should've been named `copySomeFieldsOfPerson()`.\n\n\nTo copy a complete struct value, just assign the struct value. If you have a function receiving a non-pointer `Person`, that is already a copy, so just return its address:\n\n\n\n```\nfunc copyPerson(p Person) *Person {\n    return &p\n}\n\n```\n\nThat's all, this will copy all present and future fields of `Person`.\n\n\nNow there may be cases where fields are pointers or header-like values (like a slice) which should be \"detached\" from the original field (more precisely from the pointed object), in which case you do need to make manual adjustments, e.g.\n\n\n\n```\ntype Person struct {\n    Name string\n    Age  int\n    Data []byte\n}\n\nfunc copyPerson(p Person) *Person {\n    p2 := p\n    p2.Data = append(p2.Data, p.Data...)\n    return &p2\n}\n\n```\n\nOr an alternative solution which does not make another copy of `p` but still detaches `Person.Data`:\n\n\n\n```\nfunc copyPerson(p Person) *Person {\n    var data []byte\n    p.Data = append(data, p.Data...)\n    return &p\n}\n\n```\n\nOf course, if someone adds a field which also needs manual handling, this won't help you out.\n\n\nYou could also use unkeyed literal, like this:\n\n\n\n```\nfunc copyPerson(p Person) *Person {\n    return &Person{\n        p.Name,\n        p.Age,\n    }\n}\n\n```\n\nThis will result in a compile-time error if someone adds a new field to `Person`, because an unkeyed composite struct literal must list all fields. Again, this will not help you out if someone changes the fields where the new fields are assignable to the old ones (e.g. someone swaps 2 fields next to each other having the same type), also unkeyed literals are discouraged.\n\n\nBest would be for the package owner to provide a copy constructor, next to the `Person` type definition. So if someone changes `Person`, he \/ she should be responsible keeping `CopyPerson()` still operational. And as others mentioned, you should already have unit tests which should fail if `CopyPerson()` does not live up to its name.\n\n\n## The best viable option?\n\n\nIf you can't place the `CopyPerson()` next to the `Person` type and have its author maintain it, go ahead with the struct value copying and manual handling of pointer and header-like fields.\n\n\nAnd you can create a `person2` type which is a \"snapshot\" of the `Person` type. Use a blank global variable to receive compile-time alert if the original `Person` type changes, in which case `copyPerson()`'s containing source file will refuse to compile, so you'll know it needs adjusting.\n\n\nThis is how it can be done:\n\n\n\n```\ntype person2 struct {\n    Name string\n    Age  int\n}\n\nvar _ = Person(person2{})\n\n```\n\nThe blank variable declaration will not compile if fields of `Person` and `person2` do not match.\n\n\nA variation of the above compile-time check could be to use typed-`nil` pointers:\n\n\n\n```\nvar _ = (*Person)((*person2)(nil))\n\n```\n\n"}
{"questionId":"8bdde802bac7477b8b423c52b238cfca","question":"Check if nuget package exists using command line\nHow can I check if nuget package with specific version exists in a specified package source (nuget server) using powershell or commandline outside visual studio?\n\n\n## Scenario:\n\n\nI have private NuGet server where I want to push my own packages. I have automated creation of the packages during TFS build. What I miss is check, if the package was previously uploaded and published to the NuGet Server.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"You can call this CMD script from PowerShell easy enough, with examples below it. You can just go by `$LastExitCode` to determine how to proceed, with `0` meaning you can publish:\n\n\n### check\\_nupkg.bat\n\n\n\n```\n@echo off\nSetLocal EnableDelayedExpansion EnableExtensions\npushd \"%~dp0\"\n\nset \"pkg_name=%~1\"\nset \"pkg_version=%~2\"\n\ncall nuget list %pkg_name% -AllVersions -Prerelease|findstr \/i \/r \/c:\"^%pkg_name% %pkg_version%$\" -N>nul 2>&1\n\nif not \"%errorlevel%\"==\"0\" (\n    echo This package can be published\n    exit \/b 0\n) else (\n    echo This package has already been published.\n    exit \/b 1\n)\n\n```\n\n`C:\\stuff>.\\check_nupkg.bat \"lolspeak\" \"1.0.0\"`\n\n\n\n> \n> `This package has already been published.`\n> \n> \n> \n\n\n`C:\\stuff>check_nupkg.bat \"lolspeak\" \"11.0.0\"`\n\n\n\n> \n> `This package can be published`\n> \n> \n> \n\n\n"}
{"questionId":"aab7b0cd1b9b4b979450460fabe27ee9","question":"split multiple columns in pandas dataframe by delimiter\nI have survey data which annoying has returned multiple choice questions in the following way. It's in an excel sheet There is about 60 columns with responses from single to multiple that are split by \/. This is what I have so far, is there any way to do this quicker without having to do this for each individual column\n\n\n\n```\ndata = {'q1': ['one', 'two', 'three'],\n   'q2' : ['one\/two\/three', 'a\/b\/c', 'd\/e\/f'],\n   'q3' : ['a\/b\/c', 'd\/e\/f','g\/h\/i']}\n\ndf = pd.DataFrame(data)\n\ndf[['q2a', 'q2b', 'q2c']]= df['q2'].str.split('\/', expand = True, n=0)\ndf[['q3a', 'q3b', 'q3c']]= df['q2'].str.split('\/', expand = True, n=0)\n\nclean_df = df.drop(df[['q2', 'q3']], axis=1)\n\n```\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"python"},"answer":"We can use list comprehension with `add_prefix`, then we use `pd.concat` to concatenate everything to your final df:\n\n\n\n```\nsplits = [df[col].str.split(pat='\/', expand=True).add_prefix(col) for col in df.columns]\nclean_df = pd.concat(splits, axis=1)\n\n```\n\n\n```\n     q10  q20  q21    q22 q30 q31 q32\n0    one  one  two  three   a   b   c\n1    two    a    b      c   d   e   f\n2  three    d    e      f   g   h   i\n\n```\n\n\n\n---\n\n\nIf you actually want your column names to be suffixed by a letter, you can do the following with `string.ascii_lowercase`:\n\n\n\n```\nfrom string import ascii_lowercase\n\ndfs = []\nfor col in df.columns:\n    d = df[col].str.split('\/', expand=True)\n    c = d.shape[1]\n    d.columns = [col + l for l in ascii_lowercase[:c]]\n    dfs.append(d)\n    \nclean_df = pd.concat(dfs, axis=1)\n\n```\n\n\n```\n     q1a  q2a  q2b    q2c q3a q3b q3c\n0    one  one  two  three   a   b   c\n1    two    a    b      c   d   e   f\n2  three    d    e      f   g   h   i\n\n```\n\n"}
{"questionId":"f9eab5bb4cbb47c7abaa4753cc0108da","question":"check if an string exists in a list of strings, using powershell\nI have Powershell version 3,4 and 5 in my environment. When I write below code it continously gave me false, though $CompatiableOS contains output of $OSverions.\n\n\n\n```\n   [string] $CompatiableOS = '2016','2012','2008'\n   $OSVersion=[regex]::Matches(((Get-WmiObject -class Win32_OperatingSystem).caption), \"([0-9]{4})\")\n\n   if ( $CompatiableOS -contains  $OSVersion)\n   {\n      return $TRUE\n   }\n   else\n   {\n      return $FALSE\n   }\n\n```\n\nbut when I changed above code to below, it worked. What could be the issue?\n\n\n\n```\n [string] $CompatiableOS = '2016','2012','2008'\n $OSVersion=[regex]::Matches(((Get-WmiObject -class Win32_OperatingSystem).caption), \"([0-9]{4})\")\n\n if ( $CompatiableOS.contains($OSVersion))\n {\n    return $TRUE\n }\n else\n {\n      return $FALSE\n }\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"From the MS documentation\n\n\n\n> \n> -Contains\n>  Description: Containment operator. Tells whether a collection of reference\n>  values includes a single test value. Always returns a Boolean value. Returns TRUE\n>  only when the test value exactly matches at least one of the reference values.\n> \n> \n> \n\n\nThe important part is here is \"**only when the test value exactly matches**\". In your case you try to compare the string \"2016 2012 2008\" with as an example 2016, this doese't match exactly. In this case you should use the `-like` operator. Or you define your compatibleOS variable as an string array like that `[String[]]`, then you can use the `-contains`.\n\n\nAdditonal, check whats inside the OSVersion variable. It is a machtcollection not a string. If you define your compatibleOS variable as a string array and you use `$OSVersion.Value` both of your examples will work.\n\n\n"}
{"questionId":"855eb86e27d94e7d86915b3b749a6775","question":"Makefile: exit on conditional\nI want to check that an environment variable is set before executing some code in a Makefile. If it's not set I want to throw an error with a simple error message:\n\n\n\n```\nrun:\n  [ -z \"$(MY_APP)\" ] && echo \"MY_APP must be set\" && exit 1\n  echo \"MY_APP is set. Yay!\"\n  echo \"Let's continue on with the command...\"\n\n```\n\nWhen `MY_APP` is not set I get the following error, which is desired:\n\n\n\n```\n[ -z \"\" ] && echo \"MY_APP must be set\" && exit 1\nMY_APP must be set\nmake: *** [run] Error 1\n\n```\n\nHowever, when `MY_APP` is set I get the following error:\n\n\n\n```\n[ -z \"EXAMPLE_NAME\" ] && echo \"MY_APP must be set\" && exit 1\nmake: *** [run] Error 1\n\n```\n\nAny idea what I'm doing wrong? And is there a better way to do this?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"Recall that the `&&` condition require that all conditions must be **TRUE** to pass. Since the first condition fail, the whole command will return a status of `1` (-> **false**), effectively stopping the make\n\n\nYou can use the following, so that the test will fail only when `MY_APP` is missing.\n\n\nNote that I'm using `false` instead of `exit 1`. Also better to use `\"${MY_APP}\"`, which make it easier to copy\/paste from Make to shell prompt\/script.\n\n\n\n```\nrun:\n    { [ -z \"$(MY_APP)\" ] && echo \"MY_APP must be set\" && false } || true\n    ...\n\n# Or just if-Then-Else\n    if [ -z \"${MY_APP}\" ] ; then echo \"MY_APP must be set\" ; false ; fi\n    ...\n\n```\n\n"}
{"questionId":"981123b03bb5403dbc7cccbd2d6b6532","question":"Is there any way to cast a std::any containing a derived pointer to a base pointer, without knowing the derived type?\nLet's say I have a `std::any` object which may or may not contain a pointer to some derived class of a given base class `B`. Is there any way I can do something that:\n\n\n1. Returns a `B *`, if the `std::any` object holds something convertible to `B *`, or\n2. Throws an exception if not?\n\n\nIt seems like `dynamic_cast` and `std::any_cast` each provide one half of this functionality, but I don't see any way of putting the two together.\n\n\nI'm aware that I can make this work in various ways that involve explicitly enumerating every type convertible to `B *`, but that's the mother of all DRY violations.\n\n\n\n\n---\n\n\nSample use case:\n\n\n\n```\nstd::vector<std::any> setupTools(const std::string & confFile)\n{\n  std::vector<std::any> myTools;\n\n  auto conf = parse(confFile);\n\n  for(std::string & wrenchInfo : conf[\"Wrenches\"])\n  {\n    Wrench::setup(myTools, wrenchInfo);\n  }    \n\n  for(std::string & hammerInfo : conf[\"Hammers\"])\n  {\n    Hammer::setup(myTools, hammerInfo);\n  }\n\n   \/\/ 25 more kinds of tools\n}\n\nFactory1::init(const std::vector<std::any> & tools)\n{\n  m_wrench = any_get<Wrench *>(tools);\n  m_hammer = any_get<Hammer *>(tools);\n  m_saw = any_get<Saw *>(tools);\n}\n\nFactory2::init(const std::vector<std::any> & tools)\n{\n  m_wrench = any_get<TorqueWrench *>(tools);\n}\n\nFactory3::init(const std::vector<std::any> & tools)\n{\n  m_saw = any_get<CordlessReciprocatingSaw *>(tools);\n}\n\n```\n\nI don't want to include a bunch of boilerplate code listing every single kind of saw in existence just so I can grab a saw -- *any* `Saw` -- to use in `Factory1`.\n\n\n","questionMetadata":{"type":"implementation","level":"advanced","tag":"c++"},"answer":"This is unachievable. It is only possible to get an object out from `std::any` using exactly the type that was put inside. Thus, you must know the type to get anything out of it.\n\n\nIt seems that `std::any` does not fit your use case.\n\n\n"}
{"questionId":"f337267067b64a60b6e49473cf90e00a","question":"Getting a wrong output using arraylists\nThe challenge is to find a number whose individual digits multiplied by consecutively increasing power and added up, equal the initial number.\n\n\nEg: take 89, split it into 8 and 9, then `8^1 + 9^2 = 89`\n\n\n\n```\nstatic List<Integer> sumDigPow(int a, int b) { \n        List<Integer> eureka = new ArrayList<Integer>(0);\n        List<String> digits = new ArrayList<String>();\n        String num;\n        int sum = 0, multi;\n\n    for (int i=a; i<=b; i++) {\n        num = String.valueOf(i);\n        digits.add(num);\n\n        for (int j=0; j<digits.size(); j++) {\n                multi = (int)Math.pow(Integer.parseInt(digits.get(j)), j+1);\n                sum += multi;\n        }\n\n        if (sum == i) eureka.add(i);\n\n        sum = 0;\n        digits.clear();\n    }\n\n    return eureka;\n}\n\n```\n\nWith an input of 1 and 100 (the range), the output should be [1, 2, 3, 4, 5, 6, 7, 8, 9, 89], but I'm getting all of the numbers [1, 2 ... 100].\n\n\nI've started learning java fairly recently and can't seem to find the issue in the code. Any hints would be greatly appreciated.\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"java"},"answer":"You can use the following:\n\n\n\n```\nstatic List<Integer> sumDigPow(int a, int b) {\n    List<Integer> eureka = new ArrayList<Integer>(0);\n    String num;\n    int sum = 0, multi;\n\n    for (int i = a; i <= b; i++) {\n        num = String.valueOf(i);\n        for (int j = 0; j < num.length(); j++) {\n            multi = (int) Math.pow(Character.getNumericValue(num.charAt(j)), j + 1);\n            sum += multi;\n        }\n\n        if (sum == i) {\n            eureka.add(i);\n        }\n        sum = 0;\n    }\n    return eureka;\n}\n\n```\n\nExplanation:\n\n\n1. You were not checking the second digit of the number.\n2. Loop over each character of the *String* `num`.\n3. There is no need of the `digits` arraylist, you can just use the *numeric* value of the char.\n\n\n"}
{"questionId":"415fb95360374287a69a3041786e54e8","question":"What is the best way to add an element to the beginning of a list in Kotlin\nIf I have a list like this one\n\n\n\n```\n var foo = mutableListOf(\"John\", \"Wayne\")\n\n```\n\nand if I want to add an element on top of the list, so far I am thinking of two options.\n\n\nFirst: `foo.add(0, \"Twyla\")`\n\n\nSecond: `foo = (mutableListOf(\"Twyla\") + foo).toMutableList()`\n\n\nI am not how the above two options fare in terms of performance but what is a recommended way in general?\n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"kotlin"},"answer":"These two options are not quite the same: the first one just inserts an item into the beginning of the *existing* mutable list, and the second creates a new list and then converts it into another one new mutable list.\n\n\nI think if you're fine with mutating the existing list, you should prefer the first option because it requires way less memory than the second one.\n\n\n"}
{"questionId":"4cceb95924de475ebfe1a6af1e752b42","question":"Atom-beautify not loading php-cs-fixer custom config\nI have Atom installed with PHP-CS-Fixer plugin. I'm trying to use some custom rules to apply same-line braces style.\n\n\nI have tried using the in-Atom config option, but couldn't make it work. I have tried setting `position_after_functions_and_oop_constructs` and putting it in `PHP-CS-FIXER Rules` in Atom, but didn't work.\n\n\nTherefore, I have set a custom path to my config, which is `C:\\xampp\\htdocs\\myproject\\atom.php_cs`\n\n\nThe config is: \n\n\n\n```\n<?php\n\n$finder = PhpCsFixer\\Finder::create()\n    \/\/->exclude('somedir')\n    \/\/->notPath('src\/Symfony\/Component\/Translation\/Tests\/fixtures\/resources.php'\n    ->in(__DIR__)\n;\n\nreturn PhpCsFixer\\Config::create()\n    ->setRules([\n        '@PSR2' => true,\n        'strict_param' => false,\n        'array_syntax' => ['syntax' => 'long'],\n        'braces' => [\n            'allow_single_line_closure' => true, \n            'position_after_functions_and_oop_constructs' => 'same'],\n    ])\n    ->setFinder($finder)\n;\n\n```\n\nIt didn't work and Atom is NOT doing a proper beautify. Any idea to enforce the rules?\n\n\nNotes: \n\n\nI'm interested in having the following style: \n\n\n\n```\n  public function someFunction(){ \n    \/\/ code here\n}\n\n```\n\n- I'm using Windows 10 as OS, Atom is IDE and have PHP-cs-fixer installed via Composer.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"Since the beautification is not working properly the programm might have run into an error.\nYou can run *Atom Beautify: Help Debug Editor* from the command pallette to get Debug information.  \n\nYour config works perfectly fine for me and the problem seems to be your naming.\n\n\nSimply rename your **atom.php\\_cs** to **.php\\_cs** and remove any config file path from the settings.\n\n\n"}
{"questionId":"e46f23d9478e47e7895df090fa3d6496","question":"Script cache:clear returned with error code 255\nI installed Symfony 4.2 via composer \n\n\n\n```\ncomposer create-project Symfony\/website-skeleton my-project\n\n```\n\neverything works correctly, then I put the project in gitlab. a friend of mine tried to clone it on its own computer in order to work on the same project, and when he runs `composer install`\n\n\nhe got an error :\n\n\n\n> \n> Script cache:clear returned with error code 255\n> \n> \n> \n\n\nFrankly, I tried everything I can find on the web and Stack Overflow but unfortunately, i didn't succeed.\n\n\n**Thanks in advance for any help** \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"First: How I got the error:\n\n\n1. I have a symfony 4.2 installed application in a folder called my-project\n2. I copied the content of the project to another folder called my-App using CTRL-C then CTRL-V (copy-paste)\n3. I put it to my gitlab project\n4. My friend cloned the project and run Composer install and He got the Error\n\"Script cache:clear returned with error code 255\"\n\n\nSecond: How I solved this:\n\n\n1. Actually, when you make a CTRL-C and CTRL-V to the content of the symfony project folder, **you are not copying everything**, all the .files (dot files example .env, .test, .gitignore) are hidden.\n2. in order to copy the other hidden files such as .env you should open your terminal and type \"cp .env (to your location)\" in my case, it was \"cp .env ~\/Desktop\/newApp\"\n\n\n\n> \n> **Hint:** in order to display all the folders and files in a folder use \"ll\" (ll: aliased to \"ls -lh\") instead of \"ls\" command.\n> \n> \n> \n\n\n"}
{"questionId":"42c9fd53d63241b186718f384abdfe5f","question":"Can somebody give a practical example of a many to many relationship?\nI learned about many-to-many relationships in College, and I never really understood them. So far I've been working with one-to-many relationships which are easy to understand and deal with.\n\n\nCan somebody please give a practical example of a many-to-many relationship, and explain why we need a bridging table for it. Plus, do you need a bridging table for a one-to-many relationship as well? As far as I understand you don't need a bridging table for it, but a friend of mine recently told me otherwise.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"sql"},"answer":"This question is old, but a practical example would be found in social networks like Instagram:\n\n\nYou (the **follower**) follow a person A (the **followee**).\nYou also follow person B, person C, etc..., but you are not the only one who may follow person A, as well as not the only one who may follow person B, person C, etc... Your friend or other people may as well follow them too.\n\n\nSo you end up with data shaped in the following way:\n\n\n\n```\n     Follower | Followee\n--------------|--------------\n          ... | ...\n          You | A\n          You | B\n          You | C\n  Your friend | A\n  Your friend | B\n  Your friend | C\n          ... | ...\n\n```\n\nWhich is what you call a bridging table (aka lookup table), describing a many-to-many relationship.\n\n\nContinuing with the social network example, you need a many-to-many bridging\/lookup table otherwise you would have to introduce redundancy in your `users` table, because you would need to duplicate your `You` record and that of your friend (`Your friend`) for each of your followees (`A`, `B`, `C`), which is of course non-practical and violates normalization.\n\n\n\n> \n> do you need a bridging table for a one to many relationships as well ? As far as I understand you don't need a bridging table for a one to many relationship, but a friend of mine recently told me otherwise.\n> \n> \n> \n\n\nYou may use a bridging\/lookup table for a one-to-many relationship for flexibility purposes when e.g. you don't know in advance if the relationship of your data is effectively many-to-many or the relationship is one-to-many but you think that it can evolve and become many-to-many in the future.\n\n\n"}
{"questionId":"6a5b75f8adc8458ab127a863ef74c367","question":"Understanding how EIP (RIP) register works?\nI'm a complete novice to computer architecture and the low level stuff that happens at the processor\/memory level. I'll start by saying that. What i've done with computers has pretty much always been at the high level programming level. C++, Java, etc.\n\n\nThat being said, I'm currently reading a book that is starting to delve into the low level programming stuff, assembly, registers, pointers, etc. I'm having a hard time understanding how the EIP register works. \n\n\nFrom what is said in the book, each memory address has one byte, and each byte has a memory address. \n\n\nFrom what I'm reading about the EIP register, it points to the next set of instructions for the processor to do. While using debugging tools (GDB) to follow along in the book, if you were to examine memory at a particular location, say:\n\n\nx\/8xb it allegedly lets you examine the first 8 bytes at the memory address. But if each memory address has only 1 byte, I don't understand. Can someone help me understand this? I have looked for thorough explanations of how this register works and functions but I can't really find anything\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"assembly"},"answer":"Let's begin with a concrete, x86-specific example.\n\n\n\n```\n00000000000020b0 <foo>:\n    20b0: 89 d1                         movl    %edx, %ecx\n    20b2: 89 f8                         movl    %edi, %eax\n    20b4: 0f af c6                      imull   %esi, %eax\n    20b7: 31 d2                         xorl    %edx, %edx\n    20b9: f7 f1                         divl    %ecx\n    20bb: c3                            ret\n\n```\n\nFor the sake of simplicity and of this example, think of memory as a gigantic \"array\" of bytes, and think of memory addresses as indexes into such an \"array\"0. When something lies at a given memory address, that essentially means that its first byte lies at that address, its second byte (if it's more than just a byte big) lies at the next address, and so on. For example, `foo` starts at address `0x20B0`1 and spans 12 bytes. This means that each address from `0x20B0` to `0x20BB` (inclusive) points to a byte in the function.\n\n\nThe program counter in x86, called `RIP` (`EIP` if 32-bit), points to the next instruction2. For example, if the current instruction being executed is the one at `0x20B2`, `RIP` would contain the value `0x20B4`. Because of the CISC nature of x86, instruction sizes differ, so `RIP` does not necessarily increase by a fixed amount each time.\n\n\n\n```\n00000000000020b0 <foo>:\n    20b0: 89 d1                         movl    %edx, %ecx\nEX->20b2: 89 f8                         movl    %edi, %eax\nPC->20b4: 0f af c6                      imull   %esi, %eax\n    20b7: 31 d2                         xorl    %edx, %edx\n    20b9: f7 f1                         divl    %ecx\n    20bb: c3                            ret\n\n```\n\nIn the next \"iteration\", EX (not a real register, just a way to mark what's being executed) will point to the `imull` instruction, and PC (`RIP`) will point to the `xorl` instruction, and so on until the `ret` instruction, at which point the return address, which is stored on the stack, will be loaded into `RIP` so that execution may continue at the caller of `foo`.\n\n\n\n\n---\n\n\n0 As mentioned by Peter Cordes, there are some architectures out there where this does not apply. For the sake of the question, this answer is specific to x86.  \n\n1 That's not actually the address at which this function would be found at runtime, but pretend it is for the sake of example.  \n\n2 There are some architectures where the program counter points to the current instruction (AArch64 does this), or even to two instructions ahead (AArch32 does this).\n\n\n"}
{"questionId":"a12b94ba5e594bef9913903c27c86de6","question":"Use of tidyeval based non-standard evaluation in recode in right-hand side of mutate\nConsider a tibble where each column is a character vector which can take many values -- let's say \"A\" through \"F\". \n\n\n\n```\nlibrary(tidyverse)\nsample_df <- tibble(q1 = c(\"A\", \"B\", \"C\"), q2 = c(\"B\", \"B\", \"A\"))\n\n```\n\nI wish to create a function which takes a column name as an argument, and recodes that column so that any answer \"A\" becomes an NA and the df is otherwise returned as is. The reason for designing it this way is to fit into a broader pipeline that performs a series of operations using a given column.\n\n\nThere are many ways to do this. But I am interested in understanding what the best idiomatic tidy\\_eval\/tidyverse approach would be. First, the question name needs to be on the left hand side of a mutate verb, so we use the `!!` and `:=` operators appropriately. But then, what to put on the right hand side?\n\n\n\n```\nfix_question <- function(df, question) {\n    df %>% mutate(!!question := recode(... something goes here...))\n}\n\nfix_question(sample_df, \"q1\") # should produce a tibble whose first column is (NA, \"B\", \"C\")\n\n```\n\nMy initial thought was that this would work:\n\n\n\n```\ndf %>% mutate(!!question := recode(!!question, \"A\" = NA_character_))\n\n```\n\nBut of course the bang-bang on inside the function just returns the literal character string (e.g. \"q1\"). I ended up taking what feels like a hacky route to reference the data on the right hand side, using the base R `[[` operator and relying on the `.` construct from dplyr, and it works, so in a sense I have solved my underlying problem:\n\n\n\n```\ndf %>% mutate(!!question := recode(.[[question]], \"A\" = NA_character_))\n\n```\n\nI'm interested in getting feedback from people who are very good at tidyeval as to whether there is a more idiomatic way to do this, in hopes that seeing a worked example would enhance my understanding of the tidyeval function set more generally. Any thoughts?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"r"},"answer":"Here, on the right side of `:=`, we can specify `sym` to convert to symbol and then evaluate (`!!`)\n\n\n\n```\nfix_question <- function(df, question) {\n    df %>%\n       mutate(!!question := recode(!! rlang::sym(question), \"A\" = NA_character_))\n  }\n\nfix_question(sample_df, \"q1\") \n# A tibble: 3 x 2\n#  q1    q2   \n#  <chr> <chr>\n#1 <NA>  B    \n#2 B     B    \n#3 C     A    \n\n```\n\n\n\n---\n\n\nA better approach that would work for both quoted and unquoted input is `ensym`\n\n\n\n```\nfix_question <- function(df, question) {\n    question <- ensym(question)\n    df %>%\n       mutate(!!question := recode(!! question, \"A\" = NA_character_))\n  }\n\n\nfix_question(sample_df, q1)\n# A tibble: 3 x 2\n#  q1    q2   \n#  <chr> <chr>\n#1 <NA>  B    \n#2 B     B    \n#3 C     A    \n\nfix_question(sample_df, \"q1\")\n# A tibble: 3 x 2\n#  q1    q2   \n#  <chr> <chr>\n#1 <NA>  B    \n#2 B     B    \n#3 C     A    \n\n```\n\n"}
{"questionId":"bf4204b6c6304e00b45339e176321d78","question":"Combine two state flows into new state flow\nI have two state flows. Is it possible to combine them and get new state flow? Logically it should be possible because both state flows have initials values, but as I see combine function returns just Flow and not StateFlow.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"kotlin"},"answer":"So far I created function:\n\n\n\n```\nfun <T1, T2, R> combineState(\n        flow1: StateFlow<T1>,\n        flow2: StateFlow<T2>,\n        scope: CoroutineScope = GlobalScope,\n        sharingStarted: SharingStarted = SharingStarted.Eagerly,\n        transform: (T1, T2) -> R\n): StateFlow<R> = combine(flow1, flow2) {\n    o1, o2 -> transform.invoke(o1, o2)\n}.stateIn(scope, sharingStarted, transform.invoke(flow1.value, flow2.value))\n\n```\n\n"}
{"questionId":"231db20e2ef343bf99f13715ceb09ec5","question":"The most efficient way to initialize array member of struct?\nI have declared the struct\n\n\n\n```\nstruct wnode {\n  char *word;\n  int lines[MAXLINES];\n  struct wnode *left;\n  struct wnode *right;\n};\n\n```\n\nand the pointer\n\n\n\n```\nstruct wnode *p;\n\n```\n\nThe pointer is passed to a function.\nIn that function, I first allocate memory for the pointer with malloc. Then I want to initialize the struct member **lines** to zero zero out the struct member **lines**.\n\n\nAn array initialization method will not work as it is interpreted as assignment:\n\n\n\n```\np->lines[MAXLINES] = {0};\n\n```\n\nThe compiler throws the error:\n\n\n\n> \n> error: expected expression before '{' token\n> \n> \n> \n\n\nIn the end, I'm just using a for loop to zero out the **lines** array:\n\n\n\n```\nfor (i = 0; i < MAXLINES; i++)\n  p->lines[i] = 0;\n\n```\n\nIs there a better way?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c"},"answer":"Arrays cannot be assigned to directly. You need to either use a loop to set all fields to 0 or you can use `memset`:\n\n\n\n```\nmemset(p->lines, 0, sizeof(p->lines));\n\n```\n\nNote that for non-char types you can only to do this to set all members to 0. For any other value you need a loop.\n\n\n"}
{"questionId":"c2932e0c62924a01bdbb4e3a9e4be30d","question":"How to allow all and any requests with Spring Security?\nI've just added Spring Security to my project. I've also added this configuration:\n\n\n\n```\n@Configuration\n@EnableWebSecurity\npublic class WebSecurityConfig extends WebSecurityConfigurerAdapter {\n\n    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        http.authorizeRequests().anyRequest().permitAll();\n    }\n\n}\n\n```\n\nbut now not all of my endpoints work. In fact only a single endpoint works, for the rest I get `403 Forbidden`. What could be the problem? How can I allow **any and all** requests (effectively making security a pass-through).\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"If you want to allow some URL to be accessed without authentication, it is a better practice to prepare some whitelist and pass it to the method `antMatchers()`.\n\n\nThe `antMathers()` accepts *wild cards* as well. If you surely don't want any of the endpoints to be authenticated put `\/**`. But you already have Spring Security, why not use the full power of it.\n\n\nHere is a simple way of doing it.\n\n\n\n```\nprivate static final String[] AUTH_WHITELIST = {\n   \"\/v2\/api-docs\", \"\/swagger-resources\", \"\/swagger-resources\/**\",\n};\n\n@Override\nprotected void configure(HttpSecurity http) throws Exception {\n    http.csrf().disable().authorizeRequests()\n            .antMatchers(AUTH_WHITELIST).permitAll()\n            .antMatchers(\"\/csrf\").permitAll()\n            .anyRequest().authenticated(); \n}\n\n```\n\n"}
{"questionId":"01d3d45f2b56441d94e1b9fba6bbf4bb","question":"what does register const char \\*const \\*name; mean and why is this variable outside of the function?\n\n```\nvoid\nusage (cpp)\n    register const char *const *cpp;\n{\n    (void) fprintf (stderr, *cpp++, program_name, cvs_cmd_name);\n    for (; *cpp; cpp++)\n    (void) fprintf (stderr, *cpp);\n    error_exit ();\n}\n\n```\n\nI don't get why register variable is not inside the curly brackets and what is this (void) in front of fprintf? Also register const char \\*const \\*cpp, i've never seen anything like this before\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c"},"answer":"The `register` keyword is a hint to the compiler that you'd like that value to be kept in a dedicated register on the processor. This can speed up reads and writes. With modern compilers, however, this sort of optimization is not only unnecessary but often counterproductive.\n\n\nThe reason it is between the function declaration and the block is that in old c (pre C90) you wouldn't declare parameter type next to the parameter but between the declaration of the function and the block.\n\n\nFor example:\n\n\n\n```\nint main(argc, argv)\nchar ** argv;\n{\n ...\n}\n\n```\n\nNotice I didn't do anything for `argc` because if you don't explicitly define a type it defaults to int.\n\n\nYou'll see this more often than you'd think. I ran into this a bunch when I did work on FFMPEG.\n\n\nThe (void) cast thing prevents unused parameter warnings\/errors. I've run into this when working on PortAudio with a low-level callback function.\n\n\n"}
{"questionId":"fcad66f224f04b0286278d1785af761d","question":"Identify the column name of the last occurrence of a value in R data frame\nI have a dataset like below with columns of 1s and 0s. I would like to add a final column that identifies the column name of the final occurrence of 0 per row.\n\n\n\n```\nhave = data.frame(a = c(1,0,1,1,0,0,1,1,1,0),\n                  b = c(1,0,1,1,1,0,1,1,0,0),\n                  c = c(0,0,0,1,0,1,1,1,1,0),\n                  d = c(1,0,1,1,0,0,0,1,0,1),\n                  e = c(1,1,1,1,1,1,1,1,1,1))\n> have\n   a b c d e\n1  1 1 0 1 1\n2  0 0 0 0 1\n3  1 1 0 1 1\n4  1 1 1 1 1\n5  0 1 0 0 1\n6  0 0 1 0 1\n7  1 1 1 0 1\n8  1 1 1 1 1\n9  1 0 1 0 1\n10 0 0 0 1 1\n\n```\n\nI would like the output to look like this where the final column specifies the column name of the last occurring 0 and if one does not exist return NA.\n\n\n\n```\n> want\n   a b c d e last_0\n1  1 1 0 1 1      c\n2  0 0 0 0 1      d\n3  1 1 0 1 1      c\n4  1 1 1 1 1   <NA>\n5  0 1 0 0 1      d\n6  0 0 1 0 1      d\n7  1 1 1 0 1      d\n8  1 1 1 1 1   <NA>\n9  1 0 1 0 1      d\n10 0 0 0 1 1      c\n\n```\n\nI've tried using max.col but it returns the last column name if a zero does not exist. Any other solutions? A dplyr solution is preferred.\n\n\n\n```\n> have$last_0 = names(have)[max.col(have == 0, ties.method = \"last\")]\n> have\n   a b c d e last_0\n1  1 1 0 1 1      c\n2  0 0 0 0 1      d\n3  1 1 0 1 1      c\n4  1 1 1 1 1      e\n5  0 1 0 0 1      d\n6  0 0 1 0 1      d\n7  1 1 1 0 1      d\n8  1 1 1 1 1      e\n9  1 0 1 0 1      d\n10 0 0 0 1 1      c\n\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"r"},"answer":"Here is an approach with `purrr::pmap`:\n\n\n\n```\nlibrary(dplyr);library(purrr)\nhave %>% \n   mutate(want = pmap_chr(cur_data(), \n                          ~ tail(c(NA,names(which(c(...)==0))),1)))\n   a b c d e want\n1  1 1 0 1 1    c\n2  0 0 0 0 1    d\n3  1 1 0 1 1    c\n4  1 1 1 1 1 <NA>\n5  0 1 0 0 1    d\n6  0 0 1 0 1    d\n7  1 1 1 0 1    d\n8  1 1 1 1 1 <NA>\n9  1 0 1 0 1    d\n10 0 0 0 1 1    c\n\n```\n\n`purrr:pmap` is a very useful function because it will work row wise on data and it comes in various flavors so you can control what returns. You can refer to the entire row of data with `c(...)`.\n\n\n\n\n---\n\n\nIf you wanted to apply the procedure to only a subset of columns, you might use `dplyr::select`:\n\n\n\n```\nhave %>% \n    mutate(want = pmap_chr(cur_data() %>% select(a,b,c), \n                           ~ tail(c(NA,names(which(c(...)==0))),1)))\n   a b c d e want\n1  1 1 0 1 1    c\n2  0 0 0 0 1    c\n3  1 1 0 1 1    c\n4  1 1 1 1 1 <NA>\n5  0 1 0 0 1    c\n6  0 0 1 0 1    b\n7  1 1 1 0 1 <NA>\n8  1 1 1 1 1 <NA>\n9  1 0 1 0 1    b\n10 0 0 0 1 1    c\n\n```\n\n"}
{"questionId":"047f8b2c396c42da81de2945a41a2929","question":"Build Apple Silicon binary on Intel machine\nHow can I compile a C project on macOS 11 (Intel) to work on \uf8ffSilicon?\n\n\nMy current build script is as simple as:\n\n\n\n```\n.\/configure\nmake\nsudo make install\n\n```\n\nI've tried using the `--host` and `--target` flags with `aarch64-apple-darwin` and `arm-apple-darwin` without any luck.\n\n\nThe binary always defaults to `x86_64`:\n\n\n\n```\n> file foobar.so\nfoobar.so: Mach-O 64-bit bundle x86_64\n\n```\n\n**UPDATE:**\nIt seems cc and gcc aren't found when `--host` is specified.\n\n\n\n```\nchecking for arm-apple-darwin-cc... no\nchecking for arm-apple-darwin-gcc... no\n\n```\n\n","questionMetadata":{"type":"implementation","level":"advanced","tag":"c"},"answer":"We ended up solving solving this and being able to compile `darwin-arm64` and `debian-aarch64` binaries on GitHub Actions' x86-64 machines.\n\n\nWe pre-compiled all our dependencies for arm64 and linked them statically as well as dynamically.\n\n\n\n```\nexport RELAY_DEPS_PATH=.\/build-deps\/arm64\nexport PKG_CONFIG_PATH=.\/build-deps\/arm64\/lib\/pkgconfig\n\ncd .\/relay-deps\nTARGET=.\/build-deps make install\n\ncd .\/relay\nphpize\n.\/configure CFLAGS='-target arm64-apple-macos' \\\n  --host=aarch64-apple-darwin \\\n  --enable-relay-jemalloc-prefix\n  [snip...]\n\nmake\n\n# Dynamically linked binary\ncc --target=arm64-apple-darwin \\\n  ${wl}-flat_namespace ${wl}-undefined ${wl}suppress \\\n  -o .libs\/relay.so -bundle .libs\/*.o \\\n  -L$RELAY_DEPS_PATH\/lib -lhiredis -ljemalloc_pic [snip...]\n\n# re-link to standard paths\n.\/relay-deps\/utils\/macos\/relink.sh .libs\/relay.so \/usr\/local\/lib\ncp .libs\/relay.so modules\/relay.so\n\n# Build a statically linked shared object\ncc --target=arm64-apple-darwin \\\n  ${wl}-flat_namespace ${wl}-undefined ${wl}suppress \\\n  -o .libs\/relay-static.so -bundle .libs\/*.o \\\n  $RELAY_DEPS_PATH\/lib\/libhiredis.a \\\n  $RELAY_DEPS_PATH\/lib\/libjemalloc_pic.a \\\n  [snip...]\n\n```\n\nThe `relink.sh`:\n\n\n\n```\n#!\/bin\/bash\nset -e\n\nprintUsage() {\n    echo \"$0 <shared-object> <prefix>\"\n    exit 1\n}\n\nif [[ ! -f \"$1\" || -z \"$2\" ]]; then\n    printUsage\n    exit 1\nfi\n\nINFILE=$1\nPREFIX=$2\n\nlinks=(libjemalloc libhiredis [snip...])\n\nif [ -z \"$PREFIX\" ]; then\n    PREFIX=libs\nfi\n\nfor link in ${links[@]}; do\n    FROM=$(otool -L \"$INFILE\"|grep $link|awk '{print $1}')\n    FILE=$(basename -- \"$FROM\")\n    TO=\"$PREFIX\/$FILE\"\n\n    echo \"$FROM -> $TO\"\n    install_name_tool -change \"$FROM\" \"$TO\" \"$1\"\ndone\n\n```\n\n"}
{"questionId":"21c7e7535f0b46b59cc4f370b52f01e3","question":"Using NativeCall to call the C fn `erf` gets more precise output than `erf` in C\nI have written a Raku script to call `erf` function in C standard library:\n\n\n\n```\nuse NativeCall;\nsub erf(num64) returns num64 is native { * };\n\nsay [0.5,1,2,3,4,-0.9].map: {erf($_.Num)};\n\n```\n\nThe output of this script\n\n\n\n```\n(0.5204998778130465 0.8427007929497149 0.9953222650189527 0.9999779095030014 0.9999999845827421 -0.7969082124228322)\n\n```\n\nmatches with the output from C for all values `[0.5,1,2,3,4,-0.9]` except `4`.\n\n\nFor `4` the C outputs `1.000000` while Raku gives `0.9999999845827421`.\n\n\nTo test the output for `4` in C, run this code:\n\n\n\n```\n#include <stdio.h> \/\/ Including header file for printf function\n#include <math.h>  \/\/ Including header file for erf function\n\nint main (){\n\n  double param, result;\n  param = 4.0;\n  result = erf(param);\n  printf(\"erf (%f) = %f\\n\", param, result);\n  return 0;\n}\n\n```\n\nAny idea what's going on? I need to output `1.0` from Raku too.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c"},"answer":"You're comparing apples with oranges.\n\n\n\n```\nuse NativeCall;\nsub erf(num64) returns num64 is native { * };\n\nsay .fmt(\"%f\") for [0.5,1,2,3,4,-0.9].map: {erf($_.Num)}\n\n0.520500\n0.842701\n0.995322\n0.999978\n1.000000\n-0.796908\n\n```\n\nYou're using `printf` in C, if you use `.fmt` (the easier way to say `sprintf` in Raku), then you'd also get `1.0`.\n\n\n"}
{"questionId":"dd548109671645418269db2b96f546ba","question":"Check that module implements behaviour\nI have a behaviour and a function that takes a list of modules that should implement that behaviour. I would like to check that each module passed in does in fact implement that behaviour. I can do that with `MyBehaviour.implemented_by?\/1` below, but I'm wondering if there's a more direct way about it.\n\n\n\n```\ndefmodule MyBehaviour do\n  @callback do_something(String.t(), String.t()) :: no_return()\n\n  def implemented_by?(module) do\n    :attributes\n    |> module.module_info()\n    |> Enum.member?({:behaviour, [__MODULE__]})\n  end\nend\n\n```\n\n**Is that the best way to check that?** I'm not finding anything in the docs or Elixir forum or anywhere.\n\n\n**Should I even be checking that?** Or should I just let the responsibility rest entirely upon the caller? Are behaviours more about \"I want to make sure I implement everything needed\" than \"I want everyone else to know I implement everything needed\"?\n\n\n**Is there a way to use behaviours as a type in typespecs?** Can my function spec say that args should implement my behaviour, or should I just use `module()\/atom()`?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"elixir"},"answer":"\nInteresting question. \n\n\n\n> \n> Are behaviours more about \"I want to make sure I implement everything needed\" than \"I want everyone else to know I implement everything needed\"?\n> \n> \n> \n\n\nMy understanding is that a behavior is a contract between a module author and the user of that module to say: \"I expect you to provide me with a module that can do all of these things\". So it is the responsibility of the module user to do that. \n\n\nThe fact that the keyword for a behaviour function is `@callback` seems to me to say that usually the module that defines the behaviour is also that module that will be consuming that behaviour (in other words, calling the callback). It seems to be the responsibility of the behaviour implementor to make sure it correctly implements the behaviour, with a compile-time check to help them out, but there is no run-time help for the user of a module that requires the behaviour to make sure they actually provided a valid implementation.\n\n\nYour solution to provide a run-time warning looks good to me - however it's possible to implement a behaviour without providing the `@behaviour` attribute, so it will not work well in that case.\n\n\nThere is a slightly more helpful error message if the behaviour implementor declared `@behaviour` in their code but ignored the compiler warning:\n\n\n\n> \n> warning: function foo\/0 required by behaviour ExpectBehaviour is not implemented (in module ClaimsItImplementsButDoesNot)\n> \n> \n> \n\n\n\n```\niex> ExpectBehaviour.use_behaviour(ClaimsItImplementsButDoesNot)\n** (UndefinedFunctionError) function ClaimsItImplementsButDoesNot.foo\/0 is undefined or private,\n   but the behaviour ExpectBehaviour expects it to be present\n\n```\n\nHowever this is not the case if you simply pass an unrelated module that doesn't implement the behaviour:\n\n\n\n```\niex> ExpectBehaviour.use_behaviour(DoesNotImplementOrClaimTo)\n** (UndefinedFunctionError) function DoesNotImplementOrClaimTo.foo\/0 is undefined or private\n\n```\n\n\n> \n> Is there a way to use behaviours as a type in typespecs?\n> \n> \n> \n\n\nA behaviour is not a type, it's a specification of a set of functions, and a module can implement multiple behaviours, so I don't think this makes sense. As mentioned above, it seems sensible to keep the use of a behaviour's callbacks constrained to the module where it's defined.\n\n\n"}
{"questionId":"a58fb69c3c7943a8addf041bfd9d451b","question":"Kotlin coroutine concurrency and cached value\nI'm currently trying to create a caching layer around a web request. So far I've written:\n\n\n\n```\nclass Repository(private val webServices: WebServices) {\n  private var cachedItems: List<Item>? = null\n\n  suspend fun getItems(): List<Item> {\n    cachedItems?.let { return it }\n\n    val items = withContext(Dispatchers.IO) { webServices.getItems() }\n    cachedItems = items\n    return items\n  }\n}\n\n```\n\nMy concern is what will happen when `getItems()` is invoked by two callers simultaneously. Ideally, I'd only want one web request to occur. What's the recommended approach for dealing with this issue when using coroutines?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"kotlin"},"answer":"Here's a simple solution.\n\n\n\n```\nclass Repository(private val webServices: WebServices) {\n  private val cachedItems = async(Dispatchers.IO, start = CoroutineStart.LAZY) {\n    webServices.getItems()\n  }\n\n  suspend fun getItems(): List<Item> {\n    return cachedItems.await()\n  }\n}\n\n```\n\n"}
{"questionId":"86a3440467d8469a98cb3704a2cf99ae","question":"How to make an OwnsOne property in EF Core 3.0 required when mapping to SQL Server columns?\nI have a main entity Profile that has a property Name that is a value object. The Name object has two properties First and Last. When I use the Fluent API to map the Name objects properties to columns within the Profile table I specify that they are required. When I create the migration it says nullable is true. I assume it has to do with the fact that in EF Core 3.0 owned entities are now optional but how do I tell EF that they are actually required? \n\n\n\n```\npublic class Profile\n{\n   public Name Name { get; private set; }\n   ...\n}\n\n```\n\n\n```\npublic class Name\n{\n   public string First { get; }\n   public string Last { get; }\n   ...\n}\n\n```\n\n\n```\npublic override void Configure(EntityTypeBuilder<Profile> builder)\n{\n   base.Configure(builder);\n\n   builder.OwnsOne(\n                navigationExpression: p => p.Name,\n                buildAction: n =>\n                {\n                    n.Property(n => n.First)\n                        .HasColumnName(\"NameFirst\")\n                        .HasMaxLength(25)\n                        .IsRequired();\n\n                    n.Property(n => n.Last)\n                        .HasColumnName(\"NameLast\")\n                        .HasMaxLength(25)\n                        .IsRequired();\n                });\n}\n\n```\n\nAny help you can provide would be great.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"I reached out to the EF Core team and currently the only way to do this would be to manually change the migration that is created to set nullable = false. It has been flagged as a feature request so let's hope one day they get it fixed!\n\n\n"}
{"questionId":"51b857018bf84908945672e8e32ae802","question":"How to count all stored procedures in the SQL Server for a database?\nHow can I count all stored procedures which are written by me in my database?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"sql"},"answer":"As the OP pointed out in a comment, **all** of the earlier answers are wrong, because they include **system** procedures. He specifically asked for procedures that were \"written by me\" -- and later clarified in another comment \"*other than the system procedure, written by me or anybody working on that data base*.\"\n\n\nSo to exclude system procedures, the only differentiating field I see in sys.procedures is the name. Therefore you need to add a WHERE clause to any of the other answers, like this:\n\n\n\n```\nselect count(*) from sys.procedures\nwhere name not like 'sp_%'\n\n```\n\n"}
{"questionId":"aae07526e8aa45068a5067958ff1346d","question":"How to skip the first row when reading a csv file?\nI have an awkward csv file and I need to skip the first row to read it.\n\n\nI'm doing this easily with python\/pandas\n\n\n\n```\ndf = pd.read_csv(filename, skiprows=1)\n\n```\n\nbut I don't know how to do it in Go.\n\n\n\n```\npackage main\n\nimport (\n    \"encoding\/csv\"\n    \"fmt\"\n    \"log\"\n    \"os\"\n)\n\ntype mwericsson struct {\n    id     string\n    name   string\n    region string\n}\n\nfunc main() {\n\n    rows := readSample()\n\n    fmt.Println(rows)\n    \/\/appendSum(rows)\n    \/\/writeChanges(rows)\n}\n\nfunc readSample() [][]string {\n    f, err := os.Open(\"D:\/in\/20190629\/PM_IG30014_15_201906290015_01.csv\")\n    if err != nil {\n        log.Fatal(err)\n    }\n    rows, err := csv.NewReader(f).ReadAll()\n    f.Close()\n    if err != nil {\n        log.Fatal(err)\n    }\n    return rows\n}\n\n```\n\nError:\n\n\n\n```\n2019\/07\/01 12:38:40 record on line 2: wrong number of fields\n\n```\n\n`PM_IG30014_15_201906290015_01.csv`:\n\n\n\n```\nPTN Ethernet-Port RMON Performance,PORT_BW_UTILIZATION,2019-06-29 20:00:00,33366     \nDeviceID,DeviceName,ResourceName,CollectionTime,GranularityPeriod,PORT_RX_BW_UTILIZATION,PORT_TX_BW_UTILIZATION,RXGOODFULLFRAMESPEED,TXGOODFULLFRAMESPEED,PORT_RX_BW_UTILIZATION_MAX,PORT_TX_BW_UTILIZATION_MAX\n3174659,H1095,H1095-11-ISM6-1(to ZJBSC-V1),2019-06-29 20:00:00,15,22.08,4.59,,,30.13,6.98\n3174659,H1095,H1095-14-ISM6-1(to T6147-V),2019-06-29 20:00:00,15,2.11,10.92,,,4.43,22.45\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"go"},"answer":"\n> \n> skip the first row when reading a csv file\n> \n> \n> \n\n\n\n\n---\n\n\nFor example,\n\n\n\n```\npackage main\n\nimport (\n    \"bufio\"\n    \"encoding\/csv\"\n    \"fmt\"\n    \"io\"\n    \"os\"\n)\n\nfunc readSample(rs io.ReadSeeker) ([][]string, error) {\n    \/\/ Skip first row (line)\n    row1, err := bufio.NewReader(rs).ReadSlice('\\n')\n    if err != nil {\n        return nil, err\n    }\n    _, err = rs.Seek(int64(len(row1)), io.SeekStart)\n    if err != nil {\n        return nil, err\n    }\n\n    \/\/ Read remaining rows\n    r := csv.NewReader(rs)\n    rows, err := r.ReadAll()\n    if err != nil {\n        return nil, err\n    }\n    return rows, nil\n}\n\nfunc main() {\n    f, err := os.Open(\"sample.csv\")\n    if err != nil {\n        panic(err)\n    }\n    defer f.Close()\n    rows, err := readSample(f)\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(rows)\n}\n\n```\n\nOutput:\n\n\n\n```\n$ cat sample.csv\none,two,three,four\n1,2,3\n4,5,6\n$ go run sample.go\n[[1 2 3] [4 5 6]]\n$ \n\n$ cat sample.csv\nPTN Ethernet-Port RMON Performance,PORT_BW_UTILIZATION,2019-06-29 20:00:00,33366     \nDeviceID,DeviceName,ResourceName,CollectionTime,GranularityPeriod,PORT_RX_BW_UTILIZATION,PORT_TX_BW_UTILIZATION,RXGOODFULLFRAMESPEED,TXGOODFULLFRAMESPEED,PORT_RX_BW_UTILIZATION_MAX,PORT_TX_BW_UTILIZATION_MAX\n3174659,H1095,H1095-11-ISM6-1(to ZJBSC-V1),2019-06-29 20:00:00,15,22.08,4.59,,,30.13,6.98\n3174659,H1095,H1095-14-ISM6-1(to T6147-V),2019-06-29 20:00:00,15,2.11,10.92,,,4.43,22.45\n$ go run sample.go\n[[DeviceID DeviceName ResourceName CollectionTime GranularityPeriod PORT_RX_BW_UTILIZATION PORT_TX_BW_UTILIZATION RXGOODFULLFRAMESPEED TXGOODFULLFRAMESPEED PORT_RX_BW_UTILIZATION_MAX PORT_TX_BW_UTILIZATION_MAX] [3174659 H1095 H1095-11-ISM6-1(to ZJBSC-V1) 2019-06-29 20:00:00 15 22.08 4.59   30.13 6.98] [3174659 H1095 H1095-14-ISM6-1(to T6147-V) 2019-06-29 20:00:00 15 2.11 10.92   4.43 22.45]]\n$\n\n```\n\n"}
{"questionId":"4c60285edecb4658a0060d7f5658198d","question":"ValueError: Mountpoint must not contain a space. (Colab)\nHere is my code in google colab:\n\n\n\n```\nfrom google.colab import drive\ndrive.mount('content\/drive\/My Drive\/ML')\n\n```\n\nI have a path which contains space symbol and I get this error:\n\n\n\/usr\/local\/lib\/python3.6\/dist-packages\/google\/colab\/drive.py in mount(mountpoint, force\\_remount, timeout\\_ms)\n 89 \n 90 if ' ' in mountpoint:\n---> 91 raise ValueError('Mountpoint must not contain a space.')\n 92 \n 93 mountpoint = \\_os.path.expanduser(mountpoint)\n\n\n\n> \n> ValueError: Mountpoint must not contain a space.\n> \n> \n> \n\n\nI have tried drive.mount('content\/drive\/My\\ Drive\/ML') and this doesn't work\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"python"},"answer":"Run instead:\n\n\n\n```\nfrom google.colab import drive\ndrive.mount('\/content\/drive')\n\n```\n\nThe leading `\/` is important. Once mounted at `\/content\/drive`, you'll see `My Drive\/ML` in that directory. `\/content\/drive` is the directory path on your local machine. `My Drive\/ML` is the path within your Drive. (`My Drive` distinguishes your Drive from Team drives.)\n\n\n"}
{"questionId":"c90dcc734fa24f3bb07aacf3204279ad","question":"How to exclude a property from being serialized in System.Text.Json.JsonSerializer.Serialize() using a JsonConverter\nI want to be able to exclude a property when serializing using System.Text.Json.JsonSerializer. I don't want to use a `JsonIgnore` attribute everywhere I want to do this. I would like to be able to define the properties I want to exclude during serialization only, via some kind of Fluent API, which currently does not exist.\n\n\nThe only option I was able to find is to define a `JsonConverter` and add it to the list of Converters on the `JsonSerializerOptions` that I pass to the Serialize() method like so:\n\n\n\n```\nvar options = new JsonSerializerOptions();\noptions.Converters.Add(new BookConverter());\njson = JsonSerializer.Serialize(book, options);\n\n```\n\nIn the JsonConverter I would have to write the entire JSON representation myself using a `Utf8JsonWriter`, excluding the property I don't want to serialize. This is a lot of work to just be able to exclude a property. While the JsonConverter is a great extensibility feature from the .NET team, its just too low-level for my use case. Does anyone know of any other way to acheive the exclusion of the property without having to write out the JSON representation myself?\n\n\nI don't want to have to do the following:\n\n\n- Use an attribute, or dynamically add an attribute at runtime\n- Change the access modifier of the property to something like `private` or `protected`\n- Use a 3rd party library, as my issue is solvable if I use Json.NET.\n\n\nExample:\n\n\n\n```\nclass Program\n{\n    void Main()\n    {\n        \/\/ We want to serialize Book but to ignore the Author property\n        var book = new Book() { Id = 1, Name = \"Calculus\", Author = new Author() };\n\n        var json = JsonSerializer.Serialize(book);\n        \/\/ Default serialization, we get this:\n        \/\/ json = { \"Id\": 1, \"Name\": \"Calculus\", \"Author\": {} }\n\n        \/\/ Add our custom converter to options and pass it to the Serialize() method\n        var options = new JsonSerializerOptions();\n        options.Converters.Add(new BookConverter());\n        json = JsonSerializer.Serialize(book, options);\n        \/\/ I want to get this:\n        \/\/ json = { Id: 1, Name: \"Calculus\" }\n    }\n}\n\npublic class Author { }\n\npublic class Book\n{\n    public int Id { get; set; }\n    public string Name { get; set; }\n    public Author Author { get; set; }\n}\n\npublic class BookConverter : JsonConverter<Book>\n{\n    public override Book Read(ref Utf8JsonReader reader, Type typeToConvert, JsonSerializerOptions options)\n    {\n        \/\/ Use default implementation when deserializing (reading)\n        return JsonSerializer.Deserialize<Book>(ref reader, options);\n    }\n\n    public override void Write(Utf8JsonWriter writer, Book value, JsonSerializerOptions options)\n    {\n        \/\/ Serializing. Here we have to write the JSON representation ourselves\n        writer.WriteStartObject();\n\n        writer.WriteNumber(\"Id\", value.Id);\n        writer.WriteString(\"Name\", value.Name);\n        \/\/ Don't write Author so we can exclude it\n\n        writer.WriteEndObject();\n    }\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"So I happened to stumble upon an article that demonstrates how to use the `JsonDocument` object in the new `System.Text.Json` namespace and it is the next best thing to a Fluent API. Here is how this question can be solved.\n\n\nThe BookConverter.Write() method:\n\n\n\n```\npublic override void Write(Utf8JsonWriter writer, Book value, JsonSerializerOptions options)\n{\n    writer.WriteStartObject();\n\n    using (JsonDocument document = JsonDocument.Parse(JsonSerializer.Serialize(value)))\n    {\n        foreach (var property in document.RootElement.EnumerateObject())\n        {\n            if (property.Name != \"Author\")\n                property.WriteTo(writer);\n        }\n    }\n\n    writer.WriteEndObject();\n}\n\n```\n\n"}
{"questionId":"6b9227744fc1496898bc5b62c59ccea3","question":"Convert dictionaries with list of values into a dataframe\nSay I have three dictionaries\n\n\n\n```\ndictionary_col2\n{'MOB': [1, 2], 'ASP': [1, 2], 'YIP': [1, 2]}\n\n```\n\n\n```\n dictionary_col3\n\n{'MOB': ['MOB_L001_R1_001.gz',\n         'MOB_L002_R1_001.gz'],\n 'ASP': ['ASP_L001_R1_001.gz',\n         'ASP_L002_R1_001.gz'],\n 'YIP': ['YIP_L001_R1_001.gz',\n         'YIP_L002_R1_001.gz']}\n\n\n```\n\n\n```\ndictionary_col4\n\n{'MOB': ['MOB_L001_R2_001.gz',\n         'MOB_L002_R2_001.gz'],\n 'ASP': ['ASP_L001_R2_001.gz',\n         'ASP_L002_R2_001.gz'],\n 'YIP': ['YIP_L001_R2_001.gz',\n         'YIP_L002_R2_001.gz']}\n\n\n```\n\nI wanna convert the above dictionaries into a data frame. I have tried the following,\n\n\n`df = pd.DataFrame([dictionary_col2, dictionary_col3, dictionary_col4])`\nThe `df` data frame looks like,\n\n\n\n```\n                ASP MOB YIP\n0   [1, 2]  [1, 2]  [1, 2]\n1   [ASP_L001_R1_001.gz, ASP_L002_R1_001.gz]    [MOB_L001_R1_001.gz, MOB_L002_R1_001.gz]    [YIP_L001_R1_001.gz, YIP_L002_R1_001.gz]\n2   [ASP_L001_R2_001.gz, ASP_L002_R2_001.gz]    [MOB_L001_R2_001.gz, MOB_L002_R2_001.gz]    [YIP_L001_R2_001.gz, YIP_L002_R2_001.gz]\n\n```\n\nMy aim is to have a data frame with the following columns: \n\n\n\n```\n    col1  col2 col3              col4 \n    MOB   1   MOB_L001_R1_001.gz MOB_L001_R2_001.gz      \n    MOB   2   MOB_L002_R1_001.gz MOB_L002_R2_001.gz \n    ASP   1   ASP_L001_R1_001.gz ASP_L001_R2_001.gz \n    ASP   2   ASP_L002_R1_001.gz MOB_L002_R2_001.gz \n    YIP   1   YIP_L001_R1_001.gz YIP_L001_R2_001.gz\n    YIP   2   YIP_L002_R1_001.gz YIP_L002_R2_001.gz\n\n```\n\nAny help\/suggestions are appreciated!!\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"\n```\npd.DataFrame({'col2': pd.DataFrame(col2).unstack(),\n              'col3': pd.DataFrame(col3).unstack(),\n              'col4': pd.DataFrame(col4).unstack()}).reset_index(level=0)\n\n```\n\nreturns\n\n\n\n```\n  level_0  col2                col3                col4\n0     ASP     1  ASP_L001_R1_001.gz  ASP_L001_R2_001.gz\n1     ASP     2  ASP_L002_R1_001.gz  ASP_L002_R2_001.gz\n0     MOB     1  MOB_L001_R1_001.gz  MOB_L001_R2_001.gz\n1     MOB     2  MOB_L002_R1_001.gz  MOB_L002_R2_001.gz\n0     YIP     1  YIP_L001_R1_001.gz  YIP_L001_R2_001.gz\n1     YIP     2  YIP_L002_R1_001.gz  YIP_L002_R2_001.gz\n\n```\n\n"}
{"questionId":"fa8ed374330a473c80507b807ee30769","question":"Keep the data with latest date from a pandas dataframe\nI have a csv file with company software details along with installed dates. I am reading the csv file using pandas in a dataframe. Below is the sample data containing two different software:\n\n\n\n```\nsoftware_id software_name                               installed_date    software_version\n8331        Intel(R) Graphics Media Accelerator Driver  2009-05-23 0:00   8.15.10.2008\n8331        Intel(R) Graphics Media Accelerator Driver  2010-09-15 0:00   8.15.10.2008\n8331        Intel(R) Graphics Media Accelerator Driver  2009-12-27 0:00   8.15.10.2008\n8332        Wireless Switch Utility                     2009-12-22 0:00   4.3.1400.0\n8332        Wireless Switch Utility                     2010-11-22 0:00   4.3.1400.0\n8332        Wireless Switch Utility                     2011-01-25 0:00   4.3.1400.0\n\n```\n\nSo from the above data I just need to keep one row with the latest date from each software code. For example, the output of the above file should be:\n\n\n\n```\nsoftware_id     software_name                               installed_date    software_version\n    8331        Intel(R) Graphics Media Accelerator Driver  2010-09-15 0:00   8.15.10.2008\n    8332        Wireless Switch Utility                     2011-01-25 0:00   4.3.1400.0\n\n```\n\nHow can I choose the set of rows for one software code and remove all rows but the one with the latest date and then move to the next software code until the file only has one entry per software code. I cannot hardcode the software\\_id to check as there are thousands of them.\n\n\nMy logic is to read and store the first software\\_id and installed\\_date in two variables and start reading the file line by line. Next line will check if the software\\_id matches the one stored in the variable and then compare the date and store the latest one in the variable.\nWhen the software\\_id does not match the stored software id then it means that new software\\_id block has started. It will then store the previous values in the dataframe and start executing next blocks and so on.\n\n\nFYI - I am a pandas noob.\n\n\nThanks for all the help.\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"python"},"answer":"First you need to convert the `installed_date` column to `datetime`:\n\n\n\n```\ndf['installed_date'] = pd.to_datetime(df['installed_date'])\n\n```\n\nThen you can use one of the options below:\n\n\n**Option 1:** `sort` the values on `installed_date` then `drop_duplicates` keeping only the last row per `software_id`.\n\n\n\n```\ndf.sort_values('installed_date').drop_duplicates('software_id', keep='last')\n\n```\n\n**Option 2:** `group` the dataframe on `softaware_id` and aggregate using `idxmax` to get the index of most recent date per `software_id` group, then use `loc` with this index to filter the required rows:\n\n\n\n```\nidx = df.groupby('software_id')['installed_date'].idxmax()\ndf.loc[idx]\n\n```\n\n**Result:**\n\n\n\n```\n   software_id                               software_name installed_date software_version\n1         8331  Intel(R) Graphics Media Accelerator Driver     2010-09-15     8.15.10.2008\n5         8332                     Wireless Switch Utility     2011-01-25       4.3.1400.0\n\n```\n\n"}
{"questionId":"3b7cf4dc6145437683b02bff5521a24a","question":"How to add a string literal as one selected column for select queries?\nI want to have query like below\n\n\n\n```\nselect \"RETRY\" as code , name from process ;\n\n```\n\nthen the results are\n\n\n\n```\ncode |  name\n_____________\n\nRETRY  PX1\nRETRY  PX1\nRETRY  PX3\nRETRY  PX4\nRETRY  PX5\n\n```\n\nI want to add one string literal as column for all rows returned by select query. I am trying this in PostgreSQL, but getting the following error:\n\n\n\n```\nSQL Error [42703]: ERROR: column \"RETRY\" does not exist\n  Position: 8\n\n```\n\nyny idea how to do this in a PostgreSQL select query?\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"sql"},"answer":"double quote refers column name of that table thats why you are getting error you have to use single quote \n\n\n\n```\nselect 'RETRY' as code , name from process ;\n\n```\n\n"}
{"questionId":"0b3376dc15a840389eddb95591a3266b","question":"Is there a way to pass multiple values to macro function as single defined macro value in C?\nI want to declare pin definition in global header as a simple line like:\n\n\n\n```\n#define STATUS_LED B,7\n\n```\n\nThen I want to pass this pin definition to function above:\n\n\n\n```\nCMBset_out(STATUS_LED);\n\n```\n\nI don't know how to approach this - MY\\_PIN is in proper format to be replaced during precompilation phase.\n\n\n\n```\n#define CMBsbi(port, pin) (PORT##port) |= (1<<pin)\n#define CMBset_out(port,pin) (DDR##port) |= (1<<pin)\n\/\/ define pins \n#define STATUS_LED B,7\n\n```\n\nThen, I want to pass this pin definition to function above (`hw_init_states()` is declared in the same header file called from main C file):\n\n\n\n```\n\/\/ runtime initialization\nvoid hw_init_states(){\n#ifdef STATUS_LED\n    CMBset_out(STATUS_LED);\n#endif\n}\n\n```\n\nBut I get a compilation error:\n\n\n\n```\nError   1   macro \"CMBset_out\" requires 2 arguments, but only 1 given   GENET_HW_DEF.h  68  23  Compass IO_proto\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c"},"answer":"An improvement upon the previous answer, which also allows you to call the macro with two explicit arguments.\n\n\nIt should work with any c99 (or better) compiler:\n\n\n\n```\n#define CMBset_out_X(port,pin) (DDR##port) |= (1<<pin)\n#define CMBset_out(...) CMBset_out_X(__VA_ARGS__)\n\n#define STATUS_LED B,7\nCMBset_out(STATUS_LED)\nCMBset_out(B, 7)\n\n```\n\n"}
{"questionId":"60522c37d1b74e63b6e3d9acf7f3637a","question":".NET core X509Store on linux\nWhere are the certificate files located in linux when using the .NET Core 2 `X509Store`?\n\n\nOn Windows, the certificates are accessible from the management console `certlm.msc` or with `New-SelfSignedCertificate` in powershell. Using .NET APIs, certificates can be added by something like this on both Windows and linux\n\n\n\n```\nusing (var store = new X509Store(StoreName.My, StoreLocation.CurrentUser))\n{\n    store.Open(OpenFlags.ReadWrite);\n    var cert = new X509Certificate2(\"cert.pfx\", \"1234\");\n    store.Add(cert);\n}\n\n```\n\nwhich can be accessed via `X509Store.Certificates.Find()`.\n\n\nBut where do the files get stored and how can they be added via linux tools? e.g. a sys admin would be adding the certificates and an application will be only reading them.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"~\/.dotnet\/corefx\/cryptography\/x509stores\/\n\n\n"}
{"questionId":"8b7ada02d3fb4d0fa962bf6092977155","question":"I need to change the type of few columns in a pandas dataframe. Can't do so using iloc\nIn a dataframe with around 40+ columns I am trying to change dtype for first 27 columns from float to int by using iloc:\n\n\n\n```\ndf1.iloc[:,0:27]=df1.iloc[:,0:27].astype('int')\n\n```\n\nHowever, it's not working. I'm not getting any error, but dtype is not changing as well. It still remains float.\n\n\n**Now the strangest part:**\n\n\nIf I first change dtype for only 1st column (like below):\n\n\n\n```\ndf1.iloc[:,0]=df1.iloc[:,0].astype('int')\n\n```\n\nand then run the earlier line of code:\n\n\n\n```\ndf1.iloc[:,0:27]=df1.iloc[:,0:27].astype('int')\n\n```\n\nIt works as required.\nAny help to understand this and solution to same will be grateful.\n\n\nThanks!\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"I guess it is a bug in `1.0.5`. I tested on my `1.0.5`. I have the same issue as yours. The `.loc` also has the same issue, so I guess pandas devs break something in `iloc\/loc`. You need to update to latest pandas or use a workaround. If you need a workaround, using assignment as follows\n\n\n\n```\ndf1[df1.columns[0:27]] = df1.iloc[:, 0:27].astype('int')\n\n```\n\nI tested it. Above way overcomes this bug. It will turn first 27 columns to dtype `int32`\n\n\n"}
{"questionId":"2cba8d195a904f5296a4d0dcca711f71","question":"How do I execute multiple commands in parallel on an array of parameters with bash, and fail if at least one of them failed\nI have a bash script with a function that needs to run in parallel with different arguments.\nI need to know if at least one of the executions failed (returned non-zero) - doesn't matter how many failed.\n\n\nThe command accepts an array of parameters for the execution. \nI need to limit the concurrency to 4 concurrent runs due to high load.\nI also need to print the logs in the parent process (the one that runs the bash script)\n\n\nthis is the function I'm running:\n\n\n\n```\nfunction run_and_retry {\n  EXIT_STATUS=0\n  $COMMAND || EXIT_STATUS=$?\n\n  if [ $EXIT_STATUS -ne 0 ]; then\n    EXIT_STATUS=0\n    $COMMAND || EXIT_STATUS=$?\n\n  fi\n\n  return $EXIT_STATUS\n}\n\n```\n\nI've tried using GNU parallel and xargs and encountered issues with both. \n\n\nWith xargs: (couldn't get the exit status out of it, and it also didn't work when I ran it in TravisCI)\n\n\n\n```\nPARAMETERS=(first-parameter second-parameter third-parameter)\nexport -f run_and_retry\necho \"${PARAMETERS[@]}\" | xargs -P 4 -n 1 -I {} bash -c \"run_and_retry {}\"\n\n```\n\nWith GNU parallel:\n\n\n\n```\nPARAMETERS=(first-parameter second-parameter third-parameter)\nexport -f run_and_retry\nparallel -j 4 -k --lb 2 run_and_retry {} ::: echo \"${PARAMETERS[@]}\" \n\n```\n\n","questionMetadata":{"type":"implementation","level":"advanced","tag":"bash\/shell"},"answer":"You are *so* close to getting the syntax of GNU Parallel correct:\n\n\n\n```\nCOMMAND=echo\nPARAMETERS=(first-parameter second-parameter third-parameter)\nparallel -j 4 -k --retries 2 \"$COMMAND\" {} ::: \"${PARAMETERS[@]}\" ||\n  echo $? commands failed. More than 99 if $? = 100\n\n```\n\nOr if you really insist on doing the retrying yourself:\n\n\n\n```\nPARAMETERS=(first-parameter second-parameter third-parameter)\nexport -f run_and_retry\nparallel -j 4 -k run_and_retry {} ::: \"${PARAMETERS[@]}\" ||\n  echo One or more commands failed\n\n```\n\n"}
{"questionId":"6e4c439a3bb743d684eb0dec5ce6cbe7","question":"Hive partitioned table reads all the partitions despite having a Spark filter\nI'm using spark with scala to read a specific Hive partition. The partition is `year`, `month`, `day`, `a` and `b`\n\n\n`scala> spark.sql(\"select * from db.table where year=2019 and month=2 and day=28 and a='y' and b='z'\").show`\n\n\nBut I get this error:\n\n\n\n> \n> org.apache.spark.SparkException: Job aborted due to stage failure: Task 236 in stage 0.0 failed 4 times, most recent failure: Lost task 236.3 in stage 0.0 (TID 287, server, executor 17): org.apache.hadoop.security.AccessControlException: Permission denied: user=user, access=READ, inode=\"\/path-to-table\/table\/year=2019\/month=2\/**day=27**\/a=w\/b=x\/part-00002\":user:group:-rw-rw----\n> \n> \n> \n\n\nAs you can see, spark is trying to read a different partition and I don't have permisions there.\n\n\nIt shouldn't be, because I created a filter and this filter is my partition.\n\n\nI tried the same query with Hive and it's works perfectly (No access problems)\n\n\n`Hive> select * from db.table where year=2019 and month=2 and day=28 and a='y' and b='z';`\n\n\nWhy is spark trying to read this partition and Hive doesn't?\n\n\nThere is a Spark configuration that am I missing?\n\n\n**Edit: More information**\n\n\nSome files were created with Hive, others were copied from one server and pasted to our server with different permissions (we can not change the permissions), then they should have refreshed the data. \n\n\nWe are using:\n`cloudera 5.13.2.1`\n`hive 1.1.0`\n`spark 2.3.0`\n`hadoop 2.6.0`\n`scala 2.11.8`\n`java 1.8.0_144`\n\n\n**Show create table**\n\n\n\n```\n|CREATE EXTERNAL TABLE Columns and type\nPARTITIONED BY (`year` int COMMENT '*', `month` int COMMENT '*', `day` int COMMENT '*', `a` string COMMENT '*', `b` string COMMENT '*')\nROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\nWITH SERDEPROPERTIES (\n 'serialization.format' = '1'\n)\nSTORED AS\n INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'\n OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\nLOCATION 'hdfs:\/\/path'\nTBLPROPERTIES (\n 'transient_lastDdlTime' = '1559029332'\n)\n|\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"scala"},"answer":"A parquet hive table in Spark can use following 2 read flows -\n\n\n1. Hive flow -\nThis will be used when `spark.sql.hive.convertMetastoreParquet` is set to `false`. For partitioning pruining to work in this case, you have to set `spark.sql.hive.metastorePartitionPruning=true`.\n\n\n\n> \n> spark.sql.hive.metastorePartitionPruning: When true, some predicates\n>  will be pushed down into the Hive metastore so that unmatching\n>  partitions can be eliminated earlier. This only affects Hive tables\n>  not converted to filesource relations (see\n>  HiveUtils.CONVERT\\_METASTORE\\_PARQUET and\n>  HiveUtils.CONVERT\\_METASTORE\\_ORC for more information\n> \n> \n>\n2. Datasource flow - This flow by default has partition pruning turned on.\n\n\n"}
{"questionId":"4e75b9653edc4dd2ab2eb7b3d2bd785a","question":"How do I raise X to the power of Y in Powershell?\nI (apparently wrongly) assumed there would be a built-in power `operator` or `function` in Powershell, but I it seems there is not, or is there?\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"bash\/shell"},"answer":"Never mind, I found the answer:\n\n\n\n```\n[Math]::Pow(256,3) # 256 to the power of 3 (= ~16.7 million)\n\n```\n\n"}
{"questionId":"e1c0159ffc1f4981b6cb9bea422cbbb1","question":"How to hide the bottom navigation bar in certain fragments?\nI have An activity with a navGraph and a bottom Navigation bar with 2 menu items.\nMy problem is that My Bottom Navigation Bar appears everywhere, detailFragment, aboutFragment, signInFragment and so on.\n\n\n\n```\n\n        val navController = this.findNavController(R.id.myNavHostFragment)\n\n        val appBarConfiguration = AppBarConfiguration.Builder(\n            R.id.contactsFragment,\n            R.id.profileFragment\n        ).build()\n\n        NavigationUI.setupActionBarWithNavController(this, navController, appBarConfiguration)\n\n        val navView: BottomNavigationView = findViewById(R.id.nav_view)\n        NavigationUI.setupWithNavController(navView, navController)\n\n\n\n```\n\nHow do i Limit it to just show on the 2 fragments on my menu Item?\n\n\n**This is how I solved It**\n\n\n\n```\n    navController.addOnDestinationChangedListener{ _, nd: NavDestination, _->\n        if(nd.id == R.id.contactsFragment || nd.id == R.id.profileFragment){\n            navView.visibility = View.VISIBLE\n        }else{\n            navView.visibility = View.GONE\n        }\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"kotlin"},"answer":"For your fragment where it should be visible\n\n\n\n```\nnavView.visibility = View.VISIBLE\n\n```\n\nWhere it shouldn't be visible\n\n\n\n```\nnavView.visibility = View.GONE\n\n```\n\n"}
{"questionId":"416f329b1000487f878b7e28a49fa595","question":"AttributeError:'bytes' object has no attribute 'encode'\nTrying to import a code from python2 to python 3 and this problem happens\n\n\n\n```\n    <ipython-input-53-e9f33b00348a> in aesEncrypt(text, secKey)\n     43 def aesEncrypt(text, secKey):\n     44     pad = 16 - len(text) % 16\n---> 45     text = text.encode(\"utf-8\") + (pad * chr(pad)).encode(\"utf-8\")\n     46     encryptor = AES.new(secKey, 2, '0102030405060708')\n     47     ciphertext = encryptor.encrypt(text)\n\n```\n\n\n> \n> AttributeError:'bytes' object has no attribute 'encode'\n> \n> \n> \n\n\nIf I remove `.encode(\"utf-8\")` the error is \"can't concat str to bytes\". Apparently `pad*chr(pad)` seems to be a byte string. It cannot use `encode()`\n\n\n\n```\n    <ipython-input-65-9e84e1f3dd26> in aesEncrypt(text, secKey)\n     43 def aesEncrypt(text, secKey):\n     44     pad = 16 - len(text) % 16\n---> 45     text = text.encode(\"utf-8\") + (pad * chr(pad))\n     46     encryptor = AES.new(secKey, 2, '0102030405060708')\n     47     ciphertext = encryptor.encrypt(text)\n\n```\n\n\n> \n> TypeError: can't concat str to bytes\n> \n> \n> \n\n\nHowever, the weird thing is that if i just try the part along. encode() works fine.\n\n\n\n```\ntext = { 'username': '', 'password': '', 'rememberLogin': 'true' }\ntext=json.dumps(text)\nprint(text)\npad = 16 - len(text) % 16 \nprint(type(text))\ntext = text + pad * chr(pad) \nprint(type(pad * chr(pad)))\nprint(type(text))\ntext = text.encode(\"utf-8\") + (pad * chr(pad)).encode(\"utf-8\") \nprint(type(text))\n\n{\"username\": \"\", \"password\": \"\", \"rememberLogin\": \"true\"}\n<class 'str'>\n<class 'str'>\n<class 'str'>\n<class 'bytes'>\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"If you don't know if a stringlike object is a Python 2 string (bytes) or Python 3 string (unicode). You could have a generic converter.\n\n\nPython3 shell:\n\n\n\n```\n>>> def to_bytes(s):\n...     if type(s) is bytes:\n...         return s\n...     elif type(s) is str or (sys.version_info[0] < 3 and type(s) is unicode):\n...         return codecs.encode(s, 'utf-8')\n...     else:\n...         raise TypeError(\"Expected bytes or string, but got %s.\" % type(s))\n...         \n>>> to_bytes(\"hello\")\nb'hello'\n>>> to_bytes(\"hello\".encode('utf-8'))\nb'hello'\n\n```\n\nOn Python 2 both these expressions evaluate to `True`: `type(\"hello\") == bytes` and `type(\"hello\") == str`. And `type(u\"hello\") == str` evaluates to `False`, while `type(u\"hello\") == unicode` is `True`.\n\n\nOn Python 3 `type(\"hello\") == bytes` is `False`, and `type(\"hello\") == str` is `True`. And `type(\"hello\") == unicode` raises a `NameError` exception since `unicode` isn't defined on 3.\n\n\nPython 2 shell:\n\n\n\n```\n>>> to_bytes(u\"hello\")\n'hello'\n>>> to_bytes(\"hello\")\n'hello'\n\n```\n\n"}
{"questionId":"057401e5a453453d8bb7c56106952517","question":"Which exception should be raised when a required environment variable is missing?\nThe title pretty much sums it up already.\n\n\nI have a piece of code that calls `os.getenv` to get both a URL as well as a token in order to connect to a service. The code lives in a module and will only be imported from there, i.e. it's not a script. \n\n\nIt's not a huge issue at all, since I really only need to crash and display the message saying that there are unset values, but it got me thinking about which of Python's built-in exceptions would be the best fit.\n\n\nI found the `EnvironmentError`, but that seems to function as base class from which `IOError` and other OS related exceptions inherit.\n\n\nWould it be as simple as a `ValueError`, as it's really just a value that's missing?\n\n\nThanks!\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"python"},"answer":"Well most built in concrete exception classes are for specific use cases, and this one does not really fit in any but `RuntimeError`. But I would advise you to use a custom Exception subclass.\n\n\n"}
{"questionId":"2238730e8a174e9daba93b2f49a8be4a","question":"How do you cast a dictionary to dictionary?\nLet's say I have:\n\n\n\n```\npublic class Animal\n{\n    virtual public void Attack() {};\n}\npublic class Lion : Animal\n{\n    public override void Attack() { base.Attack(); }\n}\npublic class Boar : Animal\n{\n    public override void Attack() { base.Attack(); }\n}\n\n```\n\nAnd containers of these two types of animals:\n\n\n\n```\nDictionary<int, Lion> lions;\nDictionary<int, Boar> boars;\n\n```\n\nIs there a way to cast 'lions' and 'boars' so that I can pass them into a function like this?\n\n\n\n```\nvoid IterateTable(Dictionary<int, Animal> dictionary)\n{\n    foreach(var entry in dictionary)\n        entry.Value.Attack();\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"Maybe something like this?\n\n\n\n```\nvoid IterateTable<T>(Dictionary<int, T> dictionary)\n    where T : Animal\n{\n    foreach(var entry in dictionary)\n        entry.Value.Attack();\n}\n\n```\n\n"}
{"questionId":"b271840651424e8b9aef9d049c8756b7","question":"Tax exclusive if coupon applied else tax inclusive in items\nI have setup taxes inclusive for all my items and filled MRP in price. But now I want to apply tax inclusive if customer didn't applied coupon i.e. buying on MRP. But when customer applies coupon I need to apply taxes on after discount amount.\n\n\nIs it possible with settings within Woocommerce or is there any plugin available?\n\n\n\n```\nFor e.g.\n**Case I**\nProduct MRP = 670\nShipping    =  50\nTax 18%     = 102\nFinal price = 670 (Including Taxes) \nIt's Fine.\n\n\n**Case II**\nProduct MRP = 670\nDiscount 40%= 268\nPrice       = 402\nShipping    =  50\nTax 18%     =  61\nFinal price = 452 (Including Taxes)\nBut I need tax to calculated exclusively on discounted price i.e. 402+18% = 474+50 (Ship) = 524\n\n```\n\nI have tried following filter in my custom plugin:\n\n\n\n```\nadd_filter( 'woocommerce_calc_tax', 'inc_or_exc',10,3 );\n\/\/ add_filter( 'woocommerce_calculate_totals', 'calculate_totals',11 );\nfunction inc_or_exc( $taxes,$price,$rates ) {\n    \/\/ echo \"<pre>\";\n    if(!empty(WC()->cart->coupon_discount_amounts)){\n        return  WC_Tax::calc_exclusive_tax( $price, $rates );\n    }else{\n        return  WC_Tax::calc_inclusive_tax( $price, $rates );\n    }\n}\n\n```\n\nBut it calculates taxes bit strange. If item MRP is 100, it shows 98.85 and also totals are not updating with new taxes and shipping rates after plugin run. If I disable plugin then item MRP is shown fine i.e. 100.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"php"},"answer":"Finally I have solved it.\n\n\nFirst I applied inclusive exlusive filter. Then called `woocommerce_calculated_total` with custom condition and achieved my motive.\n\n\n\n```\nadd_filter( 'woocommerce_calc_tax', 'inc_or_exc',10,3 );\n\/\/ do_action('add_points');\n\nadd_filter( 'woocommerce_calculated_total', 'custom_calculated_total', 10, 2 );\nfunction inc_or_exc( $taxes,$price,$rates ) {\n    \/\/ echo \"<pre>\";\n    if(!empty(WC()->cart->coupon_discount_amounts)){\n        return  WC_Tax::calc_exclusive_tax( $price, $rates );\n    }else{\n        return  WC_Tax::calc_inclusive_tax( $price, $rates );\n    }\n}\n\nfunction custom_calculated_total( $total, $cart ){\n    \/\/ echo \"<pre>\";\n    if(!empty(WC()->cart->coupon_discount_amounts)){\n        return round( $total + WC()->cart->get_cart_contents_tax(), $cart->dp );\n    }else{\n        return round( $total, $cart->dp );\n    }   \n}\n\n```\n\n"}
{"questionId":"d193fa2b981b4bd2a5734816ad67da2e","question":"How to remove diacritics from a String in Swift?\nHow to remove diacritics (or accents) from a `String` (like say change \"\u00e9\u00e9n\" to \"een\") in Swift? Do I have to go back to `NSString` or can it be done within Swift?\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"swift"},"answer":"This can also be done applying a `StringTransform`:\n\n\n\n```\nlet foo = \"\u00e9\u00e9n\"\nlet bar = foo.applyingTransform(.stripDiacritics, reverse: false)!\nprint(bar) \/\/ een\n\n```\n\n\n\n---\n\n\nOr implementing a custom property to `StringProtocol`\n\n\n\n```\nextension StringProtocol {\n    var stripingDiacritics: String {\n        applyingTransform(.stripDiacritics, reverse: false)!\n    }\n}\n\n```\n\n\n\n---\n\n\n\n```\nlet bar = foo.stripingDiacritics\nprint(bar) \/\/ een\n\n```\n\n"}
{"questionId":"c425e8273aae4f499be3af330aef1095","question":"Repeat items in list to required length\nI have a list of available items that I can use to create a new list with a total length of 4. The length of the available item list never exceeds 4 items. If the list has less than 4 elements I want to populate it with the available elements beginning at the start element.\n\n\nExample 1:\n\n\n\n```\navailable_items = [4, 2]\nResult -> [4, 2, 4, 2]\n\n```\n\nExample 2:\n\n\n\n```\navailable_items = [9, 3, 12]\nResult -> [9, 3, 12, 9]\n\n```\n\nExample 3:\n\n\n\n```\navailable_items = [3]\nResult -> [3, 3, 3, 3]\n\n```\n\nI have the feeling that my solution is not optimal, but I have found nothing better so far:\n\n\n\n```\navailable_items = [3, 5]\nrequired_items = 4\n\nif len(available_items) == 1:\n  new_items = [available_items[0]] * required_items\nelse:\n  new_items = available_items + []\n  for i in range(required_items - len(available_items)):\n    new_items.append(available_items[i])\n\nprint(new_items)\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"You can use `itertools.cycle`\n\n\n**Ex:**\n\n\n\n```\nfrom itertools import cycle\n\navailable_items_1 = cycle([4, 2])\navailable_items_2 = cycle([9, 3, 12])\navailable_items_3 = cycle([3])\n\nn = 4\n\nprint([next(available_items_1)for i in range(n)])\nprint([next(available_items_2)for i in range(n)])\nprint([next(available_items_3)for i in range(n)])\n\n```\n\n**Output:**\n\n\n\n```\n[4, 2, 4, 2]\n[9, 3, 12, 9]\n[3, 3, 3, 3]\n\n```\n\n"}
{"questionId":"0c8fdff8c9a7407ebe6241d31079481b","question":"Subprocess call with exit status 128\nEssentially, I am trying to use a subprocess call to checkout a git commit at a specific sha hash.\n\n\nHowever, I keep getting the error `subprocess.CalledProcessError: Command '['git', 'checkout', '62bbce43e']' returned non-zero exit status 128.`\n\n\nThis is my code below:\n\n\n\n```\nwith open(filename) as inputfile:\n    reader = csv.reader(inputfile, delimiter=\",\")\n    linecount = 0\n    for row in reader:\n        if linecount == 0:\n            linecount += 1\n        else:\n            repo = str(row[0])\n            sha = str(row[2])\n            specificfile = str(row[3])\n            linenum = int(row[4])\n            cl(\"cd\", repo)\n            subprocess.check_output(['git', 'checkout', sha])\n            print(\"checkout done\")\n            git(\"checkout\", \"-\")\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"A `subprocess.check_output()` call actually returns the output (and you can also get error output as well by passing a `stderr` parameter). You may want to have a look at that to see if it gives you an error explaining what happened.\n\n\nSince you're getting an exception (meaning the call is not completing hence output may not be returned), you should be able to get the output from one of the exception members:\n\n\n\n```\ntry:\n    output = subprocess.check_output(['git', 'checkout', sha], stderr=subprocess.STDOUT)\nexcept subprocess.CalledProcessError as e:\n    print(\"Exception on process, rc=\", e.returncode, \"output=\", e.output)\n\n```\n\n\n\n---\n\n\nOne thing I *do* know is that some `git` commands tend to return 128 if you're not actually *in* a Git repo. So I'd be looking at the path following your `cl(\"cd\", repo)` line, with:\n\n\n\n```\nos.system(\"pwd\")  # use \"cd\" for Windows.\n\n```\n\nIf that `cd` of yours is running in a *sub-process,* that will *not* affect the current process and therefore you may not necessarily be in a Git repo at all. That would certainly explain the `128` return code.\n\n\nBy way of example, the following transcript shows what happens when I try to run a `git` command outside of a repo:\n\n\n\n```\n>>> try:\n...     output = subprocess.check_output(['git', 'checkout', '12345'])\n... except subprocess.CalledProcessError as e:\n...     print(e.returncode, e.output)\n...\n\n128 b'fatal: not a git repository (or any of the parent directories): .git\\n'\n\n```\n\nIf it turns out you *are* in the wrong directory (i.e., the `cl(\"cd\", repo)` statement is running a sub-process to change directory), you should use the Python-blessed method to change directories (a):\n\n\n\n```\nimport os\nos.chdir(path)\n\n```\n\nThat actually changes the directory for the *immediate* process (the Python interpreter) rather than a transient sub-process.\n\n\n\n\n---\n\n\n(a) This is actually good advice in general - use Python-specific stuff as much as possible (since it's mostly cross-platform) rather than spawning a sub-shell (which is inherently platform-specific).\n\n\n"}
{"questionId":"0fe406b286d143739a9a4cac7489e4d6","question":"Check for 5 consecutive TRUE values from the back of a vector\nI have the following data:\n\n\n\n```\nx <- c(F, T, T, T, F, T, T, T, T, T)\nnames(x) <- letters[1:10]\ny <- c(T, F, T, T, T, F, T, T, T, T)\nnames(y) <- letters[1:10]\nz <- c(T, T, F, T, T, T, T, T, F, F)\nnames(z) <- letters[1:10]\na <- c(T, T, T, T, T, F, T, F, T, T, T, T, T)\nnames(a) <- letters[1:13]\n\n```\n\nI want to create a function which can subset the first 5 consecutive `T` values, but from the back. For example, if I pass `x` object through that function, I should get the following output:\n\n\n\n```\n#    f    g    h    i    j \n# TRUE TRUE TRUE TRUE TRUE\n\n```\n\nOr if I pass `y` through it, I should just get an `NA`. Because there are no first 5 `T` values from the back.\n\n\n`z` has first 5 consecutive `T` values in the middle and, hence, those should be returned.\n\n\n\n```\n#    d    e    f    g    h \n# TRUE TRUE TRUE TRUE TRUE\n\n```\n\nIn `a`, there are two sets of 5 consecutive values, in the beginning and in the end. Since, the first group from the back would be the one at the end and hence those values should be returned.\n\n\n\n```\n#    i    j    k    l    m \n# TRUE TRUE TRUE TRUE TRUE\n\n```\n\nHow can I make this function?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"r"},"answer":"With a basic for loop:\n\n\n\n```\nfoo <- function(x) {\n  true_in_a_row <- 0L\n  found         <- FALSE\n  for (i in length(x):1L) {\n    if (x[i]) true_in_a_row <- true_in_a_row + 1L else true_in_a_row <- 0L\n    if (true_in_a_row == 5L) {\n      found <- TRUE\n      break\n    }\n  }\n  if (found) x[i:(i+4L)] else NA\n}\n\n```\n\n\n\n---\n\n\n\n```\nfoo(x)\n#    f    g    h    i    j \n# TRUE TRUE TRUE TRUE TRUE \nfoo(y)\n# [1] NA\nfoo(z)\n#    d    e    f    g    h \n# TRUE TRUE TRUE TRUE TRUE \nfoo(a)\n#    i    j    k    l    m \n# TRUE TRUE TRUE TRUE TRUE \n\n```\n\n\n\n---\n\n\n### Benchmark\n\n\n\n```\nset.seed(42)\nx <- sample(c(TRUE, FALSE), size = 1e6, replace = TRUE)\nbench::mark(foo(x), last5(x), f_zoo(x), f_gregexpr(x), f_rle(x), f_embed(x))[1:4]\n# # A tibble: 6 \u00d7 4\n#   expression         min   median   `itr\/sec`\n#   <bch:expr>    <bch:tm> <bch:tm>       <dbl>\n# 1 foo(x)           1.9\u00b5s    6.2\u00b5s 152792.    \n# 2 last5(x)         107ms 149.53ms      5.35  \n# 3 f_zoo(x)        14.39s   14.39s      0.0695\n# 4 f_gregexpr(x) 259.58ms 283.42ms      3.53  \n# 5 f_rle(x)         1.94s    1.94s      0.514 \n# 6 f_embed(x)    187.22ms 201.41ms      5.04  \n\n# With sparser TRUEs:\nx <- sample(c(TRUE, FALSE), size = 1e6, replace = TRUE, prob = c(0.05, 0.95))\nbench::mark(foo(x), last5(x), f_zoo(x), f_gregexpr(x), f_rle(x), f_embed(x))[1:4]\n# 1 foo(x)         33.12ms  33.36ms    29.0  \n# 2 last5(x)       13.11ms   25.5ms    37.9  \n# 3 f_zoo(x)         5.14s    5.14s     0.194\n# 4 f_gregexpr(x)  75.98ms  76.72ms    12.6  \n# 5 f_rle(x)      208.37ms 221.82ms     4.58 \n# 6 f_embed(x)     69.01ms  80.64ms    11.9 \n\n```\n\n"}
{"questionId":"3126ebed15f04dc9be9293a802cca065","question":"ASP.NET Core DI in a class library?\nI have a ASP.NET Core 2.1 project that references a \"Data Access Layer\" project of typ .NET Core Class Library.\n\n\nThe Data Access Layger needs connection string from the appsettings.json in the ASP.NET Core project.\n\n\nI have created a simple container like this : \n\n\n\n```\npublic class DatabaseConnectionString : IDatabaseConnectionString\n{\n    private readonly string _connectionString;\n\n    public DatabaseConnectionString(string connectionString)\n    {\n        _connectionString = connectionString;\n    }\n\n    public string ConnectionString {\n        get { return _connectionString; }\n        set {  }\n    }\n}\n\n```\n\nIn the ASP.NET Core Startup.cs > ConfigureService I have this : \n\n\n\n```\nservices.AddScoped<IDatabaseConnectionString>(p => new DatabaseConnectionString(Configuration.GetConnectionString(\"DefaultConnection\")));\n\n```\n\nI know that I can add the IDatabaseConnectionString to a constructor of a controller in ASP.NET to get the container. But How do I get it while in the class library? I dont want to pass it all the way down from the controller and just adding the IDatabaseConnectionString to the constructor of a class in the class library do not work.\n\n\nI probably need a service where I can ask to create a object of a class and let the service fill in the constructor interfaces with the correct objects? \n\n\nFor example filling in the IDatabasConnectionString in this class : \n\n\n\n```\npublic class UserFactory : FactoryBase\n{\n    private readonly IDatabaseConnectionString _iDatabaseConnectionString;\n\n    public UserFactory(IDatabaseConnectionString connectionString)\n    {\n        _iDatabaseConnectionString = connectionString;\n    }\n\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"\n> \n> I know that I can add the IDatabaseConnectionString to a constructor of a controller in ASP.NET to get the container.\n> \n> \n> \n\n\nNo, that's not needed and it would be wrong. \n\n\n\n> \n> just adding the IDatabaseConnectionString to the constructor of a class in the class library do not work.\n> \n> \n> \n\n\nIt doesn't work because you need to create the service that will use the connection string *and* add it to the services container.\n\n\nFor example:\n\n\n\n```\npublic class Repository: IRepository\n{\n    public Repository(IDatabaseConnectionString databaseConnectionString)\n    {\n        _databaseConnectionString = databaseConnectionString;\n    }\n}\n\npublic class ServiceThatRequiresDatabase : IServiceThatRequiresDatabase\n{\n    public ServiceThatRequiresDatabase(IRepository repository)\n    {\n        _repository = repository;\n    }\n}\n\n\/\/ ...\nservices.AddScoped<IRepository, Repository>();\nservices.AddScoped<IServiceThatRequiresDatabase, ServiceThatRequiresDatabase>();\n\n\npublic class HomeController : Controller\n{\n    public HomeController(IServiceThatRequiresDatabase service)\n    {\n        _service = service;\n    }\n}\n\n```\n\n\n\n---\n\n\nBy the way, as @YeldarKurmangaliyev said, your DatabaseConnectionString should be like this if you want to make it read-only:\n\n\n\n```\npublic class DatabaseConnectionString : IDatabaseConnectionString\n{\n    public string ConnectionString { get; }\n\n    public DatabaseConnectionString(string connectionString)\n    {\n        ConnectionString = connectionString;\n    }\n}\n\n```\n\n"}
{"questionId":"4106d57658fe4bf5857e385aa0e6a8c1","question":"Is there any way to slip a \\_Static\\_assert into an expression in ISO C11?\nIn C11 it is legal to write, for instance:\n\n\n\n```\nint b = (some_function_returning_void(), 1020);\n\n```\n\nAnd you'll get back 1020. But it won't let you write:\n\n\n\n```\nint b = (_Static_assert(2 > 1, \"all is lost\"), 304);\n\n```\n\ngcc returning\n\n\n\n```\nerror: expected expression before '_Static_assert'\n\n```\n\nAnd it would be inconvenient to use `_Static_assert` outside of an expression sometimes since you're out of luck with preprocessor macros-based pseudo-functions that verify their arguments then.\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"c"},"answer":"This is doable in ISO C11. The trick is to put `_Static_assert` in a struct declaration, in a sizeof expression:\n\n\n\n```\nsizeof(struct { _Static_assert(0, \"yay it failed\"); int dummy; })\n\n```\n\nThe dummy field is necessary because `empty struct is a GNU extension`, according to `clang -std=c11 -Weverything`.\n\n\n"}
{"questionId":"d87ad4562ffb4a2eb98567c58257343a","question":"SIMD-within-a-register version of min\/max\nSuppose I have two `uint16_t[4]` arrays, `a` and `b`. Each integer in these arrays is in the range [0, 16383], so bits 14 and 15 aren't set. Then I have some code to find the minimum and maximum among `a[i]` and `b[i]` for each `i`:\n\n\n\n```\nuint16_t min[4], max[4];\nfor (int i = 0; i < 4; i++) {\n    if (a[i] < b[i]) {\n        min[i] = a[i];\n        max[i] = b[i];\n    } else {\n        min[i] = b[i];\n        max[i] = a[i];\n    }\n}\n\n```\n\nSuppose for some reason I can't\/won't use SIMD, but I'd still like to compute this as fast as possible, on a 64-bit platform. Thus, a natural solution is to use the SIMD-within-a-register (SWAR) paradigm on 64-bit registers to compute these 4 values in a single iteration, rather than over 4 iterations with 16-bit arithmetic.\n\n\nWhat bit-twiddling hacks could be used to implement either (min or max) or ideally both operations using the SWAR paradigm, so that the resulting code is faster than the loop above? My target architecture is ARMv8, so feel free to use any ARMv8 instructions that help reducing instruction counts.\n\n\nC, assembly, or C + inline assembly solutions are all welcome.\n\n\n","questionMetadata":{"type":"optimization","level":"advanced","tag":"c"},"answer":"You could use code like this, though it's really a lot longer than just doing it with SIMD:\n\n\n\n```\norr     x2, x0, #0x8000800080008000     \/\/ x2 = 0x8000 | x0\nsub     x2, x2, x1                      \/\/ x2 = (0x8000 | x0) - x1\nand     x2, x2, #0x8000800080008000      \/\/ x2 = x0 < x1 ? 0x0000 : 0x8000\nmov     x3, #0x7fff7fff7fff7fff\nadd     x2, x3, x2, lsr #15             \/\/ x2 = x0 < x1 ? 0x7fff : 0x8000\neor     x4, x0, x1                      \/\/ x4 = x0 ^ x1\nand     x3, x4, x2                      \/\/ x3 = x0 < x1 ? x0 ^ x1 : 0x0000\neor     x4, x1, x3                      \/\/ x4 = x0 < x1 ? x0 : x1\neor     x3, x0, x3                      \/\/ x3 = x0 < x1 ? x1 : x0\n\n```\n\nThe critical path of this algorithm has 6 instructions. The instructions\n\n\n\n```\nmov     x3, #0x7fff7fff7fff7fff\neor     x4, x0, x1                      \/\/ x4 = x0 ^ x1\n\n```\n\nare not on the critical path. If executed in a loop, the constant load can likely be hoisted out. The last two instructions can be evaluated independently, yielding minimum and maximum with the same latency.\n\n\n"}
{"questionId":"40f01e79fff846258720950afe73ec69","question":"Configure RestAssured to use GSON over Jackson?\nI have both GSON and Jackson in a project using RestAssured and I want to use GSON. The official documentation does not provide a clear example. I tried several configs, but it does not seem to work. Here's my config, am I missing something?\n\n\n\n```\nRestAssured.config = RestAssuredConfig.config()\n                .decoderConfig(new DecoderConfig(\"UTF-8\"))\n                .encoderConfig(new EncoderConfig(\"UTF-8\", \"UTF-8\"))\n                .objectMapperConfig(new ObjectMapperConfig(GSON));\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"In my project I solved it by wrapping original `RestAssured.given` method\n\n\n\n```\npublic static RequestSpecification given() {\n    return RestAssured.given()\n        .config(RestAssured.config()\n            .objectMapperConfig(new ObjectMapperConfig(ObjectMapperType.GSON)));\n}\n\n```\n\n"}
{"questionId":"f80fedc4398b44feacdca44150345429","question":"Why does Postgres choose index scan instead of index seek to fetch one record by primary key?\nI have the following table:\n\n\n\n```\ncreate table documents \n(\n     id serial not null primary key,\n     key varchar(50) not null,\n     document jsonb\n);\n\n```\n\nIt has over 100M records and when I run a query to get 1 record by primary key:\n\n\n\n```\n select * from documents where id = 20304050\n\n```\n\nIt uses the index scan to get it:\n\n\n\n```\nIndex Scan using documents_pkey on documents (cost=0.57..8.59 rows=1 width=533) (actual time=0.010..0.011 rows=0 loops=1)\n  Index Cond: (id = 20304050)\nPlanning Time: 0.070 ms\nExecution Time: 0.024 ms\n\n```\n\nWhy does Postgres choose to use an index scan instead of an index seek?\n\n\nEdit:\nI came from the SQL Server world where it was a distinction between an index scan and an index seek. In Postgres, there is no such thing as an index seek.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"This question has already been answered in the comments. \u201eIndex scan\u201c in postgres is the correct way of the query planner saying: \u201eI\u2018m using an index to find that row(s)\u201c. There is no concept of \u201eindex seek\u201c in postgres.\n\n\n"}
{"questionId":"50ffe419b09b4d65878aa3787dd73f78","question":"C main parameter\nI wrote a code which has to display main parameters, but when I compiled it and typed in \"\\*\" program shows my file structure.\nCommand in cmd looks like this: `program.exe 1 2 3 *`\n\n\n\n```\n#include <stdio.h>\n#include <stdlib.h>\n\nint main(int argc, char const* argv[]) {\n    for (int i=0; i<argc; i++) printf(\"%s\\n\", argv[i]);\n    return 0;\n}\n\n```\n\nThe result is:\n\n\n\n```\nprogram\n1\n2\n3\nprogram.c\nprogram.exe\n10-03-20\n11-02-20\n\n```\n\nAnd my question: Is it possible to force program to print \"\\*\" instead of listing files.\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"c"},"answer":"mingw causes the program to perform wildcard expansion of the parameters. Add the following to your program to disable this behaviour:\n\n\n\n```\nint _CRT_glob = 0;\n\n```\n\n\n\n---\n\n\nIn the unix world, the shell is expected to perform wildcard expansion.\n\n\n\n```\n$ perl -le'print for @ARGV' *\na\nb\n\n```\n\nIn the Windows world, wildcard expansion is left to the application.\n\n\n\n```\n>perl -le\"print for @ARGV\" *\n*\n\n```\n\nThat makes writing portable programs tricky. Since mingw is often used to compile programs that weren't written with Windows in mind, its C runtime library performs wildcard expansion of the parameters automatically.\n\n\n`a.c`:\n\n\n\n```\n#include <stdio.h>\n\nint main(int argc, char const* argv[]) {\n    for (int i=0; i<argc; i++)\n        printf(\"%s\\n\", argv[i]);\n\n    return 0;\n}\n\n```\n\n\n```\n>gcc -Wall -Wextra -pedantic-errors a.c -o a.exe & a *\na\na.c\na.exe\n\n```\n\nBut, mingw provides an out. Adding the following to your program disables this behaviour:\n\n\n\n```\nint _CRT_glob = 0; \n\n```\n\n`a.c`:\n\n\n\n```\n#include <stdio.h>\n\nint _CRT_glob = 0; \n\nint main(int argc, char const* argv[]) {\n    for (int i=0; i<argc; i++)\n        printf(\"%s\\n\", argv[i]);\n\n    return 0;\n}\n\n```\n\n\n```\n>gcc -Wall -Wextra -pedantic-errors a.c -o a.exe & a *\na\n*\n\n```\n\n"}
{"questionId":"35ded2fffc3d42d5b5b944b7c227f9a8","question":"Python typing for a subclass of list\nI want to be able to define what the contents of a subclass of list have to be. The class would look like the following.\n\n\n\n```\nclass A(list):\n   def __init__(self):\n      list.__init__(self)\n\n```\n\nI want to include typing such that the following would happen.\n\n\n\n```\nimport typing\n\nclass A(list: typing.List[str]):  # Maybe something like this\n   def __init__(self):\n      list.__init__(self)\n\n>> a = A()\n>> a.append(\"a\")  # No typing error\n>> a.append(1)  # Typing error\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"`typing` conveniently provides a generic version of `collections.MutableSequence`, so something to the effect of:\n\n\n\n```\nimport typing\n\nT = typing.TypeVar('T')\nclass HomogeneousList(typing.MutableSequence[T]):\n    def __init__(self, iterable: typing.Iterable[T]=()) -> None:\n        self._data: typing.List[T]  = []\n        self._data.extend(iterable)\n\n    @typing.overload\n    def __getitem__(self, index: int) -> T: ...\n    @typing.overload\n    def __getitem__(self, index: slice) -> HomogeneousList[T]: ...\n    def __getitem__(self, index):\n        return self._data[index]\n\n    @typing.overload\n    def __setitem__(self, index: int,  item: T) -> None: ...\n    @typing.overload\n    def __setitem__(self, index: slice, item: typing.Iterable[T]) -> None: ...\n    def __setitem__(self, index, item):\n        self._data[index] = item\n\n    def __delitem__(self, index: typing.Union[int, slice]) -> None:\n        del self._data[index]\n\n    def __len__(self) -> int:\n        return len(self._data)\n\n    def insert(self, index: int, item: T) -> None:\n        self._data.insert(index, item)\n\n\nstring_list = HomogeneousList[str]()\nstring_list.append('foo')\nstring_list.append(42)\n\n\nint_list = HomogeneousList[int]()\nint_list.append(42)\nint_list.append('foo')\n\n```\n\nNow, `mypy`gives the following errors:\n\n\n\n```\ntest.py:36: error: Argument 1 to \"append\" of \"MutableSequence\" has incompatible type \"int\"; expected \"str\"\ntest.py:41: error: Argument 1 to \"append\" of \"MutableSequence\" has incompatible type \"str\"; expected \"int\"\n\n```\n\nThere is some tricky aspects of typing `__getitem__` etc because they accept `slice` objects as well, but not terrible.\n\n\nNote, this is useful, because if you just try to do:\n\n\n\n```\nclass HomogeneousList(collections.abc.MutableSequence, typing.Generic[T]):\n    ....\n\n```\n\nMyPy, at least, doesn't throw an error for append. AFAIKT you'd have to explicitly add:'\n\n\n\n```\ndef append(self, item: T) -> None:\n    self._data.append(item)\n\n```\n\nWhich sort of removes a lot of the utility of `collections.abc.MutableSequence` to begin with. Anyway, thankfully, typing provides generic versions of all of these out of the box!\n\n\nNote, you can use these generically, like I've show, but you can also do something like:\n\n\n\n```\nclass StringList(HomogeneousList[str]):\n    pass\n\nmylist = StringList([1,2,3]) # mypy error\nmylist = StringList('abc') # no error\n\nmylist.append('foo') # no error\nmylist.append(42) # mypy error\n\n```\n\n"}
{"questionId":"f7c622d6dca64fcf9891a0625e0fa66a","question":"Different behavior of MSVC and clang for if constexpr branches\nGiven this helper function:\n\n\n\n```\ntemplate<typename Type>\nstd::string toString(Type const& value, bool encloseInQuotes = false) {\n  if constexpr (std::is_same<bool, Type>::value) {\n    auto s = value ? \"true\" : \"false\";\n    return encloseInQuotes ? \"\\\"\"s + s + \"\\\"\" : s;\n  }\n\n  if constexpr (std::is_arithmetic<Type>::value) {\n    if (std::isnan(value)) {\n      return encloseInQuotes ? \"\\\"NaN\\\"\" : \"NaN\";\n    }\n  }\n\n  return \"\";\n}\n\n```\n\nwhich is supposed to convert basic types (and strings) to a string expression, I get a compilation error with MSVC when using it like this:\n\n\n\n```\nint main() {\n  std::string temp = toString(true);\n  return 0;\n}\n\n```\n\nWith clang this compiles without any problem, with MSVC however I get this:\n\n\n\n> \n> 2>c:\\program files (x86)\\windows kits\\10\\include\\10.0.10240.0\\ucrt\\math.h(403): error C2668: 'fpclassify': ambiguous call to overloaded function\n> \n> \n> 2>c:\\program files (x86)\\windows kits\\10\\include\\10.0.10240.0\\ucrt\\math.h(288): note: could be 'int fpclassify(long double) noexcept'\n> \n> \n> 2>c:\\program files (x86)\\windows kits\\10\\include\\10.0.10240.0\\ucrt\\math.h(283): note: or 'int fpclassify(double) noexcept'\n> \n> \n> 2>c:\\program files (x86)\\windows kits\\10\\include\\10.0.10240.0\\ucrt\\math.h(278): note: or 'int fpclassify(float) noexcept'\n> \n> \n> 2>c:\\program files (x86)\\windows kits\\10\\include\\10.0.10240.0\\ucrt\\math.h(403): note: while trying to match the argument list '(\\_Ty)'\n> \n> \n> 2> with\n> \n> \n> 2> [\n> \n> \n> 2> \\_Ty=int\n> \n> \n> 2> ]\n> \n> \n> 2>: note: see reference to function template instantiation 'bool isnan(\\_Ty) noexcept' being compiled\n> \n> \n> 2> with\n> \n> \n> 2> [\n> \n> \n> 2> Type=int,\n> \n> \n> 2> \\_Ty=int\n> \n> \n> 2> ]\n> \n> \n> \n\n\nObviously the compiler considers the `if constexpr (std::is_arithmetic<Type>::value)` test as valid alternative too and generates the mentioned error. However, at runtime it correctly takes the path for bool (when I leave out the `if constexpr (std::is_arithmetic<Type>::value)` part or use a cast `if (std::isnan(static_cast<double>(value)))`).\n\n\nHow can I make this compile correctly on Windows as well?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c++"},"answer":"For `bool` at least two type traits return `true`:\n\n\n\n```\nstd::is_same<bool, Type>::value\nstd::is_arithmetic<Type>::value\n\n```\n\nand then you make a call `std::isnan(true)`. Use `else if`:\n\n\n\n```\nif constexpr (std::is_same<bool, Type>::value) {\n    auto s = value ? \"true\" : \"false\";\n    return encloseInQuotes ? \"\\\"\"s + s + \"\\\"\" : s;\n}\nelse if constexpr (std::is_arithmetic<Type>::value) {\n    if (std::isnan(value)) {\n        return encloseInQuotes ? \"\\\"NaN\\\"\" : \"NaN\";\n    }\n    ...\n}\nelse\n    return \"\";\n\n```\n\n"}
{"questionId":"babcbf22e6d1466aa380e3aa9d1a7a0f","question":"IntelliJ IDEA does not see my environment variables\nI've configured an environment variable in `~\/.bashrc` as well as `~\/.profile`. \n\n\nWhen I run my application via IDEA's Gradle Configuration, my environment variable is apparently not available. E.g. I am referencing this variable within `application.yml`, but the default value is being used instead. Running Gradle from the command line correctly picks up my variable.\n\n\nHow can I configure IDEA to load this environment variable in a global way, so I don't have to manually add it to every project where I need it (~20 projects)?\n\n\nNote: running on Manjaro Linux v18.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"It turns out IDEA will pick up environment variables if you define them either in e.g. `\/etc\/environment` or \/etc\/profile`or if you use ZSH,`~\/.zshrc`.\n\n\n"}
{"questionId":"e2d5f29f8afe4c5a9b1f1a79f552f504","question":"Create counter for runs of TRUE among FALSE and NA, by group\nI have a little nut to crack.\n\n\nI have a `data.frame` where runs of `TRUE` are separated by runs of one or more `FALSE` or `NA`:\n\n\n\n```\n   group criterium\n1      A        NA\n2      A      TRUE\n3      A      TRUE\n4      A      TRUE\n5      A     FALSE\n6      A     FALSE\n7      A      TRUE\n8      A      TRUE\n9      A     FALSE\n10     A      TRUE\n11     A      TRUE\n12     A      TRUE\n13     B        NA\n14     B     FALSE\n15     B      TRUE\n16     B      TRUE\n17     B      TRUE\n18     B     FALSE\n\nstructure(list(group = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, \n1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(\"A\", \n\"B\"), class = \"factor\"), criterium = c(NA, TRUE, TRUE, TRUE, \nFALSE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, NA, FALSE, \nTRUE, TRUE, TRUE, FALSE)), class = \"data.frame\", row.names = c(NA, \n-18L))\n\n\n```\n\nI want to rank the groups of `TRUE` in column `criterium` in ascending order while disregarding the `FALSE`and `NA`. The goal is to have a unique, consecutive ID for each run of `TRUE`, within each `group`.\n\n\nSo the result should look like:\n\n\n\n```\n    group criterium goal\n1      A        NA   NA\n2      A      TRUE    1\n3      A      TRUE    1\n4      A      TRUE    1\n5      A     FALSE   NA\n6      A     FALSE   NA\n7      A      TRUE    2\n8      A      TRUE    2\n9      A     FALSE   NA\n10     A      TRUE    3\n11     A      TRUE    3\n12     A      TRUE    3\n13     B        NA   NA\n14     B     FALSE   NA\n15     B      TRUE    1\n16     B      TRUE    1\n17     B      TRUE    1\n18     B     FALSE   NA\n\n\n```\n\nI'm sure there is a relatively easy way to do this, I just can't think of one. I experimented with `dense_rank()` and other window functions of `dplyr`, but to no avail.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"r"},"answer":"Another `data.table` approach:\n\n\n\n```\nlibrary(data.table)\nsetDT(dt)\ndt[, cr := rleid(criterium)][\n    (criterium), goal := rleid(cr), by=.(group)]\n\n```\n\n"}
{"questionId":"3b77e676d56941b3a5e2436cf2b6e821","question":"Laravel Livewire error \"must not be accessed before initialization\"\nI try to using Livewire Binding Directly To Model Properties, and i get this error, Anyone help please ? Thank in advance.\n\n\n\n> \n> Typed property App\\Http\\Livewire\\V2\\Settings\\Locations::$country must not be accessed before initialization\n> \n> \n> \n\n\n\n```\n class Locations extends Component{\n  public Country $country;\n  use WithPagination;\n  protected $rules = [\n    'country.name' => 'required|min:100',\n    'country.note' => 'required|min:100',\n  ];\n\n  public function SaveCountry(){\n    $this->validate();\n    $this->country->save();\n    $this->reset();\n    session()->flash('info','The country have been added successful.');\n  }\n public function render(){\n    return view('livewire.v2.settings.locations',[\n        'countries' => Country::orderBy('order_num','desc')->paginate(10),\n    ]);\n  }\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"When you define the type of a variable in PHP, you need to provide it a default value.\n\n\nIf your component is creating a `Country`, you can set it to an empty `Country` in the `mount` function:\n\n\n\n```\npublic function mount()\n{\n    $this->country = new Country();\n}\n\n```\n\nAlternatively set the `public Country $country` to be an existing `Country`.\n\n\n"}
{"questionId":"0ea61c57e992467ca4e62d51bb4d2730","question":"Does nullptr\\_t break type punning or pointer conversions?\nConsider this union:\n\n\n\n```\ntypedef union\n{\n  void*      vptr;\n  nullptr_t  nptr;\n} pun_intended;\n\n```\n\n`nullptr_t` is supposedly compatible with `void*` 1). Ok so what if we initialize the `void*` to some non-zero value?\n\n\n\n```\npun_intended foo = { .vptr = (void*)42 }; \n\n```\n\n- This conversion is supposedly legit (impl.defined) as per C23 6.3.2.3 \u00a74, or at least it was until `nullptr_t` was introduced.\n- And what about union type punning? Also supposedly legit.\n- And what about inspecting any type's internal representation in C using a character type pointer, well-defined until C23, 6.3.2.3 \u00a77.\n\n\nFull example:\n\n\n\n```\n#include <stdio.h>\n#include <inttypes.h>\n#include <stddef.h>\n\ntypedef union\n{\n  void*      vptr;\n  nullptr_t  nptr;    \n} pun_intended;\n\nint main(void)\n{\n  pun_intended foo = { .vptr = (void*)42 };\n  printf(\"Value: %\" PRIuPTR \"\\n\", (uintptr_t)foo.vptr);\n\n  if(foo.nptr != (void*)42)\n  {\n    puts(\"It does not have value 42.\");\n    if(foo.nptr == nullptr)  \n      puts(\"Because it's a nullptr.\");\n    else\n      puts(\"But it's not a nullptr.\");\n\n    unsigned int val = *(unsigned char*)&foo; \/\/ little endian assumption here\n    printf(\"And it has value %d.\\n\", val);\n\n    if(foo.vptr != nullptr)\n    {\n      puts(\"foo.vptr is however not a nullptr.\");\n    }\n  }\n}\n\n```\n\nOutput on clang 16 -std=c2x:\n\n\n\n```\nValue: 42\nIt does not have value 42\nBecause it's a nullptr\nAnd it has value 42.\nfoo.vptr is however not a nullptr\n\n```\n\nOutput on gcc 13.2 -std=c2x:\n\n\n\n```\nValue: 42\nIt does not have value 42.\nBut it's not a nullptr.\nAnd it has value 42.\nfoo.vptr is however not a nullptr.\n\n```\n\nMy question: Is anything of the above (which was previously well-defined or impl.defined) now undefined\/unspecied behavior? If so, where is that stated? Or are these scenarios simply not considered in C23 - a defect?\n\n\n\n\n---\n\n\n1) Source: C23 n3096 draft 7.21.2\n\n\n\n> \n> The size and alignment of `nullptr_t` is the same as for a pointer to character type. An object representation of the value `nullptr` is the same as the object representation of a null pointer value of type `void*`.\n> \n> \n> \n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"c"},"answer":"\n> \n> Ok so what if we initialize the `void*` to some non-zero value?\n> \n> \n> \n\n\nC 2023 N3096 7.21.2 3 explicitly answers this. After telling us that the representation of a `nullptr` value in the `nullptr_t` type is the same as for a null pointer value in the `void *` type, it tells us what happens if there is a different sequence of byte values in a `nullptr_t` object:\n\n\n\n> \n> \u2026 if the object representation is different, the behavior is undefined.\n> \n> \n> \n\n\n"}
{"questionId":"4bf1781a4b294eec9b2c8530713ebc34","question":"Why does PHP not throw an error when I pass too many parameters to a function?\nI am a n00b at php. I was learning about Default Parameters so I made this function.\n\n\n\n```\nfunction doFoo($name = \"johnny\"){\n    echo \"Hello $name\" . \"<br \/>\";\n}\n\n```\n\nI made these calls\n\n\n\n```\ndoFoo();\ndoFoo(\"ted\");\ndoFoo(\"ted\", 22);\n\n```\n\nThe first two printed what was expected i.e \n\n\n\n```\nHello johnny\nHello ted\n\n```\n\nbut the third call also printed \n\n\n\n```\nHello ted\n\n```\n\nI was expecting an error, after all the function is made for one argument whereas I am calling it with two arguments.  \n\nWhy was there no error? \n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"php"},"answer":"It is *not wrong* to pass more arguments to a function than needed. \n\n\nYou only get error if you pass to few arguments.\n\n\n\n```\nfunction test($arg1) {\n var_dump($arg1);\n}\n\ntest();\n\n```\n\n**Above will result in following error:**  \n\nUncaught ArgumentCountError: Too few arguments to function...\n\n\nIf you want to fetch first argument plus all others arguments passed to function you can do:\n\n\n\n```\nfunction test($arg1, ...$args) {\n var_dump($arg1, $args);\n}\n\ntest('test1', 'test2', 'test3');\n\n```\n\n**Resulting in:**  \n\nstring(5) \"test1\"\narray(2) {\n [0]=>\n string(5) \"test2\"\n [1]=>\n string(5) \"test3\"\n}\n\n\n"}
{"questionId":"c99f3e3f6f244c788a820944bdd175fc","question":"What does triple-single-quote mean in bash?\nI have seen lots of uses of single and double-quotes in bash, as well as backtick-quotes, but have never seen what follows. What is the meaning of the transcript below, which seems to show that triple-single-quoting is recognized as meaningful by bash and further seems to show that single quotes inside this thing also have special meaning, enabling interpolation? I have found no documentation of this.\n\n\n\n```\n$ Q=test\n$ echo '$Q'                      # <== I know, this doesn't work...\n$Q                               # <== ...and so it doesn't. \n$ echo '''$Q'''                  # <== Don't know what this could mean.\n$Q                               # <== OK, nothing special?\n$ echo ''' \"$Q\" '''              # <== Try a double-quote??\n \"$Q\"                            # <== Hmm... OK, nothing.\n$ echo ''' '$Q' '''              # <== Try a single-quote?\n test                            # <== Wow, it did interpolate!?\n$ echo '''                         \n> '''                            # <== Continuation! Proving bash  \n                                 #     thinks this is an opening \n                                 #     quote of some kind.\n$ bash --version                 # <== FYI,  version info\nGNU bash, version 3.2.57(1)-release (x86_64-apple-darwin18)\nCopyright (C) 2007 Free Software Foundation, Inc.\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"bash\/shell"},"answer":"There is no \u201ctriple quote\u201d ... for example `''' '$Q' '''` is the concatenation of several strings ... `''`, `' '`, `$Q`, `' '` and `''`. Consider each of the others in the same way.\n\n\n"}
{"questionId":"2f6ae9671ff24e37ab632d72525e9271","question":"Multipart\/mixed email attachments not showing up, but only in Windows 10 Mail\nHaving a weird problem with emails I am sending out via Python `email` \/ `smtplib`.\n\n\nI am attempting to compose an email with:\n\n\n- Alternatives of plain-text and HTML message bodies\n- An image embedded inline in the HTML body\n- A separate non-inline attachment\n\n\nThe MIME structure is setup like this:\n\n\n\n```\nmultipart\/mixed\n    multipart\/alternative\n        text\/plain\n        multipart\/related\n            text\/html\n            image\/png - inline\n    application\/pdf - attachment\n\n```\n\nThis seems to work fine on every mail client I've tested {BlueMail on Android, iOS mail client, Roundcube} **except** for the Windows 10 mail client. For some reason, the Windows 10 built-in mail client seems to show the inline image just fine, but shows no trace of the other attachment. \n\n\nThe limited information I have been able to find on the internet points to this being a bug with the Windows 10 mail client, but I have personally received other emails in this client with both inline and attached attachments, which are displayed just fine - so there obviously is some sort of workaround \/ alternative message structure that works. \n\n\nMy question is thus: *How can I format this message differently so that it will show up properly in all relevant mail clients?*\n\n\nI am composing the email like this, in Python:\n\n\n\n```\nmessage = MIMEMultipart(\"mixed\")\nmessage[\"From\"] = ...\n.\n.\n.\nbodyText = \"...\"\nbodyHTML = \"...\"\nmailFrom = \"...\"\ntargetEmail = \"...\"\nimageContent = ...\n\nmessageBody = MIMEMultipart(\"alternative\")\nmessageBody.attach(MIMEText(bodyText, \"plain\"))\n\nmessageBodyHTML = MIMEMultipart(\"related\")\nmessageBodyHTML.attach(MIMEText(bodyHTML, \"html\"))\nmessageImage = MIMEImage(imageContent)\nmessageImage.add_header(\"Content-Disposition\", 'inline; filename=\"...\"')\nmessageImage.add_header(\"Content-ID\", \"<id used in html body>\")\nmessageBodyHTML.attach(messageImage)\n\nmessageBody.attach(messageBodyHTML)\n\nmessage.attach(messageBody)\n\n\nattachment = MIMEApplication(fileContent, Name=fileName)\nattachment.add_header(\"Content-Disposition\", 'attachment; filename=\"...\"')\nmessage.attach(attachment)\n\n\nself.smtplibSession.sendmail(mailSource, targetEmail, message.as_string())\n\n```\n\n**Update:** Here's the message data from Windows 10 mail (as output via the \"save\" feature - there's no way to view the original message raw data that I can find...)\n\n\n\n```\nMIME-Version: 1.0\nDate: Thu, 30 May 2019 17:45:28 +0200\nFrom: xxxxx <xxxxx>\nSubject: xxxxx\nThread-Topic: xxxxx\nTo: \"xxxxx\" <xxxxx>\nContent-Type: multipart\/related;\n    boundary=\"_5D6C043C-FD42-42F9-B0E0-841DBFBA96D5_\"\n\n--_5D6C043C-FD42-42F9-B0E0-841DBFBA96D5_\nContent-Transfer-Encoding: quoted-printable\nContent-Type: text\/html; charset=\"utf-8\"\n\n<center><img src=3D\"cid:embedded-image\" alt=...\n\n--_5D6C043C-FD42-42F9-B0E0-841DBFBA96D5_\nContent-Type: image\/png; name=\"embedded-image.png\"\nContent-ID: <embedded-image>\nContent-Transfer-Encoding: base64\nContent-Disposition: inline; filename=\"embedded-image.png\"\n\niVBORw0KGgoAAAAN...\n\n--_5D6C043C-FD42-42F9-B0E0-841DBFBA96D5_--\n\n```\n\nI'm not sure if this is a result of saving the email from the app, or this is what the app is actually storing, but it seems that the Windows 10 Mail app is cutting out everything outside the `multipart\/related` stanza - that is, it's only taking the chosen `alternative` and not storing anything else.\n\n\nFor comparison, I've found and exported an email that displayed properly, with an image, html, and attachment, but the format seems to be a lot simpler - that email consisted only of a `multipart\/mixed` layer with `text\/html` and an `application\/pdf` attachment. That email used an external image referenced in the HTML, instead of embedding it in the message - I would like to avoid hosting the images in each email externally.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"Unlike you, there was no problem with the attachment file, instead I've had problems in displaying inline images (`Windows 10 Mail 16005.11629.20174.0`).\n\n\nUnfortunately, handling non-standard approaches in MIME messages correctly is a feature that is expected to have good email clients. Apparently Windows 10 Mail is not as \"good\" yet.\n\n\nThe structure I recommend you to use is:\n\n\n\n```\nmultipart\/mixed\n\u251c\u2500\u2500\u2500 multipart\/related\n\u2502   \u251c\u2500\u2500\u2500 multipart\/alternative\n\u2502   \u2502   \u251c\u2500\u2500\u2500 text\/plain\n\u2502   \u2502   \u2514\u2500\u2500\u2500 text\/html\n\u2502   \u2514\u2500\u2500\u2500 image\/png - inline image\n\u2514\u2500\u2500\u2500 application\/pdf - attachment\n\n```\n\nI've had no problems with this structure in the following clients.\n\n\n- Windows 10 Mail\n- Gmail Web & Android\n- Outlook Web & Android & Windows Desktop\n- Blue Mail Android\n- Roundcube Web\n- MailEnable Web\n\n\nSo, give the following code a try to see if it works for you.\n\n\n\n```\nmessage = MIMEMultipart(\"mixed\")\nmessage[\"From\"] = ...\n.\n.\n.\nbodyText = \"...\"\nbodyHTML = \"...\"\nmailFrom = \"...\"\ntargetEmail = \"...\"\nimageContent = ...\nfileContent = ...\n\nrelatedBody = MIMEMultipart(\"related\")\n\nmessageBody = MIMEMultipart(\"alternative\")\nmessageBody.attach(MIMEText(bodyText, \"plain\"))\nmessageBody.attach(MIMEText(bodyHTML, \"html\"))\n\nrelatedBody.attach(messageBody)\n\nmessageImage = MIMEImage(imageContent)\nmessageImage.add_header(\"Content-Disposition\", 'inline; filename=\"...\"')\nmessageImage.add_header(\"Content-ID\", \"<id used in html body>\")\n\nrelatedBody.attach(messageImage)\n\nmessage.attach(relatedBody)\n\nattachment = MIMEApplication(fileContent)\nattachment.add_header(\"Content-Disposition\", 'attachment; filename=\"...\"')\n\nmessage.attach(attachment)\n\n```\n\n"}
{"questionId":"6c6a37eb7bc947eab295fcd9b441249b","question":"Creating a child process WITHOUT fork()\nIs there a way to start a child process without `fork()`, using `execvp()` exclusively?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c"},"answer":"The pedantic answer to your question is no. The *only* system call that creates a new process is `fork`. The system call underlying `execvp` (called `execve`) loads a new *program* into an existing process, which is a different thing.\n\n\nSome species of Unix have additional system calls besides `fork` (e.g. `vfork`, `rfork`, `clone`) that create a new process, but they are only small variations on `fork` itself, and none of them are part of the POSIX standard that specifies the functionality you can count on on *anything* that calls itself a Unix.\n\n\nThe slightly more helpful answer is that you might be looking for `posix_spawn`, which is a library routine wrapping `fork` and `exec` into a single operation, but I find it *more* troublesome to use that correctly than to write my own `fork`+`exec` subroutine. YMMV.\n\n\n"}
{"questionId":"d0fe2f70d6ca4647832f4a9fd1ad764f","question":"How to evaluate a dynamic variable in a docker-compose.yml file?\nI want to suffix a file name with the current date in my `docker-compose.yml` file. The end goal being a file such as `my_log_123456789.log`, where `123456789` is the current unix date.\n\n\nI tried using an `.env` file, but that did not evaluate as expected. My attempt was:\n\n\n\n```\n# .env\nNOW=\"$(date +%s)\"\n\n```\n\n\n```\n# docker-compose.yml\nversion: '3.8'\nservices:\n  xxx:\n      container_name: xxx\n      image: xxx\/xxx\n      volumes:\n        - \/home\/ubuntu\/xxx_\"$(NOW)\".log:\/home\/ubuntu\/xxx.log\n      ...\n\n```\n\nI tried a few variations of this and had the following issues:\n\n\n- `ERROR: Invalid interpolation format for \"volumes\" option in service \"xxx\": \"\/home\/ubuntu\/xxx_$(NOW).log:\/home\/ubuntu\/xxx.log\"`\n- A directory was created that was named `'xxx_\"$(date +%s)\".log'`\n\n\nHow can I append the current date to a log file in a `docker-compose.yml` file?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"What you can do is to generate `.env` just before calling `docker-compose`:\n\n\n\n```\n#!\/usr\/bin\/env bash\n\ncat << EOF > .env\nNOW=$(date +%s)\nEOF\n\ndocker-compose up\n\n```\n\n"}
{"questionId":"2ae73900921b42b996cb9d90ecad679d","question":"Python PyTorch Error: ModuleNotFoundError: No module named 'torch.utils.tensorboard'\nI'm coding in Python on a Jupyter Notebook (Python 3) on Windows, and I've installed PyTorch and TensorBoard. My problem is that i'm getting this error message:\n\n\n`ModuleNotFoundError: No module named 'torch.utils.tensorboard'`\n\n\nI've already tried to use `pip install future` but the error remains, how can I fix this?\n\n\nHere's how I'm importing `torch.utils.tensorboard`\n\n\n\n```\nfrom torch.utils.tensorboard import SummaryWriter\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"Use `pip install tensorboard or pip3 install tensorboard`\nOR \n`pip install --upgrade tensorboard && pip install --upgrade torch`\n\n\n"}
{"questionId":"a4cb8da40d9e48f1bf57467e375b1844","question":"How to pass arguments in command line using dotnet?\nthe command\n`dotnet myapp.dll -- [4, 3, 2]` throws the exception `System.FormatException: Input string was not in a correct format`.\nI do not know the syntax. How should I pass arguments correctly?\nI use powershell.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"\n```\nusing System;\n\nnamespace ConsoleApp3\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            Console.WriteLine(string.Join('-', args));\n        }\n    }\n}\n\n```\n\nCall it via Powershell 6:\n\n\n\n```\ndotnet .\\ConsoleApp3.dll \"[1,2,3]\"\n\n```\n\nOutput:\n\n\n\n```\n[1,2,3]\n\n```\n\nIn the above call, your `Main` method will receive `[1,2,3]` as a single string and you have to parse\/split it in your code.\n\n\nIf you want an array reflected in the `string[]` array of `Main` you can use a PowerShell array:\n\n\n\n```\ndotnet .\\ConsoleApp3.dll @(1,2,3)\n\n```\n\nOutput:\n\n\n\n```\n1-2-3\n\n```\n\nHere the PowerShell array `@(1,2,3)` is casted to a `string[]`-array. Therefore each item of the PowerShell array is injected to the `string[]` array.\n\n\nBehavior is the same on PowerShell 5.1.\n\n\n"}
{"questionId":"f669c0ad129d4b429331e16808594551","question":"How can I print maximum value of an unsigned integer?\nI want to print the maximum value of the unsigned integer which is of 4 bytes.\n\n\n\n```\n#include \"stdafx.h\"\n#include \"conio.h\"\n\nint _tmain(int argc, _TCHAR* argv[])\n{\n    unsigned int x = 0xffffffff;\n    printf(\"%d\\n\",x);\n    x=~x;\n    printf(\"%d\",x);\n    getch();\n    return 0;\n}\n\n```\n\nBut I get output as -1 and 0. How can I print x = 4294967295?\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"c"},"answer":"Here is the code:\n\n\n\n```\n#include <stdio.h>\n\nint main(void) {\n    unsigned int a = 0;\n    printf(\"%u\", --a);\n    return 0;\n}\n\n```\n\nOutput:\n\n\n`4294967295`\n\n\nHow it works is that `0` is the minimum value for `unsigned int` and when you further decrease that value by `1` it wraps around and moves to the highest value.\n\n\n"}
{"questionId":"7666edfabbbe4f139addb5eee1d72512","question":"get post by post name instead of id\nOk i have this code currently.\n\n\n\n```\n<?php\n\n$post_id = 266;\necho \"<div id='widgets-wrapper3'><div id='marginwidgets' style='overflow: auto; max-    width: 100%; margin: 0 auto; border: none !important;'>\";\n$queried_post = get_post($post_id); \necho \"<div class='thewidgets'>\";\necho substr($queried_post->post_content, 0, 500);\necho \"<a href='\".get_permalink( 26 ).\"' title='Read the whole post' class='rm'>Read     More<\/a>\";\necho \"<\/div>\";\n\necho \"<\/div><\/div>\";\n\n?>\n\n```\n\nAs you can see to the above code, the routine is to get the post by ID, but my permalinks change into post name instead of post id for SEO purposes. How do I get the post by post name?\n\n\nHope someone here could figure it out. Thank you.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"php"},"answer":"Use `WP_Query`. This function will retrieve the first post with the given name, or `null` if nothing is found:\n\n\n\n```\nfunction get_post_by_name(string $name, string $post_type = \"post\") {\n    $query = new WP_Query([\n        \"post_type\" => $post_type,\n        \"name\" => $name\n    ]);\n\n    return $query->have_posts() ? reset($query->posts) : null;\n}\n\n```\n\nBy default this will search for an item of the type `post`:\n\n\n`get_post_by_name(\"my-post\")`\n\n\nAs a second argument you can set that to something else:\n\n\n`get_post_by_name(\"my-page\", \"page\")`\n\n\n"}
{"questionId":"658d7830422b44c492abd1aeeef3814e","question":"Why is the size of the data type different when the value is directly passed to the sizeof operator?\n\n```\n#include <stdio.h>\nint main() {\n    char a = 'A';\n    int b = 90000;\n    float c = 6.5;\n    printf(\"%d \",sizeof(6.5));\n    printf(\"%d \",sizeof(90000));\n    printf(\"%d \",sizeof('A'));\n    printf(\"%d \",sizeof(c));\n    printf(\"%d \",sizeof(b));\n    printf(\"%d\",sizeof(a));\n    return 0;\n}\n\n```\n\nThe output is:\n\n\n\n```\n8 4 4 4 4 1\n\n```\n\nWhy is the output different for the same values?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c"},"answer":"Character constants in C (opposite to C++) have the type `int`. So this call \n\n\n\n```\nprintf(\"%d\",sizeof('A'));\n\n```\n\noutputs 4. That is `sizeof( 'A' )` is equal to `sizeof( int )`.\n\n\nFrom the C Standard (6.4.4.4 Character constants)\n\n\n\n> \n> 10 **An integer character constant has type int**....\n> \n> \n> \n\n\nOn the other hand (6.5.3.4 The sizeof and alignof operators)\n\n\n\n> \n> 4 When sizeof is applied to an operand that has type char, unsigned\n>  char, or signed char, (or a quali\ufb01ed version thereof) the result is 1.\n> \n> \n> \n\n\nSo the operand of the `sizeof` operator in this expression `sizeof( 'A' )` has the type int while in this expression `sizeof( a )` where a is declared like\n\n\n\n```\nchar a = 'A';\n\n```\n\nthe operand has the type `char`.\n\n\nPay attention to that calls like this\n\n\n\n```\nprintf(\"%d\",sizeof(6.5));\n\n```\n\nuse incorrect conversion format specifier. You have to write\n\n\n\n```\nprintf(\"%zu\",sizeof(6.5));\n\n```\n\nAlso in the above call there is used a constant of the type `double` while in this call\n\n\n\n```\nprintf(\"%zu\",sizeof(c));\n\n```\n\nthe variable `c` has the type `float`.\n\n\nYou could get the same result for these calls if the first call used a constant of the type float like\n\n\n\n```\nprintf(\"%zu\",sizeof(6.5f));\n\n```\n\n"}
{"questionId":"65c07eea36474ffbaa6acc5fedc15795","question":"Rails: Installing font-awesome in Rails 6.0.0-rc1 with webpacker and yarn\nI'm trying to install FontAwesome via `yarn` by doing `yarn add @fontawesome\/fontawesome-free`, and then adding it to my `application.scss`:\n\n\n\n```\n$fa-font-path: '~@fortawesome\/fontawesome-free-webfonts\/webfonts';\n@import '~@fortawesome\/fontawesome-free-webfonts\/scss\/fontawesome';\n@import '~@fortawesome\/fontawesome-free-webfonts\/scss\/fa-solid';\n\n```\n\nI have tried several things to write in the `application.scss` that I've seen through the internet, but none of them works, as I get the following error:\n\n\n\n```\nSass::SyntaxError in Pages#index\nShowing \/Users\/foo\/dev\/project\/app\/views\/layouts\/application.html.erb where line #9 raised:\n\nFile to import not found or unreadable: ~@fortawesome\/fontawesome-free-webfonts\/scss\/fontawesome.\nLoad paths:\n  \/Users\/foo\/dev\/project\/app\/assets\/images\n  \/Users\/foo\/dev\/project\/app\/assets\/javascripts\n  \/Users\/foo\/dev\/project\/app\/assets\/stylesheets\n  \/Users\/foo\/dev\/project\/vendor\/bundle\/ruby\/2.5.0\/gems\/coffee-rails-5.0.0\/lib\/assets\/javascripts\n  \/Users\/foo\/dev\/project\/vendor\/bundle\/ruby\/2.5.0\/gems\/actioncable-6.0.0.rc1\/app\/assets\/javascripts\n  \/Users\/foo\/dev\/project\/vendor\/bundle\/ruby\/2.5.0\/gems\/activestorage-6.0.0.rc1\/app\/assets\/javascripts\n  \/Users\/foo\/dev\/project\/vendor\/bundle\/ruby\/2.5.0\/gems\/actionview-6.0.0.rc1\/lib\/assets\/compiled\n  \/Users\/foo\/dev\/project\/node_modules\n\n```\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"ruby"},"answer":"You need to remove the `~` sign at the start of your imports and it should work fine.\n\n\nIt should be something like this:\n\n\n\n```\n$fa-font-path: '@fortawesome\/fontawesome-free-webfonts\/webfonts';\n@import '@fortawesome\/fontawesome-free-webfonts\/scss\/fontawesome';\n@import '@fortawesome\/fontawesome-free-webfonts\/scss\/fa-solid';\n\n```\n\n"}
{"questionId":"aec8e6ee46084634bb8e892a450c3fdd","question":"Stop executing makefile\nI implement a recipe in order to pass all the remaining string to the command, as example in this script:\n\n\n`Makefile`\n\n\n\n```\nrun:\n#   .\/bin\/run.sh $(filter-out $@,$(MAKECMDGOALS)) \n    @echo $(filter-out $@,$(MAKECMDGOALS))\n\n```\n\nBut when I run as example:\n\n\n\n```\n>make run my custom input params\nmy custom input params\nmake: *** No rule to make target `my'.  Stop.\n\n```\n\nmakefile try to execute also the remaining string so the error:\n\n\n\n> \n> \n> ```\n> make: *** No rule to make target `my'.  Stop.\n> \n> ```\n> \n> \n\n\nHow can I prevent this?\n\n\nNB: As workaround I define a dummy recipe:\n\n\n\n```\n%:\n    @echo\n\n```\n\nSo this will print an empty string instead of the error.\n\n\nI want to avoid to do something like: \n\n\n\n```\nmake run-example param=\"my custom param\"\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"I don't think you should use a Makefile. You want to do your own parsing of the options, and that's more trouble to do in make.\n\n\nIf you're dead set on it, you could do this:\n\n\n\n```\n%:\n    @true\n\n```\n\n...which will avoid printing an empty line.\n\n\nIt would be better to do this in Bash, though. Here's one way you could do it:\n\n\n\n```\n#!\/usr\/bin\/env bash\n\nif [ $# -lt 1 ]; then\n    echo Not enough args\n    exit 1\nfi\n\ncase \"$1\" in\n    \"run\")\n        shift\n        .\/bin\/run.sh $@\n        ;;\n    *)\n        echo \"Command $1 not recognized\"\n        exit 1\n        ;;\nesac\n\n```\n\nThis seems easier and more extensible.\n\n\n"}
{"questionId":"3efec39277e74689b8ea3c5c21366809","question":"Python AWS Lambda Certificates\nHow do I add an additional CA (certificate authority) to the trust store used by my Python3 AWS Lambda function?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"If you only need a single CA, then get your crt file and encode it into a pem using the following command in linux:\n\n\n\n> \n> openssl x509 -text -in \"{your CA}.crt\" > cacert.pem\n> \n> \n> \n\n\nIf you need to add CA's to the default CA bundle, then copy python3.8\/site-packages\/certifi\/cacert.pem to your lambda folder. Then run this command for each crt:\n\n\n\n> \n> openssl x509 -text -in \"{your CA}.crt\" >> cacert.pem\n> \n> \n> \n\n\nAfter creating the pem file, deploy your lambda with the REQUESTS\\_CA\\_BUNDLE environment variable set to **\/var\/task\/cacert.pem**. \n\n\n\/var\/task is where AWS Lambda extracts your zipped up code to.\n\n\n"}
{"questionId":"d2b7800a506d4c2fb8e8dfb169784c89","question":"Print first few and last few lines of file through a pipe with \"...\" in the middle\n# Problem Description\n\n\nThis is my file\n\n\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n```\n\nI would like to send the cat output of this file through a pipe and receive this\n\n\n\n```\n% cat file | some_command\n1\n2\n...\n9\n10\n\n```\n\n\n\n---\n\n\n# Attempted solutions\n\n\nHere are some solutions I've tried, with their output\n\n\n\n```\n% cat temp | (head -n2 && echo '...' && tail -n2)\n1\n2\n...\n\n```\n\n\n```\n% cat temp | tee >(head -n3) >(tail -n3) >\/dev\/null\n1\n2\n3\n8\n9\n10\n# I don't know how to get the ...\n\n```\n\n\n```\n% cat temp | sed -e 1b -e '$!d'\n1\n10\n\n% cat temp | awk 'NR==1;END{print}'\n1\n10\n# Can only get 2 lines\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"An awk:\n\n\n\n```\nawk -v head=2 -v tail=2 'FNR==NR && FNR<=head\nFNR==NR && cnt++==head {print \"...\"}\nNR>FNR && FNR>(cnt-tail)' file file\n\n```\n\nOr if a single pass is important (and memory allows), you can use `perl`:\n\n\n\n```\nperl -0777 -lanE 'BEGIN{$head=2; $tail=2;}\nEND{say join(\"\\n\", @F[0..$head-1],(\"...\"),@F[-$tail..-1]);}' file   \n\n```\n\nOr, an awk that is one pass:\n\n\n\n```\nawk -v head=2 -v tail=2 'FNR<=head\n{lines[FNR]=$0}\nEND{\n    print \"...\"\n    for (i=FNR-tail+1; i<=FNR; i++) print lines[i]\n}' file\n\n```\n\nOr, nothing wrong with being a caveman direct like:\n\n\n\n```\nhead -2 file; echo \"...\"; tail -2 file\n\n```\n\nAny of these prints:\n\n\n\n```\n1\n2\n...\n9\n10\n\n```\n\n\n\n---\n\n\nIt terms of *efficiency*, here are some stats.\n\n\nFor small files (ie, less than 10 MB or so) all these are less than 1 second and the 'caveman' approach is *2 ms*.\n\n\nI then created a 1.1 GB file with `seq 99999999 >file`\n\n\n- The two pass awk: 50 secs\n- One pass perl: 10 seconds\n- One pass awk: 29 seconds\n- **'Caveman': 2 MS**\n\n\n"}
{"questionId":"3f24c8475499486d8968a9efeda4c46c","question":"How are strings encoded in an ELF file?\nI wanted to demonstrate that passwords in clear are easy to read from a program:\n\n\n\n```\n#include <stdio.h>\n#include <string.h>\n\nint main(int argc, char** argv)\n{\n    char password[] = \"a big refreshing lemonade\";\n    return strcmp(argv[1], password);\n}\n\n```\n\nBut it does not work as expected:\n\n\n\n```\n$ gcc foo.c\n$ hexdump -C a.out | grep -C2 'lem'\n000006c0  00 00 00 48 89 45 f8 31  c0 48 b8 61 20 62 69 67  |...H.E.1.H.a big|\n000006d0  20 72 65 48 ba 66 72 65  73 68 69 6e 67 48 89 45  | reH.freshingH.E|\n000006e0  d0 48 89 55 d8 48 b8 20  6c 65 6d 6f 6e 61 64 48  |.H.U.H. lemonadH|\n000006f0  89 45 e0 66 c7 45 e8 65  00 48 8b 45 c0 48 83 c0  |.E.f.E.e.H.E.H..|\n00000700  08 48 8b 00 48 8d 55 d0  48 89 d6 48 89 c7 e8 6d  |.H..H.U.H..H...m|\n\n```\n\nI notice some weird characters. Why is that?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c"},"answer":"It's because the strings aren't being stored as static data.\n\n\nFor example if you had this:\n\n\n\n```\nconst char* password = \"a big refreshing lemonade\";\n\n```\n\nOr even this:\n\n\n\n```\nstatic char password[] = \"a big refreshing lemonade\";\n\n```\n\nIt is stored contiguously in the binary (You see \"a big refreshing lemonade\" next to each other) in the constants section.\n\n\nIf you look at the assembly output, you see this:\n\n\n\n```\n 6:test.c        ****     char password[] = \"a big refreshing lemonade\";\n23                            .loc 1 6 0\n24 001e 48B86120              movabsq $7309940773697495137, %rax\n24      62696720\n24      7265\n25 0028 48BA6672              movabsq $7453010330678293094, %rdx\n25      65736869\n25      6E67\n26 0032 488945D0              movq    %rax, -48(%rbp)\n27 0036 488955D8              movq    %rdx, -40(%rbp)\n28 003a 48B8206C              movabsq $7233183901389515808, %rax\n28      656D6F6E\n28      6164\n29 0044 488945E0              movq    %rax, -32(%rbp)\n30 0048 66C745E8              movw    $101, -24(%rbp)\n30      6500\n\n```\n\nWhere you see a lot of `movabsq`, which loads a 64 bit constant. So, what it does load 8 bytes at a time into `password`.\n\n\nYou'll notice that the first constant (7309940773697495137) is the little-endian form of \"a big re\"\n\n\n"}
{"questionId":"c3256415d7d74c46b52cfb46106a9eab","question":"Flutter performance of StatefulWidget and StatelessWidget\nI use a lot StatelessWidgets when I have to create \"templates\" of widgets that are used multiple times inside my app because the docs say so:\n\n\n\n> \n> Stateless widget are useful when the part of the user interface you\n>  are describing does not depend on anything other than the\n>  configuration information in the object itself and the BuildContext in\n>  which the widget is inflated.\n> \n> \n> \n\n\nHere is an example:\n\n\n\n```\nclass StepInputButton extends StatelessWidget {\n\n  final int pos;\n  final String value;\n\n  const StepInputButton({\n    this.pos,\n    this.value\n  });\n\n  @override\n  Widget build(BuildContext context) {\n    return Row(\n      \/\/ Text, Icon and a tiny button\n    );\n  }\n\n}\n\n```\n\nThe above is good because I can use `const StepInputButton(val, \"val\"),` in the code with CONST which improves performances.\n\n\n\n\n---\n\n\n**PROBLEM**\n\n\nI am using the famous `Provider` widget to manage the state and the page of my apps usually look like this:\n\n\n\n```\nclass SuccessPage extends StatelessWidget {\n\n  @override\n  Widget build(BuildContext context) {\n    var prov = Provider.of<Type>(context);    \n    return Scaffold(...); \n  }\n\n}\n\n```\n\nThat's a page of my app with Scaffold that has a Drawer, a float action button and an appTitle.\nHere I use a StatelessWidget because I do not use setState() since provider does all the work for me. But still in the official flutter doc they say:\n\n\n\n> \n> For compositions that can change dynamically, e.g. due to having an\n>  internal clock-driven state, or depending on some system state,\n>  consider using StatefulWidget.\n> \n> \n> \n\n\nSo do I have to change `class SuccessPage extends StatelessWidget` to `class SuccessPage extends StatefulWidget`? Do I get advantages?\n\n\n*Note*: if you want to put the question in another way: should I use StatefulWidgets to create \"app pages\" whose state is going to change and StatelessWidgets for \"reusable widgets\" whose state doesn't change?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"dart"},"answer":"`StatefulWidget` is necessary for when the widget itself is maintaining its own state. In the example you gave, the `Provider` package is handling the state for you, assuming you're using the correct provider type higher up the widget tree (for example, `ChangeNotifierProvider`). There also doesn't seem to be anything in this code that would benefit from having access to the widget's lifecycle, so you wouldn't need access to methods like `initState` or `dispose`. \n\n\nAs such, there's nothing for the widget itself to manage, so converting your class to be stateful is unnecessary.\n\n\nOne thing I might suggest, though, is to use a `Consumer` instead of calling `Provider.of` directly. A `Consumer` handles the call for you and removes any ambiguity on whether your widget will get updated when the `Provider` detects a state change.\n\n\n"}
{"questionId":"9974ba0934a24992a7a8087c7ab013d6","question":"Remove empty line from a multi-line string with Java\nI have a multi-line string and some empty lines between other lines. It looks like:\n\n\n\n```\ndef msg = \"\"\"\n                AAAAAA\n\n                BBBBBB\n\n\n                CCCCCC\n\n                DDDDDD\n\n\n\n\n\n\n\n                EEEEEE\n                TEST\n                FFFFF\n\n\n                GGGGGG\n\"\"\"\n\n```\n\nI tried some regex expression with :\n\n\n\n```\nmsg = msg.replaceAll('(\\n\\\\\\\\s+\\n)+', '')\n\n```\n\nOr \n\n\n\n```\nmsg = msg.replaceAll('(\\r?\\n){2,}', '$1');\n\n```\n\nBut nothing is good about what I'm looking...\n\n\nIs it possible to remove only empty lines? to get something like that :\n\n\n\n```\ndef msg = \"\"\"\n                    AAAAAA\n                    BBBBBB\n                    CCCCCC\n                    DDDDDD\n                    EEEEEE\n                    TEST\n                    FFFFF\n                    GGGGGG\n\n\"\"\"\n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"java"},"answer":"Use regex `(?m)^[ \\t]*\\r?\\n\"` to remove empty lines:\n\n\n\n```\nlog.info msg.replaceAll(\"(?m)^[ \\t]*\\r?\\n\", \"\");\n\n```\n\nTo remain only 1 line use `[\\\\\\r\\\\\\n]+`:\n\n\n\n```\nlog.info text.replaceAll(\"[\\\\\\r\\\\\\n]+\", \"\");\n\n```\n\nIf you want to use the value later, then assign it \n\n\n\n```\ntext = text.replaceAll(\"[\\\\\\r\\\\\\n]+\", \"\");\n\n```\n\n"}
{"questionId":"974d6aa2d3ca4cf89153ef42d6ca34b4","question":"Lombok-related errors on Visual Studio Code with the extension installed\nI have decided I had enough of Eclipse's slowness and just moved everything to VSCode. I installed the java extension package as well as the Lombok extension.\n\n\nMy project worked perfectly fine in Eclipse and, although it does **run** fine, VSCode is finding *thousands* of errors obviously related to not finding the getter\/setter methods that Lombok generates. My solution consists of two projects that both depend on a class library (the third project) which has a dependency for Lombok registered. \n\n\nHow can I get rid of these errors \/ make Lombok work properly in VSCode?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"Seems like my issue was loading the 3 projects into my workspace before installing the Lombok plugin. I have removed the projects from my workspace and opened them again and the errors are gone.\n\n\n"}
{"questionId":"0b60841ea9c24fa6baadf94595d059c6","question":"Will a function declared inside main() have external linkage or none linkage?\nSee the following code:\n\n\n\n```\n\/* first file *\/\n\nint i; \/* definition *\/\nint main () {\n  void f_in_other_place (void);   \/* declaration *\/\n  i = 0\n  return 0;\n}\n\/* end of first file *\/\n\n\n\/* start of second file *\/\n\nextern int i; \/* declaration *\/\nvoid f_in_other_place (void){   \/* definition *\/\n  i++;\n}\n\/* end of second file *\/\n\n```\n\nI know that external objects have `external` linkage and internal objects have `none` linkage(ignoring `extern` for a moment). Now if i talk about the function `f_in_other_place()`, it is declared inside main function. So will the identifier for it be treated as an internal object ? If yes than it should have `none` linkage but as visible in program this function refers to it's definition in second file which shows that identifier for it is behaving like an object with `external` linkage. So i am confused whether this identifier here has `external` linkage or `none` linkage ?\n\n\nNow coming to the `extern` keyword, I read somewhere that function declaration implicitly prefixes `extern`. So even if i have not mentioned `extern` for this function identifier explicitly, will my function's identifier by default become an object with `external` linkage and scoped inside `main()` ? Please correct me if i am going in wrong direction.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c"},"answer":"\n> \n> I know that external objects have external linkage and internal\n> objects have none linkage\n> \n> \n> \n\n\nI think that by the term \"internal objects\" you mean objects declared in block scopes.\n\n\nAs for this declaration\n\n\n\n```\nint i; \/* definition *\/\n\n```\n\nthen it is a declaration. You may place several such declarations one after another like\n\n\n\n```\nint i; \/* definition *\/\nint i; \/* definition *\/\nint i; \/* definition *\/\n\n```\n\nThe compiler generates the so-called tentative definition of this variable at the end of the translation unit initializing it by zero.\n\n\nAs for the function declaration in main then according to the C Standard (6.2.2 Linkages of identifiers)\n\n\n\n> \n> 5 **If the declaration of an identifier for a function has no\n> storage-class specifier, its linkage is determined exactly as if it\n> were declared with the storage-class specifier extern**. If the\n> declaration of an identifier for an object has file scope and no\n> storage-class specifier, its linkage is external.\n> \n> \n> \n\n\nand\n\n\n\n> \n> 4 For an identifier declared with the storage-class specifier extern\n> in a scope in which a prior declaration of that identifier is\n> visible,31) if the prior declaration specifies internal or external\n> linkage, the linkage of the identifier at the later declaration is the\n> same as the linkage specified at the prior declaration. **If no prior\n> declaration is visible, or if the prior declaration specifies no\n> linkage, then the identifier has external linkage.**\n> \n> \n> \n\n\nSo this function declaration in main\n\n\n\n```\nvoid f_in_other_place (void);\n\n```\n\nis equivalent to\n\n\n\n```\nextern void f_in_other_place (void);\n\n```\n\nAs there is no previous function declaration in the file scope then this function has external linkage.\n\n\nIf for example in the file scope before main there would be a declaration with the keyword `static` like\n\n\n\n```\nstatic void f_in_other_place (void);\n\n```\n\nthen the function declared in main would have internal linkage.\n\n\n"}
{"questionId":"6f936fb6f54f4ed6b538d33bab97c14a","question":"Java stop executor service once one of his assigned tasks fails for any reason\nI need some kind of service that will run a few tasks simultaneously and in an interval of 1 second for 1 minute.\n\n\nIf one of the tasks fails, I want to stop the service and every task that ran with it with some kind of indicator that something went wrong, otherwise if after one minute everything went well the service will stop with an indicator that all went well.\n\n\nFor example,i have 2 functions:\n\n\n\n```\nRunnable task1 = ()->{\n      int num = Math.rand(1,100);\n      if (num < 5){\n          throw new Exception(\"something went wrong with this task,terminate\");\n      }\n}\n\nRunnable task2 = ()->{\n      int num = Math.rand(1,100)\n      return num < 50;\n}\n\n\n\nScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(2);\ntask1schedule = scheduledExecutorService.scheduleAtFixedRate(task1, 1, 60, TimeUnit.SECONDS);\ntask2schedule = scheduledExecutorService.scheduleAtFixedRate(task2, 1, 60, TimeUnit.SECONDS);\n\nif (!task1schedule || !task2schedule) scheduledExecutorService.shutdown();\n\n```\n\nAny ideas on how should I tackle this and make things as generic as possible?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"The idea is that the tasks are pushing to a common object TaskCompleteEvent. If they push an error the scheduler is stopped and all the tasks will stop.\n\n\nYou can check the results of every task-iteration in the maps \"errors\" and \"success\".\n\n\n\n```\npublic class SchedulerTest {\n\n    @Test\n    public void scheduler() throws InterruptedException {\n        ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(2);\n        TaskCompleteEvent taskCompleteEvent = new TaskCompleteEvent(scheduledExecutorService);\n        Runnable task1 = () -> {\n            int num = new Random().nextInt(100);\n            if (num < 5) {\n                taskCompleteEvent.message(\"task1-\"+UUID.randomUUID().toString(), \"Num \"+num+\" was obatined. Breaking all the executions.\", true);\n            }\n        };\n        Runnable task2 = () -> {\n            int num = new Random().nextInt(100);\n            taskCompleteEvent.message(\"task2-\"+UUID.randomUUID().toString(), num < 50, false);\n        };\n        scheduledExecutorService.scheduleAtFixedRate(task1, 0, 1, TimeUnit.SECONDS);\n        scheduledExecutorService.scheduleAtFixedRate(task2, 0, 1, TimeUnit.SECONDS);\n        scheduledExecutorService.awaitTermination(60, TimeUnit.SECONDS);\n        System.out.println(\"Success: \"+taskCompleteEvent.getSuccess());\n        System.out.println(\"Errors: \"+taskCompleteEvent.getErrors());\n        System.out.println(\"Went well?: \"+taskCompleteEvent.getErrors().isEmpty());\n    }\n\n    public static class TaskCompleteEvent {\n\n        private final ScheduledExecutorService scheduledExecutorService;\n        private final Map<String, Object> errors = new LinkedHashMap<>();\n        private final Map<String, Object> success = new LinkedHashMap<>();\n\n        public TaskCompleteEvent(ScheduledExecutorService scheduledExecutorService) {\n            this.scheduledExecutorService = scheduledExecutorService;\n        }\n\n        public synchronized void message(String id, Object response, boolean error) {\n            if (error) {\n                errors.put(id, response);\n                scheduledExecutorService.shutdown();\n            } else {\n                success.put(id, response);\n            }\n        }\n\n        public synchronized Map<String, Object> getErrors() {\n            return errors;\n        }\n\n        public synchronized Map<String, Object> getSuccess() {\n            return success;\n        }\n\n    }\n\n}\n\n```\n\n"}
{"questionId":"5f4ec02cac93487cb66ca47d84726ea7","question":"Why does the variable in this loop point at the same memory location?\nConsider the below C code. I would have thought that the variable `bar` was being instantiated every time and would thus point to different addresses in memory, but it doesn't.\n\n\n\n```\nfor (i = 2; i < 7; i++) {\n    struct foo bar;\n    printf(\"struct %u\\n\", bar);\n}\n\n```\n\nOutput:\n\n\n\n```\nstruct 13205520\nstruct 13205520\nstruct 13205520\nstruct 13205520\nstruct 13205520\n\n```\n\nIn case it is not obvious, what I want is to generate 5 different `struct`s\u2014well, actually 5 different pointers to `struct`s\u2014at 5 different locations. How can I do this?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c"},"answer":"The scope of `bar` only exists within one iteration of the loop. That means that when the next `struct foo` is created, it will be put in the same place as the old `bar`, because as far as the compiler sees it, the `bar` is no longer necessary. Looking at your example, it doesn't seem like you need to handle all of the `bar`'s at once. So you might be okay with them all being at the same location. However, if you need to deal with multiple at a time, I can think of two possible solutions.\n\n\n### Putting the scope outside of the loop\n\n\nTo do this, you'll need an array of `struct foo`'s. The scope of the array needs to be outside of the loop. For example:\n\n\n\n```\nstruct foo bar_list[5];\nfor (i = 2; i < 7; i++) {\n    printf(\"struct %p\\n\", (void *)&bar_list[i - 2]);\n    \/\/ use the foo's here\n}\n\n```\n\nThen each iteration of your for loop can modify one of the values\n\n\n### Allocating on the heap\n\n\nIf you're okay with storing five pointers in memory, you can allocate each bar somewhere on the heap. You would probably end up just using an array anyway, so this would probably only be useful if you needed to return the structs into another function. You'd need to do something like this:\n\n\n\n```\nstruct foo* bar_list[5];\nfor (i = 2; i < 7; i++) {\n    bar_list[i - 2] = malloc(sizeof(struct foo));\n    printf(\"struct %p\\n\", (void *)bar_list[i - 2]);\n    \/\/ do anything else you need to do\n}\n\n```\n\nIt's also worth mentioning, as someone else pointed out, that `%p` would be used to print a pointer address.\n\n\n"}
{"questionId":"7a86b9dd94c0429cb02a424c22cc3e06","question":"Laravel Horizon not executing pending jobs - Kubernetes and Docker environment\nWe have two different pods in Kubernetes for our Laravel app, \n\n\n- one running apache serving on port 80, (CMD \/usr\/sbin\/apache2ctl -D FOREGROUND)\n- and another running worker (Laravel Horizon) (CMD php \/var\/www\/artisan horizon)\n\n\nThe issue is when I check the horizon dashboard, it says 'Active', and I can see the Jobs in the 'Pending Jobs' section, but they never actually execute. They are just sitting there idle.\n\n\nNow, when I SSH in the pod running apache and manually and run the command 'php artisan horizon' than it actually executes all pending jobs.\n\n\nI have already ensured the followings:\n\n\n1. Both the pods are connected with the same Redis database service\n2. Horizon Prefix is the same for both the pods\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"php"},"answer":"After struggling for days, I got the answer to this problem.\n\n\nWhile using Redis as a cache, queue, or broadcast broker in the docker environment, we need to make sure that the following environment variables are defined properly and they must be the same across all the pods.\n\n\n- CACHE\\_PREFIX\n- REDIS\\_PREFIX\n- REDIS\\_CACHE\\_DB\n- HORIZON\\_PREFIX\n\n\nHope this will help others trying to deploy the Laravel apps using Kubernetes and Docker.\n\n\n"}
{"questionId":"4e4c2917ee064ad7859fe3648570533e","question":"Using Java 8 stream methods to get the last max value\nGiven a list of items with properties, I am trying to get the last item to appear with a maximum value of said property.\n\n\nFor example, for the following list of objects:\n\n\n\n```\nt  i\nA: 3\nD: 7 *\nF: 4\nC: 5\nX: 7 *\nM: 6\n\n```\n\nI can get one of the Things with the highest `i`:\n\n\n\n```\nThing t = items.stream()\n        .max(Comparator.comparingLong(Thing::getI))\n        .orElse(null);\n\n```\n\nHowever, this will get me `Thing t = D`. Is there a clean and elegant way of getting the last item, i.e. `X` in this case?\n\n\nOne possible solution is using the `reduce` function. However, the property is calculated on the fly and it would look more like:\n\n\n\n```\nThing t = items.stream()\n        .reduce((left, right) -> {\n            long leftValue = valueFunction.apply(left);\n            long rightValue = valueFunction.apply(right);\n            return leftValue > rightValue ? left : right;\n        })\n        .orElse(null);\n\n```\n\nThe `valueFunction` now needs to be called nearly twice as often.\n\n\nOther obvious roundabout solutions are:\n\n\n1. Store the object in a Tuple with its index\n2. Store the object in a Tuple with its computed value\n3. Reverse the list beforehand\n4. Don't use Streams\n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"java"},"answer":"Remove the equals option (don't return 0 if the compared numbers are equal, return -1 instead) from the comparator (ie. write your own comparator that doesn't include an equals option):\n\n\n\n```\nThing t = items.stream()\n        .max((a, b) -> a.getI() > b.getI() ? 1 : -1)\n        .orElse(null);\n\n```\n\n"}
{"questionId":"f608c7beb3074320993a56e2fa95bedb","question":"How to restart job after cancel in kotlin coroutines?\nHow to restart `job` after canceling in `kotlin coroutines`\n\n\nI have 2 buttons, 1 to start coroutine and another to cancel the job.\nBut after I cancel the job, coroutine not starts again.\n\n\n\n```\nclass TestFragment : Fragment(), CoroutineScope {\n\n    private lateinit var job: Job\n    override val coroutineContext: CoroutineContext\n        get() = Dispatchers.Main + job\n\n    override fun onCreateView(inflater: LayoutInflater, container: ViewGroup?, savedInstanceState: Bundle?): View? {\n        binding = SettingFragmentBinding.inflate(inflater, container, false)\n\n        job = Job()\n\n        button1.setOnClickListener {\n            launch {\n                val currentTime = LocalDateTime.now()\n                println(currentTime)\n            }\n        }\n\n        button2.setOnClickListener {\n            job.cancel()\n        }\n\n        return binding.root\n    }\n\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"kotlin"},"answer":"You're inappropriately using the top-level job linked to the lifecycle of the fragment as the means to cancel your coroutine on demand.\n\n\nReplace this boilerplate:\n\n\n\n```\nclass TestFragment : Fragment(), CoroutineScope {\n\n    private lateinit var job: Job\n    override val coroutineContext: CoroutineContext\n        get() = Dispatchers.Main + job\n\n```\n\nwith this:\n\n\n\n```\nclass TestFragment : Fragment(), CoroutineScope by MainScope {\n    override fun onDestroy() {\n        cancel()\n    }\n\n```\n\nThis will automatically fix one of the problems you introduced: it uses a `SupervisorJob` instead of a plain `Job`.\n\n\nNext, you need access to the job you launched in `onClick`:\n\n\n\n```\n    private var button1Job: Job?\n\n    ...\n\n    button1.setOnClickListener {\n        button1Job = launch {\n           ...\n           button1Job = null\n        }\n\n```\n\nYou can now cancel this job in `button2` listener:\n\n\n\n```\n    button1Job?.cancel()\n\n```\n\n"}
{"questionId":"ef81e3aaa7fd4a08a27b50af963a103b","question":"Is there a way to inherit constructors in Kotlin?\nI have a `Parent` class which is extended by a lot of childs and I want to avoid to copy the long constructor in each of them because it is always the same.\n\n\n\n```\nopen class Parent(arg1: Any, arg2: Any, arg3: Any...)\n\nclass ChildA(arg1: Any, arg2: Any, arg3: Any...): Parent(arg1, arg2, arg3...)\nclass ChildB(arg1: Any, arg2: Any, arg3: Any...): Parent(arg1, arg2, arg3...)\n[...]\n\n```\n\nIs there a way to inherit the constructor or maybe a function implemented on the `Parent` that instantiates a `Child` class?\n\n\n# Edit\n\n\nMy expectation is to implement the `Child` classes without having to define its constructor. The reason is that I have about 15 childs and each parameter have an optional value, so the resulting code is not so pretty.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"If it's always the same (or only extended), you can create a class for holding the parameters:\n\n\n\n```\ndata class ConstructorParams(arg1: Any, arg2: Any, arg3: Any...)\n\nopen class Parent(params: ConstructorParams)\n\nclass ChildA(params: ConstructorParams) : Parent(params)\n\nclass ChildB(params: ConstructorParams, extraParam: Int) : Parent(params)\n\n```\n\nYou could add to it a helper function to avoid explicit `ConstructorParams` when instantiating the classes, but it has a performance trade-off (though this version won't work for `Child2`):\n\n\n\n```\ninline fun <reified T : Parent> create(arg1: Any, arg2: Any, arg3: Any...) = \n    T::class.primaryConstructor.call(ConstructorParams(arg1, arg2, ...))\n\ncreate<Parent>(1, 2, \"\")\n\n```\n\n"}
{"questionId":"7a001d5f1a9a4a5ca65414bdf442bb1a","question":"How does HasCallStack influence the performance of a normal branch in Haskell?\nGenerating a call stack when reaching the error branch has runtime cost; that is easy to understand.\n\n\nBut will the `HasCallStack` constraint also influence the performance of a normal branch? How?\n\n\n","questionMetadata":{"type":"optimization","level":"advanced","tag":"haskell"},"answer":"The effect of adding a `HasCallStack` constraint to a function `foo` is more or less equivalent to:\n\n\n- adding an additional input argument for the call stack to `foo`'s argument list;\n- wherever `foo` is invoked, constructing a call stack argument for it, by pushing an information frame (consisting of the function name \"foo\" and the source location where it was invoked) onto the input call stack (if `foo` is invoked from another function with a `HasCallStack` constraint) or onto an empty call stack (if it is invoked from a function without a `HasCallStack` constraint).\n\n\nSo... if you have some functions:\n\n\n\n```\nfoo :: HasCallStack => Int -> String -> String\nfoo n = bar n '*'\n\nbar :: HasCallStack => Int -> Char -> String -> String\nbar n c str = if n >= 0 then c' ++ ' ':str ++ ' ':c'\n              else error \"bad n\"\n  where c' = replicate n c\n\nbaz :: String\nbaz = foo 3 \"hello\"\n\n```\n\nthen adding `HasCallStack` to `foo` and `bar` (but leaving `baz` alone) has basically the same effect as if you'd written:\n\n\n\n```\nfoo cs n = bar cs' n\n  where cs' = pushCallStack (\"bar\", <loc>) cs\nbar cs n c str\n  = if n >= 0 then c' ++ ' ':str ++ ' ':c'\n    else error cs' \"bad n\"\n  where c' = replicate n c\n        cs' = pushCallStack (\"error\", <loc>) cs\nbaz = foo cs' 3 \"hello\"\n  where cs' = pushCallStack (\"foo\", <loc>) emptyCallStack\n\n```\n\nSo, the baseline, unoptimized performance cost is the cost of an extra parameter for each function decorated with `HasCallStack` plus the cost of a thunk allocation to supply that parameter for every invocation point of the decorated function. (These costs are paid even if no error is triggered.)\n\n\nIn practice, optimized code will be... erm... optimized. For example, if the above example is compiled with `-O2`, `foo` will be inlined and `bar` will be specialized in the definition of `baz` in such a way that the only runtime cost of the call stack is that a static pointer (to a thunk for creating the full call stack for the `error` call) gets passed to the specialized version of `bar` (but ignored, since no error is generated).\n\n\nGHC doesn't seem to be smart enough to determine that `baz` will never follow the `error` case and so doesn't need the stack frame at all.\n\n\n"}
{"questionId":"c2090f3ba21c4dd09a38b3f704bc912b","question":"How to fix Guice error, \"An illegal reflective access operation has occurred\"\nI've a Kotlin project which uses Guice for DI and has recently been updated from JDK 8 -> 11. It now emits the following error at runtime:\n\n\n\n```\nWARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$1 (file:\/Users\/matt\/.gradle\/caches\/modules-2\/files-2.1\/com.google.inject\/guice\/4.2.2\/6dacbe18e5eaa7f6c9c36db33b42e7985e94ce77\/guice-4.2.2.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)\nWARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$1\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n\n```\n\nHow should this warning be addressed?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"kotlin"},"answer":"The issue is due to how Guice internally accesses Java objects, which is unsafe under the new (Java 9+) Java Module System.\n\n\nAlso, starting with Java 16, this warning becomes an error, and the execution flag `--illegal-access=permit` must be specified manually to reproduce the behaviour of Java 9-15.\n\n\nThere is not much you can do to fix it; the best option is to upgrade to Guice 5.0.1, which has been patched to avoid illegal accesses. Your warning will disappear, and your application will work on Java 16+ with the default JVM behaviour.\n\n\n"}
{"questionId":"0e4749cf73834c2a884832bc4cca9571","question":"max of f64 in Rust\nI have a vector of prices (`f64`). I would like to compute the highest price.\n\n\nWhat is the current easiest and most idiomatic way to compute the max of a collection of `f64` in rust ?\n\n\nThere has been some discussion about `Ord` and `f64` but I am not sure what is the most up-to-date and less hacky way to do so.\n\n\nI rely on the following but I imagined there was some built in operation\n\n\n\n```\nlet max = prices.iter().fold(None, |r, &n| match r {\n    Some(p) => Some(f64::max(p, n)),\n    None => Some(e),\n});\n\n```\n\n(which is just a fold for some free monoid)\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"rust"},"answer":"As of Rust 1.43, you can write this:\n\n\n\n```\nmy_iterator.fold(f64::NEG_INFINITY, f64::max)\n\n```\n\nExplanation: use `f64::NEG_INFINITY` as the initial value, as it is the neutral element for the `f64::max` operation.\n\n\n"}
{"questionId":"e6e5110f55ca4b7f9301249cbf3fe58b","question":"What is the standard exception for a missing value in python?\nWhich standard python exception, if any, should be used in the case where a value is missing for the proper execution of a function ? `TypeError` and `ValueError` seemed good candidates to me but according to the documentation (my italics) :\n\n\n\n> \n> Passing arguments of the wrong type (e.g. passing a `list` when an `int` is expected) should result in a `TypeError`, but passing arguments with the wrong value (e.g. a number outside expected boundaries) should result in a `ValueError`.\n> \n> \n> **ValueError** : Raised when an operation or function receives *an argument that has the right type but an inappropriate value*, and the situation is not described by a more precise exception such as IndexError.\n> \n> \n> \n\n\nNeither of these descriptions seem to me to quite capture the idea of a missing value. But none of the other exceptions in the standard seem to come even close either.\n\n\nHere's an example. Objects of type `myclass` can be instantiated in two different ways, each of which requires a specific parameter. If neither of them is provided, the init fails.\n\n\n\n```\nclass myclass(object):\n    def __init__(self,firstparam=None,secondparam=None):\n        if firstparam:\n             self.firstinit()\n        elif secondparam:\n             self.secondinit()\n        else:\n           #What to put here ?\n           raise Exception(\"Missing value for myclass\")\n\nif __name__==\"__main__\":\n    #oops, forgot to specify a parameter here !\n    myobj=myclass()\n    \n\n```\n\nOf course, I know I can always implement my own `MissingValueError` exceptions subclasses when writing a library. I am asking this question in order not to duplicate something that might already exist in the standard.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"python"},"answer":"The proper exception would be `TypeError`:\n\n\n\n```\n>>> def a(b,c) : print b,c\n... \n>>> a(1)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: a() takes exactly 2 arguments (1 given)\n>>> \n\n```\n\nAnd as a side note, I'd recommend NOT to provide the default values to the required arguments, so the user of your class will clearly understand (s)he has to provide some.\n\n\n"}
{"questionId":"a4ace79f5964497f9a8d2e231c8615c8","question":"Unity Add Default Namespace to Script Template?\nI just found Unity's script template for C# scripts. To get the script name you write `#SCRIPTNAME#` so it looks like this:\n\n\n\n```\nusing UnityEngine;\nusing System.Collections;\n\npublic class #SCRIPTNAME# : MonoBehaviour \n{\n    void Start () \n    {\n    \n    }\n    \n    void Update () \n    {\n    \n    }\n}\n\n```\n\nThen it would create the script with the right name, but is there something like `#FOLDERNAME#` so that I can put it in the right namespace directly when creating the script?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"I know it's and old question but in newer versions of Unity you can define a root namespace to be used in the project.\nYou can define the namespace in Edit > Project Settings > Editor > Root Namespace\n\n\nDoing this will add the defined namespace on newly created scripts.\n\n\n"}
{"questionId":"11c06397442f4d48b0c07f75155c7584","question":"block\\_given? always returns true in erb templates\nIn Rails 5.2.3, I need to render a partial which takes an optional block.\n\n\n\n```\n# users\/_user.html.erb\n...\n<% if block_given? %>\n  <%= yield %>\n<% else %>\n  <h1>Goodbye world<\/h1>\n<% end %>\n...\n\n\n```\n\nHowever `block_given?` returns true regardless of which version I choose to go with:\n\n\n\n```\n\n<%# Version 1 - block_given? returns true %>\n<%= render partial: \"users\/_user\" do %>\n  <h1>hello world<\/h1>\n<% end %>\n\n<%# Version 2 - block_given? also returns true %>\n<%= render partial: \"users\/_user\" %>\n\n\n```\n\nWhat's going on here and why is this happening? \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"ruby"},"answer":"Because all Rails templates support `content_for :xyz`, which is triggered by `yield :xyz`, it means all templates are always wrapped in a block that is prepared to fetch this `content_for` data.\n\n\nBecause this pre-programmed block is always there in order to accommodate `content_for`, it means `block_given?` will always return true.\n\n\nI think this may actually be a small oversight in the Rails view design. It would be nice if we'd have a separate method to detect if a partial was supplied a block.\n\n\nOne idea for workaround:\n\n\n\n```\n<% if (block = yield).empty? %>\n  <h1>Goodbye world<\/h1>\n<% else %>\n  <%= block %>\n<% end %>\n\n```\n\n"}
{"questionId":"44760c71dd094747b7964b23e6c59e90","question":"Django model fields with dynamic names\nI'd like to add to existing models new CharFields via one common mixin or abstract model but names of these fields depend on configuraton. so one model will have someprefix1\\_title field and another model - someprefix2\\_title.\n\n\nIs it possible to make this approach to work:\n\n\n\n```\nclass AbstractModel(models.Model):\n    self.fields_prefix + '_title' = models.CharField(max_length=255, blank=True, default='')\n\n    class Meta:\n        abstract = True\n\nclass ModelOne(AbstractModel):\n    fields_prefix = 'someprefix1'\n    id = models.AutoField(primary_key=True)\n\nclass ModelTwo(AbstractModel):\n    fields_prefix = 'someprefix2'\n    id = models.AutoField(primary_key=True)\n\n```\n\nso ModelOne could have fields id and someprefix1\\_title.\n\n\nupd: what about monkey-patching with add\\_to\\_class() will it work or it's an antipattern and should not be used?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"Try using a factory pattern to set up your different versions of `AbstractModel`.\n\n\nWith this approach, you can more strictly control the way `AbstractModel` is modified by way of the factory function `dynamic_fieldname_model_factory`.\n\n\nWe're also not modifying `ModelOne` or `ModelTwo` after their definitions -- other solutions have pointed out that this helps avoid maintainability problems.\n\n\nmodels.py:\n\n\n\n```\nfrom django.db import models\n\n\ndef dynamic_fieldname_model_factory(fields_prefix):\n    class AbstractModel(models.Model):\n\n        class Meta:\n            abstract = True\n\n    AbstractModel.add_to_class(\n        fields_prefix + '_title',\n        models.CharField(max_length=255, blank=True, default=''),\n    )\n    return AbstractModel\n\n\nclass ModelOne(dynamic_fieldname_model_factory('someprefix1')):\n    id = models.AutoField(primary_key=True)\n\n\nclass ModelTwo(dynamic_fieldname_model_factory('someprefix2')):\n    id = models.AutoField(primary_key=True)\n\n```\n\nHere is the migration generated by this code:\n\n\n\n```\n# Generated by Django 2.1.7 on 2019-03-07 19:53\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    initial = True\n\n    dependencies = [\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='ModelOne',\n            fields=[\n                ('someprefix1_title', models.CharField(blank=True, default='', max_length=255)),\n                ('id', models.AutoField(primary_key=True, serialize=False)),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='ModelTwo',\n            fields=[\n                ('someprefix2_title', models.CharField(blank=True, default='', max_length=255)),\n                ('id', models.AutoField(primary_key=True, serialize=False)),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n    ]\n\n```\n\n"}
{"questionId":"5bd1a1d0340e4208b27576361c6911f2","question":"How to collect multiple state flow in Android\nHow to collect two state flow in activity? Because mine only the first flow that consumed.\n\n\nFor example, inside viewmodel is like this:\n\n\n\n```\nclass ExampleViewModel: ViewModel(){\n    private val state = MutableStateFlow<HomeMainFragmentState>(HomeMainFragmentState.Init)\n    private val products = MutableStateFlow<List<ProductEntity>>(mutableListOf())\n\n\n    \/\/to be consumed\n    fun getState() : StateFlow<HomeMainFragmentState> = state\n    fun getProducts() : StateFlow<List<ProductEntity>> = products\n}\n\n```\n\nAnd then in my view is like this:\n\n\n\n```\nprivate fun observe(){\n      viewLifecycleOwner.lifecycleScope.launch {\n      viewLifecycleOwner.lifecycle.repeatOnLifecycle(Lifecycle.State.STARTED){\n            viewModel.getState().collect { state -> handleState(state) }\n            viewModel.getProducts().collect{ products -> handleProducts(products) }\n        }\n    }\n}\n\n```\n\nThe problem is, **only the first flow is consumed, for this case is the 'state'**, the **products** was never consumed\/executed by activity\/fragment.\n\n\nHow to fix this?\nI also read about combining the flow, does it mean the second flow depends on first flow to run?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"kotlin"},"answer":"Calling `collect` on a Flow suspends the coroutine until the Flow is complete. For a MutableStateFlow, it will only be complete when it's cancelled. So usually, when you call `collect` on a Flow, you don't do anything else in the coroutine underneath that call.\n\n\nIf you want to collect each of these flows individually, you need two coroutines.\n\n\nThe `flowOnLifecycle` function will give you a bit cleaner code, so it's not as painful to have two coroutines:\n\n\n\n```\nprivate fun observe(){\n    viewModel.getState()\n        .flowOnLifecycle(Lifecycle.State.STARTED)\n        .onEach { state -> handleState(state) }\n        .launchIn(lifecycleScope)\n    viewModel.getProducts()\n        .flowOnLifecycle(Lifecycle.State.STARTED)\n        .onEach { products -> handleProducts(products) }\n        .launchIn(lifecycleScope)\n}\n\n```\n\nI also want to mention, function names like `getState` are unnatural in Kotlin, except when they are heavy functions (and even if it were a heavy function that has to calculate something, I would prefer `generateState` or `calculateState`). It's more proper to use a property:\n\n\n\n```\nprivate val mutableState = MutableStateFlow<HomeMainFragmentState>(HomeMainFragmentState.Init)\nval state: StateFlow<HomeMainFragmentState> get() = mutableState\n\n```\n\nThey have said that in a **future** version of Kotlin, there will probably be a better syntax for exposing a read-only version of a mutable class that doesn't require a second property. Something like this:\n\n\n\n```\nprivate val state = MutableStateFlow<HomeMainFragmentState>(HomeMainFragmentState.Init)\n    public get(): StateFlow<HomeMainFragmentState>\n\n```\n\n"}
{"questionId":"eadc50397e5e4bf09956f4d53270f57f","question":"Java 8 - Filter a string-X from a list using startsWith and Save the String-X to a list\nI am new to Java 8 and trying to get my head around how streams and filter works with list. I have a list of predefined strings and I have a string value which I am processing. I want to add the processed string to a new list if the string starts with any of the strings in the predefined list. If the string doesn't match any strings from the list then save it to another list. \n\n\nFor example: \n\n\n\n```\nList<String> internalIpAddresses= new ArrayList<>();\nList<String> externalIpAddresses = new ArrayList<>();\n\nList<String> ipAddresseses = new ArrayList<String>();\nipAddresses.add(\"10.\");\nipAddresses.add(\"132.174.\");\nipAddresses.add(\"192.168.\");\n\n\/\/ filter internal ip addresses\nfor(String ipAddress : ipAddresseses){\n\n     if(\"10.11.12.13\".startsWith(ipAddress)) {\n          internalIpAddresses.add(\"10.11.12.13\");\n     }\n}\n\n\/\/ filter external ip addresses\nfor(String ipAddress : ipAddresseses){\n\n     if(!\"5.6.7.8\".startsWith(ipAddress)) {\n          externalIpAddresses .add(\"5.6.7.8\");\n     }\n}\n\n```\n\nResult:\n\n\n\n```\ninternalIpAddresses: 10.11.12.13\nexternalIpAddresses : 5.6.7.8 \n\n```\n\nIs there a way this can be achieved in a simpler way using stream in java 8? \n\n\nLike:\n\n\n\n```\nipAddresseses.stream()\n     .filter(ipAddress -> clientIpAddress.startsWith(ipAddress)\n     .*if the clientIpAddress starts with any of the values in the list then add to internalIpAddresses List\n     .*if clientIpAddress doesn't start with any values in list then add to externalIpAddresses List\n\n```\n\nIn the end I want to save the `clientIpAddress` (\"10.11.12.13\" or \"5.6.7.8\"), not the values from the `ipAddresses` (\"10.\" or \"192.168.\") list.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"Simply, your iterative code using streams could be represented as :\n\n\n\n```\nList<String> ipAddresses = Arrays.asList(\"10.\", \"132.174.\", \"192.168.\");\n\nList<String> internalIpAddresses = ipAddresses.stream()\n        .filter(\"10.11.12.13\"::startsWith)\n        .map(ipAddress -> \"10.11.12.13\")\n        .collect(Collectors.toList());\n\nList<String> externalIpAddresses = ipAddresses.stream()\n        .filter(ipAddress -> !\"5.6.7.8\".startsWith(ipAddress)) \/\/ I doubt this should be '.filter(\"5.6.7.8\"::startsWith)'\n        .map(ipAddress -> \"5.6.7.8\")\n        .collect(Collectors.toList());\n\n```\n\nA general approach as suggested in comments for solving this could be using:\n\n\n\n```\nList<String> internalIpAddresses = Stream.of(\"10.11.12.13\") \/\/ can add more addresses\n        .filter(ip -> ipAddresses.stream().anyMatch(ip::startsWith))\n        .collect(Collectors.toList());\n\n```\n\n"}
{"questionId":"935286bd68c6460c972ce1ea6101fe16","question":"ByteArray to IFormFile\nI am developing some REST API with C# and Net Core  \n\nI have a function in my repository which accepts a parameter of type `IFormFile`. \n\n\n\n```\npublic async Task<bool> UploadFile(IFormFile file)\n{\n    \/\/ do some stuff and save the file to azure storage\n}\n\n```\n\nThis function is called by a controller method which pass it the uploaded file \n\n\n\n```\npublic class FileController : Controller\n{\n    public async Task<IActionResult> UploadDoc(IFormFile file\n    {\n        \/\/ Call the repository function to save the file on azure\n        var res = await documentRepository.UploadFile(file);\n    }\n}\n\n```\n\nNow I have another function that calls an external API which returns a file as a byte array. I'd like to save this byte array using the `repository.UploadFile` method but I can't cast the byte array object to `IFormFile`.  \n\nIs it possible?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"Your repo shouldn't be using `IFormFile`. That's an abstraction that only applies to *one* particular method of HTTP file transfer (namely a `multipart\/form-data` encoded request body). Something like your repo should have no knowledge of the source of the file (HTTP), nor *how* it was transmitted (`multipart\/form-data` vs `application\/json` for example).\n\n\nInstead, you should use `Stream` for your param. In your `UploadDoc` action, then, you can simply do:\n\n\n\n```\nusing (var stream = file.OpenReadStream())\n{\n    await documentRepository.UploadFile(stream);\n}\n\n```\n\nAnd, where you have just a byte array:\n\n\n\n```\nusing (var stream = new MemoryStream(byteArray))\n{\n    await documentRepository.UploadFile(stream);\n}\n\n```\n\nYou might also consider adding an overload of `UploadFile` that takes a `byte[]`, as creating a new memory stream from a byte array just to have a stream is a waste of resources. However, a `byte[]` has to be handled differently than a `Stream`, so it may require some duplication of logic to go that route. You'll need to evaluate the tradeoffs.\n\n\n"}
{"questionId":"34a65e4fa31141f487af3cb6dbfef4e4","question":"Too many queries problem with JPA + Hibernate even when using @Fetch(FetchMode.JOIN)\nI am developing REST application using spring boot and I am trying to optimize the performance of the queries. I am currently using `findAll` from the repositories which is causing performance issues. Code is given below:\n\n\nPerson Entity\n\n\n\n```\n@Entity\n@Table(name = \"cd_person\")\n@Data\n@NoArgsConstructor\npublic class Person {\n    ....\n    @OneToOne(fetch = FetchType.EAGER, cascade = CascadeType.ALL)\n    @JoinColumn(name = \"password_id\")\n    @Fetch(FetchMode.JOIN)\n    private Password password;\n    ....\n    @ManyToMany(fetch = FetchType.EAGER, cascade = {CascadeType.MERGE, CascadeType.PERSIST, CascadeType.REFRESH})\n    @JoinTable(name = \"cd_person_role\",\n        joinColumns = @JoinColumn(name = \"person_id\", referencedColumnName = \"id\"),\n        inverseJoinColumns = @JoinColumn(name = \"role_id\", referencedColumnName = \"id\"))\n    @Fetch(FetchMode.JOIN)\n    private Set<Role> roles = new HashSet<>();\n}\n\n```\n\nPassword Entity\n\n\n\n```\n@Entity\n@Table(name = \"cd_password\")\n@Data\n@NoArgsConstructor\npublic class Password {\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    @Column(name = \"id\", updatable = false, nullable = false)\n    private Long id;\n\n    @Column(name = \"password_hash\", nullable = false)\n    private String passwordHash;\n    .......\n}\n\n```\n\nRole Entity\n\n\n\n```\n@Entity\n@Table(name = \"cd_role\")\n@Data\n@NoArgsConstructor\npublic class Role {\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    private Long id;\n\n    @Column(name = \"role_type\")\n    @Enumerated(EnumType.STRING)\n    private RoleType roleType;\n    ....\n}\n\n```\n\nPerson Repository\n\n\n\n```\npublic interface PersonRepository extends CrudRepository<Person, Long> {\n\n    Optional<Person> findByEmail(String email);\n\n}\n\n```\n\nWhen I do a `personRepository.findAll()` there are select queries fired for each row in the person table to fetch the password and roles when I access the person. I know I can use `@Query` annotation with `JOIN FETCH` in the repository to make it force generate the single query but I was wondering if there was any other way to do so. I am looking for something which we can do at the entity level to reduce queries. \n\n\nUsing spring boot 2.1.5-RELEASE version and related dependencies. \n\n\nPS. The `@Data` and `@NoArgsConstructor` are Lombok annotations. \n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"java"},"answer":"The most minimal code change is to use the ad-hoc EntityGraph feature from spring data . Just override `PersonRepository` 's `findAll()` and use `@EntityGraph` to configure the graph. All entities in this graph will be fetched together. \n\n\n\n```\npublic interface PersonRepository extends CrudRepository<Person, Long> {\n\n    @EntityGraph(attributePaths = { \"password\", \"roles\" })\n    public List<Person> findAll();\n\n}\n\n```\n\nBehind scene it works like `JOIN FETCH`. Only single SQL with LEFT JOIN will be generated.\n\n\n"}
{"questionId":"32d42712c99745ddabde93c292890040","question":"What exactly does it mean to say a C++ object is movable?\nGiven the following definitions:\n\n\n- object -- a contiguous region of memory holding a value of some type.\n- value -- the bits of an object interpreted according to the objects type.\n\n\nSection 6.4.1 of the C++ Programming Language book [4th ed], when discussing value categories, states:\n\n\n\n> \n> Movable: The object may be moved from (i.e., we are allowed to move\n> its value to another location and leave the object in a valid but\n> unspecified state, rather than copying.\n> \n> \n> \n\n\nQuestion:\n\n\n1. What does \"move its value to another location\" really mean?\n2. How can a value (a bit interpretation) be moved?\n3. How can bytes of an object be \"moved\" without a copy? Just doesn't make sense.\n\n\nCan someone please explain?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c++"},"answer":"\n> \n> 1. What does \"move its value to another location\" really mean?\n> \n> \n> \n\n\nIt means that the other location has the value that the original had prior to the move, **and** that it is not important what value the original location has after the move.\n\n\n\n> \n> 2. How can a value (a bit interpretation) be moved?\n> \n> \n> \n\n\nFor example, simply by copying. If one wanted to do extra work beyond that (which is not motivated with just a bag of bits), one could set the original location to some other bit pattern, such as all zeroes, and it would still be considered a successful move. The difference here with copying is that a copy should leave the original unchanged.\n\n\n\n> \n> 3. How can bytes of an object be \"moved\" without a copy? Just doesn't make sense.\n> \n> \n> \n\n\nA copy is a valid move.\n\n\nSometimes the bits in the original have semantics of *owning* a resource. If there is only *one* resource, and you simply copy the bits, now *both* locations \"own\" the resource, resulting in duplicate disposing of said resource when both objects go out of scope.\n\n\nSo a move would transfer ownership of the resource to the new location and change the original location to not own a resource. A concrete example of this is an owning pointer: copy the pointer then set the original to `nullptr`.\n\n\n\n\n---\n\n\nA copy might be more expensive than a move (and it might not). But continuing with the owning pointer example: If you make a copy of that object, then after the copy, two resources must exist (assuming unique ownership of the resource).\n\n\nSo a copy doesn't copy the pointer. The copy copies the resource and then the new location points to the new resource. If the creation of that resource is expensive, then a move can be much cheaper by just copying the pointer and nulling the original.\n\n\n\n\n---\n\n\nGenerally speaking, move should be an optimization of copy when the type supports both operations. move should never be more expensive than copy. And if move has the same expense as copy, then the move operations can simply be not implemented and copy will handle moves seamlessly. It is up to the author of each type to maintain this paradigm.\n\n\nFor scalars (ints, pointers, doubles, etc.) copy and move are the same thing: copy the bits, don't alter the source.\n\n\n"}
{"questionId":"c98480b7d0164a6889b979dc5110cc4f","question":"LINQ to SQL not supported in .net 5.0?\nMy project used .NetFramework 4.6.2 with \"LINQ to SQL\" query from MSSQL.\nA class define all tables which are needed query in database and inheritance to DataContext(System.Data.Linq).\n\n\nRecently, I am going to upgrade the program to .net5.0.\nHowever, I discover \"LINQ to SQL\" seems like doesn't support to .net5.0.\n\n\nCould anyone advise how to write \"LINQ to SQL\" in .net5.0 or using other ways please?\n\n\nThanks.\n\n\n--\n\n\nApr 23 PM6:38 Update:\nI just found `SqlSugarCore` on Nuget, and test it is okay to use. But still wondering is there any similar tools could use?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"I have tried to use EFCore and it's work.\n\n\nJust install **Microsoft.EntityFrameworkCore** and **Microsoft.EntityFrameworkCore.SqlServer** in NuGet.\n\n\nReplace those showed errors from L2S to EFCore. Example: DataContext(**L2S**) => DbContext(**EFCOre**), public Table TableA;(**L2S**) => public DbSet TableA { get; set; }(**EFCore**)...etc\n\n\nBe careful of that if your table have more than one primary key, then you have to write **override OnModelCreating** as belowed, TableAClass has three keys, and TableBClass has one key.\n\n\n\u3002Setup key of per table\n\n\n\n```\nprotected override void OnModelCreating(ModelBuilder _modelBuilder)\n        {\n            _modelBuilder.Entity<TableAClass>().HasKey(_obj => new { _obj.Key1, _obj.Key2, _obj.Key3 });\n            _modelBuilder.Entity<TableBClass>().HasKey(_obj => new { _obj.Key1 });\n        }\n\n```\n\nI have got stuck few days, and couldn't find an example on internet. Thus, I decided to put my code here.\n\n\n\u3002Setup Database Class\n\n\n\n```\npublic class DBClassName : DbContext\n    {\n        public DbSet<TableAClass> TableAs { get; set; }\n        public DbSet<TableBClass> TableBs { get; set; }\n\n        protected override void OnModelCreating(ModelBuilder _modelBuilder)\n        {\n            _modelBuilder.Entity<TableAClass>().HasKey(_obj => new { _obj.Key1, _obj.Key2, _obj.Key3 });\n            _modelBuilder.Entity<TableBClass>().HasKey(_obj => new { _obj.Key1 });\n        }\n\n        public DBClassName(string _connStr) : base(GetOptions(_connStr))\n        {\n        }\n\n        private static DbContextOptions GetOptions(string connectionString)\n        {\n            return SqlServerDbContextOptionsExtensions.UseSqlServer(new DbContextOptionsBuilder<DBClassName>(), connectionString).Options;\n        }\n\n\n    }\n\n```\n\n\u3002Setup Table Class\n\n\n\n```\n[Table(\"TableName\")]\n    public class TableClassName\n    {\n        public string ColumnA { get; set;}\n\n        public string ColumnB { get; set;}\n\n    }\n\n```\n\nThis answer should credits to all who helping me in comments.\n\n\n"}
{"questionId":"33e3b0acbfac45b89011ed7ade105829","question":"get unsigned long long addition carry\nI want to get the carry bit of adding two unsigned 64-bit integers in c.\nI can use x86-64 asm if needed.\ncode:\n\n\n\n```\n#include <stdio.h>\n\ntypedef unsigned long long llu;\n\nint main(void){\n  llu a = -1, b = -1;\n  int carry = \/*carry of a+b*\/;\n  llu res = a+b;\n  printf(\"a+b = %llu (because addition overflowed), carry bit = %d\\n\", res, carry);\n  return 0;\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c"},"answer":"As @EugeneSh. observes, the carry is either 0 or 1. Moreover, given that `a` and `b` both have the same *unsigned* type, their sum is well defined even if the arithmetic result exceeds the range of their type. Moreover, the (C) result of the sum will be less than both `a` and `b` when overflow occurs, and greater otherwise, so we can use the fact that C relational operations evaluate to either 0 or 1 to express the carry bit as\n\n\n\n```\ncarry = (a + b) < a;\n\n```\n\nThat does not require any headers, nor does it depend on a specific upper bound, or even on `a` and `b` having the same type. As long as both have unsigned types, it reports correctly on whether the sum overflows the wider of their types or `unsigned int` (whichever is wider), which is the same as their sum setting the carry bit. As a bonus, it is expressed in terms of the sum itself, which I think makes it clear what's being tested.\n\n\n"}
{"questionId":"3278cf83e0b145e79381b5c99a05c379","question":"Is the array to pointer decay changed to a pointer object?\n\n```\nint a[] = {1, 2 ,3};\n\n```\n\nI understand that array names are converted to pointers. A term often used is that they decay to pointers.\n\n\nHowever to me, a `pointer` is a region of memory that holds the address to another region of memory, so:\n\n\n\n```\nint *p = a;\n\n```\n\ncan be drawn like this:\n\n\n\n```\n-----              -----\n  p    --------->  a[0].  .....\n-----              -----\n 0x1                0x9\n\n```\n\nBut `a` itself is not pointing to another region of memory, it IS the region of memory itself. \nSo when the compiler converts it to a pointer, does it save it (like `p`) somewhere in memory or\nit's an implicit conversion?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"\n> \n> \"But `a` itself is not pointing to another region of memory, it IS the region of memory itself.\n> \n> \n> \"So when the compiler converts it to a pointer, does it save it (like `p`) somewhere in memory or it's an implicit conversion?\"\n> \n> \n> \n\n\nIt is an implicit conversion. The compiler does not implement the creation of a separate pointer object in memory (which you can f.e. assign in any manner with a different memory address) to hold the address of the first element.\n\n\nThe standard states (emphasize mine):\n\n\n\n> \n> \"Except when it is the operand of the sizeof operator, or the unary & operator, or is a string literal used to initialize an array, an expression that has type \"array of type\" is converted to an expression with type \"pointer to type\" that points to the initial element of the array object and **is not an lvalue**. If the array object has register storage class, the behavior is undefined.\"\n> \n> \n> Source: ISO\/IEC 9899:2018 (C18), 6.3.2.1\/4\n> \n> \n> \n\n\nThe array is converted to an expression of pointer type, it is not an `lvalue`.\n\n\nThe compiler just evaluates `a` to `&a[0]` (pointer to `a[0]`).\n\n\n\n\n---\n\n\n\n> \n> \"I understand that array names are converted to pointers.\"\n> \n> \n> \n\n\nAn array does not always convert to a pointer to its first element. Look at the first part of the quote above. F.e. when used as `&a`, `a` does not decay to a pointer to its first element. Rather it gains a pointer to the whole array `int (*)[3]`.\n\n\n"}
{"questionId":"bca9c5c4f4d442079cff8ceffffab20e","question":"Go file not running which is not in main package\nI have a very simple Go project setup.\nAt root directory I have `go.mod` file and `main.go` and a folder called `main2`. Inside `main2` folder there is `main2.go` file.\n\n\n\n```\n\/\n|_ go.mod\n|_ main.go\n|_ main2\n   |_ main2.go\n\n```\n\nFrom root directory I am trying to run go run command\n\n\n\n```\ngo run main2\/main2.go\n\n```\n\nand it throws error:\n\n\n\n> \n> package command-line-arguments is not a main package\n> \n> \n> \n\n\nCan someone help?\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"go"},"answer":"The package of your main2.go file must be **main**. When there is a main package, and a function main in your project, the compiler knows it will be compiled as a executable, and not as a library.\n\n\nSo try to change **package command-line-arguments** to **package main** inside the main2\/main2.go file.\n\n\n"}
{"questionId":"fc9531d2f765436c878ca95c6543bc7b","question":"Difference in printing out pointer value vs array\nI have a question on printing out pointer value and array. \n\n\n\n```\nint arr[5] = { 1, 2, 3, 4, 5 };\nint * ptr = arr;\n\nfor (int i = 0; i < 5; i++) {\n    (*ptr) += 2;\n    ptr++;\n    printf(\"%d\", (*ptr));\n}\n\n```\n\nAbove is what I typed in first but it didn't work. So I erased the printf line and entered a new code which is this. And it worked. \n\n\n\n```\nfor (int i = 0; i < 5; i++) {\n    printf(\"%d \", arr[i]);\n}\n\n```\n\nI understand why the second one worked but still don't understand why first one didn't. \n\n\nExpected output was 3 4 5 6 7 but the actual output of the first code was \n2 3 4 5 -858993460\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"c"},"answer":"\n```\nint arr[5] = { 1, 2, 3, 4, 5 };\n    int * ptr = arr;\n\n    for (int i = 0; i < 5; i++) {\n        (*ptr) += 2;\n        ptr++;\n        printf(\"%d\", (*ptr));\n    }\n\n```\n\nThe reason is you are incrementing the pointer first and then printing its content.\n\n\nPerhaps you need to print the contents first then increment it to point next element.\n\n\n\n```\nint arr[5] = { 1, 2, 3, 4, 5 };\n    int * ptr = arr;\n\n    for (int i = 0; i < 5; i++) {\n        (*ptr) += 2;\n        printf(\"%d\", (*ptr));\n        ptr++;\n    }\n\n```\n\n"}
{"questionId":"d5692d1fac8b4154befde7c6456f1131","question":"godoc command not found\n**godoc** command doesn't work on my system (I'using Linux Mint 20 Ulyana).\n\n\nI've just tried this procedure:\n\n\n1. install godoc with following command:\n\n\n`go get golang.org\/x\/tools\/cmd\/godoc`\n\n\n2. Start godoc server:\n\n\n`godoc -http=:6060`\n\n\nThe result is:\n`bash: godoc: command not found`\n\n\nI'm using this go version `go version go1.15 linux\/amd64`\n\n\nAnd this is my PATH variable `\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin:\/usr\/games:\/usr\/local\/games:\/usr\/local\/go\/bin`\n\n\nAll other go commands (go build, go run and so on) work correctly.\n\n\nWhat can I do to make godoc command work?\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"go"},"answer":"Add `$GOPATH\/bin` to your `PATH` variable. Executables, like `godoc`, are installed to `$GOPATH\/bin`.\n\n\n\n```\nexport PATH=\"$GOPATH\/bin:$PATH\"\ngodoc -http=:6060\n\n```\n\n"}
{"questionId":"f99460f8f6c4439a82ccae4d84f08f64","question":"move occurs because value has type Vec, which does not implement the `Copy` trait\nI am writing a very simple recursive program for finding all prime numbers between two numbers:\n\n\n\n```\nuse std::cmp::PartialOrd;\nuse std::ops::{Add, Div, Rem, Sub};\n\nfn _is_prime<T>(n: T, dividend: T, one: T) -> bool\nwhere\n    T: Copy + Rem<Output = T> + Sub<Output = T> + PartialOrd,\n{\n    if dividend == one {\n        true\n    } else {\n        if n % dividend < one {\n            false\n        } else {\n            _is_prime(n, dividend - one, one)\n        }\n    }\n}\n\nfn _primes_between<'a, T>(a: T, b: T, one: T, v: &'a mut Vec<T>) -> &'a mut Vec<T>\nwhere\n    T: Copy + Rem<Output = T> + Add<Output = T> + Sub<Output = T> + PartialOrd,\n{\n    if a <= b {\n        if _is_prime(a, a - one, one) {\n            v.push(a);\n        }\n\n        _primes_between(a + one, b, one, v)\n    } else {\n        v\n    }\n}\n\nfn primes_between<T>(a: T, b: T) -> Vec<T>\nwhere\n    T: Copy + Div<Output = T> + Rem<Output = T> + Add<Output = T> + Sub<Output = T> + PartialOrd,\n{\n    let one = a \/ a;\n\n    let mut v: Vec<T> = Vec::new();\n\n    *_primes_between(a, b, one, &mut v)\n}\n\nfn main() {\n    primes_between(3, 13).iter().for_each(|i| println!(\"{}\", i));\n}\n\n\n```\n\nThe problem is:\n\n\n\n```\nerror[E0507]: cannot move out of a mutable reference\n  --> src\/main.rs:42:5\n   |\n42 |     *_primes_between(a, b, one, &mut v)\n   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ move occurs because value has type `std::vec::Vec<T>`, which does not implement the `Copy` trait\n\n```\n\nHow do I solve that error?\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"rust"},"answer":"I'm not 100% sure, but I think the problem is that `_primes_between()` returns a reference that the code on line 31 is trying to make a copy of. (by taking ownership with the `*` operator) You could fix the problem by calling `.clone()` on the result, but I think in this case you don't need `_primes_between()` to return a value - you can just add the appropriate entries to the `v` parameter instead. Something like\n\n\n\n```\nfn _primes_between<T>(a: T, b: T, one: T, v: &mut Vec<T>)\nwhere\n    T: Copy + Rem<Output = T> + Add<Output = T> + Sub<Output = T> + PartialOrd,\n{\n    if a <= b {\n        if _is_prime(a, a - one, one) {\n            v.push(a);\n        }\n\n        _primes_between(a + one, b, one, v);\n    }\n}\n\nfn primes_between<T>(a: T, b: T) -> Vec<T>\nwhere\n    T: Copy + Div<Output = T> + Rem<Output = T> + Add<Output = T> + Sub<Output = T> + PartialOrd,\n{\n    let one = a \/ a;\n\n    let mut v: Vec<T> = Vec::new();\n\n    _primes_between(a, b, one, &mut v);\n\n    v\n}\n\n```\n\n"}
{"questionId":"4767fc778b1042d6a750b1f0aa259344","question":"How to calculate the difference between rows in PySpark?\nThis is my DataFrame in PySpark:\n\n\n\n```\nutc_timestamp               data    feed\n2015-10-13 11:00:00+00:00   1       A\n2015-10-13 12:00:00+00:00   5       A\n2015-10-13 13:00:00+00:00   6       A\n2015-10-13 14:00:00+00:00   10      B\n2015-10-13 15:00:00+00:00   11      B\n\n```\n\nThe values of `data` are cumulative.\n\n\nI want to get this result (differences between consecutive rows, grouped by `feed`):\n\n\n\n```\nutc_timestamp               data    feed\n2015-10-13 11:00:00+00:00   1       A\n2015-10-13 12:00:00+00:00   4       A\n2015-10-13 13:00:00+00:00   1       A  \n2015-10-13 14:00:00+00:00   10      B\n2015-10-13 15:00:00+00:00   1       B\n\n```\n\nIn `pandas` I would do it this way:\n\n\n\n```\ndf[\"data\"] -= (df.groupby(\"feed\")[\"data\"].shift(fill_value=0))\n\n```\n\nHow can I do the same thing in PySpark?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"You can use `lag` as a substitute for `shift`, and `coalesce( , F.lit(0))` as a substitute for `fill_value=0`\n\n\n\n```\nfrom pyspark.sql.window import Window\nimport pyspark.sql.functions as F\n\nwindow = Window.partitionBy(\"feed\").orderBy(\"utc_timestamp\")\n\ndata = F.col(\"data\") - F.coalesce(F.lag(F.col(\"data\")).over(window), F.lit(0))\ndf.withColumn(\"data\", data)\n\n```\n\n"}
{"questionId":"4fbc1f8b5ab34b018b8d17eaf57f51c0","question":"In Jest, how can I unit test a method that subscribes to an observable\nI have various methods in my components that subscribe to methods in injected dependencies, that return observables.\n\n\nI want to write Jest unit tests to ensure that when these observables return \/ error, my methods do the correct thing.\n\n\nIn the below example I am trying to write a test that checks if `doAThing` has fired. Neither of the below tests work. They both fail with errors like \n\n\n\n> \n> 'returnMyObservable.subscribe is not a function'.\n> \n> \n> \n\n\n\n```\n\/\/ Example method to test component \npublic testFunction (): void {\n    this.myService.returnMyObservable.subscribe(\n        ( value ) => this.doAThing( value )\n    )\n}\n\n```\n\n\n```\ndescribe( 'myComponemt', () => {\n\n    let fixture;\n    let myServiceMock;\n\n    beforeEach( () => {\n        myServiceMock = {\n            returnMyObservable: fn()\n        }\n\n        fixture = new myComponent( myServiceMock );\n    });\n\n\n    \/\/ 1) I have tried mocking with a returned value\n    it ( 'should call do a thing when value is returned', () => {\n        myServiceMock.returnMyOnservable.mockReturnValue( true );\n\n        fixture.testFunction();\n\n        expect( fixture.doAThing ).toHaveBeenCalled();\n    });\n\n    \/\/ 2) I have tried returning an observable\n    it ( 'should call do a thing when value is returned', () => {\n        myServiceMock.returnMyOnservable.mockReturnValue( of( true ) );\n\n        fixture.testFunction();\n\n        expect( fixture.doAThing ).toHaveBeenCalled();\n    });\n\n});\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"I had a few other errors else where that were masking what was actually wrong with my tests - I have found that the best way to test the above functionality is with:\n\n\n\n```\ndescribe( 'MyComponent', () => {\n    let fixture;\n    let myServiceMock;\n\n    beforeEach( () => {\n        myServiceMock = {\n            returnMyObservable: jest.fn()\n        }\n\n        fixture = new MyComponent( myServiceMock );\n    });\n\n    it ( 'should call doAThing when value is returned', () => {\n        const doAThingSpy = jest.spyOn( fixture, 'doAThing' );\n        myServiceMock.returnMyObservable.mockReturnValue( of( true ) );\n\n        fixture.testFunction();\n\n        expect( doAThingSpy ).toHaveBeenCalledWith( true );\n    });\n});\n\n```\n\n(This is pretty much the same way to do it in Jasmine too)\n\n\n"}
{"questionId":"74c6eacd2c5a40eb85a7ecee97c1a7ca","question":"Specify type for file upload event in react typescript\nI don't quite understand how event typing works, I'd like to specify a type here, but cannot quite figure out how to do it.\nI cannot seem to find a type reference for this specific case.\n\n\n\n```\nprivate handleChange (event \/*:FileUplaodEvent or something *\/): void {\n    this.setState ({\n      csv: event.target.files[0],\n    });\n}\n\n```\n\nAny help would be appreciated...\n\n\nedit: As answer by Mukesh Soni stated I used `React.FormEvent<HTMLInputElement>`, but for whatever reason the interface for files is a bit different for this type, it is actually:\n`event.currentTarget.files` rather than `event.target.files`.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"The event argument for your input's onChange is actually react's synthetic event.\n\n\nReact synthetic event types are present on React object (assuming you have installed types for react).\n\n\n\n```\n(event: React.FormEvent<HTMLInputElement>)\n\n```\n\n`React.FormEvent` consists of all syhthetic events on form input elements. It is a generic type, i.e. accepts another type as an argument when you use it. In your case, that type argument is HTMLInputElement. Which results in a complete type of a form event on input element.\n\n\n"}
{"questionId":"692879107cc34fc6985e237785e94b4e","question":"Is it a problem if @types\/react and react have different versions?\nTypescript is added to the version 16 react application created with create react app.\nI have installed the latest v.18 version of @types\/react and @types\/react-dom, but I would like to know if it is a problem that the version is different from react.\nThank you.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"typescript"},"answer":"Yes, as you'll have TS types for React 18, but JS code for React 16. For example, you may be able to import a new feature from React 18 but get a runtime error because you only have React 16.\n\n\nYou can solve this by using `@types\/react` and `@types\/react-dom` version 16 until you're ready to upgrade to React 18.\n\n\n"}
{"questionId":"43b5570ebae94dd6a947bee5e0944ff4","question":"Typescript: static method vs function defined outside class\nWhat are the practical differences between a static class method, and a function defined outside a class (at the top of the file) in TypeScript?\n\n\nI know that there are differences regarding visibilty from other classes and files. When visibility is not a concern though (the function\/method is only used in one class), when should we use static methods rather than functions defined outside any classes.\n\n\nExample:\n\n\n\n```\nexport class Foo {\n  \n  constructor(bar: string) {\n    Foo.shout(bar);\n  }\n\n  private static shout(content: string) {\n    console.log(string.toUpperCase());\n  }\n}\n\n```\n\nVS\n\n\n\n```\nexport class Foo {\n  \n  constructor(bar: string) {\n    shout(bar);\n  }\n\n}\n\nfunction shout(content: string) {\n  console.log(string.toUpperCase());\n}\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"typescript"},"answer":"\n> \n> What are the practical differences between a static class method, and a function defined outside a class (at the top of the file) in TypeScript?\n> \n> \n> \n\n\nNot all that many:\n\n\n1. Code using the static method must have access to the class; code using a standalone function must have access to the standalone function instead.\n2. Using the static method technically involves a property lookup on the class's constructor function; using a standalone function doesn't. In practice, this will be optimized such that it doesn't matter.\n3. The code **in** the static method will have access to the constructor function's parent constructor function via `super` (something that's fairly unique to JavaScript); code in a standalone function does not (because there is no parent constructor).\n4. Specifically in regard to your example: Your standalone `shout` function is private to the module in which it appears; the static method on `Foo` is accessible from other modules that import `Foo`.\n\n\nSlightly closer to opinion-land:\n\n\n5. A standalone function is *standalone*, it doesn't provide much context for what it is or where it's defined to someone reading the code. A static method provides more context, via the class name. **But**, if you're using modules (as you seem to be), the `import` provides some context for the standalone function.\n\n\n\n> \n> ...when should we use static methods rather than functions defined outside any classes.\n> \n> \n> \n\n\nThat's largely a matter of opinion and style, so off-topic for Stack Overflow. If your team feels the need for consistency using some set of rules, agree a set and be consistent.\n\n\n"}
{"questionId":"8b0c175d974942319fe93120cf5f97f9","question":"How to use ES6 Proxy in Typescript?\nI would like to try some things around Proxies, but i struggle to get the simplest form running. \n\n\nI have the following code\n\n\n\n```\nconst myObj = {\n  x: 'Hello',\n};\n\nconst p = new Proxy(myObj, {\n  get: (target, key) => {\n    return key in target ? target[key] + ' World!' : 'nope';\n  },\n});\n\nconsole.log(p.x);\n\n```\n\nAnd i get the following error, but i have no clue why and how to solve it:\n\n\n\n```\nindex.ts:7:28 - error TS7053: Element implicitly has an 'any' type because expression of type 'string | number | symbol' can't be used to index type '{ x: string; }'.\n  No index signature with a parameter of type 'string' was found on type '{ x: string; }'.\n\n7     return key in target ? target[key] + ' World!' : 'nope';\n                             ~~~~~~~~~~~\n\nFound 1 error.\n\n```\n\nI think TS should be able to infer everything. What do i miss here?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"This is because the `key` is defined as `PropertyKey`. Meaning it could be `string | number | symbol` which is different index signature than your object which only has one key `x`. It's not really error in types - TS cannot be statically sure that your object won't be called with something other than `x` and assumes broader range on possible `key` values. One solution to this could be assuring TypeScript that it indeed will be a `keyof myObj`, since you are already doing a runtime check yourself:\n\n\n\n```\nreturn key in target ? target[key as keyof typeof target] + ' World!' : 'nope';\n\n```\n\n"}
{"questionId":"5cb76c19e69a41ecba602ea8158de87b","question":"Why is this borrow still \"active\"?\nThis is a simplified example of something I've encountered:\n\n\n\n```\ntrait Bla<'a> {\n    fn create(a: &'a Foo) -> Self;\n    fn consume(self) -> u8;\n}\n\nstruct Foo;\nimpl Foo {\n    fn take(&mut self, u: u8) {}\n}\n\nstruct Bar {\n    foo: Foo,\n}\n\nimpl Bar {\n    fn foobar<'a, 'b, B: Bla<'b>>(&'a mut self)\n    where 'a: 'b {\n        let u = {\n            \/\/ immutable borrow occurs here\n            \/\/ type annotation requires that `self.foo` is borrowed for `'b`\n            let foo: &'b Foo = &self.foo;\n            let b = B::create(foo);\n            b.consume()\n        };\n\n        \/\/ error[E0502]: cannot borrow `self.foo` as mutable because it is also borrowed as immutable\n        self.foo.take(u);\n    }\n}\n\n\n```\n\nWhy is `self.foo` still considered to be borrowed even after exiting the block it was borrowed on, with everything that used the borrow also dropped?\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"rust"},"answer":"The data might *flow* to another place.  \n\nTo understand why the compiler is correct that the immutable borrow *might* still exist, consider the following implementation of `Bla` that is valid:\n\n\n\n```\n#![feature(once_cell)]\n\n\/\/ The data stored in this global static variable exists for the entirety of runtime.\nstatic REFS: SyncLazy<Mutex<Vec<&'static Foo>>> = SyncLazy::new(|| Mutex::new(vec![]));\n\nstruct Bad;\nimpl Bla<'static> for Bad {\n    fn create(a: &'static Foo) -> Self {\n        \/\/ The reference can go somewhere other than `Self`!\n        REFS.lock().unwrap().push(a);\n        Bad\n    }\n\n    fn consume(self) -> u8 {\n        \/\/ Even if `self` is consumed here, \n        \/\/ `self` doesn't necessarily destory the immutable borrow.\n        0\n    }\n}\n\n```\n\nWhen your generic `foobar` function accepts any valid implementation of `Bla`, a valid implementation might take a `&'static` borrow that guarantees the borrow to last for the entire runtime. Therefore, it **is** an error because `consume` does not guarantee that the immutable borrow goes away.\n\n\n\n\n---\n\n\nYou are probably looking for the `for<>` syntax as that will allow the `Bad` implementation to exist but will disallow anyone from calling `foobar` with `Bad`:\n\n\n\n```\nimpl Bar {\n    fn foobar<B: for<'b> Bla<'b>>(&mut self) {\n        let u = {\n                let foo: &Foo = &self.foo;\n                let b = B::create(foo);\n                b.consume()\n        };\n        self.foo.take(u);\n    }\n}\nfn main() {\n    Bar::foobar::<Bad>(&mut Bar { foo: Foo })\n    \/\/ ^ error: implementation of `Bla` is not general enough\n    \/\/ | `Bad` must implement `Bla<'0>`, for any lifetime `'0`...\n    \/\/ | ...but it actually implements `Bla<'static>`\n}\n\n```\n\nIf this isn't the case, you might have to provide more implementation details rather than an example causing the diagnostic error.\n\n\n"}
{"questionId":"eb098df419b84391ac64323537e60ffe","question":"What is @context and why is it red?\nI'm using the BlazoredTypeahead component in a blazor server side app and I'd like to know where the @context keyword is coming from.\n\n\nThe following code runs fine, but VS is reporting that it Cannot resolve symbol 'context'. Naturally, I'm curious why it works and where context is coming from.\n\n\n\n```\n@inject IEquipmentService EquipmentService\n@inject AppDataService AppDataService\n<h3>ModelSelect<\/h3>\n\n<BlazoredTypeahead SearchMethod=\"SearchModels\"\n                   @bind-Value=\"equipModel\">\n    <SelectedTemplate>\n        @context.model\n    <\/SelectedTemplate>\n    <ResultTemplate>\n        @context.model (@context.model_desc)\n    <\/ResultTemplate>\n<\/BlazoredTypeahead>\n\n@if (equipModel != null)\n{\n    <p>Selected model is: @equipModel.model<\/p>\n}\n\n@code {\n\n\n    private EquipModel equipModel;\n\n\n    private async Task<IEnumerable<EquipModel>> SearchModels(string searchText)\n    {\n        var result = await EquipmentService.SearchModels(AppDataService.CurrentContact, searchText);\n\n        return await Task.FromResult(result.ToList());\n    }\n\n\n}\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c#"},"answer":"When you define a RenderFragment property in a component, you actually define a delegate that encapsulates a method that has a single parameter of type T, as for instance:\n\n\n\n```\nRenderFragment<TItem> RowTemplate\n\n```\n\nWhen you execute the delegate like this: \n\n\n\n```\n@foreach (var item in Items)\n {\n     <tr>@RowTemplate(item)<\/tr>\n }\n\n```\n\nYou're calling the delegate for each item in a list of items, passing the delegate an item object, say a Pet object (Note that the type specifier of delegate here is generic. **@context** is an internal variable provided by Blazor that contain the value passed to the delegate, as for instance:\n\n\n\n```\n<RowTemplate>\n  <td>@context.ID<\/td>\n  <td>@context.Name<\/td>\n<\/RowTemplate>  \n\n```\n\nNote: The content within the opening and closing of the RowTemplate element represents the fragment of content that is produced by the RowTemplate RenderFragment delegate... once again when the content is produced, you get the value passed to the delegate and use it. If the value is a Pet object, for example, with two fields, you can access the value of the fields by using expression like @context.Name.\n\n\nNote that there is a way to use the type name of the object passed instead of using @context, but that's another story. \n\n\n\n> \n> but VS is reporting that it Cannot resolve symbol 'context'\n> \n> \n> \n\n\nThis is a warning, right ? I did not inspect your code, but it seems to me that the type specifier used is the generic type, and VS cannot resolve the underlying type. In my example the type specifier is TItem, but the type of the list of items passed to the `RenderFragment<TItem> RowTemplate` property is of type Pet.\n\n\nAs far as I recall, previously there was an issue with the generic type, and you had to pass the underlying type passed to a templated component... You had to write your code like this:\n\n\nUsage of templated component:\n\n\n\n```\n <TableTemplate TItem=\"Pet\" Items=\"pets\">\n\n <\/TableTemplate>\n\n```\n\nAs you can see we assign a list of Pet objects named `\"pets\"` to the Items attribute, but we had also to specify the type of the objects passed in the list (`TItem=\"Pet\"`). As far as I know this issue was solved, perhaps the code of BlazoredTypeahead was written before?\n\n\n"}
{"questionId":"0c46b10c1d63428691d089778582b226","question":"list return from Inline::Perl5 gives a count of items, not the list\nSome simple Inline::Perl5 code returns a list, but it seems to return the count of the items rather than the actual list.\n\n\nChanging the number of items involved changes the count.\n\n\n\n```\nuse Inline::Perl5;                                             \nmy $p5 = Inline::Perl5.new;                                    \n                                                               \nmy $perl5_code = q:to\/END\/;                                    \n   sub p5_data {                                               \n      my @monsters = qw( godzilla blob tingler kong dorisday );\n      return @monsters;                                        \n   }                                                           \n                                                               \n   p5_data();                                                  \nEND                                                            \n                                                               \nmy @stuff = $p5.run: $perl5_code;                              \nsay @stuff; # [5]                                              \n\n```\n\nI thought I'd get the list of strings stored in the array, but instead it acts like something is switching it to scalar context.\n\n\n**Update:**\n\n\nikeami points out that it works better to return the reference to\nthe array:\n\n\n\n```\nreturn \\@monsters;\n\n```\n\nThough, then you end up with an array in the first element of the\n@stuff array when you do this:\n\n\n\n```\nmy @stuff = $p5.run: $perl5_code;                              \n\n```\n\nAn alternate approach (from reading the Inline::Perl5 docs), is after doing a `$p5.run` to define the perl5 sub, to call it from perl6:\n\n\n\n```\nmy @stuff = $p5.call('p5_data');  \n\n```\n\nThen the list return (`return @monsters;`) gets loaded into the\narray as I expected:\n\n\n\n```\n[godzilla blob tingler kong dorisday]\n\n```\n\nThis is a recently installed Inline::Perl5 of version 0.40, on\n\"Rakudo Star version 2019.03.1 built on MoarVM version 2019.03 implementing\nPerl 6.d\".\n\n\n**Update2:** So, it seems that \"run\" implies a scalar context and \"call\" is a list context.\n\n\n\n```\nuse Inline::Perl5;\nmy $p5 = Inline::Perl5.new;\n\nmy $perl5_defsub = q:to\/END\/;\n   sub whadaya_want {\n       wantarray() ? 'LIST' : 'SCALAR';\n   }\nEND\n\n$p5.run: $perl5_defsub;\n\nmy $run_context  = $p5.run(  'whadaya_want' );\nmy $call_context = $p5.call( 'whadaya_want' );  \n\nsay \"run: \", $run_context;\nsay \"call: \", $call_context;\n# run: SCALAR\n# call: LIST\n\n```\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"perl"},"answer":"`Perl5::Inline` puts return values into scalar context.\n\n\nAs background, in Perl 5, context flows inwards into routines, so a routine always knows which context it's in.\n\n\nIn Perl 6, context flows outwards, so a routine returns an object that can behave differently in different context.\n\n\nThis impedance mismatch between Perl 5 and Perl 6 means that `Inline::Perl5` has to decide to call Perl 5 routines in one context, and that's scalar.\n\n\nAs ikegami pointed out, the proper solution is to return a proper scalar, aka reference (in Perl 5 speak). Limitations in `Inline::Perl5` might mean you need to explicitly dereference on the Perl 6 side.\n\n\n"}
{"questionId":"046eeb7c6b41437d9cb09348d72e6a83","question":"Firefox refresh current tab from command-line\nI'd like to trigger a tab refresh on Firefox from a command line. I'm working on a web app and the refresh goes after the app compiles. I trigger the compile from a command in my IDE. It's not a fixed url, nor can it be derived from the open file in the IDE. Thus, the currently open url in the active tab.\n\n\nThe thing is, I'm in a double headed box with no Xinerama support, which means I can't alt+tab to Firefox, instead I must move the mouse to the other screen, click on Firefox and then Ctrl+R. That can't be right.\n\n\nI tried some sort of bookmarklet stuff, like `DISPLAY=':0.1' firefox -remote 'openurl(javascript:alert(1);)'` , but FF wouldn't run that.\n\n\nAny ideas?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"Here's an updated version of @geekQ's script that will return focus to the previous program (however, the firefox window will still be pulled to the top).\n\n\n\n```\n#!\/bin\/bash\n\nCURRENT_WID=$(xdotool getwindowfocus)\n\nWID=$(xdotool search --name \"Mozilla Firefox\")\nxdotool windowactivate $WID\nxdotool key F5\n\nxdotool windowactivate $CURRENT_WID\n\n```\n\n"}
{"questionId":"13595f2ea6aa472082b61d6103e1e09b","question":"Bash evaluation of expression .[].[]\nConsider the following expression evaluations:\n\n\n\n```\n$ echo .\n.\n$ echo .[]\n.[]\n$ echo .[].\n.[].\n$ echo .[].[]\n..                # <- WAT??\n$ echo .[].[].\n.[].[].\n$ echo .[].[].[]\n.[].[].[]\n\n```\n\nCan someone explain why `.[].[]` has this special behavior?\n\n\n(Tested in bash `3.2.57(1)-release (x86_64-apple-darwin18)` and `4.4.23(1)-release (arm-unknown-linux-androideabi)`.\n\n\nI suspect it has something to do with `..` being a valid \"file\" (the parent directory). But then why doesn't e.g. `.[].` produce the same result?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"bash\/shell"},"answer":"That's because `[]` creates a character class. Normally, `]` must be escaped (backslashed) to be included in the class, but you don't have to escape it if it immediately follows the opening bracket. Therefore, `[].[]` in fact means `[\\].[]` which matches any of the characters `.`, `]`, and `[`.\n\n\nYou can verify it by creating files named `.]` and `.[`.\n\n\n\n```\ntouch .\\[ .\\]\necho .[].[]  # .. .[ .]\n\n```\n\n"}
{"questionId":"169536e048314f1fb0a12e95cd3e4611","question":"How to write generic function with two inputs?\nI am a newbee in programming, and I run into an issue with R about generic function: how to write it when there are multiple inputs?\n\n\nFor an easy example, for dataset and function\n\n\n\n```\nz <- c(2,3,4,5,8)\ncalc.simp <- function(a,x){a*x+8}\n# Test the function:\ncalc.simp(x=z,a=3)\n[1] 14 17 20 23 32\n\n```\n\nNow I change the class of z:\nclass(z) <- 'simp'\nHow should I write the generic function 'calc' as there are two inputs?\nMy attempts and errors are below:\n\n\n\n```\ncalc <- function(x) UseMethod('calc',x)\ncalc(x=z)\nError in calc.simp(x = z) : argument \"a\" is missing, with no default\n\n```\n\nAnd\n\n\n\n```\ncalc <- function(x,y) UseMethod('calc',x,y)\nError in UseMethod(\"calc\", x, y) : unused argument (y)\n\n```\n\nMy confusion might be a fundamental one as I am just a beginner. Please help! Thank you very much!\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"r"},"answer":"I'd suggest you model your generic function off of the template used by innumerable base R functions as, e.g., `mean`:\n\n\n\n```\n> mean\nfunction (x, ...) \nUseMethod(\"mean\")\n\n```\n\nIn your case, that would translate to the following generic which (if I understand your question correctly) works just fine:\n\n\n\n```\ncalc <- function(x, ...) UseMethod('calc')\n\ncalc.simp <- function(a, x) {\n    x <- unclass(x)\n    a * x + 8\n}\n\n\n## Try it out\n\nz <- c(2,3,4,5,8)\nclass(z) <- \"simp\"\n\ncalc.simp(x = z, 10)\n## [1] 28 38 48 58 88\n\ncalc(x = z, 10)\n## [1] 28 38 48 58 88\n\n```\n\n"}
{"questionId":"185af3648ab3473884e2c7d464da8724","question":"Fetching python class name while using abstract classes with `abc` library\nI want to extract the python class name while using abstract classes with `abc` library. I unfortunately instead receive the class name `ABCMeta`.\n\n\n\n```\n\nimport abc\n\nclass A(abc.ABC)\n    pass\n\nclass B(A)\n    pass\n\nprint(A.__class__.__name__)  # output: 'ABCMeta'\nprint(B.__class__.__name__)  # output: 'ABCMeta'\nprint(str(A))  # output: \"<class '__main__.A'>\"\nprint(str(B))  # output: \"<class '__main__.B'>\"\n\n\n```\n\nI expect that I should receive the output as below\n\n\n\n```\nprint(A.__class__.__name__)  # output: 'A'\nprint(B.__class__.__name__)  # output: 'B'\n\n```\n\nThe `str(A)` and `str(B)` seems to print the class name so I assume the class name can be extracted from somewhere. But nonetheless, I am not interested to use `str` to parse and get the class name.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"Recall that a metaclass is the *type of a class*, while a class is the type of its instances.\n\n\nIf we have `a = A()`, `a` is of type `A`, and `A` is of type `abc.ABCMeta`. Therefore, you should naturally expect that `A.__class__` and `B.__class__` both return `abc.ABCMeta`, since they are instances of it!\n\n\nWhat you want is the names of `A` and `B` themselves, which you can get with `A.__name__` and `B.__name__` respectively.\n\n\n"}
{"questionId":"515f8b574cb449868d5a44dcc7fec760","question":"Calling a static method from a generic constraint Dart\nI'm trying to call a static method from a generic type I receive.\nIs that even possible?\n\n\nFurthermore, I apply a Type constraint in order to only manipulate the object from its parent class.\n\n\nHere is a short example of what I'm trying to achieve:\n\n\n\n```\nclass A {\n  static func() {\n    print(\"A\");\n  }\n}\n\nclass B extends A {\n  static func() {\n    print(\"B\");\n  }\n}\n\nconcret<T extends A>() {\n  T.func(); \/\/ I expected a print('B')\n}\n\nmain() {\n    concret<B>();\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"dart"},"answer":"No, it's not possible.\n\n\nDart static method invocations are resolved at compile-time, so it's not possible to call them on type variables which only have a value at run-time.\n\n\nIf it was possible, it would be completely unsafe. Anyone can create a class `C` extending `A` which does not have a static `func` member and invoke `concret<C>();`. Since static members are not inherited, it would have to give you a run-time error, and there is nothing you can do to detect that at compile-time. That is the primary reason why it is not allowed.\n\n\n"}
{"questionId":"52901ce6782442ba876dc212d54fcccf","question":"Pointer to one before first element of array\nIt is said in C that when pointers refer to the same array or one element past the end of that array the arithmetics and comparisons are well defined. Then what about one before the first element of the array? Is it okay so long as I do not dereference it?\n\n\nGiven\n\n\n\n```\nint a[10], *p;\np = a;\n\n```\n\n(1) Is it legal to write `--p`?\n\n\n(2) Is it legal to write `p-1` in an expression?\n\n\n(3) If (2) is okay, can I assert that `p-1 < a`?\n\n\nThere is some practical concern for this. Consider a `reverse()` function that reverses a C-string that ends with `'\\0'`.\n\n\n\n```\n#include <stdio.h>\n\nvoid reverse(char *p)\n{\n    char *b, t;\n\n    b = p;\n    while (*p != '\\0')\n        p++;\n    if (p == b)      \/* Do I really need *\/\n        return;      \/* these two lines? *\/\n    for (p--; b < p; b++, p--)\n        t = *b, *b = *p, *p = t;\n}\n\nint main(void)\n{\n    char a[] = \"Hello\";\n\n    reverse(a);\n    printf(\"%s\\n\", a);\n    return 0;\n}\n\n```\n\nDo I really need to do the check in the code?\n\n\nPlease share your ideas from language-lawyer\/practical perspectives, and how you would cope with such situations.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c"},"answer":"\n> \n> (1) Is it legal to write --p?\n> \n> \n> \n\n\nIt's \"legal\" as in the C syntax allows it, but it invokes undefined behavior. For the purpose of finding the relevant section in the standard, `--p` is equivalent to `p = p - 1` (except `p` is only evaluated once). Then:\n\n\n\n> \n> C17 6.5.6\/8\n> \n> \n> If both the pointer\n>  operand and the result point to elements of the same array object, or one past the last\n>  element of the array object, the evaluation shall not produce an overflow; otherwise, the behavior is undefined.\n> \n> \n> \n\n\nThe *evaluation* invokes undefined behavior, meaning it doesn't matter if you de-reference the pointer or not - you already invoked undefined behavior.\n\n\nFurthermore:\n\n\nC17 6.5.6\/9:\n\n\n\n> \n> When two pointers are subtracted, both shall point to elements of the same array object, or one past the last element of the array object;\n> \n> \n> \n\n\nIf your code violates a \"shall\" in the ISO standard, it invokes undefined behavior.\n\n\n\n> \n> (2) Is it legal to write p-1 in an expression?\n> \n> \n> \n\n\nSame as (1), undefined behavior.\n\n\n\n\n---\n\n\nAs for examples of how this could cause problems in practice: imagine that the array is placed at the very beginning of a valid memory page. When you decrement outside that page, there could be a hardware exception or a pointer trap representation. This isn't a completely unlikely scenario for microcontrollers, particularly when they are using segmented memory maps.\n\n\n"}
{"questionId":"901f88f0539b42a0a84f31dbd324bdcd","question":"How can I fix unused imports in Rust automatically?\nIs there any way to fix ONLY unused imports in Rust automatically? I have seen `cargo fix`, which does work, but it makes a whole bunch of other fixes too. Is there any way to tell `cargo fix` to only fix unused imports (preferably in a specified file) and nothing else?\n\n\nSide question: Is there any way to make IntelliJ do this? The usual shortcut (Cmd+Option+O) works in other languages, but for Rust, it just re-orders the imports.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"rust"},"answer":"Individual lints are currently only configurable via attributes in your source code. There is no way to tell `cargo check` or `cargo fix` to ignore certain lint rules through the command line tool.\n\n\nFrom my experience with JetBrains tools, it's unlikely that they'd use `cargo fix` under the hood anyway. They have built their own Rust tooling for parsing and analysing code, and that is where this feature would go.\n\n\nI suggest submitting a feature request to JetBrains.\n\n\n"}
{"questionId":"90d11c5ad6a14030a99fa340935b8631","question":"is there a way to convert h2oframe to pandas dataframe\nI am able to convert dataframe to h2oframe but how can I convert back to a dataframe? If this is possible not can I convert it to a python list?\n\n\n\n```\nimport pandas as pd\nimport h2o\ndf = pd.DataFrame({'1': [2838, 3222, 4576, 5665, 5998], '2': [1123, 3228, 3587, 5678, 6431]})\ndata = h2o.H2OFrame(df)\n\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"Convert h2o frame to List\n\n\n\n```\ndata_as_list = h2o.as_list(data, use_pandas=False)\n\n```\n\nConvert h2o frame to pandas DataFrame. (Default `use_pandas=True`)\n\n\n\n```\ndata_as_df = h2o.as_list(data)\n\n```\n\n"}
{"questionId":"42c74273b3cb48f580e17ea612f868e2","question":"What is the time complexity of type casting function in python?\nFor example, \n `int(x)`\n`float(x)`\n`str(x)`\nWhat is time complexity of them?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"python"},"answer":"There is no definite answer to this because it depends not just what type you're converting to, but also what type you're converting from.\n\n\nLet's consider just numbers and strings. To avoid writing \"log\" everywhere, we'll measure the size of an `int` by saying n is how many bits or digits it takes to represent it. (Asymptotically it doesn't matter if you count bits or digits.) For strings, obviously we should let n be the length of the string. There is no meaningful way to measure the \"input size\" of a `float` object, since floating-point numbers all take the same amount of space.\n\n\n- Converting an `int`, `float` or `str` to its own type ought to take \u0398(1) time because they are immutable objects, so it's not even necessary to make a copy.\n- Converting an `int` to a `float` ought to take \u0398(1) time because you only need to read at most a fixed constant number of bits from the `int` object to find the mantissa, and the bit length to find the exponent.\n- Converting an `int` to a `str` ought to take \u0398(n2) time, because you have to do \u0398(n) division and remainder operations to find n digits, and each arithmetic operation takes \u0398(n) time because of the size of the integers involved.\n- Converting a `str` to an `int` ought to take \u0398(n2) time because you need to do \u0398(n) multiplications and additions on integers of size \u0398(n).\n- Converting a `str` to a `float` ought to take \u0398(n) time. The algorithm only needs to read a fixed number of characters from the string to do the conversion, and floating-point arithmetic operations (or operations on bounded `int` values to avoid intermediate rounding errors) for each character take \u0398(1) time; but the algorithm needs to look at the rest of the characters anyway in order to raise a `ValueError` if the format is wrong.\n- Converting a `float` to any type takes \u0398(1) time because there are only finitely many distinct `float` values.\n\n\nI've said \"ought to\" because I haven't checked the actual source code; this is based on what the conversion algorithms need to do, and the assumption that the algorithms actually used aren't asymptotically worse than they need to be.\n\n\nThere could be special cases to optimise the `str`-to-`int` conversion when the base is a power of 2, like `int('11001010', 2)` or `int('AC5F', 16)`, since this can be done without arithmetic. If those cases are optimised then they should take \u0398(n) time instead of \u0398(n2). Likewise, converting an `int` to a `str` in a base which is a power of 2 (e.g. using the `bin` or `hex` functions) should take \u0398(n) time.\n\n\n"}
{"questionId":"e2f39b4f3fef4cc487bb2ee2c159ecb4","question":"Postgres race condition involving subselect and foreign key\nWe have 2 tables defined as follows\n\n\n\n```\nCREATE TABLE foo (\n  id BIGSERIAL PRIMARY KEY,\n  name TEXT NOT NULL UNIQUE\n);\n\nCREATE TABLE bar (\n  foo_id BIGINT UNIQUE,\n  foo_name TEXT NOT NULL UNIQUE REFERENCES foo (name)\n);\n\n\n```\n\nI've noticed that when executing the following two queries concurrently\n\n\n\n```\nINSERT INTO foo (name) VALUES ('BAZ')\n\n```\n\n\n```\nINSERT INTO bar (foo_name, foo_id) VALUES ('BAZ', (SELECT id FROM foo WHERE name = 'BAZ'))\n\n```\n\nit is possible under certain circumstances to end up inserting a row into `bar` where `foo_id` is `NULL`. The two queries are executed in different transactions, by two completely different processes.\n\n\nHow is this possible? I'd expect the second statement to either fail due to a foreign key violation (if the record in `foo` is not there), or succeed with a non-null value of `foo_id` (if it is).\n\n\nWhat is causing this race condition? Is it due to the subselect, or is it due to the timing of when the foreign key constraint is checked?\n\n\nWe are using isolation level \"read committed\" and postgres version 10.3.\n\n\n**EDIT**\n\n\nI think the question was not particularly clear on what is confusing me. The question is about how and why 2 different states of the database were being observed during the execution of a single statement. The subselect is observing that the record in foo as being absent, whereas the fk check sees it as present. If it's just that there's no rule preventing this race condition, then this is an interesting question in itself - why would it not be possible to use transaction ids to ensure that the same state of the database is observed for both?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"sql"},"answer":"The subselect in the `INSERT INTO bar` cannot see the new row concurrently inserted in `foo` because the latter is not committed yet.\n\n\nBut by the time that the query that checks the foreign key constraint is executed, the `INSERT INTO foo` has committed, so the foreign key constraint doesn't report an error.\n\n\nA simple way to work around that is to use the `REPEATABLE READ` isolation level for the `INSERT INT bar`. Then the foreign key check uses the same snapshot as the `INSERT`, it won't see the newly committed row, and a constraint violation error will be thrown.\n\n\n"}
{"questionId":"da8e0d209a224f24bf3574b734087e31","question":"Fill missing values rowwise (right \/ left)\nI'm looking for a way to \"fill\" `NA`s to the right (as opposed to down\/up) with dplyr. In other words, I would like to convert d into d2 without having to explicitly reference any columns in a mutate call.\n\n\nMy real dataframe has several 10s of fields with staggered blocks of NAs spanning variable numbers of columns. I'm curious whether there's a short way to globally inherit the first non-NA value to the left, regardless of what field it occurs in.\n\n\n\n```\nd<-data.frame(c1=c(\"a\",1:4), c2=c(NA,2,NA,4,5), c3=c(NA,3,4,NA,6))\nd2<-data.frame(c1=c(\"a\",1:4), c2=c(\"a\",2,2,4,5), c3=c(\"a\",3,4,4,6))\nd\nd2\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"r"},"answer":"We can do a `gather` into 'long' format, do the `fill` grouped by the row number and then `spread` back to 'wide' format\n\n\n\n```\nlibrary(tidyverse)\nrownames_to_column(d, 'rn') %>% \n    gather(key, val, -rn) %>%\n    group_by(rn) %>% \n    fill(val) %>% \n    spread(key, val) %>%\n    ungroup %>%\n    select(-rn)\n# A tibble: 5 x 3\n#  c1    c2    c3   \n#  <chr> <chr> <chr>\n#1 a     a     a    \n#2 1     2     3    \n#3 2     2     4    \n#4 3     4     4    \n#5 4     5     6 \n\n```\n\n\n\n---\n\n\nor another option without reshaping would be doing rowwise fill with `na.locf`\n\n\n\n```\nlibrary(zoo)\nd %>% \n    mutate(c1 = as.character(c1)) %>%\n    pmap_dfr(., ~ na.locf(c(...)) %>%\n                      as.list %>%\n                      as_tibble)\n\n```\n\n\n\n---\n\n\nAlso, if we use `na.locf`, it run columnwise, so the data can be transposed and apply `na.locf` directly\n\n\n\n```\nd[] <- t(na.locf(t(d)))\nd\n#  c1 c2 c3\n#1  a  a  a\n#2  1  2  3\n#3  2  2  4\n#4  3  4  4\n#5  4  5  6\n\n```\n\nAs @G.Grothendieck mentioned in the comments, inorder to take care of the elements that are NA at the beginning of the row, use `na.locf0` instead of `na.locf`\n\n\n"}
{"questionId":"bd40bc3d4c64472a820dc48935e76a60","question":"Struct pointer (address), and default constructor\nDoes taking address of a C# struct cause default constructor call?\n\n\nFor example, I got such structs:\n\n\n\n```\n[StructLayout(LayoutKind.Sequential, Pack = 1)]\npublic struct HEADER {\n    public byte OPCODE;\n    public byte LENGTH;\n}\n\n[StructLayout(LayoutKind.Sequential, Pack = 1)]\npublic struct S {\n    public HEADER Header;\n    public int Value;\n}\n\n```\n\nThen, of course, I can't do this:\n\n\n\n```\nS s;                \/\/ no constructor call, so...\nvar v = s.Value;    \/\/ compiler error: use of possibly unassigned field 'Value'\n\n```\n\nBut once I obtain pointer to the struct, I can read its fields even without using the pointer, and even embedded struct's fields:\n\n\n\n```\nS s;\nS* ps = &s;\nvar v1 = ps->Value;        \/\/ OK, expected\nvar v2 = s.Value;          \/\/ OK!\nvar len = s.Header.LENGTH; \/\/ OK!\n\n```\n\nSo, does it call the default constructor somehow, or - once I take the address - C# stops caring about the memory?\n\n\nPS: The memory seems to be zero-initialized anyway.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c#"},"answer":"\n> \n> Does taking address of a C# struct cause default constructor call?\n> \n> \n> \n\n\nNo. It just circumvents the compiler check. \n\n\nThe \"use of possibly unassigned field\" is a nicety to protect you against yourself. But it can easily be worked around. And in this case it does not seem so critical. \n\n\n\n> \n> PS: The memory seems to be zero-initialized anyway.\n> \n> \n> \n\n\nYes, that will almost always (see below) be the case in .NET, making the \"default constructor call\" question a bit academic. What happens to your memory is not so tightly coupled to the compiler warning. \n\n\n"}
{"questionId":"85bdd85f7b7949d8a8b2eb5613497440","question":"Can't customize color palette types on Material UI theme in TypeScript\nI create a type interface to add custom properties to the Palette like so:\n\n\n\n```\ndeclare module @material-ui\/core\/styles\/createMuiTheme {\n  interface PaletteColor {\n    transparent?: string;\n    gradient?: PaletteColor;\n  }\n  interface Palette {\n    gradient?: PaletteColor;\n    transparent?: PaletteColor;\n    secondary?: {\n      transparent?: string;\n    }\n  }\n\n  interface PaletteColorOptions {\n    main?:string;\n    dark?:string;\n    light?:string;\n    transparent?: string;\n    gradient?: string;\n    test?: string\n  }\n}\n\n```\n\nI am trying to a lot of interfaces to get it to work...\n\n\nThen I use those types in my theme like this:\n\n\n\n```\nexport default function createMyTheme(options: ThemeOptions) {\n    return createMuiTheme({\n      ...options,\n    })\n  }\n\n```\n\nI use that function to create my main theme by importing it and calling it:\n\n\n\n```\nconst theme = createMyTheme({});\n\n```\n\nAnd then I set my component style like this:\nbackground: theme.palette.gradient.main,\n\n\nand it tells me this:\n\n\n\n```\nProperty 'gradient' does not exist on type 'Palette'.\n\n```\n\nEnvironment:\n\"@material-ui\/core\": \"^4.9.2\",\n\"react\": \"^16.12.0\",\n\"typescript\": \"^3.7.5\"\n\n\nHere is my full theme:\n\n\n\n```\nconst theme = createMyTheme({\n  palette: {\n    primary: {\n      main: '#5FB9A6',\n      dark: 'linear-gradient(-68deg, #151E27 , #335850)',\n      gradient: 'linear-gradient(-68deg, #151E27 , #335850)'\n    },\n    secondary: {\n      main: '#C68A77',\n      transparent: 'rgba(198, 138, 119, 0.7)'\n    },\n    error: {\n      light: '#e5a0a0',\n      main: '#CD5C5C',\n      dark: '#992c2c',\n    },\n    text: {\n      primary:'#20383C',\n      secondary: '#151E27',\n      hint: 'rgba(32, 56, 60, 0.7)'\n    },\n    background: {\n      paper: '#fff'\n    },\n    common: {\n      white: \"#FFF\"\n    }\n  },\n  typography: {\n     fontFamily: '\"Work Sans\"'\n  }\n\n```\n\nAny help would be greatly appreciated! Thanks!\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"According to the documentation you need `module augmentation` to add custom properties to the palette.\n\n\nWhat I did was:\n\n\nCreated a file `expanded-theme.ts` in the root directory (same directory where `App.tsx` is)\n\n\n\n```\nimport '@material-ui\/core\/styles';\n\ndeclare module '@material-ui\/core\/styles\/createPalette' {\n  interface Palette {\n    myCustomColor?: Palette['primary'];\n  }\n  interface PaletteOptions {\n    myCustomColor?: PaletteOptions['primary'];\n  }\n}\n\n```\n\nNext, you can define the custom properties in your theme (I did not need to import `expanded-theme.ts`)\n\n\n\n```\nimport React from 'react';\nimport { createMuiTheme, MuiThemeProvider } from '@material-ui\/core';\n\n\/\/ ...\n\nconst theme = createMuiTheme({\n  palette: {\n    myCustomColor: {\n      main: 'blue'\n    }\n  }\n});\n\n\/\/ ...\n\n<MuiThemeProvider theme={theme}>\n\n\/\/...\n\n\n```\n\nNow you can use `myCustomColor` in your styles :).\n\n\n"}
{"questionId":"373ebcf86e964b0ab23b597cfbb4903d","question":"How to correctly import custom types in typescript\nI have custom type in `src\/@types\/app.d.ts` ( `export type AuthKeyT = string` )\n\n\nI would like to use it in `tsx` react file (using `import { AuthKeyT } from '@types\/app'`), but I am getting an error:\n\n\n\n> \n> Cannot import type declaration files. Consider importing 'app' instead of '@types\/app' \n> \n> \n> \n\n\nSo, when I use `import { AuthKeyT } from 'app'` I am getting:\n\n\n\n> \n> Cannot find module 'app'.\n> \n> \n> \n\n\nSo.. what is wrong? How should I import custom type?\n\n\nI am using typescript `3.7.2` in react app.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"As the error says, you can't import a `.d.ts` file in TypeScript.\n\n\n`import ... from 'app'` mean implicitly `import ... from 'app.ts'` or this file doesn't exist so you get the second error.\n\n\nIf you want to use your types using this import, you have to write them in a simple `.ts` file like that:\n\n\n*src\/@types\/app.ts:*\n\n\n\n```\nexport type AuthKeyT = string\n\n```\n\nWith this structure you will be able to import this type using:\n\n\n\n```\nimport { AuthKeyT } from 'app'\n\n```\n\n"}
{"questionId":"df31508ddb234084b50737920b3b70a0","question":"Type 'Subscription' is missing the following properties\nIDE shows error when I write this code.\n\n\nI have a component that calls service in ngOnInit to get the data. Service calls the other service to get some data and use it to get data then returns it.\n\n\ncomponent:\n\n\n\n```\nngOnInit() {\n    const token = \"abc\";\n    this.service.getContactPersons(token).subscribe(\n      persons => this.contactPersons = persons\n    );\n\n  }\n\n```\n\nservice:\n\n\n\n```\ngetContactPersons(token: string): Observable<ContactPerson[]> {\nreturn this.tokenService.getParameters(token).pipe(\n      switchMap((data: Params) => {\n        return this.http.get<Properties>(\n          this.baseUrl + `\/abc\/${data.param1}\/properties\/${data.param2}`\n        );\n      })\n    ).subscribe((data: Properties) => data.contactPersons);\n}\n\n```\n\nI've got this error: \"Type 'Subscription' is missing the following properties from type 'Observable': \\_isScalar, source, operator, lift, and 6 more.\"\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"`subscribe` is not an rxjs equivalent of `then`. Specifically, with promises you can do `somePromise.then(doSomething).then(doSomethingElse)`, but you can't `someRxjsStream$.subscribe(doSomething).subscribe(doSomethingElse)`. If you want to transform the data stream, you should use one of several available rxjs operators, and in your case it's `map`:\n\n\n\n```\ngetContactPersons(token: string): Observable<ContactPerson[]> {\n    return this.tokenService.getParameters(token).pipe(\n        switchMap((data: Params) => this.http.get<Properties>(\n            `${this.baseUrl}\/abc\/${data.param1}\/properties\/${data.param2}`)),\n        map((data: Properties) => data.contactPersons));\n}\n\n```\n\n"}
{"questionId":"f6acf391be884a7fb6e11fb1a93211f4","question":"Specify a Zod schema with a non-optional but possibly undefined field\nIs it possible to define a Zod schema with a field that is possibly `undefined`, but non-optional. In TypeScript this is the difference between:\n\n\n\n```\ninterface IFoo1 {\n  somefield: string | undefined;\n}\n\ninterface IFoo2 {\n  somefield?: string | undefined;\n}\n\nconst schema = z.object({\n  somefield: z.union([z.string(), z.undefined()]),\n}); \/\/ Results in something like IFoo2\n\n```\n\nAs far as I can tell using `z.union([z.string(), z.undefined()])` or `z.string().optional()` results in the field being equivalent to `IFoo2`.\n\n\nI'm wondering if there is a way to specify a schema that behaves like `IFoo1`.\n\n\n## Context \/ Justification\n\n\nThe reason that I might want to do something like this is to force developers to think about whether or not the field should be `undefined`. When the field is optional, it can be missed by accident when constructing objects of that type. A concrete example might be something like:\n\n\n\n```\ninterface IConfig {\n  name: string;\n  emailPreference: boolean | undefined;\n}\nenum EmailSetting {\n  ALL,\n  CORE_ONLY,\n}\n\nfunction internal(config: IConfig) {\n  return {\n    name: config.name,\n    marketingEmail: config.emailPreference ? EmailSetting.ALL : EmailSetting.CORE_ONLY,\n  }\n}\n\nexport function signup(userName: string) {\n  post(internal({ name: userName }));\n}\n\n```\n\nThis is sort of a contrived example, but this occurs a lot in our codebase with React props. The idea with allowing the value to be `undefined` but not optional is to force callers to specify that, for example, there was no preference specified vs picking yes or no. In the example I want an error when calling `internal` because I want the caller to think about the email preference. Ideally the type error here would lead me to realize that I should ask for email preference as a parameter to `signup`.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"You can use the `transform` function to explicitly set the field you're interested in. It's a bit burdensome, but it works.\n\n\n\n```\nconst schema = z\n    .object({\n        somefield: z.string().optional(),\n    })\n    .transform((o) => ({ somefield: o.somefield }));\n\ntype IFoo1 = z.infer<typeof schema>;\n\/\/ is equal to { somefield: string | undefined }\n\n```\n\n"}
{"questionId":"3a6ac8d94b55499e99ea97e853bcf675","question":"How to test Java Spring Boot application without @SpringBootApplication using JUnit?\nI created a Spring Boot Java library which has a set of utilities that perform certain functions. This library can then be used across multiple applications that require its utilities. This library was designed as a Spring Boot application to allow these utilities to be injected in the application context.\n\n\nNow I wish to execute a JUnit test on one of the utilities to ensure it is working correctly. However, since this application is basically a collection of utilities and not a stand-alone application, it does not have a main class or the main method annotated with `@SpringBootApplication`. Now, when I run the JUnit test, it comes up with an error.\n\n\n\n```\njava.lang.IllegalStateException: Unable to find a @SpringBootConfiguration, you need to use @ContextConfiguration or @SpringBootTest(classes=...)\n\n```\n\nIs it possible to test this Java library, or should we write the test cases only in the application that will be using this library?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"I think there is some contradiction in what you say:\n\n\n1. Created a Library...that was designed as a Spring Boot Application.\n2. Library can be used across multiple applications that require its utilities.\n\n\nIf you implement \"1\" then there is a module with spring boot maven\/gradle plugin configured to create a JAR of the application which is a library.\n\n\nBut if you have, say, module X that wishes to use your library, its impossible to add the dependency on your library in this module, spring boot JAR artifacts are not JARs in a Java Sense... So this won't work in both IDE and maven (I mean technically you'll have compilation errors).\n\n\nThen you write something that completely makes sense: You say that the library by itself doesn't have a main class\/`@SpringBootApplication` annotated class. From this I conclude that its not a spring boot application, but rather a spring boot starter module.\n\n\nIn this case you should not use `@SpringBootTest` annotation since it mimics the way of starting up the spring boot application (finds main class, scans the packages according to the package structure, loads the configurations and so forth). You don't need all this. Well, maybe technically you can still create a main class annotated with `@SpringBootApplication` and put it into `src\/test\/java\/...\/` in a relevant package, but it doesn't really makes sense.\n\n\nSo basically you have two choices:\n\n\n1. You can test the utilities without spring at all as if the utility is just a Java class, mock the dependency with Mockito and you're good to go. Since these tests are fast, it you be the best option.\n2. You can run the integartion test by means of loading the spring context with all the required beans created by the application.\n\n\n\n```\n\n@ExtendWith(SpringExtension.class)\n@ContextConfiguration(classes = {MyLibraryConfiguration.class})\npublic class SampleLibraryTest {\n\n    @Autowired \n    private SomeBeanFromTheLibrary theBean;\n\n    @Test\n    public void testFoo() {\n      ....\n    }\n}\n\n```\n\nNow although you can use component scanning (in this case you'll need slightly different annotations), in the example I've assumed that you're using java config, register all the beans of the library there and create a `spring.factories` that uses this Java Configuration file to create an autoconfiguration (you add a dependency on the library in module X and it loads the beans defined in the library automatically).\n\n\nThis `@ExtendsWith` (`@RunWith` for junit 4) has nothing to do with Spring Boot, it behaves as a \"plain\" spring, you can autowire beans, create mock beans, there is caching of configurations, etc.\n\n\n"}
{"questionId":"c3b2ade608244105a71d8643f275042f","question":"Difference in Perl regex variable $+{name} and $-{name}\nWhat is the difference between Perl regex variables `$+{name}` and `$-{name}` when both are used to refer to the same regex group from Perl statement\/expression code?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"perl"},"answer":"While `$+{name}` holds the captured substring referred by `name`\nas a scalar value, `$-{name}` refers to an *array* which holds capture\ngroups with the name.  \n\nHere is a tiny example:\n\n\n\n```\n#!\/usr\/bin\/perl\n\nuse strict;\nuse warnings;\n\n'12' =~ \/(?<foo>\\d)(?<foo>\\d)\/; # '1' and '2' will be captured individually\n\nprint $+{'foo'}, \"\\n\";          # prints '1'\n\nfor (@{$-{'foo'}}) {            # $-{'foo'} is a reference to an array\n    print $_, \"\\n\";             # prints '1' and '2'\n}\n\n```\n\nAs `$+{name}` can hold only a single scalar value, it is assigned\nto the first (leftmost) element of the capture groups.\n\n\n"}
{"questionId":"ed0c568661794dd4a5719558b8127a76","question":"How to fix linter warning `Error return value is not checked`?\nI'm calling the method with error type value (foo() in code example). I don't care this result. What's the rigth code style way to write? Errcheck linter makes me check this error. \n\n\n\n```\n\/\/for example, same method may be called from imported entity\nfunc foo() error {\n   if err := someFunction(); err != nil {\n       return err\n   }\n   return nil\n}\n\nfunc process() {\n   \/\/linter doesn't like this\n   foo()\n\n   \/\/this way leads to unused variable error\n   err := foo()\n\n   \/\/is this clean way?\n   _ = foo()\n\n  return \n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"go"},"answer":"Yes, assigning it to a wildcard variable would be a good way to ignore the error. But the whole practice (of ignoring errors) is strongly discouraged. Here's what \"Effective Go\" has to say about this: \n\n\n\n> \n> Occasionally you'll see code that discards the error value in order to ignore the error; this is terrible practice. Always check error returns; they're provided for a reason.\n> \n> \n> \n> ```\n>    \/\/ Bad! This code will crash if path does not exist.\n>     fi, _ := os.Stat(path)\n>     if fi.IsDir() {\n>         fmt.Printf(\"%s is a directory\\n\", path)\n>     }\n> \n> ```\n> \n> \n\n\n"}
{"questionId":"316075f3e62e47c8b879c047fa02cc14","question":"Why does this function return the correct length of a string? (Incrementing a char pointer)\nThis is a function that counts the number of characters in a string:\n\n\n\n```\nint str_len(const char* s) {\n    int i = 0;\n    while(*(s++)) {\n        i++;\n    }\n    return i;\n}\n\n```\n\nWhy does this return the correct length? \n\n\nLet's say I call this function with a simple String `\"a\"`. Then `s` is incremented in the while loop, therefore the value of `s` and `i` are both 0.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c"},"answer":"The value of `s++` is the original value of `s`, before increment, the increment occurs at an unspecified time before the next sequence point.\n\n\nHence `*s++` and `*(s++)` are equivalent: they both dereference the original value of `s`. Another equivalent expression is `*(0, s++)` and, not for the faint of heart, such is this one: `0[s++]`\n\n\nNote however that your function should use type `size_t` for `i` and its return type:\n\n\n\n```\nsize_t str_len(const char *s) {\n    size_t i = 0;\n    while (*s++) {\n        i++;\n    }\n    \/* s points after the null terminator *\/\n    return i;\n}\n\n```\n\nHere is a potentially more efficient version with a single increment per loop:\n\n\n\n```\nsize_t str_len(const char *s) {\n    const char *s0 = s;\n    while (*s++) {\n        \/* nothing *\/\n    }\n    return s - 1 - s0;\n}\n\n```\n\n\n\n---\n\n\nFor those who wonder about the weird expressions in the second paragraph:\n\n\n- `0, s++` is an instance of the comma operator `,` that evaluates its left part, then its right part which constitutes its value. hence `(0, s++)` is equivalent to `(s++)`.\n- `0[s++]` is equivalent to `(s++)[0]` and `*(0 + s++)` or `*(s++ + 0)` which simplify as `*(s++)`. Transposing the pointer and the index expressions in `[]` expressions is not very common nor particularly useful but conforms to the C standard.\n\n\n"}
{"questionId":"99e9ba0506304dc0a2493bc2bbbc96de","question":"Why is int x{ y = 5 } possible?\n\n```\nint main() {\n    int y;\n    int x{ y = 5 };\n    \/\/x is 5\n}\n\n```\n\nHow is this possible, since y = 5 is not a calculable expression?\n\n\nAlso, why doesn't the compiler or IDE complain about main() not returning an int?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c++"},"answer":"I will start from your last question\n\n\n\n> \n> Also, why doesn't the compiler or IDE complain about main() not\n>  returning an int?\n> \n> \n> \n\n\nAccording to the C++ Standard (6.6.1 main function)\n\n\n\n> \n> 5 A return statement in main has the effect of leaving the main\n>  function (destroying any objects with automatic storage duration) and\n>  calling std::exit with the return value as the argument. **If control\n>  flows off the end of the compound-statement of main, the effect is\n>  equivalent to a return with operand 0** (see also 18.3).\n> \n> \n> \n\n\nAnd relative to this question\n\n\n\n> \n> How is this possible, since y = 5 is not a calculable expression?\n> \n> \n> \n\n\nFrom the C++ Standard (8.18 Assignment and compound assignment operators)\n\n\n\n> \n> 1 The assignment operator (=) and the compound assignment operators\n>  all group right-to-left. All require a modifiable lvalue as their left\n>  operand and return an lvalue referring to the left operand.\n> \n> \n> \n\n\nSp this declaration\n\n\n\n```\nint x{ y = 5 };\n\n```\n\ncan be equivalently split into two statements\n\n\n\n```\ny = 5;\nint x{ y };\n\n```\n\nMoreover in C++ you can even to make a reference to the variable y the following way\n\n\n\n```\nint &x{ y = 5 };\n\n```\n\nHere is a demonstrative program\n\n\n\n```\n#include <iostream>\n\nint main() \n{\n    int y;\n    int &x{ y = 5 };    \n\n    std::cout << \"y = \" << y << '\\n';\n\n    x = 10;\n\n    std::cout << \"y = \" << y << '\\n';\n}\n\n```\n\nIts output is\n\n\n\n```\ny = 5\ny = 10\n\n```\n\nYou may this declaration\n\n\n\n```\nint x{ y = 5 };\n\n```\n\nrewrite also like\n\n\n\n```\nint x = { y = 5 };\n\n```\n\nHowever take into account that there is a difference between these (looking similarly as the above declarations) two declarations.\n\n\n\n```\nauto x{ y = 5 };\n\n```\n\nand\n\n\n\n```\nauto x = { y = 5 };\n\n```\n\nIn the first declaration the variable `x` has the type `int`. \nIn the second declaration the variable `x` has the type `std::initializer_list<int>`.\n\n\nTo make the difference more visible see how the values of the objects are outputted.\n\n\n\n```\n#include <iostream>\n\nint main() \n{\n    int y;\n    auto x1 { y = 5 };  \n\n    std::cout << \"x1 = \" << x1 << '\\n';\n\n    auto x2 = { y = 10 };   \n\n    std::cout << \"*x2.begin()= \" << *x2.begin() << '\\n';\n\n    std::cout << \"y = \" << y << '\\n';\n\n    return 0;\n}\n\n```\n\nThe program output is\n\n\n\n```\nx1 = 5\n*x2.begin()= 10\ny = 10\n\n```\n\n"}
{"questionId":"fa4089bc0ff34c2888c3e536652ea410","question":"How to modify kernel DTB file\n**Summary**\n\n\nI am currently compiling the Linux kernel (kernel, modules and DTB) with some custom drivers for a custom board. Occasionally I'll compile the kernel and realize that the compatibility string in the DTB file is not what the custom driver is looking for. Right now the only way i can remedy this is modify the DTS or kernel driver so the strings match and then recompile the kernel again. Is there are way I can just edit the DTB file to update the compatibility string?\n\n\n**Failed Attempts**\n\n\nI have been able to decompile the DTB file back to a DTS file using the command:\n\n\n\n```\ndtc -I dtb -o <filename>.dts -<filename>.dtb\n\n```\n\nHowever if I modify the DTS file and recompile using the command:\n\n\n\n```\ndtc -I dts -o <filename>.dtb -<filename>.dts\n\n```\n\nThe kernel will not load the recompiled DTB file\n\n\n","questionMetadata":{"type":"implementation","level":"advanced","tag":"c"},"answer":"Just want to update this with 2 years more experience on the subject.\n\n\nThe DTS files in the Linux repository are a mixture of DTS **and** C preprocessor directives (#include, #define, etc.). So when the original DTB is compiled, the preprocessor links to the referenced files to create a pure DTS file. `dtc` converts the single DTS file into a DTB file.\n\n\nSo if you want to modify a kernel DTS file and compile it, then you have two options:\n\n\n1. Just run `make dtbs` which automatically handles all of this\n2. Manually run the preprocessor (`cpp -nostdinc -I <include dir> -undef -x assembler-with-cpp ...`) and then compile the output with `dtc`.\n\n\n"}
{"questionId":"aaecdd7aeb474bb6ae8fa44d132a58df","question":"What is the difference between get() and addListenerForSingleValueEvent?\nI see a lot of tutorials, documentation and questions about using the Firebase Realtime Database on Android talk about the `addListenerForSingleValueEvent` method. This method reads the value from the database once, which is what I want to do.\n\n\nBut in the auto-complete in my IDE and in the documentation I also see a method called `get()`, which also seems to read a value from the database once. From the name and signature it seems simpler, and more modern (as it returns a `Task`). But since the other method is mentioned so much more, I want to make sure I\u2019m using the right code.\n\n\nSo what is the difference between `get()` and `addListenerForSingleValueEvent`, why is the latter mentioned so much more in documentation, tutorials and questions, and which one should I use when I want to read a value from the database once?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"java"},"answer":"First off: keep in mind that Firebase Realtime Database is best when used to keep data in sync between the client and the database server (or between multiple clients) by using a long-lived listener. On Android you do this with `addValueEventListener`, which you should try to use whenever possible.\n\n\nBut in some cases you\u2019ll want to read a value from the database only once. So let's see if I can answer the questions in turn, starting with the most important one:\n\n\n# Which method should I use when I want to read a value from the database once?\n\n\n**If you need to read a value from the database only once, use the new `get()` method.**\n\n\nIn Java that looks like this:\n\n\n\n```\nref.get().addOnCompleteListener(new OnCompleteListener<DataSnapshot>() {\n\u00a0\u00a0\u00a0\u00a0@Override\n\u00a0\u00a0\u00a0\u00a0public void onComplete(@NonNull Task<DataSnapshot> task) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if (!task.isSuccessful()) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Log.e(\"firebase\", \"Error getting data\", task.getException());\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0else {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Log.d(\"firebase\", String.valueOf(task.getResult().getValue()));\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0\u00a0\u00a0}\n});\n\n```\n\nAnd in Kotlin it is:\n\n\n\n```\nref.get().addOnSuccessListener {\n\u00a0\u00a0\u00a0\u00a0Log.i(\"firebase\", \"Got value ${it.value}\")\n}.addOnFailureListener{\n\u00a0\u00a0\u00a0\u00a0Log.e(\"firebase\", \"Error getting data\", it)\n}\n\n```\n\n## Why do you recommend using `get()` when `addListenerForSingleValueEvent` is mentioned so much more?\n\n\nWe introduced `addListenerForSingleValueEvent` in our first Android SDK, and it has been there ever since. Over the years a lot of tutorials have been written, and a lot of questions have been asked and answered.\n\n\nWe\u2019re updating the documentation of course. But there\u2019s no way we can get all tutorials updated. So for the foreseeable future, there will be more mentions of `addListenerForSingleValueEvent` than of the new `get()` method.\n\n\n### what is the difference between `get()` and `addListenerForSingleValueEvent`?\n\n\nAs said: the `addListenerForSingleValueEvent` method has been part of the Firebase Android SDK for as long as it exists, and is used to read a value from the database once. It does this by:\n\n\n1. Attaching a listener with `addValueEventListener`\n2. Waiting for the value to show up from the persistence layer\n3. Calling `onDataChange`\n4. Removing the listener\n\n\nThis worked really well... until we introduced disk caching in version 2.0 of the SDK (way back at I\/O 2015). Before that, all values in step 2 would always come from the server, either because the client already had a listener, or because this would attach the first listener to the server. But with disk caching, if you had previously read the value but currently had no listener to it, step 2 will read the value from the disk cache, and your `onDataChange` will be called with that value immediately. Even if the value on the server has been updated since. In fact, behind the scenes the listener will update the value in the disk cache, but only after calling your `onDataChange` with the (possibly stale) value from the cache.\n\n\nWhile this behavior can be explained, it is not what almost anyone wanted. Unfortunately we found this edge case too late to classify it as a simple implementation bug and fix it. So we left it in, and recommended that folks *either* use disk persistence *or* use `addListenerToSingleValueEvent`, but not both. Or you could call `keepSynced(true)` on the same reference\/query as a workaround. All messy, and not good.\n\n\nFast forward 5+ years, and we finally introduced a new method that doesn\u2019t have this awkward behavior anymore. And since Android APIs have moved on quite a bit since 2015, we also use a (slightly) more modern method signature: `Task<DataSnapshot> get()`.\n\n\n"}
{"questionId":"fb3a9fd52c1c426b8d2ac0b3f3b1aaa2","question":"POSIX: abcdef to ab bc cd de ef\nUsing POSIX `sed` or `awk`, I would like to duplicate every second character in every pair of neighboring characters and list every newly-formed pair on a new line.\n\n\nexample.txt:\n\n\n\n```\nabcd 10001.\n\n```\n\nExpected result:\n\n\n\n```\nab\nbc\ncd\nd \n 1\n10\n00\n00\n01\n1.\n\n```\n\nSo far, this is what I have (N.B. omit \"--posix\" if on macOS). For some reason, adding a literal newline character  before `\\2` does not produce the expected result. Removing the first group and using `\\1` has the same effect. What am I missing?\n\n\n\n```\nsed --posix -E -e 's\/(.)(.)\/&\\2\\\n\/g' example.txt\n\nabb\ncdd\n100\n000\n1..\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"Try:\n\n\n\n```\n$ echo \"abcd 10001.\" | awk '{for(i=1;i<length($0);i++) print substr($0,i,2)}'\nab\nbc\ncd\nd \n 1\n10\n00\n00\n01\n1.\n\n```\n\n"}
{"questionId":"0b033a7899dd4822bd6ab83b838d0b21","question":"How do I check if current code is part of a try-except-block?\nI'm debugging a function that I wrote as part of some form of plug-in framework. The function does not appear to be doing what it should, and I have the suspicion that, somewhere up the stack, someone is catching exceptions, or raise (either a very specific or a very generic) exception and test what happens (but if it's swallowed, it still doesn't tell me where). I could enter the debugger and inspect the source code at every stack level. Is there a more direct way to list any try-except blocks that the current code may be part of \u2014 specifically, the try-part of any such block?\n\n\nThis is, of course, exclusively for debugging purposes.\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"python"},"answer":"It's *spectacularly* possible I'm missing something here (I just eyeballed the `dis.dis()` output for the `catcher` function), but at least this catches simple cases of catching things on Python 3.7:\n\n\n\n```\nimport sys\nimport dis\n\n\ndef are_we_being_caught():\n    frame = sys._getframe(1)\n    while frame:\n        bytecode = dis.Bytecode(frame.f_code)\n        except_stack = 0\n        for instr in bytecode:\n            if instr.opname == \"SETUP_EXCEPT\":  # Going into a try: except: block\n                except_stack += 1\n            elif instr.opname == \"POP_EXCEPT\":  # Exiting a try: except: block\n                except_stack -= 1\n            if instr.offset > frame.f_lasti:  # Past the current instruction, bail out\n                break\n        if except_stack:  # If we `break`ed in the middle of a SETUP\/POP pair\n            print(frame, \"may be catching exceptions now\")\n        frame = frame.f_back\n\n\ndef catcher(fn):\n    try:\n        x = fn()\n    except:\n        x = None  # YOLO :D\n    return x\n\n\ndef f1():\n    return 8\n\n\ndef f2():\n    are_we_being_caught()\n    raise ValueError(\"foo\")\n\n\nprint(catcher(f1))\nprint(catcher(f2))\n\n```\n\noutputs\n\n\n\n```\n8\n<frame at 0x109d2d238, file 'so55729254.py', line 24, code catcher> may be catching exceptions now\nNone\n\n```\n\n"}
{"questionId":"7de6a5b46329408b85b289115687dc53","question":"Get the next element of list in Python\nI have a list of sentences like:\n\n\n\n```\nlst = ['A B C D','E F G H I J','K L M N']\n\n```\n\nWhat i did is\n\n\n\n```\nl = []\nfor i in lst:\n    for j in i.split():\n        print(j)\n        l.append(j) \n\nfirst = l[::2]\nsecond = l[1::2]\n\n[m+' '+str(n) for m,n in zip(first,second)]\n\n```\n\nThe Output i got is \n\n\n\n```\nlst = ['A B', 'C D', 'E F', 'G H', 'I J', 'K L', 'M N']\n\n```\n\nThe Output i want is: \n\n\n\n```\nlst = ['A B', 'B C','C D','E F','F G','G H','H I','I J','K L','L M','M N']\n\n```\n\nI am struggling to think how to achieve this.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"First format your list of string into a list of list, then do a mapping by `zip`.\n\n\n\n```\ni = [i.split() for i in lst]\n\nf = [f\"{x} {y}\" for item in i for x,y in zip(item,item[1::])]\n\nprint (f)\n\n#['A B', 'B C', 'C D', 'E F', 'F G', 'G H', 'H I', 'I J', 'K L', 'L M', 'M N']\n\n```\n\n"}
{"questionId":"6cbf904686b24b5faf789db391293b3c","question":"Does the C standard have any guarantees on the amount of stack space used?\nI am doing embedded programming where saving memory is important.\n\n\nHow much stack space would the following C code occupy at run-time?\n\n\n\n```\nif (send_small_message) {\n    uint8_t buffer[16000];\n    \/\/ do something with the buffer\n} else {\n    uint8_t buffer[32000];\n    \/\/ do something the with buffer\n}\n\n```\n\nCould some compiler decide to allocate 16000 + 32000 = 48kB stack space for both buffers? Or is it guaranteed that since both buffers will never be used at the same time, the compiler will allocate only 32kB - the size of the larger buffer?\n\n\nFOLLOW UP QUESTION:\n\n\n\n```\nvoid SendSmallMessage() {\n    uint8_t buffer[16000];\n    \/\/ do something with the buffer\n}\n\nvoid SendLargeMessage() {\n    uint8_t buffer[32000];\n    \/\/ do something with the buffer\n}\n\n```\n\nCan a code compiled by some compiler use 16000 + 32000 bytes at run-time to execute the snippet below:\n\n\n\n```\nif (send_small_message) {\n   SendSmallMessage(); \n} else {\n   SendLargeMessage();\n}\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"\n> \n> Does the C standard guarantee amount of stack used?\n> \n> \n> \n\n\nNo guarantees whatsoever exist for this. The C standard does not mention concepts like stacks. You can even write C for low level CPUs that completely lack a stack.\n\n\nThe C standard does however guarantee that `uint8_t` is 1 byte large and that 1 byte is 8 bits on your system (or otherwise `uint8_t` wouldn't be available).\n\n\n\n> \n> How much stack space would the following C code occupy at run-time?  \n> \n> Could some compiler decide to allocate 16000 + 32000 = 48kB stack space for both buffers?\n> \n> \n> \n\n\nSystem-specific, but also depends on exactly how the function is written and what optimizations that take place. Generally though, real world systems allocate room for as much stack as the function requires, given all possible execution paths. So it is quite likely that many compilers would allocate 16k + 32k.\n\n\nBut who cares, since it doesn't make sense to allocate that large amount of memory on the stack in any known system. Not on high-end, PC-like systems and certainly not on memory-restricted embedded systems. You'll get stack overflows all over the place.\n\n\nThe general rule of thumb in embedded is to never allocate any form of buffers on the stack, but always with static storage duration. On PC-like systems, heap allocation is another option.\n\n\n"}
{"questionId":"ad8ee29ff0994b7ca114eaa780779037","question":"Why does this simple program compiled with gcc,-mfpmath=387, and an optimization level of -O2 or -O3 produce NaN values?\nI have a short program that performs a numerical computation, and obtains an incorrect NaN result when some specific conditions hold. I cannot see how this NaN result can arise. Note that I am not using compiler options that allow the reordering of arithmetic operations, such as `-ffath-math`.\n\n\n**Question:** I am looking for an explanation of how the NaN result arises. Mathematically, there is nothing in the computation that leads to division by zero or similar. Am I missing something obvious?\n\n\nNote that I am not asking how to fix the problem\u2014that is easy. I am simply looking for an understanding of how the NaN appears.\n\n\n### Minimal example\n\n\nNote that this example is very fragile and even minor modifications, such as adding `printf()` calls in the loop to observe values, will change the behaviour. This is why I was unable to minimize it further.\n\n\n\n```\n\/\/ prog.c\n\n#include <stdio.h>\n#include <math.h>\n\ntypedef long long myint;\n\nvoid fun(const myint n, double *result) {\n    double z = -1.0;\n    double phi = 0.0;\n    for (myint i = 0; i < n; i++) {\n        double r = sqrt(1 - z*z);\n\n        \/* avoids division by zero when r == 0 *\/\n        if (i != 0 && i != n-1) {\n            phi += 1.0 \/ r;\n        }\n\n        double x = r*cos(phi);\n        double y = r*sin(phi);\n\n        result[i + n*0] = x;\n        result[i + n*1] = y;\n        result[i + n*2] = z;\n\n        z += 2.0 \/ (n - 1);\n    }\n}\n\n#define N 11\n\nint main(void) {\n    \/\/ perform computation\n    double res[3*N];\n    fun(N, res);\n\n    \/\/ output result\n    for (int i=0; i < N; i++) {\n        printf(\"%g %g %g\\n\", res[i+N*0], res[i+N*1], res[i+N*2]);\n    }\n\n    return 0;\n}\n\n```\n\nCompile with:\n\n\n\n```\ngcc -O3 -mfpmath=387 prog.c -o prog -lm\n\n```\n\nThe last line of the output is:\n\n\n\n```\nnan nan 1\n\n```\n\nInstead of NaN, I expect a number close to zero.\n\n\n#### Critical features of the example\n\n\nThe following must all hold for the NaN output to appear:\n\n\n- Compile with GCC on an x86 platform. I was able to reproduce with this GCC 12.2.0 (from MacPorts) on macOS 10.14.6, as well as with GCC versions 9.3.0, 8.3.0 and 7.5.0 on Linux (openSUSE Leap 15.3).\n\n\nI *cannot* reproduce it with GCC 10.2.0 or later on Linux, or GCC 11.3.0 on macOS.\n- Choose to use x87 instructions with `-mfpmath=387`, and an optimization level of `-O2` or `-O3`.\n- `myint` must be a *signed* 64-bit type.\n- Thinking of `result` as an n-by-3 matrix, it must be stored in column-major order.\n- No `printf()` calls in the main loop of `fun()`.\n\n\nWithout these features, I do get the expected output, i.e. something like `1.77993e-08 -1.12816e-08 1` or `0 0 1` as the last line.\n\n\n#### Explanation of the program\n\n\nEven though it doesn't really matter to the question, I give a short explanation of what the program does, to make it easier to follow. It computes `x`, `y`, `z` three-dimensional coordinates of `n` points on the surface of a sphere in a specific arrangement. `z` values go from -1 to 1 in equal increments, however, the last value won't be precisely 1 due to numerical round-off errors. The coordinates are written into an `n`-by-3 matrix, `result`, stored in column-major order. `r` and `phi` are polar coordinates in the (x, y) plane.\n\n\nNote that when `z` is `-1` or `1` then `r` becomes 0. This happens in the first and last iteration steps. This would lead to division by 0 in the `1.0 \/ r` expression. However, `1.0 \/ r` is excluded from the first and last iteration of the loop.\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"c"},"answer":"This is caused by interplay of x87 80-bit internal precision, non-conforming behavior of GCC, and optimization decisions differing between compiler versions.\n\n\nx87 supports IEEE binary32 and binary64 only as storage formats, converting to\/from its 80-bit representation on loads\/stores. To make program behavior predictable, the C standard requires that extra precision is dropped on assignments, and allows to check intermediate precision via the `FLT_EVAL_METHOD` macro. With `-mfpmath=387`, `FLT_EVAL_METHOD` is 2, so you know that intermediate precision corresponds to the `long double` type.\n\n\nUnfortunately, GCC does not drop extra precision on assignments, unless you're requesting stricter conformance via `-std=cNN` (as opposed to `-std=gnuNN`), or explicitly passing `-fexcess-precision=standard`.\n\n\nIn your program, the `z += 2.0 \/ (n - 1);` statement should be computed by:\n\n\n1. Computing `2.0 \/ (n - 1)` in the intermediate 80-bit precision.\n2. Adding to previous value of `z` (still in the 80-bit precision).\n3. **Rounding to the declared type of `z` (i.e. to binary64)**.\n\n\nIn the version that ends up with NaNs, GCC instead does the following:\n\n\n1. Computes `2.0 \/ (n - 1)` just once before the loop.\n2. Rounds this fraction from binary80 to binary64 and stores on stack.\n3. In the loop, it reloads this value from stack and adds to `z`.\n\n\nThis is non-conforming, because the `2.0 \/ (n - 1)` undergoes rounding twice (first to binary80, then to binary64).\n\n\n\n\n---\n\n\nThe above explains why you saw different results depending on compiler version and optimization level. However, in general you cannot expect your computation to not produce NaNs in the last iteration. When `n - 1` is not a power of two, `2.0 \/ (n - 1)` is not representable exactly and may be rounded up. In that case, 'z' may be growing a bit faster than the true sum `-1.0 + 2.0 \/ (n - 1) * i`, and may end up above 1.0 for `i == n - 1`, causing `sqrt(1 - z*z)` to produce a NaN due to a negative argument.\n\n\nIn fact, if you change `#define N 11` to `#define N 12` in your program, you will deterministically get a NaN both with 80-bit and 64-bit intermediate precision.\n\n\n"}
{"questionId":"2785d9b567cb48d9be5a4c200b5feb00","question":"python class attribute not updating when updated in a function\nI have a stopwatch of sorts going in the background using threading, and while it's updating the global variable, it doesn't change the output of my class attribute.\n\n\nThis is what I have:\n\n\n\n```\nimport time\nfrom threading import Thread\ns = 0\nm = 0\nh = 0\nstopped = False\n\ndef stopwatch():\n    global s\n    global m\n    global h\n    global stopped\n    while stopped == False:\n        s = s + 1\n        if s >= 60:\n            s = 0\n            m += 1\n        if m >= 60:\n            m = 0\n            h += 1\n        time.sleep(1)\n\nclass foo:\n    name = 'shirb'\n    time = str(h) + 'h' + str(m) + 'm' + str(s) +'s'\n\nThread(target = stopwatch).start()\ninput('press enter to stop the stopwatch')\nstopped = True\nprint('Name: ' + foo.name + '\\nTime: ' + foo.time)\n\n```\n\nLets say I wait for one minute and 34 seconds.\nThe output should be:\n\n\n\n```\npress enter to stop the stopwatch\nName: shirb\nTime: 0h1m34s\n\n```\n\nBut this is what it actually puts out:\n\n\n\n```\npress enter to stop the stopwatch\nName: shirb\nTime: 0h0m0s\n\n```\n\nI have no idea what is causing it to not update. When I try to print the variable itself with \"print(s)\" i get the correct amount of seconds, so there is something wrong with the class attribute that I don't know how to fix.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"Class variables are initialized at module load time, so `foo.time` is set when h, m, and s, are zero. If you make it a class method, however, you will get the right result:\n\n\n\n```\nclass foo:\n    name = 'shirb'\n    \n    @classmethod\n    def cls_time(cls):\n        return str(h) + 'h' + str(m) + 'm' + str(s) +'s'\n\nThread(target = stopwatch).start()\ninput('press enter to stop the stopwatch')\nstopped = True\nprint('Name: ' + foo.name + '\\nTime: ' + foo.cls_time())\n\n```\n\n"}
{"questionId":"6d9e77a8a7174116a82a0a3d32afebfa","question":"Spying a lambda with mockito\nI have encountered an interesting issue while writing an unit test which involved mocking a lambda.\n\n\n\n```\n@Test\npublic void spyingLambda() {\n    final Supplier<String> spy = Mockito.spy((Supplier) () -> \"1\");\n    spy.get();\n}\n\n```\n\nRunning this test fails with the following error:\n\n\n\n> \n> Mockito cannot mock\/spy because :\n>  - final class\n> \n> \n> \n\n\nOne workaround for the above issue is replacing the lambda with anonymous implementation:\n\n\n\n```\n@Test\npublic void spyingAnonymousImplementation() {\n    final Supplier<String> spy = Mockito.spy(new Supplier<String>() {\n        @Override\n        public String get() {\n            return \"1\";\n        }\n    });\n    spy.get();\n}\n\n```\n\nThough both tests are exactly the same (the IDE suggest even replacing the anonymous implementation with lambda), the second test doesn't fail.\n\n\nI was wondering if this is a known issue which could be fixed in mockito or are there any other workarounds. \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"Another way of dealing with this issue is the following:\n\n\n\n```\n\/**\n * This method overcomes the issue with the original Mockito.spy when passing a lambda which fails with an error\n * saying that the passed class is final.\n *\/\n@SuppressWarnings(\"unchecked\")\nstatic <T, P extends T> P spyLambda(Class<T> lambdaType, P lambda) {\n    return (P) mock(lambdaType, delegatesTo(lambda));\n}\n\n```\n\nWhich allows spying the lambda by changing the first method as following:\n\n\n\n```\n@Test\nvoid spyingLambda() {\n    Supplier<String> spy = spyLambda(Supplier.class, () -> \"1\");\n    spy.get();\n}\n\n```\n\nHopefully the above examples might help others who encounter the same issue.\n\n\n"}
{"questionId":"a6121db85f1e4d6cac438cf210eb6304","question":"Recommended method for reloading `.zshrc`? (`source` VS `exec`?)\nMost people seem to recommend using `source` for reloading `.zshrc`. Why?\n\n\nFirst I tried out `source ~\/.zshrc`. But it resulted in a `compinit` error (at reload, not for new shell instances at first `.zshrc` load).\n\n\nMy reload alias is now:\n\n\n\n```\nalias zsh-reload=\"exec zsh\"\n\n```\n\ninstead of:\n\n\n\n```\nsource ~\/.zshrc\n\n```\n\nThe reason for this is that my previous reload method (`source`) triggered a compinit error;\n\n\n\n```\nzsh compinit: insecure directories, run compaudit for list.\nIgnore insecure directories and continue [y] or abort compinit [n]?\n\n```\n\nSo I'm now doing `exec` because I believie it ensures the previous environment doesn't pollute the new one.\n\n\n- What are the downsides of using the `exec` method?\n- Is there an explicit reason for that you're doing `exec` or `source`?\n- Could `exec` in theory cause unexpected problems compared to the `source` method?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"bash\/shell"},"answer":"With `exec`, all (unexported) variables in your shell are lost, which is probably not what you want. Instead, it might be better to fix the compinit problem, by using `compinit -i`. See the zsh man page for *compinit*, paragraph *Use of compinit*:\n\n\n\n> \n> to make compinit silently ignore all insecure files and directories use the option -i\n> \n> \n> \n\n\n"}
{"questionId":"8e4e5dac2ae34937901ceb21fc4ef0af","question":"Using PowerShell sls (Select-String) vs grep vs findstr\nCould someone clarify how sls (Select-String) works compared to grep and findstr?\n\n\ngrep: `grep <pattern> files.txt`\n\n\nsls: `sls <pattern> files.txt`\n(default parameter position for sls is pattern then file)\n\n\ngrep examples: `grep \"search text\" *.log` ; `cat *.log | grep \"search text\"`\n\n\nsls examples: `sls \"search text\" *.log` ; `cat *.log | grep \"search text\"`\n\n\nAs an aside, all PowerShell Cmdlets are case-insensitive, unlike Linux tools which are generally always case-sensitive but also older tools like findstr which are case sensitive too, but findstr can be used in PowerShell, and works in situations where sls does not, for example: `Get-Service | findstr \"Sec\"` (this works without a problem!), but when we try to use sls in a similar way `Get-Service | sls \"Sec\"` we get nothing (presumably this fails because sls works with strings, but Get-Service returns an object, so that's understandable - but what is findstr doing then in that it can see the output as a string?).\n\n\nSo, my thinking is \"ok, I need to make the output from Get-Service into a string to work with PowerShell Cmdlets\", but that doesn't work (or not in a way that I would expect):\n\n\n`Get-Service | Out-String | sls \"Sec\"` (gives results, but odd)\n\n\n`(Get-Service).ToString() | sls \"Sec\"` (.ToString() just returns \"System.Object[]\")\n\n\nHow in general should I turn an object into a string so that it can manipulate the information (in the same way that `Get-Service | findstr \"Sec\"` can do so easily)?\n\n\nWould appreciate if someone could clarify how things fit together in the above so that I can make more use of sls. In particular, `Get-Service | Out-String | sls \"Sec\"` does return stuff, just not the stuff I was expecting (is it searching for each character of \"s\" and \"e\" and \"c\" so is returning lots - that would not be very intuitive if so in my opinion)?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"bash\/shell"},"answer":"When you use `Out-String` by default, it turns the piped input object (an array of service objects in this case) into a single string. Luckily, the `-Stream` switch allows each line to be output as a single string instead. Regarding case-sensitivity, `Select-String` supports the `-CaseSensitive` switch.\n\n\n\n```\n# For case-insensitive regex match\nGet-Service | Out-String -Stream | Select-String \"Sec\"\n\n# For case-sensitive regex match\nGet-Service | Out-String -Stream | Select-String \"Sec\" -CaseSensitive\n\n# For case-sensitive non-regex match\nGet-Service | Out-String -Stream | Select-String \"Sec\" -CaseSensitive -SimpleMatch\n\n```\n\nIn either case, `Select-String` uses regex (use the `-SimpleMatch` switch to do a string match) to pattern match against each input string and outputs the entire string that matched the pattern. So if you only pipe into it a single string with many lines, then all lines will be returned on a successful match.\n\n\n"}
{"questionId":"d5b8ac4662bc488a9a06eef977c43037","question":"Is volatile needed when variable is only read during interrupt\nThe C standard states that the volatile keyword should be used in the definition of a variable when there's a chance that the variable's value could change outside the normal flow of execution of the program.\n\n\nIf a global variable is changed (written) during normal execution flow and only read outside this normal flow (in an interrupt). Does this variable need to be volatile ? And why ?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c"},"answer":"\n> \n> If a global variable is changed (written) during normal execution flow and only read outside this normal flow (in an interrupt). Does this variable need to be volatile?\n> \n> \n> \n\n\nYes absolutely.\n\n\n\n> \n> And why?\n> \n> \n> \n\n\nTo ensure proper execution of the interrupt handler (rather than the normal flow).\n\n\n\n\n---\n\n\nLet me elaborate.\n\n\nImagine you have a variable like this:\n\n\n\n```\nint x;\n\n```\n\nYou are modifying this variable in the normal flow of the program as this:\n\n\n\n```\nvoid modify(int *x){...some code here...}\n\n```\n\nAnd in the interrupt service routine, you are reading the variable.\n\n\nRemember that an interrupt can occur asynchronously(at any time). Also remember that the compiler will first convert your code into a series of machine instructions which could look something like this:\n\n\n\n```\nload val from mem to register\nperform modification operations\nstore value from registers to memory\nperform other operations where the variable is used\n\n```\n\nNow, the compiler may optimise this program to reduce the number of memory read-writes as such:\n\n\n\n```\nload value\nperform modifications\nuse value\n...\nmodify register content\n...\nuse\n...\nkeep using value from the registers without ever storing in the memory.\n\n```\n\nIn such a case, if an interrupt occurs, (note that the interrupt context is usually different from the normal context and thus has different set of registers in a lot of architectures (say arm...)), it will attempt to read the value of the variable from the memory. But the contents of the memory have never been changed because of compiler optimisation.\n\n\nSo, the ISR could read an old value (worse still-we can't definitely say how old a value) which would result in unintended behaviour.\n\n\nSo, the variable should be declared as volatile to prevent the compiler from meddling with the program.\n\n\n"}
{"questionId":"70aec3c2bd3047bd83dfba171d20204d","question":"Concatenate two columns in pandas with NaN\nI have a dataframe like this\n\n\n\n```\ndf = (pd.DataFrame({'ID': ['ID1', 'ID2', 'ID3'], \n                        'colA': ['A', 'B', 'C'], \n                        'colB': ['D', np.nan, 'E']}))\n\ndf\n\n    ID  colA   colB\n0   ID1 A      D\n1   ID2 B      NaN\n2   ID3 C      E\n\n```\n\nI want to combine the two columns, however keep only column A if column B is NaN. Hence Expected output is\n\n\n\n```\n    ID  colA    colB    colC\n0   ID1 A       D       A_D\n1   ID2 B       NaN     B\n2   ID3 C       E       C_E\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"Idea is add `_` to second column with `_`, so after replace missing value by empty string is not added `_` for missing values:\n\n\n\n```\ndf['colC'] = df['colA'] + ('_' + df['colB']).fillna('')\nprint (df)\n    ID colA colB colC\n0  ID1    A    D  A_D\n1  ID2    B  NaN    B\n2  ID3    C    E  C_E\n\n```\n\nIf not sure where are missing values (in `colA` or `colB`):\n\n\n\n```\ndf['colC'] = (df['colA'].fillna('') + '_' + df['colB'].fillna('')).str.strip('_')\n\n```\n\nAlso is possible test each column separately:\n\n\n\n```\nm1 = df['colA'].isna()\nm2 = df['colB'].isna()\n\ndf['colC'] = np.select([m1, m2, m1 & m2], \n                        [df['colB'], df['colA'], np.nan], \n                        default=df['colA'] + '_' + df['colB'])\nprint (df)\n\n    ID colA colB colC\n0  ID1    A    D  A_D\n1  ID2    B  NaN    B\n2  ID3  NaN    E    E\n3  ID4  NaN  NaN  NaN\n\n```\n\n"}
{"questionId":"d5faabbd1c944c46b1c41b87699f999b","question":"removing duplicated strings within a column with shell\nI have a file with two columns separated by tabs as follows:\n\n\n\n```\nOG0000000   PF03169,PF03169,PF03169,MAC1_004431-T1,\nOG0000002   PF07690,PF00083,PF00083,PF07690,PF00083,\nOG0000003   MAC1_000127-T1,\nOG0000004   PF13246,PF00689,PF00690,\nOG0000005   PF00012,PF01061,PF12697,PF00012,\n\n```\n\nI just want to remove duplicate strings within the second column, while not changing anything in the first column, so that my final output looks like this:\n\n\n\n```\nOG0000000   PF03169,MAC1_004431-T1,\nOG0000002   PF07690,PF00083,\nOG0000003   MAC1_000127-T1,\nOG0000004   PF13246,PF00689,PF00690,\nOG0000005   PF00012,PF01061,PF12697,\n\n```\n\nI tried to start this by using awk.\n\n\n\n```\nawk 'BEGIN{RS=ORS=\",\"} !seen[$0]++' file.txt\n\n```\n\nBut my output looks like this, where there are still some duplicates if the duplicated string occurs first.\n\n\n\n```\nOG0000000   PF03169,PF03169,MAC1_004431-T1,\nOG0000002   PF07690,PF00083,PF07690,\nOG0000003   MAC1_000127-T1,\nOG0000004   PF13246,PF00689,PF00690,\nOG0000005   PF00012,PF01061,PF12697,PF00012,\n\n```\n\nI realize that the problem is because the first line that awk grabs is everything until the first comma, but I'm still rough with awk commands and couldn't figure out how to fix this without messing up the first column. Thanks in advance!\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"This `awk` should work for you:\n\n\n\n```\nawk -F '[\\t,]' '\n{\n   printf \"%s\", $1 \"\\t\"\n   for (i=2; i<=NF; ++i) {\n      if (!seen[$i]++)\n         printf \"%s,\", $i\n   }\n   print \"\"\n   delete seen\n}' file\n\nOG0000000   PF03169,MAC1_004431-T1,\nOG0000002   PF07690,PF00083,\nOG0000003   MAC1_000127-T1,\nOG0000004   PF13246,PF00689,PF00690,\nOG0000005   PF00012,PF01061,PF12697,\n\n```\n\nPS: As per the expected output shown this solution also shows a trailing comma in each line.\n\n\n"}
{"questionId":"083079f18f5444b2bf374020a19d7ff4","question":"Get first value that matches condition (loop too slow)\nI've got a lot of matrices similar to this but with thousands of rows :\n\n\n\n```\nr <- 10\nc <- 2\nset.seed(333)\n\nm1 <- matrix(runif(r*c)+1, r, c)\n\n> m1\n          [,1]     [,2]\n [1,] 1.467001 1.393902\n [2,] 1.084598 1.474218\n [3,] 1.973485 1.891222\n [4,] 1.571306 1.665011\n [5,] 1.020119 1.736832\n [6,] 1.723557 1.911469\n [7,] 1.609394 1.637850\n [8,] 1.306719 1.864651\n [9,] 1.063510 1.287575\n[10,] 1.305353 1.129959\n\n```\n\nI've got a loop that tells me, for each value of the first column, what is the index of the first value in the second column that is 10% higher like so :\n\n\n\n```\nresult <- 1:nrow(m1)\n\nfor (i in 1:nrow(m1)){\n    result[i] <- which(m1[,2]>(1.1*m1[,1][i]))[1]\n}\n> result\n [1]  3  1 NA  3  1  6  3  2  1  2\n\n```\n\nI've got so much matrices that it's taking hours, and after profiling my code, the biggest time consuming task by far is this loop. What is, according to you, the fastest way to do it ?\n\n\nFor example, with r = 30000 :\n\n\n\n```\nstart_time <- Sys.time()\n\nfor (i in 1:nrow(m1)){\n    result[i] <- which(m1[,2]>(1.1*m1[,1][i]))[1]\n}\n\nend_time <- Sys.time()\na <- end_time - start_time\n\n> a\nTime difference of 11.25815 secs\n\n```\n\nThanks for you help !\n\n\n","questionMetadata":{"type":"optimization","level":"advanced","tag":"r"},"answer":"There are some shortcuts you can take here. You are looking for the first value in column 2 that is higher than some other value. This means that it is never worth looking at values that are lower than what we have previously seen in column 2.\n\n\nIn your example with 10 rows, that would be as follows:\n\n\n\n```\n> cummax(m1[, 2])\n [1] 1.393902 1.474218 1.891222 1.891222 1.891222 1.911469 1.911469 1.911469 1.911469 1.911469\n> which(cummax(m1[, 2]) == m1[, 2])\n[1] 1 2 3 6\n\n```\n\nAnd as you can see, these are the only values in your result vector.\n\n\nA second optimisation that could be made is to order the first column. If you start looking for the lowest value first, and work your way up, you don't have to look through the second column each time. You only have to step to the next row there if there are no matches with the left row anymore.\n\n\nThis does bear the cost of sorting the matrix, but afterwards the result can be found using a single pass through both columns.\n\n\n\n```\ndostuff <- function(m1){\n  orderColumn1 <- order(m1[, 1])\n\n  plus.10 <- m1[, 1] * 1.1\n\n  results <- rep(NA, length(plus.10))\n\n  IndexColumn1 <- 1\n  IndexColumn2 <- 1\n  row2CurrentMax <- 0\n  while(IndexColumn2 <= nrow(m1)){\n    row2Current <- m1[IndexColumn2, 2]\n    if(row2Current > row2CurrentMax){\n      row2CurrentMax <- row2Current\n      while(TRUE){\n        row1Current <- plus.10[orderColumn1[IndexColumn1]]\n        if(row1Current <= row2CurrentMax){\n          results[orderColumn1[IndexColumn1]] <- IndexColumn2\n          IndexColumn1 <- IndexColumn1 + 1\n        } else {\n          break\n        }\n      }\n    }\n    IndexColumn2 <- IndexColumn2 + 1\n  }\n  results\n}\n\n```\n\nWith 30000 rows:\n\n\n\n```\n> result <- dostuff(m1)\n> end_time <- Sys.time()\n> a <- end_time - start_time\n> a\nTime difference of 0.0600059 secs\n\n```\n\n"}
{"questionId":"3e5594e0e0e64974a603f63614a28f76","question":"Error \"None of the following functions can be called with the arguments supplied:\" with Toast\nI want to create a code to click on items of RecyclerView. I found one from Internet, however it keep getting this error:\n\n\n\n> \n> None of the following functions can be called with the arguments supplied:\n> \n> \n> public open fun makeText(p0: Context!, p1: CharSequence!, p2: Int): Toast! defined in android.widget.Toast\n> \n> \n> public open fun makeText(p0: Context!, p1: Int, p2: Int): Toast! defined in android.widget.Toast\n> \n> \n> \n\n\nHere's my code:\n\n\n\n```\noverride fun onCreate(savedInstanceState: Bundle?) {\n        super.onCreate(savedInstanceState)\n        setContentView(R.layout.activity_main)\n\n        recyclerView.layoutManager = LinearLayoutManager(this, RecyclerView.VERTICAL, false)\n        val users = ArrayList<User>()\n\n        val adapter = CustomAdapter(users)\n\n        recyclerView.adapter = adapter\n\n        recyclerView.addOnItemClickListener(object : OnItemClickListener {\n            override fun onItemClicked(position: Int, view: View) {\n                Toast.makeText(this, \"Clicked on  \" + users.get(position).name, Toast.LENGTH_LONG).show()\n            }\n        })\n\n\n    }\n\n    interface OnItemClickListener {\n        fun onItemClicked(position: Int, view: View)\n    }\n\n    fun RecyclerView.addOnItemClickListener(onClickListener: OnItemClickListener) {\n        this.addOnChildAttachStateChangeListener(object : RecyclerView.OnChildAttachStateChangeListener {\n            override fun onChildViewDetachedFromWindow(view: View) {\n                view.setOnClickListener(null)\n            }\n\n            override fun onChildViewAttachedToWindow(view: View) {\n                view.setOnClickListener {\n                    val holder = getChildViewHolder(view)\n                    onClickListener.onItemClicked(holder.adapterPosition, view)\n                }\n            }\n        })\n    }\n\n```\n\nHow can I fix that error message?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"kotlin"},"answer":"\n```\nToast.makeText(this@YOUR_ACTIVITY_NAME, \"Clicked on  \" + users.get(position).name, Toast.LENGTH_LONG).show()\n\n```\n\n"}
{"questionId":"859f9dca21bc4ac1ba9599de10ef1898","question":"PHP Warning Deprecated: Creation of dynamic property is deprecated\nI'm seeing this more and more, and I'm not sure what I need to do to stop this warning:\n\n\n\n> \n> Deprecated: Creation of dynamic property ... is deprecated\n> \n> \n> \n\n\nThis is my class:\n\n\n\n```\nclass database {\n\n    public $username = \"root\";\n    public $password = \"password\";\n    public $port = 3306;\n\n    public function __construct($params = array())\n    {\n        foreach ($params as $key => $value)\n        {\n            $this->{$key} = $value;\n        }\n    }\n}\n\n\n```\n\nThis is how I'm instantiating it.\n\n\n\n```\n$db = new database(array(\n    'database' => 'db_name',\n    'server' => 'database.internal',\n));\n\n```\n\nWhich gives me two messages:\n\n\n\n> \n> Deprecated: Creation of dynamic property database::$database is\n> deprecated\n> \n> \n> Deprecated: Creation of dynamic property database::$server is\n> deprecated\n> \n> \n> \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"So the warnings are coming from the constructor adding dynamic class properties. If you don't have to pass in those fields dynamically and really, it does seem like you're overcomplicating something simple then try it like this.\n\n\n\n```\nclass database {\n\n    public $username = \"root\";\n    public $password = \"pasword\";\n    public $port = 3306;\n    public $database = 'db_name';\n    public $server = 'database.internal';\n}\n\n\n$db = new database();\n\n```\n\nIs there a reason you needed dynamic parameters? You could also do this:\n\n\n\n```\nclass database {\n\n    public $username = \"root\";\n    public $password = \"pasword\";\n    public $port = 3306;\n    public $database;\n    public $server;\n\n    public function __construct($params = array())\n    {\n\n        foreach ($params as $key => $value)\n        {\n            $this->{$key} = $value;\n        }\n    }\n}\n\n\n```\n\nIf you add the parameters ahead of time, they're not dynamic, and you're just assigning a value to something already existing.\n\n\nThis should work now without any warnings.\n\n\n\n```\n\n$db = new database(array(\n    'database' => 'db_name',\n    'server' => 'database.internal',\n));\n\n\n```\n\n"}
{"questionId":"2ba0d6fe315a4d8faf35ad3ebe533bfb","question":"Could not locate MSBuild instance to register with OmniSharp\nI have found many questions about this but non have helped me. I am trying to write c# code and the omnisharp auto complete doesn't work and I get this back from the Omnisharp Log:\n\n\n\n```\nOmniSharp server started.\n    Path: c:\\Users\\GeorgV.216\\.vscode\\extensions\\ms-dotnettools.csharp-1.24.1\\.omnisharp\\1.38.3-beta.31\\OmniSharp.exe\n    PID: 11536\n\n[info]: OmniSharp.Stdio.Host\n        Starting OmniSharp on Windows 6.2.9200.0 (x64)\n[info]: OmniSharp.Services.DotNetCliService\n        Checking the 'DOTNET_ROOT' environment variable to find a .NET SDK\n[info]: OmniSharp.Services.DotNetCliService\n        Using the 'dotnet' on the PATH.\n[info]: OmniSharp.Services.DotNetCliService\n        DotNetPath set to dotnet\n[info]: OmniSharp.MSBuild.Discovery.MSBuildLocator\n        Located 0 MSBuild instance(s)\nCould not locate MSBuild instance to register with OmniSharp.\n\n```\n\nWhat could be a possible solution?\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"c#"},"answer":"The solution that worked for me was to change the \"omnisharp.path\": from \"latest\" to \"\" in the setting.json file and delete the 1.38.3-beta.31 folder in C:\\Users{username}.vscode\\extensions\\ms-dotnettools.csharp-1.24.1.omnisharp\n\n\n"}
{"questionId":"76f21d48089d4cc4b35ac4b8ca40b033","question":"What is calling void(); doing?\nI came across `void();` being used as a 'do-nothing' in the 'else' branch of a ternary operator, as a shorthand for a null pointer check\n\n\n\n> \n> \n> ```\n> if(var){\n>    var->member();\n> }\n> \n> ```\n> \n> \n\n\nas\n\n\n\n> \n> `var ? var->member() : void();`\n> \n> \n> \n\n\nbut I can't seem to find any reference to the `void` keyword being used in this way, is this a function or functor call on the `void` keyword itself? or is it casting nothing to the type of `void`? or is this just the c++ syntax of something like `pass`?\n\n\nEdit:\nThe return type of `member()` is `void` in this situation.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c++"},"answer":"You are just \"constructing\" a prvalue (not a variable, for the reason suggested in the comments) of type `void`, just as `int()` would default-construct an `int`.\n\n\nAs others said in the comments, the second alternative is pejorative. The ternary operator is, well, *ternary* because it has the `if`, the `then`, and the `else` parts. If you don't need an `else`, why would you write one and leave it empty?\n\n\nThat alternative is even uglier and more cryptic than this,\n\n\n\n```\nif(var){\n   var->member();\n} else {}\n\n```\n\nwhich may just look stupid.\n\n\n"}
{"questionId":"aa1cab10ddea421d84813d1b888ba91b","question":"Determine if an Excel workbook has ever been saved?\nHow can I tell, using VBA, if an Excel workbook has ever been saved?\n\n\nI want to know if clicking save will save the document to an existing location or prompt me to choose a save location.\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"vba"},"answer":"\n```\nIf ActiveWorkbook.Path = vbNullString Then\n    'actions if the active workbook hasn't been saved yet go here\nElse\n    '....\nEnd If\n\n```\n\n"}
{"questionId":"e802121041e14426bfdeea38bb556c3e","question":"rvalue for a std::string parameter\nWhat's the difference in practice between LVALUE and RVALUE in the following code when I pass the text?\nI mean, in this specific case of a string (where the string is a string literal), is there any benefit of using RVALUE (&&)?\n\n\n\n```\nvoid write_Lvalue(const std::string &text) {\n    \/\/...\n}\n\nvoid write_Rvalue(const std::string &&text) {\n    \/\/...\n}\n\nint main() {\n    write_Lvalue(\"writing the Lvalue\");\n    write_Rvalue(\"writing the Rvalue\");\n}\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c++"},"answer":"First, constant rvalue reference are not really useful, since you cannot move them. Moving value need mutable references to work.\n\n\nLet's take your corrected example:\n\n\n\n```\nvoid write_lvalue(std::string const& text) {\n    \/\/...\n}\n\nvoid write_rvalue(std::string&& text) {\n    \/\/...\n}\n\nint main() {\n    write_lvalue(\"writing the Lvalue\");\n    write_rvalue(\"writing the Rvalue\");\n}\n\n```\n\nIn this case, the two are **completely equivalent**. In these two case, the compiler has to create a string and send it by reference:\n\n\n\n```\nint main() {\n    \/\/ equivalent, string created\n    \/\/ and sent by reference (const& bind to temporaries)\n    write_lvalue(std::string{\"writing the Lvalue\"}); \n\n    \/\/ equivalent, string created\n    \/\/ and sent by reference (&& bind to temporaries)\n    write_rvalue(std::string{\"writing the Rvalue\"});\n}\n\n```\n\nSo why have function that takes rvalue references?\n\n\nIt depends on what you do with the string. A mutable reference can be moved from:\n\n\n\n```\nstd::string global_string;\n\nvoid write_lvalue(std::string const& text) {\n    \/\/ copy, might cause allocation\n    global_string = text;\n}\n\nvoid write_rvalue(std::string&& text) {\n    \/\/ move, no allocation, yay!\n    global_string = std::move(text);\n}\n\n```\n\nSo why using rvalue reference at all? Why not using mutable lvalue reference?\n\n\nThat is because mutable lvalue references cannot be bound to temporaries:\n\n\n\n```\nvoid write_lvalue_mut(std::string& text) {\n    \/\/ move, no allocation... yay?\n    global_string = std::move(text);\n}\n\nint main() {\n    std::string s = \/* ... *\/;\n    write_lvalue_mut(std::move(s)); \/\/ fails\n    write_lvalue_mut(\"some text\"); \/\/ also fails\n}\n\n```\n\nBut mutable rvalue reference can be bound to rvalue, as shown above.\n\n\n"}
{"questionId":"e36e115a88f8451fafca3c48c3251392","question":"How can I update a table to insert decimal points at a fixed position in numbers?\nI am using Microsoft SQL Server 2014 and have a table with three columns and the field data type is `Decimal(38,0)`.\n\n\nI want to update each row of my table to insert a decimal point after the first two digits. For example, I want `123456` to become `12.3456`. The numbers are different lengths; some are five digits, some are seven digits, etc.\n\n\nMy table is:\n\n\n\n```\n+-------------+-------+-------+\n| ID          |   X   |   Y   |\n+-------------+-------+-------+\n| 1200        | 321121| 345000|\n| 1201        | 564777| 4145  |\n| 1202        | 4567  | 121444|\n| 1203        | 12747 | 789887|\n| 1204        | 489899| 124778|\n+-------------+-------+-------+\n\n```\n\nAnd I want to change this to:\n\n\n\n```\n+-------------+--------+--------+\n| ID          |   X    |   Y    |\n+-------------+--------+--------+\n| 1200        | 32.1121| 34.5000|\n| 1201        | 56.4777| 41.45  |\n| 1202        | 45.67  | 12.1444|\n| 1203        | 12.747 | 78.9887|\n| 1204        | 48.9899| 12.4778|\n+-------------+--------+--------+\n\n```\n\nMy code is:\n\n\n\n```\nUpdate [dbo].[UTM]\n     SET [X] = STUFF([X],3,0,'.')\n         [Y] = STUFF([X],3,0,'.')\n\n```\n\nAnd I tried this:\n\n\n\n```\nBEGIN\nDECLARE @COUNT1 int;\nDECLARE @COUNT2 int;\nDECLARE @TEMP_X VARCHAR(255);\nDECLARE @TEMP_Y VARCHAR(255);\nDECLARE @TEMP_main VARCHAR(255);\n\nSELECT @COUNT1 = COUNT(*) FROM [UTM];\nSET @COUNT2 = 0;\n\n    WHILE(@COUNT2<@COUNT1)\n    BEGIN\n        SET @TEMP_main = (SELECT [id] from [UTM] order by [id] desc offset @COUNT2 rows fetch next 1 rows only);\n        SET @TEMP_X = (SELECT [X] from [UTM] order by [id] desc offset @COUNT2 rows fetch next 1 rows only);\n        SET @TEMP_Y = (SELECT [Y] from [UTM] order by [id] desc offset @COUNT2 rows fetch next 1 rows only);\n\n        UPDATE [dbo].[UTM]\n           SET [X] = CONVERT(decimal(38,0),STUFF(@TEMP_X,3,0,'.'))\n              ,[Y] = CONVERT(decimal(38,0),STUFF(@TEMP_Y,3,0,'.'))\n           WHERE [id] = @TEMP_main;\n\n        SET @COUNT2 = @COUNT2  +  1\n    END\n\nEND\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"sql"},"answer":"This runs on an assumption from a previously deleted post (that you have negative number as well). \n\n\nFirstly, as you're using a `decimal(38,0)` you can't store values with any kind of precision, thus you need to change the data type as well. This provides the results you appear to be looking for:\n\n\n\n```\nUSE Sandbox;\nGO\n\nCREATE TABLE dbo.SampleTable (ID int,\n                              X decimal(38,0),\n                              Y decimal(38,0));\nINSERT INTO dbo.SampleTable (ID,\n                             X,\n                             Y)\nVALUES (1200,321121,345000), \n       (1201,564777,4145  ), \n       (1202,4567  ,121444), \n       (1203,12747 ,789887), \n       (1204,489899,124778),\n       (1205,-32472,-27921);\nGO\n--Fix the datatype\nALTER TABLE dbo.SampleTable ALTER COLUMN X decimal(10,4); --Based on data provided, may need larger scale\nALTER TABLE dbo.SampleTable ALTER COLUMN Y decimal(10,4); --Based on data provided, may need larger scale\nGO\n\n--update the data\nUPDATE dbo.SampleTable\nSET X = STUFF(ABS(CONVERT(int,X)),3,0,'.') * CONVERT(decimal(10,4),CASE WHEN X < 0 THEN -1.0 ELSE 1.0 END),\n    Y = STUFF(ABS(CONVERT(int,Y)),3,0,'.') * CONVERT(decimal(10,4),CASE WHEN Y < 0 THEN -1.0 ELSE 1.0 END);\n\nSELECT *\nFROM dbo.SampleTable;\nGO\n\nDROP TABLE dbo.SampleTable;\n\n```\n\nNote that you won't get a value like `41.45`, but instead `41.4500`. If you don't want to display trailing 0's you need to do the formatting in your presentation layer (otherwise you'd have to store the values as a `varchar`, and that's a very bad idea).\n\n\n"}
{"questionId":"0cf64b81212a4a008e1b9534c9aa768d","question":"Typescript: union and intersection \"inverted\" for object keys\nFrom what I know of set theory, when you take a union of two sets, you overlay two sets and take the resulting set. When you take the intersection of two sets, you overlay two sets and take the parts that overlap with each other.\n\n\nFor numbers this works just fine:\n\n\n\n```\ntype SomeUnion = (1 | 2 | 3) | (2 | 3 | 4)\nconst someUnion: SomeUnion \/\/ typeof someUnion is 1 | 2 | 3 | 4\n\ntype SomeIntersect = (1 | 2 | 3) & (2 | 3 | 4) \nconst someIntersect: SomeIntersect \/\/ typeof someIntersect is 2 | 3\n\n```\n\nFor object keys the intersection and union operates work quite unintuitively in my opinion.\n\n\n\n```\ntype ObjUnion = { one: string, two: string } | { two: string, three: string }\nconst objUnionKeys: keyof ObjUnion \/\/ typeof objUnionKeys is 'two' while I would expect it to be all keys -> 'one' | 'two' | 'three'\n\ntype ObjIntersection = { one: string, two: string } & { two: string, three: string }\nconst objIntersectionKeys: keyof ObjIntersection \/\/ typeof objIntersectionKeys is 'one' | 'two' | 'three' while I would expect it only to be the keys in common -> 'one'\n\n```\n\nI imagine that there is a solid reason for why it works like this. Can anyone fill me in?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"typescript"},"answer":"You're right that the behaviour of objects unions and intersections paradoxically seems to behave in the exact opposite way to that of literals but it actually makes perfect sense :) let's dive into why!\n\n\nIf you have a union type for string or number literals \u00e0 la\n\n\n\n```\ntype SomeUnion = 1 | 2 | 3 | 2 | 3 | 4\n\n```\n\nwhat you're saying is that `SomeUnion` could be any one of these numbers.\n\n\nWhen you have\n\n\n\n```\ntype SomeIntersect = (1 | 2 | 3) & (2 | 3 | 4)\n\n```\n\nwhat you're saying is that `SomeIntersect` *has* to satisfy the constraints of both groups. In this case the only numbers that satisfy both are 2 and 3, hence the above is equivalent to `type SomeIntersect = 2 | 3`\n\n\n\n\n---\n\n\nThe semantics of unions and intersections are different for objects though.\n\n\nWhen you have\n\n\n\n```\ntype ObjUnion = { one: string, two: string } | { two: string, three: string }\n\n```\n\nWhat you're saying is that `ObjUnion` could have *either* shape - meaning that the only field you know for sure exists on `ObjUnion` is `\"two\"`. The others may or may not exist depending on which of the two shapes it actually is. But you do have certainty that `{ two: string }` exists on the object.\n\n\n\n\n---\n\n\nWhen it comes to\n\n\n\n```\ntype ObjIntersection = { one: string, two: string } & { two: string, three: string }\n\n```\n\nwhat you're saying is that `ObjIntersection` *has* to have all 3 fields in the two object types, otherwise it wouldn't satisfy the constraints of the intersection.\n\n\nThis means that if you have an object of type `ObjIntersection` you *know* that it has all 3 fields and so TypeScript will let you access any one of them without a problem!\n\n\n"}
{"questionId":"5bd45cca92e14b37a67a6f07b192f226","question":"typescript: question mark vs type union with undefined\nWhen specifying a member with a question mark at the end of it's name, the type signature automatically gets extended to include `undefined`. It's okay to create an instance without this member:\n\n\n\n```\ninterface Option{\n    val? : number; \/\/ hover over val and it tells you that the type is number|undefined\n}\n\nlet o: Option = {};\n\n```\n\nThe inferred type of val is `number|undefined`. So far I thought that this was the only effect of the question mark. But manually annotating the type union does not have the same effect:\n\n\n\n```\ninterface Union{\n    val : number|undefined;\n}\n\nlet u: Union = {}; \/\/ compiler complains about missing member val\n\n```\n\nWhy is the second code sample incorrect?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"typescript"},"answer":"Your first option says \"val is an optional property with type number\". It CAN be there, but it doesn't have to.\n\n\nYour second options says \"val is REQUIRED property, which can have a value of either number or undefined\". Hence, it will throw compiler error.\n\n\n"}
{"questionId":"3d1d547e3b744e49974094aaac007c23","question":"Java Stream Reduce of array of objects\nI have a list of array of 2 objects:\n\n\n\n```\nList<Object[2]>\n\n```\n\nWhere object[0] is an Integer and object[1] is a String.\n\n\nHow can I stream the list and apply different functions on each object?\nSo that, the result will be an array having:\n\n\n\n```\nresult[0] = multiplication of all object[0]\nresult[1] = concatenation of all object[1]\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"You can achieve this with `reduce()` :\n\n\n\n```\npublic void testStacko() {\n    List<Object[]> list = new ArrayList<>();\n    list.add(new Object[] {1, \"foo\"});\n    list.add(new Object[] {6, \"|bar\"});\n    list.add(new Object[] {15, \"|baz\"});\n    Object[] array = list.stream()\n                         .reduce(\n                                  (obj1, obj2) -> \n                                   new Object[] {(int) obj1[0] * (int) obj2[0], \n                                                 (String) obj1[1] + (String) obj2[1]\n                                                }\n                                )\n                         .get();\n    System.out.println(array[0]); \/\/ 90\n    System.out.println(array[1]); \/\/ foo|bar|baz\n}\n\n```\n\n"}
{"questionId":"460f6bae7c774aea9ef0636bc7cba7b2","question":"TypeError: cannot unpack non-iterable int objec\nHow can I solve this error After running my code as follows . I am using the function below and implementin running window for loop on it but end up getting the error below. The for loop works and hungs at a point.\n\n\n\n```\ndef get_grps(s, thresh=-1, Nmin=3):\n    \"\"\"\n    Nmin : int > 0\n    Min number of consecutive values below threshold.\n    \"\"\"\n    m = np.logical_and.reduce([s.shift(-i).le(thresh) for i in range(Nmin)])\n    if Nmin > 1:\n        m = pd.Series(m, index=s.index).replace({False: np.NaN}).ffill(limit=Nmin - 1).fillna(False)\n    else:\n        m = pd.Series(m, index=s.index)\n\n    # Form consecutive groups\n    gps = m.ne(m.shift(1)).cumsum().where(m)\n\n    # Return None if no groups, else the aggregations\n    if gps.isnull().all():\n        return 0\n    else:\n        agg = s.groupby(gps).agg([list, sum, 'size']).reset_index(drop=True)\n        # agg2 = s2.groupby(gps).agg([list, sum, 'size']).reset_index(drop=True)\n        return agg, gps\n\n\ndata_spi = [-0.32361498 -0.5229471   0.15702732  0.28753752   -0.01069884 -0.8163699\n  -1.3169327   0.4413181   0.75815576  1.3858147   0.49990863-0.06357133\n-0.78432    -0.95337325 -1.663739    0.18965477  0.81183237   0.8360347\n  0.99537593 -0.12197364 -0.31432647 -2.0865853   0.2084263    0.13332903\n -0.05270813 -1.0090573  -1.6578217  -1.2969246  -0.70916456   0.70059913\n -1.2127264  -0.659762   -1.1612778  -2.1216285  -0.8054617    -0.6293912\n -2.2103117  -1.9373081  -2.530625   -2.4089663  -1.950846    -1.6129876]\nlon = data_spi.lon\nlat = data_spi.lat\nprint(len(data_spi))\n\nn=6\n\nfor x in range(len(lat)):\n    for y in range(len(lon)):\n        if data_spi[0, x, y] != 0:\n            for i in range(len(data_spi)-70):\n                ts = data_spi[i:i+10, x, y].fillna(1)\n                print(ts)\n                # print(np.array(ts))\n\n                agg, gps = get_grps(pd.Series(ts), thresh=-1, Nmin=3)\n\n                duration = np.nanmean(agg['sum'])\n                frequency = len(agg['sum'])\n                severity = np.abs(np.mean(agg['sum']))\n                intensity = np.mean(np.abs(agg['sum'] \/ agg['size']))\n                print(f'intensity {intensity}')\n\n```\n\nI get this error\n\n\n\n```\n Traceback (most recent call last):\n File \"\/Users\/mada0007\/PycharmProjects\/Research_ass \/FREQ_MEAN_INT_DUR_CORR.py\", line 80, in <module>\n agg, gps = get_grps(pd.Series(ts), thresh=-1, Nmin=3)\n typeError: cannot unpack non-iterable int object\n\n```\n\nHow can I resolve this error?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"Just replace `return 0` by `return 0, 0`, or better: raise an error instead of returning 0\n\n\nWhen your `if` condition is True, you only return `0`. Then later, when you do `agg, gps = get_grps(...)`, you tell python to unpack the result of the function. Then, python is expecting a 2-length iterable, and try to unpack it, but as it says: it 'cannot unpack non-iterable int object'...\n\n\nSo a quick workaround is to return a tuple (0, 0) with `return 0, 0`, but it is quite bad because you return integers where objects are expected. your script will crash on the next line `duration = np.nanmean(agg['sum'])` (since `agg` is 0).\n\n\nSome cleaner solutions to handle this case would be to unpack in a second time:\n\n\n\n```\ndef get_grps(s, thresh=-1, Nmin=3):\n    # ...\n    if gps.isnull().all():\n        return None\n    else:\n        # ...\n        return agg, gps\n\nfor i in range(len(data_spi)-70):\n    ts = data_spi[i:i+10, x, y].fillna(1)\n\n    result = get_grps(pd.Series(ts), thresh=-1, Nmin=3)\n    if result is None:\n        break\n\n    agg, gps = result\n\n    duration = np.nanmean(agg['sum'])\n    frequency = len(agg['sum'])\n\n```\n\n"}
{"questionId":"464a9928ae6b4492a40e4882d29d9001","question":"How to select the last record of a table in SQL?\nThis is a sample code to select all records from a table. Can someone show me how to select the last record of that table?\n\n\n\n```\nselect * from table\n\n```\n\nWhen I use: `SELECT * FROM TABLE ORDER BY ID DESC LIMIT` \nI get this error: Line 1: Incorrect syntax near 'LIMIT'.\nThis is the code I use: \n\n\n\n```\nprivate void LastRecord()\n{\n    SqlConnection conn = new SqlConnection(ConfigurationManager.ConnectionStrings[\"HELPDESK_OUTLOOKConnectionString3\"].ToString());\n\n    conn.Open();\n    SqlDataReader myReader = null;\n    SqlCommand myCommand = new SqlCommand(\"SELECT * FROM HD_AANVRAGEN ORDER BY \" +\n                \"aanvraag_id DESC LIMIT 1\", conn);\n    myReader = myCommand.ExecuteReader();\n    while (myReader.Read())\n    {\n        TextBox1.Text = (myReader[\"aanvraag_id\"].ToString());\n        TextBox1.Text += (myReader[\"wijziging_nummer\"].ToString());\n        TextBox1.Text += (myReader[\"melding_id\"].ToString());\n        TextBox1.Text += (myReader[\"aanvraag_titel\"].ToString());\n        TextBox1.Text += (myReader[\"aanvraag_omschrijving\"].ToString());\n        TextBox1.Text += (myReader[\"doorlooptijd_id\"].ToString());\n        TextBox1.Text += (myReader[\"rapporteren\"].ToString());\n        TextBox1.Text += (myReader[\"werknemer_id\"].ToString());\n        TextBox1.Text += (myReader[\"outlook_id\"].ToString());\n    }\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"sql"},"answer":"MS SQL Server has supported ANSI SQL `FETCH FIRST` for many years now:\n\n\n\n```\nSELECT * FROM TABLE\nORDER BY ID DESC \nOFFSET 0 ROWS FETCH FIRST 1 ROW ONLY\n\n```\n\n(Works with most modern databases.)\n\n\n"}
{"questionId":"bac3d5f065654eab9e1f77460a22ca69","question":"What is the meaning of \"producing negative zeroes\" in a system that doesn't support it?\nC17 6.2.6.2\/4 says:\n\n\n\n> \n> If the implementation does not support negative zeros, the behavior of the &, |, ^, ~, <<,\n>  and >> operators with operands that would produce such a value is undefined.\n> \n> \n> \n\n\nIf I have a 2's complement system, it does not support negative zeroes. And it always utilizes all possible combinations of a binary number to express a value. Therefore it is impossible to ever produce a negative zero, no matter which bitwise operation that is used. So what is the meaning of this text?\n\n\nMy take is that this part refers to systems with 1's complement or signed magnitude that does not support negative zeroes, but instead use a padding bit or trap representation. Is this correct?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"Your interpretation is correct.\n\n\nGoing up to paragraph 2 of 6.2.6.2:\n\n\n\n> \n> For signed integer types, the bits of the object \n>  representation shall be divided into three groups: value bits, \n>  padding bits, and the sign bit. There need not be any\n>  padding bits; signed char shall not have any padding bits.\n>  There shall be exactly one sign bit. Each bit that is a \n>  value bit shall have the same value as the same bit in the\n>  object representation of the corresponding unsigned type (if there are\n>  M value bits in the signed type and N in the unsigned type, then M \u2264 N\n>  ). If the sign bit is zero, it shall not affect the resulting \n>  value. If the sign bit is one, the value shall be modified\n>  in one of the following ways:\n> \n> \n> - the corresponding value with sign bit 0 is negated ( sign and magnitude );\n> - the sign bit has the value \u2212 (2M)( two\u2019s complement );\n> - the sign bit has the value \u2212 (2M \u2212 1) ( ones\u2019 complement ).\n> \n> \n> Which of these applies is implementation-defined, **as is whether the\n>  value with sign bit 1 and all value bits zero (for the first \n>  two), or with sign bit and all value bits 1 (for ones\u2019\n>  complement), is a trap representation or a normal value. In \n>  the case of sign and magnitude and ones\u2019 complement, if this \n>  representation is a normal value it is called a negative zero.**\n> \n> \n> \n\n\nThis means an implementation using either one's complement or sign and magnitude has, for a given size integer type, a specific representation which must be either negative zero or a trap representation. It's then up to the implementation to choose which one of those applies.\n\n\nAs an example, suppose a system has sign and magnitude representation and a 32 bit `int` with no padding. Then the representation that would be negative zero, if it is supported, is `0x80000000`.\n\n\nNow suppose the following operations are performed:\n\n\n\n```\n int x = 0x7fffffff;\n x = ~x;\n\n```\n\nIf the implementation supports negative zero, the `~` operator will generate `-0` as the result and store it in `x`. If it does not, it creates a trap representation and invokes undefined behavior as per paragraph 4.\n\n\n"}
{"questionId":"bfe61baddeac48db911c9fea48bf90df","question":"checking if a string exists in an array of strings in bash\nI know how to compare two string in bash:\n\n\n\n```\nif [ \"$build_type\" = \"devite\" ]; then\n  echo \"building'\"\nfi\n\n```\n\nBut what I need is to check if \"$build\\_type\" is in [\"devite\", \"relite\"]\n\n\nso something similar to this:\n\n\n\n```\nif [ \"$build_type\" in [\"devite\", \"relite\"] ]; then\n  echo \"building'\"\nfi\n\n```\n\nCan anyone shed light on this?\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"bash\/shell"},"answer":"Join two `test`\/`[` commands with `||`:\n\n\n\n```\nif [ \"$build_type\" = devite ] || [ \"$build_type\" = relite ]; then\n  echo \"building\"\nfi\n\n```\n\nor use a `case` statement.\n\n\n\n```\ncase $build_type in\n  devite|relite) echo \"building\" ;;\nesac\n\n```\n\nIf the targets are in an associative array, you can check for the existence of a key.\n\n\n\n```\ndeclare -A targets=([devite]= [relite]=)\n\nif [[ -v targets[$build_type] ]]; then\n    echo \"building\"\nfi\n\n```\n\n"}
{"questionId":"f2505ef2a54c4ddd92cdec1860c31c63","question":"Convert datetime.min into offset aware datetime\nI need to subtract a timezone aware datetime.now() with datetime.min, but i keep getting this error **TypeError: can't subtract offset-naive and offset-aware datetimes**. Please help!\n\n\n\n```\nfrom datetime import datetime\nfrom pytz import timezone\nnow = datetime.now(timezone('Europe\/Dublin'))\nresult = now - datetime.min\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"You can convert it to `UTC`:\n\n\n\n```\nIn [1]: from datetime import datetime\n\nIn [2]: import pytz\n\nIn [3]: dt_min = datetime.min\n\nIn [4]: print(dt_min)\n0001-01-01 00:00:00\n\nIn [5]: dt_min = dt_min.replace(tzinfo=pytz.UTC)\n\nIn [6]: print(dt_min)\n0001-01-01 00:00:00+00:00\n\n```\n\nSo your code would be:\n\n\n\n```\nfrom datetime import datetime\nimport pytz\nnow = datetime.now(pytz.timezone('Europe\/Dublin'))\ndt_min = datetime.min\nresult = now - dt_min.replace(tzinfo=pytz.UTC)\nprint(result)\n\noutput:\n737202 days, 7:27:48.839353\n\n```\n\n"}
{"questionId":"272ae7d3e6284f9780fd44b6cf834b01","question":"'IConfigurationBuilder' does not contain a definition for 'AddAzureAppConfiguration'\nTrying to setup Azure App Configuration with Azure Key Vault in Program.cs and getting following error:\n\n\n\n> \n> 'IConfigurationBuilder' does not contain a definition for\n> 'AddAzureAppConfiguration'\n> \n> \n> \n\n\n\n```\npublic static IHostBuilder CreateHostBuilder(string[] args) =>\n        Host.CreateDefaultBuilder(args)\n        .ConfigureWebHostDefaults(webBuilder =>\n        webBuilder.ConfigureAppConfiguration((hostingContext, config) =>\n        {\n            var settings = config.Build();\n\n            config.AddAzureAppConfiguration(options =>\n            {\n                options.Connect(settings[\"ConnectionStrings:AppConfig\"])\n                        .ConfigureKeyVault(kv =>\n                        {\n                            kv.SetCredential(new DefaultAzureCredential());\n                        });\n            });\n        })\n        .UseStartup<Startup>());\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"adding following package fixed it:\n\n\n\n```\ndotnet add package Microsoft.Azure.AppConfiguration.AspNetCore\n\n```\n\n"}
{"questionId":"365848db451a47dba4889a709bed0ae1","question":"Dart - Send an UDP broadcast\nI'm asking for help since it seems I cannot find a way to send an UDP broadcast inside a local network using Dart.\n\n\nSo far I managed to communicate using UDP with `RawDatagramSocket`. I'm able to send a message to a specific address.\n\n\nWhat I'm not able to do is to send a broadcast to any device inside a local network (network mask is 255.255.255.0), and to wait for possible (multiple) answer(s). Here is the code I'm using:\n\n\n\n```\nRawDatagramSocket.bind('127.0.0.1', 8889)\n    .then((RawDatagramSocket udpSocket) {\n        udpSocket.listen((e) {\n            Datagram dg = udpSocket.receive();\n            if (dg != null) {\n                \/\/stuff\n            }\n        });\n        udpSocket.send(utf8.encode('TEST'), DESTINATION_ADDRESS, 8889);\n});\n\n```\n\nI tried to replace `DESTINATION_ADDRESS` with `InternetAddress.anyIPv4`, but I had no luck. I also found the property `broadcastEnabled` inside `RawDatagramSocket`, but I cannot find further informations about how to make use of it.\n\n\nThanks in advance for you help.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"dart"},"answer":"There are two problems:\n\n\n1. Use `InternetAddress.anyIPv4` for binding on all network interfaces;\n2. Enable permission for broadcasting with property `broadcastEnabled`\n\n\nObviously use a broadcast address: for a `\/24` network use `x.y.z.255` address.\n\n\nThis snippet works:\n\n\n\n```\nimport 'dart:io';\nimport 'dart:convert';\n\nmain() {\n\n  var DESTINATION_ADDRESS=InternetAddress(\"x.y.z.255\");\n\n  RawDatagramSocket.bind(InternetAddress.anyIPv4, 8889).then((RawDatagramSocket udpSocket) {\n    udpSocket.broadcastEnabled = true;\n    udpSocket.listen((e) {\n      Datagram dg = udpSocket.receive();\n      if (dg != null) {\n        print(\"received ${dg.data}\");\n      }\n    });\n    List<int> data =utf8.encode('TEST');\n    udpSocket.send(data, DESTINATION_ADDRESS, 8889);\n  });\n}\n\n```\n\n"}
{"questionId":"68eec7227a8242a2bc0435885063a78c","question":"How to use rows.Scan of Go's database\/sql\nI use database\/sql and define a struct mapping to DB table columns(tag field):\n\n\n\n```\n\/\/ Users ...\ntype Users struct {\n    ID            int64  `field:\"id\"`                      \n    Username      string `field:\"username\"`           \n    Password      string `field:\"password\"`           \n    Tel           string `field:\"tel\"`                   \n}\n\n```\n\nthen I query:\n\n\n\n```\n        rows, err := db.Query(sql)  \/\/ select * from users\n        if err != nil {\n            fmt.Println(err)\n        }\n        defer rows.Close()\n        for rows.Next() {\n            user := new(Users)\n\n            \/\/ works but I don't think it is good code for too many columns\n            err = rows.Scan(&user.ID, &user.Username, &user.Password, &user.Tel)\n\n            \/\/ TODO: How to scan in a simple way \n\n\n            if err != nil {\n                fmt.Println(err)\n            }\n            fmt.Println(\"user: \", user)\n            list = append(list, *user)\n        }\n        if err := rows.Err(); err != nil {\n            fmt.Println(err)\n        }\n\n```\n\nAs you can see for `rows.Scan()` , I have to write all columns , and I don't think it's a good way for 20 or more columns . \n\n\nHow to scan in a clear way.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"go"},"answer":"It's a good practice for using reflect:\n\n\n\n```\n    for rows.Next() {\n        user := Users{}\n\n        s := reflect.ValueOf(&user).Elem()\n        numCols := s.NumField()\n        columns := make([]interface{}, numCols)\n        for i := 0; i < numCols; i++ {\n            field := s.Field(i)\n            columns[i] = field.Addr().Interface()\n        }\n\n        err := rows.Scan(columns...)\n        if err != nil {\n            log.Fatal(err)\n        }\n        log.Println(user)\n    }\n\n```\n\n"}
{"questionId":"68ffa98af9f143b3ab4290cfc672027c","question":"Collectors.toMap write a merge function on a different attribute of object than the one which is not used as value\nI need to create `Map<String, String>` from `List<Person>` using Stream API.\n\n\n\n```\npersons.stream()\n       .collect(Collectors\n            .toMap(Person::getNationality, Person::getName, (name1, name2) -> name1)\n\n```\n\nBut in the above case, I want to resolve conflict in name attribute by using Person's age. is there any way to pass merge function something around the lines `(age1, age2) -> \/\/ if age1 is greater than age2 return name1, else return name2` ?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"To select a person based on its age, you need the `Person` instance to query the age. You cannot reconstitute the information after you mapped the `Person` to a plain name `String`.\n\n\nSo you have to collect the persons first, to be able to select the oldest, followed by mapping them to their names:\n\n\n\n```\npersons.stream()\n    .collect(Collectors.groupingBy(Person::getNationality, Collectors.collectingAndThen(\n        Collectors.maxBy(Comparator.comparingInt(Person::getAge)),\n        o -> o.get().getName())));\n\n```\n\n"}
{"questionId":"9ea14d726d644dd0bfd27745676a4598","question":"Get second minimum values per column in 2D array\nHow can I get the second minimum value from each column? I have this array:\n\n\n\n```\nA = [[72 76 44 62 81 31]\n     [54 36 82 71 40 45]\n     [63 59 84 36 34 51]\n     [58 53 59 22 77 64]\n     [35 77 60 76 57 44]]\n\n```\n\nI wish to have output like:\n\n\n\n```\nA = [54 53 59 36 40 44]\n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"python"},"answer":"Try this, in just one line:\n\n\n\n```\n[sorted(i)[1] for i in zip(*A)]\n\n```\n\nin action:\n\n\n\n```\nIn [12]: A = [[72, 76, 44, 62, 81, 31], \n    ...:      [54 ,36 ,82 ,71 ,40, 45], \n    ...:      [63 ,59, 84, 36, 34 ,51], \n    ...:      [58, 53, 59, 22, 77 ,64], \n    ...:      [35 ,77, 60, 76, 57, 44]] \n\nIn [18]: [sorted(i)[1] for i in zip(*A)]                                                                                                                                                                           \nOut[18]: [54, 53, 59, 36, 40, 44]\n\n```\n\n\n> \n> `zip(*A)` will transpose your list of list so the columns become rows.\n> \n> \n> \n\n\nand if you have duplicate value, for example:\n\n\n\n```\nIn [19]: A = [[72, 76, 44, 62, 81, 31], \n    ...:  [54 ,36 ,82 ,71 ,40, 45], \n    ...:  [63 ,59, 84, 36, 34 ,51], \n    ...:  [35, 53, 59, 22, 77 ,64],   # 35\n    ...:  [35 ,77, 50, 76, 57, 44],]  # 35\n\n```\n\nIf you need to skip both `35`s, you can use `set()`:\n\n\n\n```\nIn [29]: [sorted(list(set(i)))[1] for i in zip(*A)]                                                                                                                                                                \nOut[29]: [54, 53, 50, 36, 40, 44]\n\n```\n\n"}
{"questionId":"91939e44b1c74d8ba68b1df82ada59fd","question":"bigquery group by all columns except a few\nI have a table with loads of fields, and I am trying to group by all except two values which I am summing on. I would like to do something like \n\n\n\n```\nSELECT my_table.* except(value_1, value_2)\n    , sum(value_1)\n    , sum(value_2)\nFROM my_table\nGROUP BY my_table.* except(value_1, value_2)\n\n```\n\nBut unfortunately `GROUP BY my_table.* except(value_1, value_2)` do not work. Any suggestions please?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"sql"},"answer":"Below is for BigQuery Standard SQL \n\n\n\n```\n#standardSQL\nSELECT DISTINCT * EXCEPT(value_1, value_2, grp),\n  SUM(value_1) OVER(PARTITION BY grp) sum_value_1,\n  SUM(value_2) OVER(PARTITION BY grp) sum_value_2\nFROM (\n  SELECT *, REGEXP_REPLACE(TO_JSON_STRING(t), r'\"(?:value_1|value_2)\":.+?[,}]', '') grp\n  FROM `project.dataset.table` t\n)\n\n```\n\nYou can test, play with above using dummy data as in below example \n\n\n\n```\n#standardSQL\nWITH `project.dataset.table` AS (\n  SELECT 1 value_1, 2 value_2, 3 value_3, 4 value_4 UNION ALL\n  SELECT 11, 12, 3, 14 UNION ALL\n  SELECT 21, 22, 3, 14\n)\nSELECT DISTINCT * EXCEPT(value_1, value_2, grp),\n  SUM(value_1) OVER(PARTITION BY grp) sum_value_1,\n  SUM(value_2) OVER(PARTITION BY grp) sum_value_2\nFROM (\n  SELECT *, REGEXP_REPLACE(TO_JSON_STRING(t), r'\"(?:value_1|value_2)\":.+?[,}]', '') grp\n  FROM `project.dataset.table` t\n)\n\n```\n\nwith result as \n\n\n\n```\nRow value_3 value_4 sum_value_1 sum_value_2  \n1   3       14      32          34   \n2   3       4       1           2    \n\n```\n\nAbove will work with any number of columns and you don't need to reference them all explicitly - only those columns to be excluded to be explicitly referenced - value\\_1 and value\\_2 in this example \n\n\n"}
{"questionId":"100f669545c044389219b555aa38c6e4","question":"Weird enum name resolution in C#\nConsider the following code:\n\n\n\n```\nusing System;\n\nnamespace Test\n{\n    enum Foo\n    {\n        A = 1,\n        B = 1,\n        C = 1\n    }\n    \n    public static class Program\n    {\n        public static void Main()\n        {\n            Console.WriteLine(\"{0}, {1}, {2}\", Foo.A, Foo.B, Foo.C);\n        }\n    }\n}\n\n```\n\nKnowing that enums are just integers under the hood, I expected it to be either `A, A, A` or `C, C, C`. But surprisingly, it prints out `B, B, B`! This behaviour appears to be consistent across .NET Framework, .NET Core 3.x and .NET 5.\n\n\nWhy does it choose `B`?\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"c#"},"answer":"It's undefined according to the documentation for Enum.GetName():\n\n\n\n> \n> If multiple enumeration members have the same underlying value, the\n> GetName method guarantees that it will return the name of one of those\n> enumeration members. However, it does not guarantee that it will\n> always return the name of the same enumeration member.\n> \n> \n> \n\n\nSo it can do what it likes in this regard.\n\n\nAs to *why* it returns `B` in your example, we can inspect the implementation of `GetEnumName()`:\n\n\n\n```\npublic virtual string GetEnumName(object value)\n{\n    if (value == null)\n        throw new ArgumentNullException(\"value\");\n\n    if (!IsEnum)\n        throw new ArgumentException(Environment.GetResourceString(\"Arg_MustBeEnum\"), \"enumType\");\n    Contract.EndContractBlock();\n\n    Type valueType = value.GetType();\n\n    if (!(valueType.IsEnum || Type.IsIntegerType(valueType)))\n        throw new ArgumentException(Environment.GetResourceString(\"Arg_MustBeEnumBaseTypeOrEnum\"), \"value\");\n\n    Array values = GetEnumRawConstantValues();\n    int index = BinarySearch(values, value);\n\n    if (index >= 0)\n    {\n        string[] names = GetEnumNames();\n        return names[index];\n    }\n\n    return null;\n}\n\n```\n\nAha! All is explained. To make the lookup faster, they used a binary search. And where is the first place a binary search looks when starting the search? That's right - it starts halfway through the list. And that's why it's finding the `B` first - after the list is ordered, the `B` in in the middle.\n\n\n(Note that the list is ordered by enum value, not enum name, so for your case the list is already ordered since all the values are the same.)\n\n\n"}
{"questionId":"660cdd3d5c834473888797eda559ca09","question":"How to create table using select query in SQL Server?\nI try to make 50-100 tables using SYS queries \n\n\n\n```\nSELECT windows_release, windows_service_pack_level, \n       windows_sku, os_language_version\nFROM sys.dm_os_windows_info OPTION (RECOMPILE);     -- DE\u011e\u0130\u015e\u0130RSE INSERT ETSIN AYNI ISE DE\u011e\u0130\u015eMES\u0130N\n\n-- Gives you major OS version, Service Pack, Edition, and language info for the operating system\n\n-- SQL Server Services information (SQL Server 2008 R2 SP1 or greater)\nSELECT servicename, startup_type_desc, status_desc, \nlast_startup_time, service_account, is_clustered, cluster_nodename\nFROM sys.dm_server_services OPTION (RECOMPILE);\n\n\n-- Hardware information from SQL Server 2008 \n-- (Cannot distinguish between HT and multi-core)\nSELECT cpu_count AS [Logical CPU Count], hyperthread_ratio AS [Hyperthread Ratio],\ncpu_count\/hyperthread_ratio AS [Physical CPU Count], \nphysical_memory_in_bytes\/1048576 AS [Physical Memory (MB)], \nsqlserver_start_time --, affinity_type_desc -- (affinity_type_desc is only in 2008 R2)\nFROM sys.dm_os_sys_info OPTION (RECOMPILE);\n\n```\n\nHow to create table from SYS tables queries result?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"sql"},"answer":"An example statement that uses a sub-select :\n\n\n\n```\nselect * into MyNewTable\nfrom\n(\nselect \n  * \nfrom \n[SomeOtherTablename]\nwhere \n  EventStartDatetime >= '01\/JAN\/2018' \n)\n) mysourcedata\n;\n\n```\n\nnote that the sub query must be given a name .. any name .. e.g. above example gives the subquery a name of mysourcedata. Without this a syntax error is issued in SQL\\*server 2012.\n\n\nThe database should reply with a message like:\n(9999 row(s) affected)\n\n\n"}
{"questionId":"4b3df3a0256247349ebe9a5c539972e4","question":"Android Studio setting node directory for build (Cause: error=2, No such file or directory)\nMy project build fails because it's using the incorrect directory to run Node. How do I go about setting the Node directory for these compile-time tasks?\n\n\nThe specific task is:\n\n\n\n> \n> app:recordFilesBeforeBundleCommandDebug\n> \n> \n> \n\n\nAnd the related error:\n\n\n\n```\norg.gradle.api.tasks.TaskExecutionException: Execution failed for task ':app:recordFilesBeforeBundleCommandDebug'.\nCaused by: org.gradle.process.internal.ExecException: A problem occurred starting process 'command 'node''\nCaused by: net.rubygrapefruit.platform.NativeException: Could not start 'node'\nCaused by: java.io.IOException: Cannot run program \"node\" (in directory \"\/Users\/me\/Code\/appname\/android\/app\"): error=2, No such file or directory\nCaused by: java.io.IOException: error=2, No such file or directory\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"Add this to the build.gradle if you use React-Native\n\n\n\n```\next {    \n    react = [\n        nodeExecutableAndArgs:  [\"\/usr\/local\/bin\/node\"]\n    ]\n}\n\n```\n\n"}
{"questionId":"3ba5388fb1f74e008f0ee725ce21aaa5","question":"How to fix \"error: call to 'abs' is ambiguous\"\nI'm running a simple C++ program from HackerRank about pointers and it works fine on the website. However,\nwhen I run it on MacOS, I get `error: call to 'abs' is ambiguous` and I'm not sure exactly what is ambiguous.\n\n\nI've looked at other answers to similar issues, but the error message tends to be `Ambiguous overload call to abs(double)`, which is not the issue I'm having, since I haven't used any doubles. I've also tried including the header files `cmath` and `math.h`, but the problem persists.\n\n\n\n```\n#include <stdio.h>\n#include <cmath>\n\nvoid update(int *a,int *b) {\n    int num1 = *a;\n    int num2 = *b;\n    *a = num1 + num2;\n    *b = abs(num1 - num2);\n}\n\nint main() {\n    int a, b;\n    int *pa = &a, *pb = &b;\n\n    scanf(\"%d %d\", &a, &b);\n    update(pa, pb);\n    printf(\"%d\\n%d\", a, b);\n\n    return 0;\n}\n\n```\n\nMy issue occurs with line 8.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c++"},"answer":"The full error message is:\n\n\n\n```\n$ clang++ test.cpp\ntest.cpp:8:10: error: call to 'abs' is ambiguous\n    *b = abs(num1 - num2);\n         ^~~\n...\/include\/c++\/v1\/math.h:769:1: note: candidate function\nabs(float __lcpp_x) _NOEXCEPT {return ::fabsf(__lcpp_x);}\n^\n...\/include\/c++\/v1\/math.h:769:1: note: candidate function\nabs(double __lcpp_x) _NOEXCEPT {return ::fabs(__lcpp_x);}\n^\n...\/include\/c++\/v1\/math.h:769:1: note: candidate function\nabs(long double __lcpp_x) _NOEXCEPT {return ::fabsl(__lcpp_x);}\n^\n1 error generated.\n\n```\n\nThe three overloads of `abs` that you have from `<cmath>` are `abs(float)`, `abs(double)` and `abs(long double)`; it's ambiguous because you have an `int` argument and the compiler doesn't know which floating-point type to convert to.\n\n\n`abs(int)` is defined in `<cstdlib>`, so `#include <cstdlib>` will resolve your problem.\n\n\nIf you're using Xcode, you can get more details about the error in the Issues navigator (\u23185) and clicking the triangle next to your issue.\n\n\n"}
{"questionId":"59cf2216cc33425881c40348271424f2","question":"Why is the function return in C a statement?\nAn expression generates a value, statements alter the status of the machine, aka, side effects. However, I keep reading that function return is a statement. If I call a function that returns a void, how does that change any status of the machine? Or if I call a function that returns a non-void value, if I don't use it but just calling it how this changes any status?\n\n\nI just don't get why the **return** is a statement?\n\n\nSource: ***Concepts in Programming Languages. Cambridge: Cambridge University Press, 3.4.1 Statements and Expressions, p. 26***\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"It changes the call stack and program counter. It puts the return value in a known place (depending on calling conventions)\n\n\nEven if you don\u2019t use the return value, the compiler still needs to store it somewhere as it may be called from different compiler units that are unknown.\n\n\n"}
{"questionId":"6cc796d562754240a4c2c18cfaf149d8","question":"Capture all Regex matches into a vector\nI'd like to capture all the numbers in a string and return a vector of integers, something like this (the result can be an empty vector):\n\n\n\n```\nfn str_strip_numbers(s: &str) -> Vec<isize> {\n    unimplemented!()\n}\n\n```\n\nA Python prototype:\n\n\n\n```\ndef str_strip_numbers(s):\n    \"\"\"\n    Returns a vector of integer numbers\n    embedded in a string argument.\n    \"\"\"\n    return [int(x) for x in re.compile('\\d+').findall(s)]\n\n```\n\nFor `\"alfa\"` the result is `[]`, for `\"42by4\"` it is `[42, 4]`.\n\n\nWhat is the idiomatic way to get it in Rust?\n\n\nUPD:\n\n\n\n```\nfn str_strip_numbers(s: &str) -> Vec<String> {\n    lazy_static! {\n        static ref RE: Regex = Regex::new(r\"\\d+\").unwrap();\n    }\n    RE.captures(s).and_then(|cap| {cap})\n}\n\n```\n\nI tried something like this, which is grossly wrong on more than one count. What would be the right approach?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"rust"},"answer":"If you want all of the matches then you probably want to use `find_iter()`, which gives you an iterator over all of the matches. Then you'll need to convert the string matches into integers, and finally collect the results into a vector.\n\n\n\n```\nuse lazy_static::lazy_static;\nuse regex::Regex;\n\nfn str_strip_numbers(s: &str) -> Vec<i64> {\n    lazy_static! {\n        static ref RE: Regex = Regex::new(r\"\\d+\").unwrap();\n    }\n    \/\/ iterate over all matches\n    RE.find_iter(s)\n        \/\/ try to parse the string matches as i64 (inferred from fn type signature)\n        \/\/ and filter out the matches that can't be parsed (e.g. if there are too many digits to store in an i64).\n        .filter_map(|digits| digits.as_str().parse().ok())\n        \/\/ collect the results in to a Vec<i64> (inferred from fn type signature)\n        .collect()\n}\n\n```\n\n"}
{"questionId":"4ead0575b6524188aa64b64a7aebd161","question":"where is ruby 3.0.0 on rbenv\n`rbenv install --list-all` shows me, among many other things of course,\n\n\n\n```\n3.0.0-dev\n3.0.0-preview1\n3.0.0-preview2\n\n```\n\nBut where is `3.0.0` itself, the actual final release? I'd like to try it out. Not all announced features made it into the previews so I need the real thing.\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"ruby"},"answer":"`rbenv install` passes thru `ruby-build`. You need to update (`brew upgrade` if installed via Homebrew) your `ruby-build` in order to see the latest versions.\n\n\n"}
{"questionId":"da536e7c67f540d787c0368f95d61c60","question":"How to remove all pipenv virtualenvs when the directory was deleted?\nI have a directory `A\/` containing a Pipfile and a Pipfile.lock obtained by running `pipenv install`. So this directory has a corresponding virtualenv at `\/home\/username\/.local\/share\/virtualenvs\/A-...`.\n\n\nThen I delete this `A\/` directory. **Is there a way to check all pipenv virtualenvs, find directories that were deleted and remove the corresponding virtualenvs ?**\n\n\nIn this case it should find that `A\/` was deleted and so remove the virtualenv: `\/home\/username\/.local\/share\/virtualenvs\/A-...`.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"`pipenv` has no special option for it. But you can use these bash scripts (I made them using the venvs directory `~\/.local\/share\/virtualenvs\/` so you should change it if your venvs folder is different)\n\n\n# PRINT useless venvs:\n\n\n\n```\nfor f in $(find ~\/.local\/share\/virtualenvs\/*\/.project -type f); do proj_path=\"$(cat $f)\"  && [ ! -d \"$proj_path\" ] && echo ${f\/\/\\\/.project};  done;\n\n```\n\n**PRINT not existing projects paths** that still have corresponding venvs:\n\n\n\n```\nfor f in $(find ~\/.local\/share\/virtualenvs\/*\/.project -type f); do proj_path=\"$(cat $f)\"  && [ ! -d \"$proj_path\" ] && echo $proj_path;  done;\n\n```\n\n**PRINT both** (useless venvs and not existing project folders):\n\n\n\n```\nfor f in $(find ~\/.local\/share\/virtualenvs\/*\/.project -type f); do proj_path=\"$(cat $f)\" && [ ! -d \"$proj_path\" ] && echo $proj_path \"\\n\"${f\/\/\\\/.project} \"\\n\";  done;\n\n```\n\n# DELETE useless venvs\n\n\n\n```\nfor f in $(find ~\/.local\/share\/virtualenvs\/*\/.project -type f); do proj_path=\"$(cat $f)\" && [ ! -d \"$proj_path\" ] && rm -rif ${f\/\/\\\/.project} && echo DELETING ${f\/\/\\\/.project};  done;\n\n```\n\nTry printing the venvs before deleting!\n\n\np.s. If your venv folder is not `~\/.local\/share\/virtualenvs\/`, make sure to change it to your venv path!\n\n\n"}
{"questionId":"dbeb1214068640ec9f98c5a5cfd50170","question":"Pandas 'Passing list-likes to .loc or [] with any missing labels is no longer supported' on train\\_test\\_split returned data\nFor some reason train\\_test\\_split, despite lengths being identical and indexes look the same, triggers this error.\n\n\n\n```\nfrom sklearn.model_selection import KFold\n\ndata = {'col1':[30.5,45,1,99,6,5,4,2,5,7,7,3], 'col2':[99.5, 98, 95, 90,1,5,6,7,4,4,3,3],'col3':[23, 23.6, 3, 90,1,9,60,9,7,2,2,1]} \ndf = pd.DataFrame(data)\n\ntrain, test = train_test_split(df, test_size=0.10)\nX = train[['col1', 'col2']]\ny2 = train['col3']\n\nX = np.array(X)\n\nkf = KFold(n_splits=3, shuffle=True)\nfor train_index, test_index in kf.split(X):\n    X_train, y_train = X[train_index], y[train_index]\n\n```\n\ny is a pandas Series (same length as x).\nx was a dataframe with about 20 numerical columns casted to numpy array.\n\n\nFor some reason train\\_test\\_split triggers the error despite the lengths being identical.\n\n\nIf i dont call train\\_test\\_split it works fine.\n\n\nthe last line triggering the error due to trying to index numpy array this way:\ny[train\\_ind]\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"I've tried to create a scenario for your situation.\n\n\nI've created following dataframe:\n\n\n\n```\n    col1  col2  col3\n0      1     2     1\n1      3     4     0\n2      5     6     1\n3      7     8     0\n4      9    10     1\n5     11    12     0\n6     13    14     1\n7     15    16     0\n8     17    18     1\n9     19    20     0\n10    21    22     1\n11    23    24     0\n12    25    26     1\n13    27    28     0\n14    29    30     1\n\n```\n\nI set `col1` and `col2` for X and `col3` for y. After this I've converted X to numpy array as following. Only difference is I've used `shuffle` in `KFold`.\n\n\n\n```\nX = df[['col1', 'col2']]\ny = df['col3']\nX = np.array(X)\nkf = KFold(n_splits=3, shuffle=True)\nfor train_index, test_index in kf.split(X):\n    X_train, y_train = X[train_index], y[train_index]\n\n```\n\nAnd it worked well. So please check my code and your code and clarify it if there is something I missed.\n\n\n## Update\n\n\nI assume y2 is y. So y type is still `Series`, you need to use `.iloc` for it. Following code worked well.\n\n\n\n```\ndata = {'col1':[30.5,45,1,99,6,5,4,2,5,7,7,3], 'col2':[99.5, 98, 95, 90,1,5,6,7,4,4,3,3],'col3':[23, 23.6, 3, 90,1,9,60,9,7,2,2,1]}\ndf = pd.DataFrame(data)\ntrain, test = train_test_split(df, test_size=0.10)\n\nX = train[['col1', 'col2']]\ny = train['col3']\n\nX = np.array(X)\n\nkf = KFold(n_splits=3, shuffle=True)\nfor train_index, test_index in kf.split(X):\n    X_train, y_train = X[train_index], y.iloc[train_index]\n\n```\n\n"}
{"questionId":"4dac18b756a74d2eba83e46c0f632b44","question":"How can I print the offset of a struct member at compile time?\nGiven a struct, for instance:\n\n\n\n```\nstruct A {\n    char a;\n    char b;\n} __attribute__((packed));\n\n```\n\nI want the offset of `b` (in this example, 1) in the struct to be printed at compile time - I don't want to have to run the program and call something like `printf(\"%zu\", offsetof(struct A, b));` because printing is non-trivial on my platform. I want the offset to be printed by the compiler itself, something like:\n\n\n\n```\n> gcc main.c\nThe offset of b is 1\n\n```\n\n\n\n---\n\n\nI've tried a few approaches using `#pragma message` and `offsetof`, with my closest being:\n\n\n\n```\n#define OFFSET offsetof(struct A, b)\n#define XSTR(x) STR(x)\n#define STR(x) #x\n\n#pragma message \"Offset: \" XSTR(OFFSET)\n\n```\n\nwhich just prints:\n\n\n\n```\n> gcc main.c\nmain.c:12:9: note: #pragma message: Offset: __builtin_offsetof (struct A, b)\n\n```\n\nwhich does not print the numeric offset. It's possible to binary-search the offset at compile time by using `_Static_assert` - but my real structs are big and this can get a bit cumbersome.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c"},"answer":"Given this macro:\n\n\n\n```\n#define PRINT_OFFSETOF(A, B) char (*__daniel_kleinstein_is_cool)[sizeof(char[offsetof(A, B)])] = 1\n\n```\n\nUse it into your `main()` function (or whatever function):\n\n\n\n```\nstruct Test {\n  char x;\n  long long y;\n  int z;\n};\n\nint main(void) {\n  PRINT_OFFSETOF(struct Test, z);\n  return 0;\n}\n\n```\n\nAnd you will get this warning:\n\n\n\n```\nwarning: initialization of \u2018char (*)[16]\u2019 from \u2018int\u2019 makes pointer from integer without a cast [-Wint-conversion]\n\n```\n\nHence, `offsetof(struct Test, z) == 16`.\n\n\n\n\n---\n\n\nNOTE: in case `offsetof()` returns `0` (e.g.: `PRINT_OFFSETOF(struct Test, x)`), the compiler warning will have `char (*)[]` instead of `char (*)[16]`.\n\n\nNOTE 2: I only tested this with GCC.\n\n\n"}
{"questionId":"aa85ef2077a64645904be7f15e3ab793","question":"Access an array from the end in C?\nI recently noticed that in C, there is an important difference between `array` and `&array` for the following declaration: \n\n\n\n```\nchar array[] = {4, 8, 15, 16, 23, 42};\n\n```\n\nThe former is *a pointer to a char* while the latter is *a pointer to an array of 6 chars*. Also it is notable that the writing `a[b]` is a syntactic sugar for `*(a + b)`. Indeed, you could write `2[array]` and it works perfectly according to the standard.\n\n\nSo we could take advantage of this information to write this: \n\n\n\n```\nchar last_element = (&array)[1][-1];\n\n```\n\n`&array` has a size of 6 chars so `(&array)[1])` is a pointer to chars located right after the array. By looking at `[-1]` I am therefore accessing the last element. \n\n\nWith this I could for example swap the entire array :\n\n\n\n```\nvoid swap(char *a, char *b) { *a ^= *b; *b ^= *a; *a ^= *b; }\n\nint main() {\n    char u[] = {1,2,3,4,5,6,7,8,9,10};\n\n    for (int i = 0; i < sizeof(u) \/ 2; i++)\n        swap(&u[i], &(&u)[1][-i - 1]);\n}\n\n```\n\nDoes this method for accessing an array by the end have flaws?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"The C standard does not define the behavior of `(&array)[1]`.\n\n\nConsider `&array + 1`. This is defined by the C standard, for two reasons:\n\n\n- When doing pointer arithmetic, the result is defined for results from the first element (with index 0) of an array to one beyond the last element.\n- When doing pointer arithmetic, a pointer to a single object behaves like a pointer to an array with one element. In this case, `&array` is a pointer to a single object (that is itself an array, but the pointer arithmetic is for the pointer-to-the-array, not a pointer-to-an-element).\n\n\nSo `&array + 1` is defined pointer arithmetic that points just beyond the end of `array`.\n\n\nHowever, by definition of the subscript operator, `(&array)[1]` is `*(&array + 1)`. While the `&array + 1` is defined, applying `*` to it is not. C 2018 6.5.6 8 explicitly tells us, about result of pointer arithmetic, \u201cIf the result points one past the last element of the array object, it shall not be used as the operand of a unary `*` operator that is evaluated.\u201d\n\n\nBecause of the way most compilers are designed, the code in the question may move data around as you desire. However, this is not a behavior you should rely on. You can obtain a good pointer to just beyond the last element of the array with `char *End = array + sizeof array \/ sizeof *array;`. Then you can use `End[-1]` to refer to the last element, `End[-2]` to refer to the penultimate element, and so on.\n\n\n"}
{"questionId":"7c92b913aeea4993b17446c478c78473","question":"How to create a list of lists where each sub-list 'increments' as follows: [1, 0, 0], [1, 1, 0], [1, 1, 1]\nThis works but is unwieldy and not very 'Pythonic'. I'd also like to be able to run through different values for 'numValues', say 4 to 40...\n\n\n\n```\ninnerList = []\nouterList = []\nnumValues = 12\nloopIter = 0\n\nfor i in range(numValues):\n    innerList.append(0)\n\nfor i in range(numValues):\n    copyInnerList = innerList.copy()\n    outerList.append(copyInnerList)\n\nfor i in range(len(innerList)):\n    for j in range(loopIter + 1):\n        outerList[i][j] = 1\n    loopIter += 1\n\nprint(outerList)\n\n```\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"python"},"answer":"\n```\nnumValues = 12\nresult = [ [1] * i + [0] * (numValues - i) for i in range(1, numValues+1) ]\n\n```\n\n"}
{"questionId":"2230f79bb6ee45938c380d4d54ef4891","question":"List.of gives error: cannot find symbol error\nI am using springboot with gradle and I am trying to execute below code in the controller.\n\n\n\n```\nList<String> planets\n    = List.of(\"Mercury\", \"Venus\", \"Earth\", \"Mars\",\n    \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\");\n\n```\n\nOn compiling I get the following error\n\n\n\n> \n> error: cannot find symbol\n> = List.of(\"Mercury\", \"Venus\", \"Earth\", \"Mars\",\n> ^ symbol: method of(String,String,String,String,String,String,String,String)  \n> \n> location: interface List\n> \n> \n> \n\n\nmy gradle file has\n\n\n\n> \n> sourceCompatibility = '1.8'\n> \n> \n> \n\n\nI do understand that its a java 9 feature but unsure why would it fail on compile\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"`List.of` isn't a Java 9 feature, it's a method that was added in JDK 9. If you're using JDK 8, it just doesn't contain this method, and thus can't compile against it.\n\n\nIn short - use a newer JDK (and set the compatibility levels to 9 while you're at it, so you don't create a mix of valid Java 8 program that can only work with a newer JDK).\n\n\n"}
{"questionId":"855a1f81d52240b19b3512a67fdbc7b7","question":"Rails - Rubocop - Begin + Rescue syntax\nI have the following code:\n\n\n\n```\n  def payload\n    begin\n      @payload ||= Warden::JWTAuth::TokenDecoder.new.call(token)\n    rescue JWT::ExpiredSignature => e\n      Rollbar.warning(e)\n    end\n  end\n\n```\n\nFrom brief reading of a few blogs I'm supposed to be using begin rescue and end to handle the error as I'm doing above, however I'm getting a redundant 'begin' rubocop warning.\n\n\nIs begin only used when specifying a bit of code that may cause an error within a larger block? And is it therefore redundant here?\n\n\nThanks in advance\n\n\nEDIT: And if I don't need it, is it written as\n\n\n\n```\n  def payload\n    @payload ||= Warden::JWTAuth::TokenDecoder.new.call(token)\n  rescue JWT::ExpiredSignature => e\n    Rollbar.warning(e)\n  end\n\n```\n\n? \n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"ruby"},"answer":"Do this when the begin would be the first thing in your method\n\n\n\n```\ndef payload\n  @payload ||= Warden::JWTAuth::TokenDecoder.new.call(token)\nrescue JWT::ExpiredSignature => e\n  Rollbar.warning(e)\nend\n\n```\n\n"}
{"questionId":"4c92136982134dcdb4c1ceaa03760f45","question":"Use Haskell like Prelude modules in a module in raku\nI'm writing a drawing package with some parts, and I have operators and data types scattered througout. However I don't want the users to add the corresponding modules every time, since it would be quite messy, for instance I'd have a `Point` class, a `Monoid` role and a `Style` class\nin different paths like this\n\n\n\n```\nunit module Package::Data::Monoid;\n# $?FILE = lib\/Package\/Data\/Monoid.pm6\n\nrole Monoid {...}\n\n```\n\n\n```\nunit module Package::Data::Point;\n# $?FILE = lib\/Package\/Data\/Point.pm6\n\nclass Point {...}\n\n```\n\n\n```\nunit module Package::Data::Style;\n# $?FILE = lib\/Package\/Data\/Style.pm6\n\nclass Style {...}\n\n```\n\nI would like to have a `haskell` like prelude in `lib\/Package\/Prelude.pm6`\nwith the effect that I can write such scripts\n\n\n\n```\nuse Package::Prelude;\n\n# I can use Point right away, Style etc...\n\n```\n\ninstead of doing\n\n\n\n```\nuse Package::Data::Style;\nuse Package::Data::Point;\nuse Package::Data::Monoid;\n\n# I can too use point right away, but for users not knowing the\n# inner workings it's too overwhelming\n\n```\n\nI've tried many things:\n\n\n- This version doesn't give me the right effect, I have to type\nthe whole path to point, i.e., `Package::Data::Point`...\n\n\n\n```\nunit module Package::Prelude;\n# $?FILE = lib\/Package\/Prelude.pm6\nuse Package::Data::Style;\nuse Package::Data::Point;\nuse Package::Data::Monoid;\n\n```\n\n- This version gives me the `Point` right away, but I get\nproblems with the operators and so on, also I would just like to\nadd automatically everything from the exported routines in the mentioned\nexample packages.\n\n\n\n```\n# $?FILE = lib\/Package\/Prelude.pm6\nuse Package::Data::Style;\nuse Package::Data::Point;\nuse Package::Data::Monoid;\n\nsub EXPORT {\n  hash <Point> => Point\n     , <Style> => Style\n     , <mappend> => &mappend\n     ...\n}\n\n```\n\nDo you people know a better and quick way of getting such a prelude-like\nfile?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"haskell"},"answer":"\nUsing `EXPORT` is in the right direction. The key things to know are:\n\n\n- Imports are lexical\n- We can use introspection to obtain and access the symbols in the current lexical scope\n\n\nSo the recipe is:\n\n\n- `use` all the modules inside of `EXPORT`\n- Then extract all the imported symbols and return them as the result from `EXPORT`\n\n\nAs an example, I create a module `Foo::Point`, including an operator and a class:\n\n\n\n```\nunit module Foo::Point;\n\nclass Point is export {\n    has ($.x, $.y);\n}\n\nmulti infix:<+>(Point $a, Point $b) is export {\n    Point.new(x => $a.x + $b.x, y => $a.y + $b.y)\n}\n\n```\n\nAnd, just to demonstrate it can work with multiple modules, also a `Foo::Monad`:\n\n\n\n```\nunit module Foo::Monad;\n\nclass Monad is export {\n    method explain() { say \"Just think of a burrito...\" }\n}\n\n```\n\nThe goal is to make this work:\n\n\n\n```\nuse Foo::Prelude;\nsay Point.new(x => 2, y => 4) + Point.new(x => 3, y => 5);\nMonad.explain;\n\n```\n\nWhich can be achieved by writing a `Foo::Prelude` that contains:\n\n\n\n```\nsub EXPORT() {\n    {\n        use Foo::Point;\n        use Foo::Monad;\n        return ::.pairs.grep(*.key ne '$_').Map;\n    }\n}\n\n```\n\nThere's a few oddities in here to explain:\n\n\n1. A `sub` has implicit declarations of `$_`, `$\/`, and `$!`. Exporting these would result in a compile-time symbol clash error when the module is `use`'d. A block only has an implicit `$_`. Thus we make our life easier with a nested bare block.\n2. The `grep` is to make sure we don't export our implicitly declared `$_` symbol (thanks to the nested block, it's the only one we have to care about).\n3. `::` is a way to reference the current scope (etymology: `::` is the package separator). `::.pairs` thus obtains `Pair` objects for each symbol in the current scope.\n\n\nThere's a speculated re-export mechanism that may appear in a future Raku language release that would eliminate the need for this bit of boilerplate.\n\n\n"}
{"questionId":"14b9d98a20ca4dfd9c09afbe5ee48548","question":"Blazor: Adding a custom AuthenticationStateProvider in Startup.cs not recognized\nI am trying to implement a login using a custom database. As far as I can tell, I need to override AuthenticationStateProvider in order to accomplish this.\n\n\nIn MyServerAuthenticationStateProvider.cs:\n\n\n\n```\npublic class MyServerAuthenticationStateProvider : AuthenticationStateProvider\n{\n    string UserId;\n    string Password;\n\n    public void LoadUser(string _UserId, string _Password)\n    {\n        UserId = _UserId;\n        Password = _Password;\n    }\n\n\n    public override async Task<AuthenticationState> GetAuthenticationStateAsync()\n    {\n        var securityService = new SharedServiceLogic.Security();\n\n        var userService = new UserService();\n\n        var validPassword = await securityService.ValidatePassword(UserId, Password);\n\n        var authenticated = validPassword == true ? true : false;\n\n\n        var identity = authenticated\n            ? new ClaimsIdentity(await userService.GetClaims(UserId), \"AuthCheck\")\n            : new ClaimsIdentity();\n\n        var result = new AuthenticationState(new ClaimsPrincipal(identity));\n\n        return result;\n    }\n\n}\n\n```\n\nIn Startup.cs:\n\n\n\n```\nusing Microsoft.AspNetCore.Builder;\nusing Microsoft.AspNetCore.Components;\nusing Microsoft.AspNetCore.Hosting;\nusing Microsoft.Extensions.Configuration;\nusing Microsoft.Extensions.DependencyInjection;\nusing Microsoft.Extensions.Hosting;\nusing BadgerWatchWeb.Services;\n\nnamespace BadgerWatchWeb\n{\n    public class Startup\n    {\n        public Startup(IConfiguration configuration)\n        {\n            Configuration = configuration;\n        }\n\n        public IConfiguration Configuration { get; }\n\n    public void ConfigureServices(IServiceCollection services)\n    {\n        services.AddRazorPages();\n        services.AddServerSideBlazor();\n        services.AddSingleton<UserService>();\n        services.AddAuthorizationCore();\n        services.AddScoped<AuthenticationStateProvider, MyServerAuthenticationStateProvider > ();\n        \/\/services.AddScoped<AuthenticationStateProvider>(provider => provider.GetRequiredService<MysServerAuthenticationStateProvider>());\n    }\n\n    public void Configure(IApplicationBuilder app, IWebHostEnvironment env)\n    {\n        if (env.IsDevelopment())\n        {\n            app.UseDeveloperExceptionPage();\n        }\n        else\n        {\n            app.UseExceptionHandler(\"\/Error\");\n        }\n\n        app.UseStaticFiles();\n        app.UseAuthentication();\n        app.UseAuthorization();\n\n        app.UseRouting();\n\n        app.UseEndpoints(endpoints =>\n        {\n            endpoints.MapBlazorHub<App>(selector: \"app\");\n            endpoints.MapFallbackToPage(\"\/_Host\");\n        });\n    }\n}\n\n```\n\n}\n\n\nWhen I then try to use this service in a .razor class, I get an error saying 'MyServerAuthenticationStateProvider does not contain a definition for LoadUser.'\n\n\n\n```\n@page \"\/\"\n@using BadgerWatchWeb.Services  \n@inject AuthenticationStateProvider AuthenticationStateProvider\n\n<h1>Sup<\/h1>\n\n\n<AuthorizeView>\n    <Authorized>\n        <h1>Hello, @context.User.Identity.Name!<\/h1>\n        <p>You can only see this content if you're authenticated.<\/p>\n    <\/Authorized>\n    <NotAuthorized>\n        <h1>Authentication Failure!<\/h1>\n        <p>You're not signed in.<\/p>\n    <\/NotAuthorized>\n    <Authorizing>\n        <h1>Authorizing<\/h1>\n    <\/Authorizing>\n<\/AuthorizeView>\n\n\n@code {\n[CascadingParameter] Task<AuthenticationState> authenticationStateTask { get; set; }\n\n    AuthenticationState AuthState;\n\n    protected override async Task OnInitializedAsync()\n    {\n\n        AuthenticationStateProvider.LoadUser(\"mperry\", \"testtest\");\n        AuthState = await AuthenticationStateProvider.GetAuthenticationStateAsync();\n    }\n\n}\n\n```\n\nI am not sure if I am not sure if I am not using AuthenticationStateProvider correctly, and I have not been able to find any examples online of how to implement a custom login in razor. But my question is: why can't my code recognize LoadUser even though MyServerAuthenticationProvider is declaed as scoped to AuthenticationStateProvider in Startus.cs.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"On DI you did the right thing injecting your custom provider:\n\n\n\n```\n    services.AddScoped<AuthenticationStateProvider, \n                       MyServerAuthenticationStateProvider > ();\n\n```\n\nTo access your custom provider, just make a cast:\n\n\n\n```\n@inject AuthenticationStateProvider AuthenticationStateProvider\n\n@code {\n\n\nprotected override async Task OnInitializedAsync()\n{\n    var myStateProv = AuthenticationStateProvider as\n                        MyServerAuthenticationStateProvider;\n    myStateProv.LoadUser(\"mperry\", \"testtest\");\n\n```\n\n### Edited ( october 2020 )\n\n\nor:\n\n\n\n```\n    services.AddAuthorizationCore();\n\n    services.AddScoped<\n      MyServerAuthenticationStateProvider,\n      MyServerAuthenticationStateProvider>();\n\n    services.AddScoped<AuthenticationStateProvider>(\n      p => p.GetService<MyServerAuthenticationStateProvider>() );\n\n```\n\nAnd just get it via DI:\n\n\n\n```\n@inject MyServerAuthenticationStateProvider MyAuthenticationStateProvider\n\n```\n\n"}
{"questionId":"445d6aae346b4ab7af653507be15562e","question":"Sonarqube test report \"report refers to a file which is not configured as a test file\" when tests and source are together\nI am using TypeScript and Jest and have my tests next to my source files. e.g:\n\n\n- someDir\n\t- someCode.ts\n\t- someCode.spec.ts\n\n\nWhen I try and import the text-report.xml (which looks to be fine and matches the format), I get an error saying:\n\n\n'Line X report refers to a file which is not configured as a test file: \/someDir\/someCode.spec.ts'\n\n\nWhat configuration do I need in in the Sonarqube properties so that it understand which files are tests and which are source?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"Seems it doesn't detect files in sub-folders using \".\". Only way I was able to get it working was to list all of the folders.\n\n\n\n```\nsonar.sources=helpers,managers,routes,schemas,types\nsonar.tests=helpers,managers,routes,schemas,types\nsonar.exclusions=**\/*.js,test-data,dist,coverage\nsonar.test.inclusions=**\/*.spec.ts\nsonar.testExecutionReportPaths=test-report.xml\nsonar.javascript.lcov.reportPaths=coverage\/lcov.info\n\n```\n\n"}
{"questionId":"a87e052f9a054d80942b377360f62425","question":"Assign value to `readonly` properties from methods called by the constructor\nI have a simple class, and I want to assign a value to a readonly property in a method initiated by the constructor, but it says `[ts] Cannot assign to 'readOnlyProperty' because it is a constant or a read-only property.`\nWhy can't I assign a value to the property even though I am calling `process` from the constructor?\n\n\nSample Code:\n\n\n\n```\nclass C {\n    readonly readOnlyProperty: string;\n    constructor(raw: string) {\n        this.process(raw);\n    }\n    process(raw: string) {\n        this.readOnlyProperty = raw; \/\/ [ts] Cannot assign to 'readOnlyProperty' because it is a constant or a read-only property.\n    }\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"I usually use this workaround:\n\n\n\n```\nget myValue(): boolean { return this._myValue }\nprivate _myValue = true\n\n```\n\nResult:\n\n\n- `myValue` \u2013 is `readonly` from the outside\n- `_myValue` \u2013 can be modified within the class\n\n\nThe advantage of this workaround is that **your IDE can refactor your property**. Also, we do not misuse a non-writable `readonly` property, which can lead to errors when compiling, optimizing, or code review. If not today, then in the near future. That's why I wouldn't use something like:\n\n\n\n```\n\/\/ hack, may cause problems (refactoring is not possible)\n(this as any).readOnlyProperty = raw\n\n\/\/ hack, may cause problems (we write a non-writable)\n(this.readOnlyProperty as boolean) = raw\n\n\/\/ hack, may cause problems (refactoring is not possible)\nObject.assign(this, {readOnlyProperty: raw})\n\n```\n\n"}
{"questionId":"b223d03242114271ad922fff7b76cb0b","question":"ref.current.contains is not a function in React\nI made a dropdown toggle in React. My dropdown working perfectly fine. But when I try to close the dropdown while clicking outside of the dropdown menu. It shows error. I used ref to find the container element.\nSample code\n\n\n\n```\n\nclass Search extends React.Component{\n    constructor()\n    {\n        super()\n        this.state={\n            notificationStatus:false,\n            isFocus:false\n\n\n        }\n        this.container=React.createRef()\n    }\n    toggleNotification=()=>\n    {\n        this.setState({notificationStatus:!this.state.notificationStatus});\n    }\n\n    componentDidMount() {\n        document.addEventListener(\"mousedown\", this.handleClickOutside);\n      }\n      componentWillUnmount() {\n        document.removeEventListener(\"mousedown\", this.handleClickOutside);\n      }\n      handleClickOutside = event => {\n          if(this.container.current)\n          {\n        if (this.container.current && !this.container.current.contains(event.target)) {\n          this.setState({\n            notificationStatus: false,\n          });\n        }\n    }\n      };\n    render()\n    {\n        const {isFocus,notificationStatus}=this.state;\n        return(\n            <div>\n                    <div className=\"col-md-1 col-sm-1 bell-container flex all-center relative\">\n                        <img src={bell} onClick={this.toggleNotification} alt=\"bell icon\" \/>\n                    <\/div>\n                {\n                    notificationStatus ?  <NotificationList ref={this.container} \/> : null\n                    \n                }\n                \n            <\/div>\n\n            \n        )\n    }\n}\n\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"Adding a **ref on NotificationList** component will not give you the reference of the DOM element rendered in it, you need to pass down the ref to the `div` within `NotificationList`\n\n\n\n```\n<NotificationList innerRef={this.container} \/>\n\n```\n\nand in NotificationList\n\n\n\n```\nclass NotificationList extends React.Component {\n   render() {\n      <div ref={this.props.innerRef}>{\/* *\/}<\/div>\n   }\n}\n\n```\n\nP.S. a short solution is to use `ReactDOM.findDOMNode(this.container.current)` but its not longer recommended to use in React\n\n\n"}
{"questionId":"a10f6943edf34b06b8e5ec3baa973dc4","question":"How to use WeakRef in Typescript?\nI want to use WeakRef in Typescript. I tried using the last version available at the moment (4.1.5).\nI have a compilation error:\n\n\n\n```\nconst toIdString = (o: Object): string =>\n  o.constructor.name + JSON.stringify(o);\n\nexport class MemoCache {\n  static handle<T>(o: T): T {\n    const id = toIdString(o);\n    const cached = MemoCache.map.get(id);\n    if (cached) {\n      return cached.deref() as T;\n    }\n    MemoCache.map.set(id, new WeakRef(o));\n    return o;\n  }\n\n  static map = new Map<string, WeakRef>();\n}\n\n```\n\nI have compilation errors.\n\n\n\n```\n\nsrc\/Memoizer.ts:11:31 - error TS2304: Cannot find name 'WeakRef'.\n\n11     MemoCache.map.set(id, new WeakRef(o));\n                                 ~~~~~~~\n\nsrc\/Memoizer.ts:15:32 - error TS2304: Cannot find name 'WeakRef'.\n\n15   static map = new Map<string, WeakRef>();\n                                  ~~~~~~~\n\n\n```\n\nHowever, this is ECMAScript 2021. Chrome 88 seems to understand it.\nI have node 15.8.0 (the last one)\nDo you have any idea how to make Typescript understand WeakRef ?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"On the 18 Feb 2021, in order to use `WeakRef` in Typsecript, you need to have and configure `tsconfig.json` by adding `ESNext` to the `lib` property.\n\n\n\n```\n{\n  \"compilerOptions\": {\n\/\/ ...\n    \"lib\": [\"ESNext\"],\n\/\/ ...\n  },\n\/\/ ...\n}\n\n\n```\n\n"}
{"questionId":"0179af093a0b44cda8f0fe9038667160","question":"How do you inject a service in NestJS into a typeorm repository?\nI have a `UserRepository` which handles creating\/authenticating users infront of the database. I want to perform hashing & validation for the user's password, so I created a seperate service for that purpose, trying to follow single repsonsibility principle, which is declared like this:\n\n\n\n```\n@Injectable()\nexport default class HashService\n\n```\n\nAnd I import it in my module:\n\n\n\n```\n@Module({\n    imports: [TypeOrmModule.forFeature([UserRepository])],\n    controllers: [AuthController],\n    providers: [AuthService, HashService],\n})\nexport class AuthModule {}\n\n```\n\nI wish to inject it into `UserRepository`, I tried passing in it as a constructor parameter but it didn't work because it's base class already accepts 2 parameters there, so I tried injecting my service after them like so:\n\n\n\n```\n@EntityRepository(User)\nexport default class UserRepository extends Repository<User> {\n    constructor(\n        entityManager: EntityManager,\n        entityMetadata: EntityMetadata,\n        @Inject() private readonly hashService: HashService,\n    ) {\n        super();\n    }\n\n    \/\/ Logic...\n}\n\n```\n\nBut `hashService` was undefined, I also tried without the `@Inject()` decorator.\nWhat would be the best way to inject `HashService` into my repository? Do I have to create a new instance of it?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"Short answer: you don't.\n\n\nTypeORM's custom repositories, repository classes, and entities are technically outside of Nest's DI system, so it's not possible to inject any values into them. If you really want to go about it, you can figure out what a `Repository` class requires for it's constructor parameters and add them to a factory to instantiate the repository class and use it directly instead of via the `TypeOrmModule.forFeature`, but that's quite a bit of extra work.\n\n\nIn my opinion, the custom repository pattern doesn't help too much in Nest, as services essentially hold the logic that the `CustomRepository` would. The repository classes are your gateway to the database, but don't need any extra logic added to them.\n\n\n"}
{"questionId":"f7a77b3b67ab43559b0ae69d1c725571","question":"How to add an event listener to useRef in useEffect\nI'm building a custom hook where I want to add an event listener to a ref, but I'm not sure how to clean up properly, since `listRef` and `listRef.current` could be null:\n\n\n\n```\nexport const myHook: MyHook = () => {\n  const listRef = useRef<HTMLDivElement>(null)\n\n  useEffect(() => {\n    \/\/ I can check for the presence of `listRef.current` here ...\n    if (listRef && listRef.current) {\n      listRef.current.addEventListener(...)\n    }\n\n    \/\/ ... but what's the right way for the return function?\n    return listRef.current.removeEventListener(...)\n  })\n\n  return [listRef]\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"# Edit:\n\n\n\n> \n> but I have to check for the presence of `listRef` in the return function too, right?\n> \n> \n> \n\n\nYes, and what you could do is wrap everythinig around the if statement\n\n\n\n```\n  useEffect(() => {\n    \/\/ Everything around if statement\n    if (listRef && listRef.current) {\n      listRef.current.addEventListener(...)\n    \n      return () => {\n        listRef.current.removeEventListener(...)\n      }\n    }\n  }, [listRef])\n\n```\n\nIf you don't call `addEventListener`, you don't need to call `removeEventListener`, so that is why you put everything inside the `if`.\n\n\n\n\n---\n\n\nYou need to pass a function in the return that do what you want to do in the cleanup.\n\n\n\n```\nexport const myHook: MyHook = () => {\n  const listRef = useRef<HTMLDivElement>(null)\n\n  useEffect(() => {\n    \/\/ This is ok\n    if (listRef && listRef.current) {\n      listRef.current.addEventListener(...)\n    }\n\n    \/\/ Passing a function that calls your function\n    return () => {\n        listRef.current.removeEventListener(...)\n    }\n  }, [listRef])\n\n  return [listRef]\n}\n\n```\n\nAnother thing you need to notice is that inside the `fooEventListener`, the `...` should be the same reference of the function, this means:\n\n\nYou shoudn't do this:\n\n\n\n```\n  useEffect(() => {\n    if (listRef && listRef.current) {\n      listRef.current.addEventListener(() => console.log('do something'))\n    }\n\n    return () => {\n        listRef.current.removeEventListener(() => console.log('do something'))\n    }\n  })\n\n```\n\nAnd you should do :\n\n\n\n```\n  useEffect(() => {\n    const myFunction = () => console.log('do something')\n\n    if (listRef && listRef.current) {\n      \/\/ Passing the same reference\n      listRef.current.addEventListener(myFunction)\n    }\n\n    return () => {\n      \/\/ Passing the same reference\n      listRef.current.removeEventListener(myFunction)\n    }\n  }, [listRef])\n\n```\n\n"}
{"questionId":"8ec73f8669cc46b790ba7b812a64bcc2","question":"Is there a pytorch method to check the number of cpus?\nI can use this `torch.cuda.device_count()` to check the number of GPUs. I was wondering if there was something equivalent to check the number of CPUs.\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"python"},"answer":"just use this :\n\n\n\n```\nos.cpu_count()\n\n```\n\n"}
{"questionId":"44a54bcbcf4641d59d71bc776f035df9","question":"How do I create a JSON in PowerShell?\nHow do I create a Json in PowerShell?  \n\nI assumed it would be similar to creating Xmls in PowerShell but it doesn't work:\n\n\n\n```\n[Json]$MyJsonVariable = @\"\n{\n  \"MyList\"{\n    \"Item1\" {\n      \"Name\": \"AMD Ryzen 5 3600x\"\n      \"Type\":\"CPU\"\n      \"Price\":\"`$69.99\"\n      \"Where\":\"Amazon.com\"\n    }\n  }\n}\n\"@\n\n```\n\nBut it didn't work:\n\n\n\n```\nUnable to find type [Json].\nAt line:1 char:1\n+ [Json]$MyJsonVariable = @\"\n+ ~~~~~~\n    + CategoryInfo          : InvalidOperation: (Json:TypeName) [], RuntimeException\n    + FullyQualifiedErrorId : TypeNotFound\n\n```\n\nHelp Please\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"bash\/shell"},"answer":"Like the other answer shows, you can use PowerShell hashtables as JSONs, however, PowerShell does have a way to work with JSONs.\n\n\nFirst of all, even if there was a JSON datatype in Powershell, you would still be getting a syntax error for your JSON. This is what your JSON should look like:\n\n\n\n```\n$MyJsonVariable = @\"\n{\n  \"MyList\":{\n    \"Item1\":{\n      \"Name\":\"AMD Ryzen 5 3600x\",\n      \"Type\":\"CPU\",\n      \"Price\":\"`$69.99\",\n      \"Where\":\"Amazon.com\"\n    }\n  }\n}\n\"@\n\n```\n\nNotice the commas and the colons before the curly braces.\n\n\nOnce we define this variable, we can convert it to an actual JSON with the `ConvertFrom-JSON` cmdlet:\n\n\n\n```\n$MyJsonVariable = $MyJsonVariable | ConvertFrom-JSON\n\n```\n\nWhich would convert it to a JSON:\n\n\n\n```\nPS C:\\> $MyJsonVariable.MyList\n\nItem1\n-----\n@{Name=AMD Ryzen 5 3600x; Type=CPU; Price=$69.99; Where=Amazon.com}\n\n\nPS C:\\> $MyJsonVariable.MyList.Item1\n\nName              Type Price  Where\n----              ---- -----  -----\nAMD Ryzen 5 3600x CPU  $69.99 Amazon.com\n\n\nPS C:\\> $MyJsonVariable.MyList.Item1.Where\nAmazon.com\n\n```\n\nAnd to see the structure of it you can use the `ConvertTo-JSON` cmdlet:\n\n\n\n```\nPS C:\\> $testjson | ConvertTo-Json\n{\n    \"MyList\":  {\n                   \"Item1\":  {\n                                 \"Name\":  \"AMD Ryzen 5 3600x\",\n                                 \"Type\":  \"CPU\",\n                                 \"Price\":  \"$69.99\",\n                                 \"Where\":  \"Amazon.com\"\n                             }\n               }\n}\n\n```\n\n"}
{"questionId":"306db7d4e6db49bc8c4ea099c9662218","question":"Cocoapods - framework not found FirebaseCoreDiagnostics\nI recently updated to the most recent version of firebase so that I could integrate apple sign in for my ios app (obj-c) and I can't for the life of me work out why I'm getting this error:\n\n\nld: framework not found FirebaseCoreDiagnostics\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\n\n\nHere's my podfile:\n\n\n\n```\n    platform :ios, '13.7'\n    \n    pod 'Firebase\/Database'\n    pod 'Firebase\/Auth'\n    pod 'Firebase\/AdMob'\n    pod 'Firebase\/Messaging'\n    pod 'Firebase\/Analytics'\n    pod 'Google-Mobile-Ads-SDK'\n    pod 'SDWebImage', '~> 5.0'\n    pod 'FBSDKCoreKit', '~> 5.2'\n    pod 'FBSDKLoginKit', '~> 5.2'\n    pod 'FBSDKPlacesKit', '~> 5.2'\n    pod 'FBSDKShareKit', '~> 5.2'\n    pod 'FBSDKMarketingKit'\n    \n    target 'WriteAnythingPrototype' do\n    \n    end\n\n```\n\nI've checked my Pods and can see that FirebaseCoreAnalytics is there. Also if I try and manually add the framework into the frameworks folder, I get the error that the framework is duplicated.\n\n\nI've also tried deleting my derived data and deleting my pod folder and then reinstalling the pods.\n\n\nI'm at my wits end and can't seem to find this problem online, can anyone please explain why this is happening?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"objective-c"},"answer":"Something about the Xcode project is corrupt. Try the following:\n\n\n- `pod deintegrate`\n- Examine the resulting xcproject source and remove any FirebaseCoreDiagnostic references.\n- Rerun `pod install`\n\n\n"}
{"questionId":"ef626b33163e4d41a214acab9188bf02","question":"How to use Automapper with Autofac\nI've upgraded to the latest version of AutoMapper (9.0) and I've changed the static configuration to:\n\n\n\n```\npublic static IMapper RegisterAutoMapper()\n{\n    var config = new MapperConfiguration(cfg =>\n    {\n        cfg.CreateMap<MyModel MyDto>;\n        \/\/etc...\n   });\n  \n   var mapper = config.CreateMapper();\n   return mapper;\n}\n\n```\n\nUsing the previous static API I used to do the following in `Global.asax`:\n\n\n\n```\nprotected void Application_Start()\n{\n    GlobalConfiguration.Configure(WebApiConfig.Register);\n    AutoMapping.Map();\n}\n\n```\n\n`WebApiConfig.Register` registers the routes and also `Autofac`\n\n\nHow do I register AutoMapper with Autofac because currently I'm getting compiler errors on such lines:\n\n\n\n```\nvar myDto = Mapper.Map<MyModel>(model);\n\n```\n\nAnd the compiler error:\n\n\n\n> \n> An object reference is required for the non-static field, method, or property 'Mapper.Map(object)'\n> \n> \n> \n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"Here's one I made earlier:\n\n\n\n```\npublic class YourAutofacModule : Module\n{\n    protected override void Load(ContainerBuilder builder)\n    {\n        \/\/Also register any custom type converter\/value resolvers\n        builder.RegisterType<CustomValueResolver>().AsSelf();\n        builder.RegisterType<CustomTypeConverter>().AsSelf();\n\n        builder.Register(context => new MapperConfiguration(cfg =>\n        {\n            cfg.CreateMap<MyModel MyDto>;\n            \/\/etc...\n        })).AsSelf().SingleInstance();\n\n        builder.Register(c =>\n        {\n            \/\/This resolves a new context that can be used later.\n            var context = c.Resolve<IComponentContext>();\n            var config = context.Resolve<MapperConfiguration>();\n            return config.CreateMapper(context.Resolve);\n        })\n        .As<IMapper>()\n        .InstancePerLifetimeScope();\n    }\n}\n\n```\n\nIn the global.asax.cs\n\n\n\n```\npublic class MvcApplication : System.Web.HttpApplication\n{\n    protected void Application_Start()\n    {\n        var builder = new ContainerBuilder();\n\n        builder.RegisterModule<MyAutofacModule>();\n        \/\/ Register anything else needed\n\n        var container = builder.Build();\n\n        \/\/ MVC resolver\n        DependencyResolver.SetResolver(new AutofacDependencyResolver(container));\n\n        \/\/ API Resolver\n        GlobalConfiguration.Configuration.DependencyResolver = new AutofacWebApiDependencyResolver(container);\n    }\n}\n\n```\n\nThen all you need to do is inject IMapper\n\n\n"}
{"questionId":"c6cdbfe995344124baff81d9728b8fdc","question":"How can I make all tmux panes have their own unique shell history?\nPreface: My current tmux configuration has multiple panes in multiple windows in multiple sessions.\n\n\nThe problem that keeps arising is that when I work in one window, all my history is good and separate between that window's panes, and when I swap windows\/sessions, that history is initially separate too, but only until I enter one command in a different window\/session.\n\n\nOnce that happens, all the history for all panes gets merged together and it's sometimes impossible to find a certain pane's actual last command, depending on how long it had been since I was on the pane.\n\n\nIs there any way to avoid this and have each pane have its own shell history?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"If you're using the `bash` shell, your command history is written to a file defined by the `HISTFILE` variable, which defaults to `~\/.bash_history`. Inside a `tmux` pane, you have access to the variable `$TMUX_PANE` which looks something like this:\n\n\n\n```\n$ echo $TMUX_PANE\n%3\n\n```\n\nYou could use this to create a per-pane history by adding something like this to your `~\/.bashrc` file:\n\n\n\n```\nif [[ $TMUX_PANE ]]; then\n  HISTFILE=$HOME\/.bash_history_tmux_${TMUX_PANE:1}\nfi\n\n```\n\nThis will store the history for pane 2, for example, in `~\/.bash_history_tmux_2`.\n\n\nThe downside to this idea is that you're going to end up with a bunch of `.bash_history_tmux_*` files in your home directory.\n\n\n"}
{"questionId":"5f354a2fc1aa41d893fa3df9402331f6","question":"Python: TypeError: Invalid comparison between dtype=datetime64[ns] and date\nFor a current project, I am planning to filter a JSON file by timeranges by running several loops, each time with a slightly shifted range. The code below however yields the error `TypeError: Invalid comparison between dtype=datetime64[ns] and date` for line `after_start_date = df[\"Date\"] >= start_date`.\n\n\nI have already tried to modify the formatting of the dates both within the Python code as well as the corresponding JSON file. Is there any smart tweak to align the date types\/formats?\n\n\nThe JSON file has the following format:\n\n\n\n```\n[\n{\"No\":\"121\",\"Stock Symbol\":\"A\",\"Date\":\"05\/11\/2017\",\"Text Main\":\"Sample text\"}\n]\n\n```\n\nAnd the corresponding code looks like this:\n\n\n\n```\nimport string\nimport json\n\nimport pandas as pd\nimport datetime\nfrom dateutil.relativedelta import *\n\n\n# Loading and reading dataset\nfile = open(\"Glassdoor_A.json\", \"r\")\ndata = json.load(file)\ndf = pd.json_normalize(data)\ndf['Date'] = pd.to_datetime(df['Date'])\n\n\n# Create an empty dictionary\nd = dict()\n\n# Filtering by date\n\nstart_date = datetime.date.fromisoformat('2017-01-01')\nend_date = datetime.date.fromisoformat('2017-01-31')\n\nfor i in df.iterrows():\n    start_date += relativedelta(months=+3)\n    end_date += relativedelta(months=+3)\n\n    print(start_date)\n    print(end_date)\n\n    after_start_date = df[\"Date\"] >= start_date\n    before_end_date = df[\"Date\"] <= end_date\n\n    between_two_dates = after_start_date & before_end_date\n    filtered_dates = df.loc[between_two_dates]\n\n    print(filtered_dates)\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"You can use `pd.to_datetime('2017-01-31')` instead of `datetime.date.fromisoformat('2017-01-31')`.\n\n\nI hope this helps!\n\n\n"}
{"questionId":"5bb7b8067b994845886dda5a9aed51f9","question":"How to prevent error : this old-style function\nI am following a tutorial and my code seems normal but I got a message which says\n\n\n\n```\nThis old-style function definition is not preceded by a prototype\n\n```\n\n**code.c :**\n\n\n\n```\nvoid viderBuffer()\n{\n    int c = 0;\n    while (c != '\\n' && c != EOF)\n    {\n        c = getchar();\n    }\n}\n\n```\n\nThanks you for your help. Sorry if my post is not perfect I am new here.\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"c"},"answer":"Declare the function before main (or before referencing it in main) like\n\n\n\n```\nvoid viderBuffer( void );\n\n```\n\nAnd define it also like\n\n\n\n```\nvoid viderBuffer( void )\n{\n    \/\/...\n}\n\n```\n\n"}
{"questionId":"15d44b695c8e45d298097577be9d9958","question":"Bar chart in matplotlib using a colormap\nI have a df with two columns:\n\n\n- y: different numeric values for the y axis\n- days: the names of four different days (Monday, Tuesday, Wednesday, Thursday)\n\n\nI also have a colormap with four different colors that I made myself and it's a ListedColorMap object.\n\n\nI want to create a bar chart with the four categories (days of the week) in the x axis and their corresponding values in the y axis. At the same time, I want each bar to have a different color using my colormap.\n\n\nThis is the code I used to build my bar chart:\n\n\n\n```\ndef my_barchart(my_df, my_cmap):\n  fig = plt.figure()\n  ax = fig.add_axes([0,0,1,1])\n  ax.bar(my_df['days'], my_df['y'], color=my_cmap)\n  return fig\n\n```\n\nHowever, I get the following error: \"object of type 'ListedColormap' has no len()\", so it seems that I'm not using my\\_cmap correctly.\n\n\nIf I remove that from the function and run it, my bar chart looks ok, except that all bars have the same color. So my question is: what is the right way to use a colormap with a bar chart?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"Okay, I found a way to do this without having to scale my values:\n\n\n\n```\ndef my_barchart(my_df, my_cmap):\n  fig = plt.figure()\n  ax = fig.add_axes([0,0,1,1])\n  ax.bar(my_df['days'], my_df['y'], color=my_cmap.colors)\n  return fig\n\n```\n\nSimply adding `.colors` after `my_cmap` works!\n\n\n"}
{"questionId":"edfea9fcec69497790c7a874945e3e63","question":"ValueError: The number of FixedLocator locations (5), usually from a call to set\\_ticks, does not match the number of ticklabels (12)\nthis piece of code was working before, however, after creating a new environment , it stopped working for the line\n\n\n\n```\nplt.xticks(x, months, rotation=25,fontsize=8)\n\n```\n\nif i comment this line then no error, after putting this line error is thrown\n\n\n\n```\nValueError: The number of FixedLocator locations (5), usually from a call to set_ticks, does not match the number of ticklabels (12).\n\n```\n\n\n\n---\n\n\n\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = df\ndfsize = dataset[df.columns[0]].size\nx = []\nfor i in range(dfsize):\n    x.append(i)\n\ndataset.shape\n# dataset.dropna(inplace=True)\ndataset.columns.values\nvar = \"\"\nfor i in range(dataset.shape[1]):  ## 1 is for column, dataset.shape[1] calculate length of col\n\n    y = dataset[dataset.columns[i]].values\n    y = y.astype(float)\n    y = y.reshape(-1, 1)\n    y.shape\n    from sklearn.impute import SimpleImputer\n\n    missingvalues = SimpleImputer(missing_values=np.nan, strategy='mean', verbose=0)\n    missingvalues = missingvalues.fit(y)\n    y = missingvalues.transform(y[:, :])\n\n    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n    from sklearn.compose import ColumnTransformer\n\n    labelencoder_x = LabelEncoder()\n    x = labelencoder_x.fit_transform(x)\n\n    from scipy.interpolate import *\n\n    p1 = np.polyfit(x, y, 1)\n    # from matplotlib.pyplot import *\n    import matplotlib\n    matplotlib.use('Agg')\n    import matplotlib.pyplot as plt\n\n    plt.figure()\n    plt.xticks(x, months, rotation=25,fontsize=8)\n    #print(\"-->\"+dataset.columns[i])\n    plt.suptitle(dataset.columns[i] + ' (xyz)', fontsize=10)\n    plt.xlabel('month', fontsize=8)\n    plt.ylabel('Age', fontsize=10)\n    plt.plot(x, y, y, 'r-', linestyle='-', marker='o')\n    plt.plot(x, np.polyval(p1, x), 'b-')\n    y = y.round(decimals=2)\n    for a, b in zip(x, y):\n        plt.text(a, b, str(b), bbox=dict(facecolor='yellow', alpha=0.9))\n\n    plt.grid()\n    # plt.pause(2)\n    # plt.grid()\n\n    var = var + \",\" + dataset.columns[i]\n    plt.savefig(path3 + dataset.columns[i] + '_1.png')\n    plt.close(path3 + dataset.columns[i] + '_1.png')\n    plt.close('all')\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"I also stumbled across the error and found that making both your xtick\\_labels and xticks a list of equal length works. So in your case something like :\n\n\n\n```\ndef month(num):\n  # returns month name based on month number\n\n\nnum_elements = len(x)\nX_Tick_List = []\nX_Tick_Label_List=[]\n\nfor item in range (0,num_elements):\n    X_Tick_List.append(x[item])\n    X_Tick_Label_List.append(month(item+1))\n\nplt.xticks(ticks=X_Tick_List,labels=X_Tick_LabeL_List, rotation=25,fontsize=8)\n       \n\n```\n\n"}
{"questionId":"a1f724ae9d75414ea36573d6d3558ddb","question":".Net Core Middleware - Getting Form Data from Request\nIn a .NET Core Web Application I am using middleware (app.UseMyMiddleware) to add some logging on each request:\n\n\n\n```\n        public void Configure(IApplicationBuilder app, IHostingEnvironment env)\n        {\n            if (env.IsDevelopment())\n            {\n                app.UseDeveloperExceptionPage();\n            }\n            else\n            {\n                app.UseExceptionHandler(MyMiddleware.GenericExceptionHandler);\n                app.UseHsts();\n            }\n\n            app.UseHttpsRedirection();\n            app.UseStaticFiles();\n            app.UseMyMiddleware();\n\n            app.UseMvc(routes =>\n            {\n                routes.MapRoute(\n                    name: \"default\",\n                    template: \"{controller=Home}\/{action=Index}\/{id?}\");\n            });\n        }\n\n```\n\n\n```\n        public static void UseMyMiddleware(this IApplicationBuilder app)\n        {\n            app.Use(async (context, next) =>\n            {\n                await Task.Run(() => HitDetails.StoreHitDetails(context));\n                await next.Invoke();\n            });\n        }\n\n```\n\n\n```\n        public static void StoreHitDetails(HttpContext context)\n        {\n            var config = (IConfiguration)context.RequestServices.GetService(typeof(IConfiguration));\n            var settings = new Settings(config);\n            var connectionString = config.GetConnectionString(\"Common\");\n            var features = context.Features.Get<IHttpRequestFeature>();\n            var url = $\"{features.Scheme}:\/\/{context.Request.Host.Value}{features.RawTarget}\";\n\n            var parameters = new\n            {\n                SYSTEM_CODE = settings.SystemName,\n                REMOTE_HOST = context.Connection.RemoteIpAddress.ToString(),\n                HTTP_REFERER = context.Request.Headers[\"Referer\"].ToString(),\n                HTTP_URL = url,\n                LOCAL_ADDR = context.Connection.LocalIpAddress.ToString(),\n                AUTH_USER = context.User.Identity.Name\n            };\n\n            using (IDbConnection db = new SqlConnection(connectionString))\n            {\n                db.Query(\"StoreHitDetails\", parameters, commandType: CommandType.StoredProcedure);\n            }\n        }\n\n```\n\nThis all works fine and I can grab most of what I need from the request but what I need next is the Form Data on a POST method.\n\n\ncontext.Request.Form is an available option but when debugging I hover over it and see \"The function evaluation requires all thread to run\". If I try to use it the application just hangs.\n\n\nWhat do I need to do to access Request.Form or is there an alternative property with POST data that I'm not seeing?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"You can create a separate middleware rather than an inline one and then call the `HitDetails.StoreHitDetails` from there.\n\n\n\n```\npublic class MyMiddleware\n{\n    private readonly RequestDelegate _next;\n\n    public MyMiddleware(RequestDelegate next)\n    {\n        _next = next;\n    }\n\n    public async Task Invoke(HttpContext context)\n    {\n        HitDetails.StoreHitDetails(context);\n\n        await _next(context);\n    }\n}\n\n\/\/ Extension method used to add the middleware to the HTTP request pipeline.\npublic static class MiddlewareExtensions\n{\n    public static IApplicationBuilder UseMyMiddleware(this IApplicationBuilder builder)\n    {\n        return builder.UseMiddleware<MyMiddleware>();\n    }\n}\n\n```\n\nThat way you can continue using `app.UseMyMiddleware();` and you don't have to run it using `Task.Run` as you mentioned.\n\n\nOr you can just try calling `HitDetails.StoreHitDetails(context)` without wrapping it in `Task.Run`\n\n\n**Edited**\n\n\nCheck if your `Request` has a correct content type:\n\n\n\n```\nif (context.Request.HasFormContentType)\n{\n    IFormCollection form;\n    form = context.Request.Form; \/\/ sync\n    \/\/ Or\n    form = await context.Request.ReadFormAsync(); \/\/ async\n\n    string param1 = form[\"param1\"];\n    string param2 = form[\"param2\"];\n }\n\n```\n\n"}
{"questionId":"60b6417e7d544d31a6f19d7c11cc986b","question":"Why is the return type for ftell not fpos\\_t?\nAccording to C99, the prototype for `ftell` is:\n\n\n\n```\nlong int ftell(FILE *stream);\n\n```\n\nFrom what I undersood it should be the following instead:\n\n\n\n```\nfpos_t ftell(FILE *stream);\n\n```\n\nWhy is that?\n\n\nFrom \u00a77.19.1-2\n\n\n\n> \n> `fpos_t` which is an object type other than an array type capable of recording all the information needed to specify uniquely every position within a file.\n> \n> \n> \n\n\nI understand that `fpos_t` should be used to record a position within a file. So `ftell` which returns a position within a file should be of that type. Instead it is: \n\n\n- `signed`\n- of type `long` which can be too small or too big to access a file on certain architectures.\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"Notice that `fpos_t` is \n\n\n\n> \n> [...] a complete object type other than an array type capable of recording all the information needed to specify uniquely every position within a file.\n> \n> \n> \n\n\nSo it can can be even a structure, totally unusable for anything else besides calling `fsetpos`! \n\n\nOn the other hand the return value of `ftell` is a scalar which is guaranteed to be possible to use in telling the exact byte position in a binary file:\n\n\n\n> \n> For a binary stream, the value is the number of characters from the beginning of the file.\n> \n> \n> \n\n\n\n\n---\n\n\nOther than that, the reason is *backwards-compatibility*. `ftell` debuted in C89, and perhaps then the expectation was that `long` would scale fast enough to contain all file sizes, something that is not always true nowadays. Unfortunately it is not possible to change the type returned by `ftell` but it is too late to change that now - even those platforms that support larger files now have functions with another name, such as `ftello`.\n\n\n\n\n---\n\n\nthe signedness is required because the function returns `-1` on error.\n\n\n"}
{"questionId":"c71c4a7aba844103950ddb33e1337cb9","question":"Is the initialization order of the vector elements guaranteed by the standard?\nTo fill a `std::vector<struct_name>` from `std::cin`, I usually write as following code:\n\n\n\n```\nstruct point{\n    int x, y;\n};\n\nint main()\n{\n   std::size_t n;\n   std::cin>>n; \n   std::vector<point> vec(n);\n    for (auto& p:vec) \n        std::cin>>p.x>>p.y;\n   \/\/...\n}\n\n```\n\nBut today I found out another way to do it with default constructor:\n\n\n\n```\nstruct point{\nint x, y;\n   point(){\n      std::cin>>x>>y;\n   }\n};\n\nint main()\n{\n    std::size_t n;\n    std::cin>>n; \n    std::vector<point> vec(n);\n    \/\/...\n}\n\n```\n\n**Questions**:\n\n\n1. Is the initialization order of the vector elements guaranteed by the standard(0,1,2,n-1...)?\n2. (If answer of previous question is true) Is really the second variant has twice effective?\n\n\nI am interested in behavior according to the C++11(and newer) standard\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c++"},"answer":"It is not guaranteed that the elements are initialized in the order of their indices. In C++11, see [vector.cons]\/3:\n\n\n\n> \n> *Effects:* Constructs a `vector` with `n` value-initialized elements.\n> \n> \n> \n\n\nThis says nothing about the ordering, so nothing can be assumed. The wording changes in later editions of the standard, but no ordering ever seems to have been imposed.\n\n\n"}
{"questionId":"1c01fc8be55f4caa84bda2be2ee18cb9","question":"The injection point has the following annotations: - @org.springframework.beans.factory.annotation.Autowired(required=true)\nI am new to Spring Boot and I'm getting the following error when writing a file upload API: \n\n\n\n```\nError:Description:\nField fileStorageService in com.primesolutions.fileupload.controller.FileController required a bean of type 'com.primesolutions.fileupload.service.FileStorageService' that could not be found.\nThe injection point has the following annotations:\n    - @org.springframework.beans.factory.annotation.Autowired(required=true)\nAction:\nConsider defining a bean of type 'com.primesolutions.fileupload.service.FileStorageService' in your configuration.*\n\n```\n\nController class:\n\n\n\n```\npublic class FileController \n{\n    private static final Logger logger = LoggerFactory.getLogger(FileController.class);\n\n    @Autowired\n    private FileStorageService fileStorageService;\n\n    @PostMapping(\"\/uploadFile\")\n    public UploadFileResponse uploadFile(@RequestParam(\"file\") MultipartFile file) {\n        String fileName = fileStorageService.storeFile(file);\n\n        String fileDownloadUri = ServletUriComponentsBuilder.fromCurrentContextPath()\n                .path(\"\/downloadFile\/\")\n                .path(fileName)\n                .toUriString();\n\n        return new UploadFileResponse(fileName, fileDownloadUri,\n                file.getContentType(), file.getSize());\n    }\n\n    @PostMapping(\"\/uploadMultipleFiles\")\n    public List<UploadFileResponse> uploadMultipleFiles(@RequestParam(\"files\") MultipartFile[] files) {\n        return Arrays.asList(files)\n                .stream()\n                .map(file -> uploadFile(file))\n                .collect(Collectors.toList());\n    }\n}\n\n```\n\nService class:\n\n\n\n```\nprivate final Path fileStorageLocation;\n\n\n    @Autowired\n    public FileStorageService(FileStorageProperties fileStorageProperties) {\n        this.fileStorageLocation = Paths.get(fileStorageProperties.getUploadDir())\n                .toAbsolutePath().normalize();\n\n        try {\n            Files.createDirectories(this.fileStorageLocation);\n        } catch (Exception ex) {\n            throw new FileStorageException(\"Could not create the directory where the uploaded files will be stored.\", ex);\n        }\n    }\n\n    public String storeFile(MultipartFile file) {\n        \/\/ Normalize file name\n        String fileName = StringUtils.cleanPath(file.getOriginalFilename());\n\n        try {\n            \/\/ Check if the file's name contains invalid characters\n            if(fileName.contains(\"..\")) {\n                throw new FileStorageException(\"Sorry! Filename contains invalid path sequence \" + fileName);\n            }\n\n            \/\/ Copy file to the target location (Replacing existing file with the same name)\n            Path targetLocation = this.fileStorageLocation.resolve(fileName);\n            Files.copy(file.getInputStream(), targetLocation, StandardCopyOption.REPLACE_EXISTING);\n\n            return fileName;\n        } catch (IOException ex) {\n            throw new FileStorageException(\"Could not store file \" + fileName + \". Please try again!\", ex);\n        }\n    }\n\n```\n\nConfiguration class:\n\n\n\n```\n@ConfigurationProperties(prefix = \"file\")\npublic class FileStorageProperties {\n\n    private String uploadDir;\n\n    public String getUploadDir()\n    {\n        return uploadDir;\n    }\n\n    public void setUploadDir(String uploadDir) {\n        this.uploadDir = uploadDir;\n    }\n}\n\n```\n\nMain:\n\n\n\n```\n@SpringBootApplication\n@EnableConfigurationProperties({\n        FileStorageProperties.class\n})\npublic class FileApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(FileApplication.class, args);\n    }\n}\n\n```\n\nproperties file\n\n\n\n```\n## MULTIPART (MultipartProperties)\n# Enable multipart uploads\nspring.servlet.multipart.enabled=true\n# Threshold after which files are written to disk.\nspring.servlet.multipart.file-size-threshold=2KB\n# Max file size.\nspring.servlet.multipart.max-file-size=200MB\n# Max Request Size\nspring.servlet.multipart.max-request-size=215MB\n\n## File Storage Properties\n# All files uploaded through the REST API will be stored in this directory\nfile.upload-dir=C:\/Projects\/SpringBootProject\/Primesolutions\/PrimeSolutions\/FileUpload\n\n```\n\nI'm trying to read the file upload property and pass it to the controller class.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"The error seems to indicate that Spring does not know any bean of type `com.primesolutions.fileupload.service.FileStorageService`.\n\n\nAs said in the comment, make sure you class `FileStorageService`is annotated by `@Service` or `@Component`:\n\n\n\n```\n@Service\npublic class FileStorageService {\n...\n}\n\n```\n\nMake also sure that this class is located in a sub-package of your class `FileApplication`. For example, if your `FileApplication` class is located in a package `com.my.package`, make sure your `FileStorageService` is located in the package com.my.package.\\*\\* (same package or any sub package).\n\n\nFew notes to improve your code by the way :\n\n\n- When your class has only one not default constructor, the use of `@Autowired` on the constructor is optional.\n- Do not put too much code in your constructor. Use instead the `@PostConstruct` annotation.\n\n\n\n```\n\n    @Service\n    public class FileStorageService {\n        private FileStorageProperties props;\n        \/\/ @Autowired is optional in this case\n        public FileStorageService (FileStorageProperties fileStorageProperties) {\n            this.props = fileStorageProperties;\n            this.fileStorageLocation = Paths.get(fileStorageProperties.getUploadDir())\n                    .toAbsolutePath().normalize();\n        }\n\n        @PostConstruct\n        public void init() {\n            try {\n                Files.createDirectories(this.fileStorageLocation);\n            } catch (Exception ex) {\n                throw new FileStorageException(\"Could not create the directory where the uploaded files will be stored.\", ex);\n            }\n        }\n    }\n\n\n```\n\n- It is better to avoid the `@Autowired` on a field. Use the constructor instead. It is better for your tests, and more maintainable:\n\n\n\n```\npublic class FileController {\n    private FileStorageService service;\n\n    public FileController(FileStorageService service) {\n        this.service = service;\n    }\n}\n\n```\n\n"}
{"questionId":"71fd446f755b46fbb6e631e295f2238b","question":"Find symmetric pairs quickly in numpy\n\n```\nfrom itertools import product\nimport pandas as pd\n\ndf = pd.DataFrame.from_records(product(range(10), range(10)))\ndf = df.sample(90)\ndf.columns = \"c1 c2\".split()\ndf = df.sort_values(df.columns.tolist()).reset_index(drop=True)\n#     c1  c2\n# 0    0   0\n# 1    0   1\n# 2    0   2\n# 3    0   3\n# 4    0   4\n# ..  ..  ..\n# 85   9   4\n# 86   9   5\n# 87   9   7\n# 88   9   8\n# 89   9   9\n# \n# [90 rows x 2 columns]\n\n```\n\nHow do I quickly find, identify, and remove the last duplicate of all symmetric pairs in this data frame?\n\n\nAn example of symmetric pair is that '(0, 1)' is equal to '(1, 0)'. The latter should be removed. \n\n\nThe algorithm must be fast, so it is recommended to use numpy. Converting to python object is not allowed.\n\n\n","questionMetadata":{"type":"optimization","level":"advanced","tag":"python"},"answer":"You can sort the values, then `groupby`:\n\n\n\n```\na= np.sort(df.to_numpy(), axis=1)\ndf.groupby([a[:,0], a[:,1]], as_index=False, sort=False).first()\n\n```\n\n**Option 2**: If you have a lot of pairs `c1, c2`, `groupby` can be slow. In that case, we can assign new values and filter by `drop_duplicates`:\n\n\n\n```\na= np.sort(df.to_numpy(), axis=1) \n\n(df.assign(one=a[:,0], two=a[:,1])   # one and two can be changed\n   .drop_duplicates(['one','two'])   # taken from above\n   .reindex(df.columns, axis=1)\n)\n\n```\n\n"}
{"questionId":"c06344fc58564a81be4c2a2cddbf03a0","question":"Direnv not loading environment when shell starts in a directory with .envrc\nImagine I am in a shell with the working directory set to a directory that contains an `.envrc` file. When I now open up a new tab in my terminal emulator, a new shell is launched into the same working directory as the original shell.\n\n\nFor some reason, the `.envrc` file is not loaded in this new shell. If manually exit the working directory and enter it again, the environment variables are then loaded.\n\n\nIs there a way to make the variables load automatically when the shell is created?\n\n\nI am running MacOS Mojave (10.14.4) with direnv 2.20.1, using bash as my shell. I am loading the direnv hooks through my `.bashrc`.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"Under macOS Mojave (and probably older versions), saving a `.bashrc` has no effect. Instead, use a `.bash_profile`.\n\n\n"}
{"questionId":"0738c36e4c44483da0a9e171b9974885","question":"Are the keys of a kwargs argument to Python function guaranteed to be type string?\n\n```\ndef func(**kwargs):\n   for key, value in kwargs.items():\n      # Is key always going to be a string?\n      # Could it have spaces?\n      pass\n\n```\n\nTwo questions about kwargs in Python.\n\n\n1. Is every key of kwargs guaranteed to be of type str? If so, I can avoid type checking.\n2. If #1 is true, would every key be guaranteed to not have spaces?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"python"},"answer":"A keyword argument passed directly must be a valid Python identifier, and yes it will always be treated as strings. Anything else is a `SyntaxError`.\n\n\n\n```\nf(foo=1) # Works\nf($=1) # Fails\nf(1=1) # Fails\n\n```\n\nAlthough, you can also give keyword arguments through unpacking. In this case, your keyword arguments must be strings still, but they can take any format.\n\n\nLet's define a dummy function to test this.\n\n\n\n```\ndef f(**kwargs):\n    print(kwargs)\n\n```\n\nA keyword argument can contain a space or be a string of digits. It can even contain special characters.\n\n\n\n```\nf(**{\"hello world\": 'foo'}) # prints {'hello world': 'foo'}\nf(**{\"1\": 'foo'}) # prints {'1': 'foo'}\nf(**{\"$\": 'foo'}) # prints {'$': 'foo'}\n\n```\n\nA keyword argument must be a string. Anything else is a `TypeError`.\n\n\n\n```\nf(**{1: 'foo'}) # TypeError: f() keywords must be strings\nf(**{b'foo': 1}) # TypeError: f() keywords must be strings\n\n```\n\n"}
{"questionId":"1459ece8ffa9432bad8fa47fe0fdace9","question":"Cannot start service web: OCI runtime create failed:\ncan I ask some help please when I execute the command `docker-compose up -d`\n\n\nI get error can you help me how can I execute php using nginx\n\n\n\n```\nRemoving nginx-container\nmysql-container is up-to-date\nphp-container is up-to-date\nRecreating 2e6f7b9915c6_nginx-container ... error\n\nERROR: for 2e6f7b9915c6_nginx-container  Cannot start service web: OCI runtime create failed: container_linux.go:345: starting container process caused \"process_linux.go:430: container init caused \\\"rootfs_linux.go:58: mounting \\\\\\\"\/host_mnt\/c\/webdock\/firstweb\/default.conf\\\\\\\" to rootfs \\\\\\\"\/var\/lib\/docker\/overlay2\/8261a085184069473ca52f3ad508386e84f0636baafbbc754e1447ea72427433\/merged\\\\\\\" at \\\\\\\"\/var\/lib\/docker\/overlay2\/8261a085184069473ca52f3ad508386e84f0636baafbbc754e1447ea72427433\/merged\/etc\/nginx\/conf.d\\\\\\\" caused \\\\\\\"not a directory\\\\\\\"\\\"\": unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type\n\nERROR: for web  Cannot start service web: OCI runtime create failed: container_linux.go:345: starting container process caused \"process_linux.go:430: container init caused \\\"rootfs_linux.go:58: mounting \\\\\\\"\/host_mnt\/c\/webdock\/firstweb\/default.conf\\\\\\\" to rootfs \\\\\\\"\/var\/lib\/docker\/overlay2\/8261a085184069473ca52f3ad508386e84f0636baafbbc754e1447ea72427433\/merged\\\\\\\" at \\\\\\\"\/var\/lib\/docker\/overlay2\/8261a085184069473ca52f3ad508386e84f0636baafbbc754e1447ea72427433\/merged\/etc\/nginx\/conf.d\\\\\\\" caused \\\\\\\"not a directory\\\\\\\"\\\"\": unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type\nERROR: Encountered errors while bringing up the project.\n\n```\n\nHere is my docker-compose\n\n\n\n```\nversion: \"3.7\"\nservices:\n\n  web:\n    image: nginx:latest\n    container_name: nginx-container\n    ports:\n      - \"8080:80\"\n    volumes:\n      - .\/:\/var\/www\/firstweb\n      - .\/default.conf:\/etc\/nginx\/conf.d\/\n    links:\n      - php\n\n  php:\n    image: php:7-fpm\n    container_name: php-container\n  db:\n    image: mysql\n    container_name: mysql-container\n    command: --default-authentication-plugin=mysql_native_password\n    volumes:\n      - .\/mysql-data:\/var\/lib\/mysql\n    expose:\n      - 3306\n    ports:\n      - \"3306:3306\"\n    environment:\n      MYSQL_ROOT_PASSWORD: rootpass\n\n```\n\nand my default.conf\n\n\n\n```\nserver {\n        listen  80;\n         index index.php;\n         server_name app.dev;\n         error_log  \/var\/log\/nginx\/error.log;\n         access_log \/var\/log\/nginx\/access.log;\n         root \/var\/www\/firstweb;\n\n         location ~ \\.php$ {\n                 try_files $uri =404;\n                 fastcgi_split_path_info ^(.+\\.php)(\/.+)$;\n                 fastcgi_index index.php;\n                 fastcgi_pass php:9000;\n                 include fastcgi_params;\n                 fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\n                 fastcgi_param PATH_INFO $fastcgi_path_info;\n             }\n     }\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"change this `- .\/default.conf:\/etc\/nginx\/conf.d\/` to :\n\n\n\n```\n- .\/default.conf:\/etc\/nginx\/conf.d\/default.conf\n\n```\n\nyou can see in the error that `Docker` trying to mount a file `default.conf` to a folder `conf.d`\n\n\n"}
{"questionId":"3d3e3a2c43de484b99a4c4912f5198aa","question":"TypeScript: get type of instance method of class\nI'm trying to get the type of an instance method of a class. Is there a built-in (better) way other than looking up the type in the prototype of the class?\n\n\n\n```\nclass MyClass\n{\n    private delegate: typeof MyClass.prototype.myMethod; \/\/ gets the type ( boolean ) => number;\n\n    public myMethod( arg: boolean )\n    {\n        return 3.14;\n    }\n}\n\n```\n\nThanks in advance!\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"you can use the following type:\n\n\n\n```\ntype TypeOfClassMethod<T, M extends keyof T> = T[M] extends Function ? T[M] : never;\n\n```\n\nWith that, you can write the following:\n\n\n\n```\nclass MyClass\n{\n  private delegate: TypeOfClassMethod<MyClass, 'myMethod'>; \/\/ gets the type (boolean) => number;\n\n  public myMethod( arg: boolean )\n  {\n    return 3.14;\n  }\n}\n\n```\n\n"}
{"questionId":"0ecf0a639f124ed6bb23e2b4b267ab43","question":"Using Mocha Test Explorer with VScode when not in default `\/test' folder\nI am currently working on a project where the test files are stored in `\/code_tests` instead of the default `\/test`. I am trying to avoid having to rename the folder but I can't seem to change the default location for the Mocha Test Explorer.\n\n\nThe project is using Typescript on vscode.\n\n\nThe documentation provides this line of code. `\"mochaExplorer.files\": \"test\/**\/*.ts\"` which I assume would need to be changed to `\"mochaExplorer.files\": \"code_tests\/**\/*.ts\"` but I can't seem to find where to store this command for it to work as intended.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"add these to vscode `settings.json`\n\n\n\n```\n\"mochaExplorer.require\": \"ts-node\/register\",\n\"mochaExplorer.files\": \"code_tests\/**\/*.ts\",\n\n```\n\n"}
{"questionId":"f147daf60da04dfaab302efa1dd950a4","question":"How to debug Rust in Intellij idea?\nI have searched for rust debugging and found guides for vs code only. My question is how to configure intellij rust for debugging?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"rust"},"answer":"You cannot debug Rust in IntelliJ, but CLion supports debugging. However CLion is not free.\n\n\n"}
{"questionId":"79a56d6dec284d7194439ad66cb04546","question":"How to suppress Possible Null Reference warnings\nI am playing with the nullable types in c# 8 and I found a problem that is bugging me.\nSuppose I have a method which takes a nullable parameter. When a parameter is null, I want to throw a specific Exception. But I want the method to be clean and check the parameter somewhere else. The check method throws an exception, so after the method the parameter can not be null. \nUnfortunately, the compiler does not see that and throws warnings at me.\nHere's the method:\n\n\n\n```\n    public void Foo(string? argument)\n    {\n        GuardAgainst.Null(argument, nameof(argument));\n        string variable = argument; \/\/ <-- Warning CS8600  Converting null literal or possible null value to non - nullable type\n        var length = argument.Length; \/\/<--Warning CS8602  Dereference of a possibly null reference\n    }\n\n```\n\nHere's the check method:\n\n\n\n```\n    public static void Null(string? text, string paramName)\n    {\n        if (text == null)\n            throw new ArgumentNullException(paramName);\n    }\n\n```\n\nNow, I can suppress the warning like this:\n\n\n\n```\n#pragma warning disable CS8602\nvar length = argument.Length;\n#pragma warning restore CS8602\n\n```\n\nbut it kind of kills my intention to keep my code clean.\nSo my question is: is there a nicer way to suppress the warnings? Or maybe tell a compiler that from now on the parameter is guaranteed to not be null?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"This does what you want:\n\n\n\n```\npublic static void Null<T>([NotNull] T? value, string paramName)\n{\n    if (value == null)\n        throw new ArgumentNullException(paramName);\n}\n\n```\n\nThe `[NotNull]` attribute instructs the analysis that, after calling this method, `value` will not be `null`.\n\n\nThis means you don't need the `!` operator, which is much cleaner and more natural.\n\n\n\n```\nvoid M(string? argument)\n{\n    GuardAgainst.Null(argument, nameof(argument));\n    string variable = argument; \/\/ no warning\n    \/\/ ...\n}\n\n```\n\nThe use of an unconstrained generic type parameter `T` here means that this approach works for both reference types (such as `string`) and nullable value types (such as `int?`).\n\n\nIf you're using .NET 6, you can simplify this even further via `CallerArgumentExpressionAttribute` as follows:\n\n\n\n```\npublic static void Null<T>(\n    [NotNull] T? value,\n    [CallerArgumentExpression(parameterName: \"value\")] string? paramName = null)\n{\n    if (value == null)\n        throw new ArgumentNullException(paramName);\n}\n\n```\n\nWith that, the second argument can be omitted, and the caller can be simplified to:\n\n\n\n```\nGuardAgainst.Null(argument);\n\n```\n\n\n\n---\n\n\nThink of the `?` specifier on a type as meaning two things: 1) the value can be null before the call, and 2) the value can be null afterwards. Another way of writing it is `[AllowNull, MaybeNull]`. The absence of `?` in a nullable context equally means `[DisallowNull, NotNull]`. In the case of your `Null` method, we end up with `[AllowNull, NotNull]` due to the manual specification of `NotNull`.\n\n\n"}
{"questionId":"e424da4e0efd4f65b5a3f92e68b85d4e","question":"ERROR: function round(double precision, integer) does not exist\nI am in the middle of migrating some queries which have been running for ages with MySQL database which is now in Postgres having the same structure. I got stuck with a simple round function which ends with the following error message.\n\n\n\n> \n> ERROR: function round(double precision, integer) does not exist\n> \n> \n> \n\n\npart of the select which does not work:\n\n\n\n```\nround(floor(pools.available_capacity_in_kb\/1024\/1024\/1024*100)\/100,2) as free,\n\n```\n\n`pools.available_capacity_in_kb` is stored as BIGINT in the database (Postgres 10.9)\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"The core of the problem is somewhere else. PostgreSQL uses long division for integer and bigint numbers (when on both parts of division are int, bigint values). So result of `pools.available_capacity_in_kb\/1024\/1024\/1024*100)\/100` is bigint. Probably this is not, what you expect.\n\n\n\n```\npostgres=# \\df round\n                          List of functions\n+------------+-------+------------------+---------------------+------+\n|   Schema   | Name  | Result data type | Argument data types | Type |\n+------------+-------+------------------+---------------------+------+\n| pg_catalog | round | double precision | double precision    | func |\n| pg_catalog | round | numeric          | numeric             | func |\n| pg_catalog | round | numeric          | numeric, integer    | func |\n+------------+-------+------------------+---------------------+------+\n(3 rows)\n\n```\n\nThere is not any `round` function for `bigint` (because it has not any sense).\nPlease try to fix it by using float division like\n\n\n\n```\npools.available_capacity_in_kb\/1024\/1024\/1024*100)\/100.0\n\n```\n\nNow, the result will be `numeric`, and the function `round(numeric, int)` exists - so it should works.\n\n\n"}
{"questionId":"ed8d55b677684bbbadb3b563e452a011","question":"poetry can't find version of dependency even though it exists\nWhen bumping my python version from 3.7 to 3.8 in poetry, reinstalling all the dependencies fail with a version of the following:\n\n\n\n```\nERROR: No matching distribution found for...\n\n```\n\nThe distribution for that version is available at pypa, and is often the most recent version.\n\n\nSimply removing the offending package doesn't fix the issue, as poetry will likely fail with other packages. After some investigation, it appears that somehow poetry isn't using pip3 to install underneath, but is instead using pip2.7.\n\n\nIndeed this is supported by a deprecation alert, and the error is always reproducible if I attempt to install the same version with pip (globally or otherwise) and not pip3.\n\n\nThis issue is frustrating and deleting the venv alone doesn't seem to help. How can I resolve this dependency issue that shouldn't exist in the first place?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"There are two issues here which feed into each other. 1. poetry seems to consistently botch the upgrade of a venv when you modify the python versions. According to finswimmer, the upgrade should create a new virtual env for the new python version, however this process can fail when poetry uses the wrong `pip` version or loses track of which virtual env it's using. 2. poetry uses whatever `pip` is no questions asked - with no way to override and force usage of pip3.\n\n\nHere are the distilled steps I used to solve this issue\n\n\n1. delete the virtual env ( sometimes poetry loses track of the venv\/thinks it's already activated. Best to clear the slate )\n\n\n\n```\nrm -rf `poetry env list --full-path`\n\n```\n\n2. create a new virtual env ( the command should fail, but the venv will be created )\n\n\n\n```\npoetry install\n\n```\n\n3. manually activate the virtual env\n\n\n\n```\nsource \"$(poetry env list --full-path | tail -1 | sed 's\/.\\{12\\}$\/\/')\/bin\/activate\"\n\n```\n\n4. poetry install within the virtual env ( this ensures poetry is using the correct version of pip )\n\n\n\n```\npoetry install\n\n```\n\n"}
{"questionId":"959b62f2d45f4b1b9e99c88296ac5389","question":"A question about returning local pointer variable in function\nI know the variables in function are using stack space. When function exit, the space are freed. That's why we should declare the pointer variable as static in function. However, I found that the code below works well.\n\n\nThe gcc version is: `gcc version 9.3.0 (Ubuntu 9.3.0-17ubuntu1~20.04)` \n\n\n\n```\n#include <stdio.h>\n\nchar *month_name(int n) {\n    char *name[] = {\"Invalid name\", \"Jan.\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"June\",\n                    \"July\",         \"Aug\",  \"Sep\", \"Oct\", \"Nov\", \"Dec\"};\n\n    return n < 1 || n > 12 ? name[0] : name[n];\n}\n\nint main() {\n    char *month;\n    month = month_name(2);\n    printf(\"%s\\n\", month); \/\/ The output is Feb\n}\n\n```\n\nIt seems the variable in function is translate to static implicitly. Can anyone explain for me? Thanks in advance.\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"You declared an array of pointers to string literals (to their first characters)\n\n\n\n```\nchar *name[] = {\"Invalid name\", \"Jan.\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"June\",\n                \"July\",         \"Aug\",  \"Sep\", \"Oct\", \"Nov\", \"Dec\"};\n\n```\n\nString literals have static storage duration. That is they are alive after exiting the function.\n\n\nFor example in the C Standard (6.4.5 String literals) there is written\n\n\n\n> \n> 6 In translation phase 7, a byte or code of value zero is appended to\n> each multibyte character sequence that results from a string literal\n> or literals.78) **The multibyte character sequence is then used to\n> initialize an array of static storage duration** and length just\n> sufficient to contain the sequence....\n> \n> \n> \n\n\nOn the other hand, the array itself has automatic storage duration that is it is not alive after exiting the function. But the function returns a pointer to a string literal instead of a pointer to the array itself.\n\n\nThe function would be incorrect if you tried to return a pointer to the array itself as for example\n\n\n\n```\nchar * ( *month_name(int n) )[13] {\n    char *name[] = {\"Invalid name\", \"Jan.\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"June\",\n                    \"July\",         \"Aug\",  \"Sep\", \"Oct\", \"Nov\", \"Dec\"};\n\n    \/\/...     \n    return &name;\n}\n\n```\n\nor the following way\n\n\n\n```\nchar ** month_name(int n) {\n    char * name[] = {\"Invalid name\", \"Jan.\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"June\",\n                    \"July\",         \"Aug\",  \"Sep\", \"Oct\", \"Nov\", \"Dec\"};\n\n    return n < 1 || n > 12 ? name : name + n;\n}\n\n```\n\nOr if you would declare a two dimensional array like\n\n\n\n```\nchar *month_name(int n) {\n    char name[][13] = {\"Invalid name\", \"Jan.\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"June\",\n                    \"July\",         \"Aug\",  \"Sep\", \"Oct\", \"Nov\", \"Dec\"};\n\n    return n < 1 || n > 12 ? name[0] : name[n];\n}\n\n```\n\nthen in this case the return statement\n\n\n\n```\nreturn n < 1 || n > 12 ? name[0] : name[n];\n\n```\n\nindeed would invoke undefined behavior by the same reason that the array itself will not be alive after exiting the function.\n\n\nPay attention to that in C++ opposite to C string literals have types of constant character arrays. So to compile your function as a C++ code you have to define the function the following way\n\n\n\n```\nconst char *month_name(int n) {\n    const char *name[] = {\"Invalid name\", \"Jan.\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"June\",\n                    \"July\",         \"Aug\",  \"Sep\", \"Oct\", \"Nov\", \"Dec\"};\n\n    return n < 1 || n > 12 ? name[0] : name[n];\n}\n\n```\n\nAlso in C it is much better to define the function the same way because though in C string literals have types of non-constant character arrays nevertheless any attempt to change a string literal invokes undefined behavior. Such a function definition allows to avoid program bugs.\n\n\n"}
{"questionId":"e13bac3a46904c30a886b6fe43cb61b1","question":"What's the difference between a coroutine and a function with static variables?\nI've been learning about what's new in C++20 and I'm trying to understand the commonly discussed \"Generator\" use case of co-routines. I've tried to create a small example here but apologies if there is some mistake:\n\n\n\n```\ngenerator<int> Generate() {\n    int i = 0;\n    while(1) {\n        co_yield i++;\n    }\n}\n\nint main()\n{\n    auto gen { Generate() };\n    for (int x = 0; x < 10; ++x) {\n        gen.next();\n        std::cout << gen.getValue() << std::endl;\n    }\n    return 0;\n}\n\n```\n\nBut I don't see how this is any different from a function with static variables like:\n\n\n\n```\nauto gen() {\n    static int i = 0;\n    return i++;\n}\n\nint main()\n{\n    for (int x = 0; x < 10; ++x)\n        std::cout << gen() << std::endl;\n    return 0;\n}\n\n```\n\nI think I can maybe see how asynchronous I\/O is a usecase, especially with the `co_await` keyword, but for this generator example I'm sure I'm misunderstanding something about how they should be used. I'd be very grateful for any explanation\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c++"},"answer":"Perhaps the most obvious difference is that `static` local variables means you effectively have one instance... *total*. Whereas each generator is completely independent.\n\n\n\n```\n\/\/ with coroutines\nassert(Generator().next() == 0);\nassert(Generator().next() == 0);\nassert(Generator().next() == 0);\nassert(Generator().next() == 0);\n\n```\n\nEvery call to `Generator()` is creating a new generator, each of which starts counting at `0`. So every new generator's `next()` gives me zero. As expected.\n\n\nBut that's not the case with static local variables:\n\n\n\n```\nassert(gen() == 0);\nassert(gen() == 1);\nassert(gen() == 2);\nassert(gen() == 3);\n\n```\n\nSo you could imagine how if what you wanted was to create a generator that gave you an infinite stream of integers, it'd be nice if you could use that function reliably more than one time total in your entire program.\n\n\nThis isn't to say that static local variables aren't useful. It's just that they're not usable for this particular use-case.\n\n\n"}
{"questionId":"86d28fe5f63444b9b4454b1179146872","question":"Does `#[test]` imply `#[cfg(test)]`?\nConventionally, unit tests in Rust are given a separate module, which is conditionally compiled with `#[cfg(test)]`:\n\n\n\n```\n#[cfg(test)]\nmod tests {\n    #[test]\n    fn test1() { ... }\n\n    #[test]\n    fn test2() { ... }\n}\n\n```\n\nHowever, I've been using a style where tests are more inline:\n\n\n\n```\npub fn func1() {...}\n\n#[cfg(test)]\n#[test]\nfn test_func1() {...}\n\npub fn func2() {...}\n\n#[cfg(test)]\n#[test]\nfn test_func2() {...}\n\n```\n\nMy question is, does `#[test]` imply `#[cfg(test)]`? That is, if I tag my test functions with `#[test]` but not `#[cfg(test)]`, will they still be correctly absent in non-test builds?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"rust"},"answer":"\n> \n> My question is, does `#[test]` imply `#[cfg(test)]`? That is, if I tag my test functions with `#[test]` but not `#[cfg(test)]`, will they still be correctly absent in non-test builds?\n> \n> \n> \n\n\nYes. If you are not using a separate module for tests then you don't need to use `#[cfg(test)]`. Functions marked with `#[test]` are already excluded from non-test builds. This can be verified very easily:\n\n\n\n```\n#[test]\nfn test() {}\n\nfn main() {\n    test(); \/\/ error[E0425]: cannot find function `test` in this scope\n}\n\n```\n\n"}
{"questionId":"f3a452aac8594afcb12f5269b016ccc5","question":"Grep - how to output only the content of a capturing group\nI am trying to find a way for grep to output only the content of a capturing group. For instance, if I have the following file:\n\n\n\n```\nhello1, please match me\nhello2, please do not match me\n\n```\n\nI would like\n\n\n\n```\ngrep -Eo '(hello[0-9]+), please match me' file\n\n```\n\nTo output `hello1`. However it outputs `hello1, please match me`.\n\n\nNow, I know that `grep -Po 'hello[0-9]+(?=, please match me)'` will do the trick, but I'm thinking there must be a way to simply return a capturing group, but I couldn't find any info (on the net and in `man grep`).\n\n\nIs it possible, or are capturing groups only meant to be backrefenced ? It would seem weird to me if there was no way of doing that.\n\n\nThank you for your time, and feel free to critique the way this post is constructed!\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"If you have either `pcregrep` or `pcre2grep` you can use the `-o1` command-line flag to request that only capture group 1 is output. (Or change 1 to some other number if there are more captures in the regex.)\n\n\nYou can use the `-o*N*` command more than once if you want to output more than one capture group.\n\n\nAs far as I know, `grep -P` does not implement this extension. You'll find `pcre2grep` in Debian\/Ubuntu package `pcre2-utils`. `pcregrep` is in package `pcregrep`.\n\n\n"}
{"questionId":"1fdf1a6ed3274cf087a781c6466076b9","question":"Is there an efficient alternative to table()?\nI use the following command:\n\n\n\n```\ntable(factor(\"list\",levels=1:\"n\")\n\n```\n\nwith \"list\": (example) `a = c(1,3,4,4,3)`\nand `levels = 1:5`, to also take the 2 and 5 into consideration.\nFor really big datasets, my code seems to be very ineffective.\n\n\nDoes anyone know a hidden library or a code snippet to make it faster?\n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"r"},"answer":"We could use `fnobs` from `collapse` which would be efficient\n\n\n\n```\nlibrary(collapse)\nfnobs(df, g = df$X1)\n\n```\n\n\n\n---\n\n\nIn `base R`, `tabulate` is more efficient compared to `table`\n\n\n\n```\n tabulate(df$X1)\n [1]  9  6 15 13 11  9  7  9 11 10\n\n```\n\n"}
{"questionId":"ef7bbaa9a5744b64b329c1bf0bc3feb9","question":"Set-AzContext works in Azure Cloud Shell but doesn't in Azure PowerShell\nWhen i execute following command \n\n\n\n```\nClear-AzureProfile\nConnect-AzAccount -TenantID xxxxxxxxxxxxxxxxxxx\nSet-AzContext -SubscriptionId xxxxxxxxxxxxxxxxxxx\n\n```\n\nin `Azure PowerShell` i get this error. \n\n\n\n```\nSet-AzContext : Please provide a valid tenant or a valid subscription.\nAt line:6 char:1\n+ Set-AzContext -SubscriptionId xxxxxxxxxxxxxxxxxxx\n\n```\n\nand if i run the same command in `Azure Cloud Shell` it works \n\n\n\n```\nName        Account         SubscriptionName    Environment    TenantId         \nxxxx        xxxxxxx         xxxx                 xxxx             xxxx\n\n```\n\nI switched from free-trial to pay-as-you-go subscription and using credentials for pay-as-you-go in both environment but it doesn't work. can anyone help \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"Close your powershell and open a new one, or use `Clear-AzContext`, not `Clear-AzureProfile`. Then use `Connect-AzAccount -Tenant xxxxx -Subscription xxxxx`, it should work.\n\n\n"}
{"questionId":"b70699dc69124ca38f69b4984cfe13d3","question":"Add generated column to an existing table Postgres\nI am trying to add a generated column to an existing table with this script. \n\n\n\n```\nalter table Asset_Store add column\n\nmd5_hash VARCHAR(100) GENERATED ALWAYS AS \n\n(CAST(UPPER(    \n        case\n             when OR_ID is not null then MD5(cast(OR_ID as varchar(100)))\n             when Asset_ID is not null then MD5(Asset_ID)\n             else null\n        end \n) as VARCHAR(100)))\n\nSTORED\n\n;\n\n```\n\nbut I am getting an error: \n\n\n\n```\nSQL Error [42601]: ERROR: syntax error at or near \"(\"\n Position: 88\n ERROR: syntax error at or near \"(\"\n Position: 88\n ERROR: syntax error at or near \"(\"\n Position: 88\n\n```\n\nWhat is the issue? I don't get it. \n\n\nIn the schema of my Asset\\_Store table the column  \n\nOR\\_ID is `int` and Asset\\_ID is `varchar(100)`. \n\n\nI guess it expects a slightly different syntax... but what is the right syntax? \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"Your syntax is correct. Your version of PostgreSQL apparently is not.\n\n\nIn version 12:\n\n\n\n```\ncreate table asset_store(or_id text, asset_id text);\n\nalter table Asset_Store add column\nmd5_hash VARCHAR(100) GENERATED ALWAYS AS \n(CAST(UPPER(    \n        case\n             when OR_ID is not null then MD5(cast(OR_ID as varchar(100)))\n             when Asset_ID is not null then MD5(Asset_ID)\n             else null\n        end \n) as VARCHAR(100)))\nSTORED\n;\n\n```\n\n\n> \n> \n> ```\n> ALTER TABLE\n> Time: 17.678 ms\n> \n> ```\n> \n> \n\n\n"}
{"questionId":"0508b78836704b77bf9dd152f8e5ae95","question":"How can I get url of my attachment stored in active storage in my rails controller\nHow can I get url of my has\\_one model attachment stored in active storage in my rails controller. So, that I would be able to send it as full link as api in json.\nSo far, I have tried following methods but each of them are giving various issues:\n\n\n1. `current_user.image.service_url` ---- undefined method `service\\_url' for #<ActiveStorage::Attached::One:0x....\n2. `Rails.application.routes.url_helpers.rails_disk_blob_path(current_user.image, only_path: true)`, it gives me an output like:\n\n\n\"\/rails\/blobs\/%23%3CActiveStorage::Attached::One:0x007f991c7b41b8%3E\"\n\n\nbut this is not a url, right? I am not able to hit and get image on browser.\n\n\n3. `url_for` ----\n\n\nundefined method `active\\_storage\\_attachment\\_url' for #<Api::V1::UsersController:0x007f991c1eaa98\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"ruby"},"answer":"# Use the method `rails_blob_path` for attachements in a controller and models\n\n\nFor example, if you need to assign a variable (e.g. `cover_url`) in a controller, first you should include `url_helpers` and after use method `rails_blob_path` with some parameters. You can do the same in any model, worker etc.\n\n\nComplete example below:\n\n\n\n```\nclass ApplicationController < ActionController::Base\n\n  include Rails.application.routes.url_helpers\n\n  def index\n    @event = Event.first\n    cover_url = rails_blob_path(@event.cover, disposition: \"attachment\", only_path: true)\n  end\n\nend\n\n```\n\n"}
{"questionId":"e0083dabc18d4883a8948a693aa57691","question":"Why Kotlin's Type's vararg is treated as Array and not Array\nThe following method gets compiled in Java:\n\n\n\n```\npublic class Main {\n    public static void main(String[] args) {\n        varargMethod(1, 2.0);\n    }\n\n    static void varargMethod(Number... va) {\n        arrayMethod(va);\n    }\n\n    static void arrayMethod(Number[] arr) {\n        for (Number number : arr) {\n            System.out.println(number);\n        }\n    }\n}\n\n```\n\nIf I try to write similar code in Kotlin i get type mismatch error:\n\n\n\n```\nfun main() {\n    varargFun(1, 2.0)\n}\n\nfun varargFun(vararg va: Number) {\n    arrayFun(va) \/\/ Error:(6, 14) Kotlin: Type mismatch: inferred type is Array<out Number> but Array<Number> was expected\n}\n\nfun arrayFun(arr: Array<Number>) {\n    arr.forEach {\n        println(it)\n    }\n}\n\n```\n\nI expected `va` to be of type `Array<String>`, but it is `Array<out String>`. If I cast it: `va as Array<Number>`, I get a warning:\n\n\n\n> \n> Warning:(6, 21) Kotlin: Unchecked cast: Array to Array\n> \n> \n> \n\n\nHow am I supposed to pass `vararg` as an `Array` to another function without getting warning and errors?\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"kotlin"},"answer":"The difference is that in Java arrays are covariant, i.e. the following is valid:\n\n\n\n```\npublic static void main(String[] args) {\n    Number[] numbers = new Number[0];\n    Integer[] ints = new Integer[0];\n\n    numbers = ints;\n}\n\n```\n\nHowever, arrays are not covariant in Kotlin, i.e. the following gives a compilation error:\n\n\n\n```\nvar numbers: Array<Number> = arrayOf()\nval ints: Array<Int> = arrayOf()\n\nnumbers = ints \/\/ error: required Array<Number>, found Array<Int>\n\n```\n\nHowever you can declare the array is a producer (i.e. you promise you'll never insert anything inside it; the compiler will make sure of that) with the keyword `out`. That makes the array covariant, i.e. the following is valid:\n\n\n\n```\nvar numbers: Array<out Number> = arrayOf() \/\/ we will only extract Numbers out of this array\nval ints: Array<Int> = arrayOf()\n\nnumbers = ints \/\/ this is ok\n\n```\n\nGiven that, if `vararg va: Number` was not treated as a `Array<out Number>`, then you could have called your method only with `Number` objects and not with its subclasses. I.e., the following would fail:\n\n\n\n```\nfun main() {\n    varargFun(arrayOf<Int>(1, 2)) \/\/ error: required Array<Number>, found Array<Int>\n}\n\nfun varargFun(va: Array<Number>) {\n    arrayFun(va)\n}\n\n```\n\nBut again, with an `out` (which is what `vararg` does), it magically works:\n\n\n\n```\nfun main() {\n    varargFun(arrayOf<Int>(1, 2))\n}\n\nfun varargFun(va: Array<out Number>) {\n    arrayFun(va)\n}\n\n```\n\n"}
{"questionId":"db0e807b4d1e4cfca8c3dfad05822a1f","question":"Is a.b always (&a)->b?\nIn C, `a.b` is normally synonymous with `(&a)->b`.\n\n\nIs this true in absolutely all cases, even if `a` is a macro for some messy compound term? Or are there any edge cases, in any version of C, in which the equivalence fails to hold?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"Here are three counterexamples, all based on constraints on applying `&`:\n\n\n- `a` is an rvalue because it is a structure returned by a function:\n\n```\nint bar(void)\n{\n    extern struct S { int b; } foo(void);\n    return (&foo())->b;\n}\n\n```\n\nClang says \u201cerror: cannot take the address of an rvalue of type 'struct S'\u201d. But it accepts `return foo().b;`.\n- `a` is an rvalue because it is the result of an assignment:\n\n```\n  int bar(void)\n  {\n      struct S { int b; } x = {0};\n      struct S y;\n      return (&(y=x))->b;\n  }\n\n\n```\n\nClang says \u201cerror: cannot take the address of an rvalue of type 'struct S'\u201d. But it accepts `return (y=x).b;`.\n- `a` is declared with `register`, so its address may not be taken:\n\n```\nint bar(void)\n{\n    register struct S { int b; } a = {0};\n    return (&a)->b;\n}\n\n```\n\nClang says \u201cerror: address of register variable requested\u201d.\n\n\n"}
{"questionId":"077fae262dbd4c7485c1a653e4cebced","question":"Spring R2DBC DatabaseClient.as(\u2026)\nIn my spring-boot 2.3 application, I have a simple data method using `DatabaseClient`:\n\n\n\n```\nfun getCurrentTime(): Mono<LocalDateTime> =\n    databaseClient\n        .execute(\"SELECT NOW()\")\n        .asType<LocalDateTime>()\n        .fetch()\n        .first()\n}\n\n```\n\nWith spring-boot 2.4 (and spring 5.3 and spring-data-r2dbc 1.2), `org.springframework.data.r2dbc.core.DatabaseClient` from spring-data-r2dbc is deprecated in favor of `org.springframework.r2dbc.core.DatabaseClient` of spring-r2dbc - which has a different API.\n\n\nAdapting that is pretty much straightforward - with the exception of the kotlin extension `asType`, which is not a part of the new DatabaseClientExtensions.\n\n\n\n```\nfun getCurrentTime(): Mono<LocalDateTime> =\n    databaseClient\n        .sql(\"SELECT NOW()\")\n        .map { row: Row ->\n            row.get(0, LocalDateTime::class.java)!!\n        }\n        .one()\n\n```\n\nAre those extensions somewhere else or how can I convert using a reified type parameter?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"# TL;DR\n\n\nThere's no `as(Class)` API after the migration to Spring R2DBC.\n\n\n# A bit of background\n\n\n`DatabaseClient` started its journey in the experimental Spring Data R2DBC project, trying out various approaches. One of them evaluated how close a textual SQL API and an object-mapping API could be brought together. `DatabaseClient` in Spring Data exposed various API methods such as `select().from(\"table\").as(targetType)`.\n\n\nIt turned out that this functionality is useful but draws certain limitations because the more an API goes into an entity or even aggregate-oriented direction, the more complexity the actual API becomes and at some point, boundaries between simple object mapping and entities (for example, entity lifecycle callbacks) blur.\n\n\nWe decided to introduce `R2dbcEntityTemplate` as an abstraction for all entity-bound operations to support most common use-cases. Looking at the fluent API that was previously in place, there's still a gap for all use-cases that require ad-hoc SQL queries, aggregations, function calls, etc.\n\n\nIn the meantime, the project proved useful, and we've identified core support classes that could be migrated into Spring Framework 5.3, so Spring Data R2DBC 1.2 could be based on top of Spring R2DBC.\n\n\nWe weren't able to come up with a proper approach while migrating the code. To be fair, `DatabaseClient` offers almost the same level of abstraction (except for stored procedures) as `NamedParameterJdbcTemplate`. Spring JDBC ships clearly with several `RowMapper` implementations such as `SingleColumnRowMapper` or `DataClassRowMapper` that would be useful for Spring R2DBC, too.\n\n\n## Final thoughts\n\n\nFrom a user-perspective, `as(\u2026)` sees a lot of demand and we should investigate, how this functionality (or a variant of it) could be surfaced.\n\n\n"}
{"questionId":"bd0432ff211040bba24a9c9edc812dca","question":"Use module from parent directory in rust\nIs it possible to structure a rust project in this way?\n\n\nDirectory structure:\n\n\n\n```\nsrc\n\u251c\u2500\u2500 a\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 bin1.rs\n\u251c\u2500\u2500 b\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bin2.rs\n\u2514\u2500\u2500 common\n    \u251c\u2500\u2500 mod.rs\n\n```\n\nfrom Cargo.toml:\n\n\n\n```\n[[bin]]\nname = \"bin1\"\npath = \"src\/a\/bin1.rs\"\n\n[[bin]]\nname = \"bin2\"\npath = \"src\/b\/bin2.rs\"\n\n```\n\nI would like to be able to use the `common` module in `bin1.rs` and `bin2.rs`. It's possible by adding the path attribute before the import:\n\n\n\n```\n#[path=\"..\/common\/mod.rs\"]\nmod code;\n\n```\n\nIs there a way for `bin1.rs` and `bin2.rs` to use `common` without having to hardcode the path?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"rust"},"answer":"The recommended method to share code between binaries is to have a `src\/lib.rs` file. Both binaries automatically have access to anything accessible through this `lib.rs` file as a separate crate.\n\n\nThen you would simply define a `mod common;` in the `src\/lib.rs` file. If your crate is called `my_crate`, your binaries would be able to use it with\n\n\n\n```\nuse my_crate::common::Foo;\n\n```\n\n"}
{"questionId":"1a46f60a591f4100a876bcba19654576","question":"How can I read a file which will be upload from a form in .Net Core API?\nI create a method in my .Net Core API which will upload a file.\n\n\n\n```\n[HttpPost]\npublic async Task<IActionResult> ReadFile(IFormFile file)\n{\n    return BadRequest(file);\n}\n\n```\n\nI do a `return BadRequest(file)` in order to read what it send me on **postman**.\n\n\nThe result is this :\n\n\n\n```\n{\n    \"contentDisposition\": \"form-data; name=\\\"file\\\"; filename=\\\"data.dat\\\"\",\n    \"contentType\": \"application\/octet-stream\",\n    \"headers\": {\n        \"Content-Disposition\": [\n            \"form-data; name=\\\"file\\\"; filename=\\\"data.dat\\\"\"\n        ],\n        \"Content-Type\": [\n            \"application\/octet-stream\"\n        ]\n    },\n    \"length\": 200,\n    \"name\": \"file\",\n    \"fileName\": \"data.dat\"\n\n```\n\n}\n\n\nI see on Microsoft documentation this : \n\n\n\n```\nusing (StreamReader sr = new StreamReader(\"TestFile.txt\"))\n{\n    \/\/ Read the stream to a string, and write the string to the console.\n        String line = sr.ReadToEnd();\n        Console.WriteLine(line);\n}\n\n```\n\nBut the user will have to choose a file to read in it, the application won't have to go in a folder and read a file.\n\n\nIt is possible to do this ? And can I have some link to help me to do this ?\n\n\n**Update** \n\n\nI want to the method ReadFile read the content of my file which will be upload thinks to a form.\n\n\nSo I will have a string which will have the content of my file and after that I will can all I wanted to do in this file.\n\n\nFor example I have a file and in this file it is wrote **LESSON**, with the method **ReadFile** I will get the word lesson in a string.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"In you Controller :\n\n\n1. Check if `IFormFile file` contains something\n2. Check if the file's extension is the one you are looking for (.dat)\n3. Check if the file's Mime type is correct to avoid attacks\n\n\nThen, if it is all right, call a Service class to read your file.\n\n\nIn your Service, you can do something like following :\n\n\n\n```\nvar result = new StringBuilder();\nusing (var reader = new StreamReader(file.OpenReadStream()))\n{\n    while (reader.Peek() >= 0)\n        result.AppendLine(await reader.ReadLineAsync()); \n}\nreturn result.ToString();\n\n```\n\nHope it helps.\n\n\n"}
{"questionId":"4c3adde437fe49639865d3fca2ce1b2e","question":"How can i find the source of this font-related coretext warning in IOS13?\nWorking on an update of my app i notice that i get tons of warnings in the log when running the app in Xcode 11.2 on IOS13.\n\n\n\n> \n> CoreText note: Client requested name \".SFUI-Regular\", it will get\n>  TimesNewRomanPSMT rather than the intended font. All system UI font\n>  access should be through proper APIs such as\n>  CTFontCreateUIFontForLanguage() or +[UIFont systemFontOfSize:].\n> \n> \n> \n\n\nI dug around a bit and found this quote from WWDC:\n\n\n\n> \n> As mentioned in numerous WWDC sessions, dot-prefixed font names are\n>  not to be directly used.\n> \n> \n> \n\n\nI am myself almost exclusively using IB and nibs to set fonts for textfields etc., and there is no reference to \"SFUI-Regular\" in my code anywhere, so i am not sure how to find the actual reason for these warnings (i have something like 20-30 rows of these in the logs).\n\n\nDoes anyone have any tips on how i can find where the warning comes from, and how to fix it?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"objective-c"},"answer":"There is another output in console, you can try to add a symbolic breakpoint\n\n\n\n> \n> CoreText note: Set a breakpoint on CTFontLogSystemFontNameRequest to debug.\n> \n> \n> \n\n\n"}
{"questionId":"6608d049743d4c1f8234618fe646ddf1","question":"Execute custom SQL script as part of Entity Framework migration\nI have been using the standard add-migration approach to updating my aspnet core database on Entity Framework. I now have to move two \"image\" columns into a new table (and their image data), remove the columns from the original table, and set up a foreign key relationship between the old and new tables. I have a working SQL script to do all this.\n\n\n**How can I execute this sql script as part of a normal EF migration, AND make sure subsequent add-migration changes will reflect the changes that my sql script will do (adding new table\/columns, removing image columns from original table)?**\n\n\nI've seen a few references to SqlFile and deriving from DbMigration, but nothing that fits my scenario nicely. I am using EF Core, aspnet core 2.0.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"sql"},"answer":"**Moving a populated, non-nullable column**\n\n\nGet Entity Framework to create the base migration and then enhance the output.\n\n\nSome example code that moves an `EmailAddress` field from `OldTable` to `NewTable` (MS SQL Server):\n\n\n\n```\nmigrationBuilder.AddColumn<string>(\n    name: \"EmailAddress\",\n    table: \"NewTable\",\n    defaultValue: \"\");\n\nmigrationBuilder.Sql(\"UPDATE NewTable SET NewTable.EmailAddress = OldTable.EmailAddress FROM NewTable JOIN OldTable ON NewTable.Id = OldTable.NewTableId\");\n\nmigrationBuilder.AlterColumn<string>(\n    name: \"EmailAddress\",\n    table: \"NewTable\",\n    nullable: false,\n    defaultValue: \"\");\n\nmigrationBuilder.DropColumn(\n    name: \"EmailAddress\",\n    table: \"OldTable\");\n\n```\n\nRemember, this needs to happen for `Up()` and `Down()`, except `Down()` undoes the operation.\n\n\n"}
{"questionId":"3c6a9c573b89415da0162bafac7d28ef","question":"Macos Catalina error after update: unable to run colorls\nUpdating to macOS **Catalina** gives error on opening terminal\n\n\n\n```\nERROR:  Can't find Ruby library file or shared library colorls\nusage: dirname path\n\/Users\/varunsukheja\/.zshrc:source:91: no such file or directory: \/tab_complete.sh\n\n```\n\nWhen I checked for the .zshrc file for line 91, I found below line\n\n\n\n```\nsource $(dirname $(gem which colorls))\/tab_complete.sh\n\n```\n\nAnd on checking `gem which colorls` I got below error\n\n\n\n```\nERROR:  Can't find Ruby library file or shared library colorls\n\n```\n\nBut when I check `which colorls` I get below path\n\n\n\n```\n\/usr\/local\/bin\/colorls\n\n```\n\nAlso I tried installing colorls again using gem\n\n\n\n```\nBuilding native extensions. This could take a while...\nERROR:  Error installing colorls:\n    ERROR: Failed to build gem native extension.\n\n    current directory: \/Library\/Ruby\/Gems\/2.6.0\/gems\/clocale-0.0.4\/ext\/clocale\n\/System\/Library\/Frameworks\/Ruby.framework\/Versions\/2.6\/usr\/bin\/ruby -I \/Library\/Ruby\/Site\/2.6.0 -r .\/siteconf20191009-43313-16ndnb.rb extconf.rb\nmkmf.rb can't find header files for ruby at \/System\/Library\/Frameworks\/Ruby.framework\/Versions\/2.6\/usr\/lib\/ruby\/include\/ruby.h\n\nYou might have to install separate package for the ruby development\nenvironment, ruby-dev or ruby-devel for example.\n\nextconf failed, exit code 1\n\nGem files will remain installed in \/Library\/Ruby\/Gems\/2.6.0\/gems\/clocale-0.0.4 for inspection.\nResults logged to \/Library\/Ruby\/Gems\/2.6.0\/extensions\/universal-darwin-19\/2.6.0\/clocale-0.0.4\/gem_make.out\n\n```\n\nPlease help how to make it work.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"Here is how I solved it:\n\n\n1. `sudo xcode-select --install`\n2. Install rbenv with `brew install rbenv`\n3. Add `eval \"$(rbenv init -)\"` to the end of `~\/.zshrc` or `~\/.bash_profile`\n4. Install a ruby version `rbenv install 2.6.0`\n5. Select a ruby version by rbenv `rbenv global 2.6.0`\n6. Open a new terminal window.\n7. Verify that the right gem folder is being used with `gem env home` (this should report something in your user folder, not system wide).\n\n\nAfter this I installed colorls again using `sudo gem install colorls`\n\n\nIf getting error:\n\n\n\n```\nERROR:  While executing gem ... (Gem::FilePermissionError)\n    You don't have write permissions for the \/usr\/bin directory.\n\n```\n\nTry installing colorls using cmd:\n\n\n\n```\nsudo gem install colorls -n \/usr\/local\/bin\n\n```\n\nAfter this you see colorls is installed successfully and new terminal works fine.\n\n\n"}
{"questionId":"1ace0e8808f148b381ef16185838e6be","question":"A faster strptime?\nI have code which reads vast numbers of dates in 'YYYY-MM-DD' format. Parsing all these dates, so that it can add one, two, or three days then write back in the same format is slowing things down quite considerably.\n\n\n\n```\n 3214657   14.330    0.000  103.698    0.000 trade.py:56(effective)\n 3218418   34.757    0.000   66.155    0.000 _strptime.py:295(_strptime)\n\n day = datetime.datetime.strptime(endofdaydate, \"%Y-%m-%d\").date()\n\n```\n\nAny suggestions how to speed it up a bit (or a lot)?\n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"python"},"answer":"### Python 3.7+: `fromisoformat()`\n\n\nSince Python 3.7, the `datetime` class has a method `fromisoformat`. It should be noted that this can also be applied to this question:\n\n\n### Performance vs. `strptime()`\n\n\nExplicit string slicing may give you about a 9x increase in performance compared to normal `strptime`, but you can get about a **90x increase with the built-in `fromisoformat` method!**\n\n\n\n```\n%timeit isofmt(datelist)\n569 \u00b5s \u00b1 8.45 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n%timeit slice2int(datelist)\n5.51 ms \u00b1 48.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n%timeit normalstrptime(datelist)\n52.1 ms \u00b1 1.27 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n```\n\n\n```\nfrom datetime import datetime, timedelta\nbase, n = datetime(2000, 1, 1, 1, 2, 3, 420001), 10000\ndatelist = [(base + timedelta(days=i)).strftime('%Y-%m-%d') for i in range(n)]\n\ndef isofmt(l):\n    return list(map(datetime.fromisoformat, l))\n    \ndef slice2int(l):   \n    def slicer(t):\n        return datetime(int(t[:4]), int(t[5:7]), int(t[8:10]))\n    return list(map(slicer, l))\n\ndef normalstrptime(l):\n    return [datetime.strptime(t, '%Y-%m-%d') for t in l]\n    \nprint(isofmt(datelist[0:1]))\nprint(slice2int(datelist[0:1]))\nprint(normalstrptime(datelist[0:1]))\n\n# [datetime.datetime(2000, 1, 1, 0, 0)]\n# [datetime.datetime(2000, 1, 1, 0, 0)]\n# [datetime.datetime(2000, 1, 1, 0, 0)]\n\n```\n\n\n\n---\n\n\nPython 3.8.3rc1 x64 \/ Win10\n\n\n"}
{"questionId":"a14af2cfdd4146b3b4fe31301ec08d3a","question":"Amazon Redshift Error - ERROR: 0A000: Specified types or functions (one per INFO message) not supported on Redshift tables\nWhen running the below query:\n\n\n\n```\nWITH sublevels AS (\n    SELECT 1 UNION ALL \n    SELECT 1 UNION ALL\n    SELECT 1\n), FIELDA AS (\n   SELECT (ROW_NUMBER() OVER ())::INT sublevel \n   FROM sublevels sl1, sublevels sl2, sublevels sl3\n)\nSELECT TOP 10 \nFIELDB, \nsublevel, \nREPLACE(REGEXP_REPLACE(REGEXP_SUBSTR(UPPER(FIELDC), 'FROM \\\\S+', 1, sublevel), 'FROM ', ''),')','') ALIASA\nFROM TABLEA\nJOIN FIELDA ON sublevel <= REGEXP_COUNT(UPPER(FIELDC), 'FROM ')\nWHERE ALIASA != 'ABC'\n    AND lower(split_part(ALIASA, '.', 2)) IN (\n        SELECT DISTINCT lower(t.table_name)\n        FROM information_schema.tables t\n        INNER JOIN information_schema.columns c on c.table_name = t.table_name AND c.table_schema = t.table_schema\n        WHERE lower(column_name) similar TO '%(aaa|bbb|ccc)%')\n\n```\n\nI am getting the following error:\n\n\n\n> \n> ERROR: 0A000: Specified types or functions (one per INFO message) not supported on Redshift tables.\n> \n> \n> \n\n\nI have no idea why, if I run the queries individually they work fine:\n\n\n**Query1**\n\n\n\n```\nWITH sublevels AS (\n    SELECT 1 UNION ALL \n    SELECT 1 UNION ALL\n    SELECT 1\n), FIELDA AS (\n    SELECT (ROW_NUMBER() OVER ())::INT sublevel \n    FROM sublevels sl1, sublevels sl2, sublevels sl3\n)\nSELECT TOP 10 \nFIELDB, \nsublevel, REPLACE(REGEXP_REPLACE(REGEXP_SUBSTR(UPPER(FIELDC), 'FROM \\\\S+', 1, sublevel), 'FROM ', ''),')','') ALIASA\nFROM TABLEA\nJOIN FIELDA ON sublevel <= REGEXP_COUNT(UPPER(FIELDC), 'FROM ')\nWHERE ALIASA != 'ABC'\n\n```\n\n**Query2**\n\n\n\n```\nSELECT DISTINCT lower(t.table_name)\nFROM information_schema.tables t\nINNER JOIN information_schema.columns c on c.table_name = t.table_name AND c.table_schema = t.table_schema\nWHERE lower(column_name) similar TO '%(aaa|bbb|ccc)%'\n\n```\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"sql"},"answer":"In Redshift:\n\n\n1. queries against information\\_schema run on the leader node only\n2. queries against any \"normal\" tables run on compute nodes only\n\n\nYou cannot mix and match 1. and 2.\n\n\n"}
{"questionId":"1a323d8ef98b4d0d89a8801a292003f5","question":"'\\0' and printf() in C\nIn an introductory course of C, I have learned that while storing the strings are stored with null character `\\0` at the end of it. But what if I wanted to print a string, say `printf(\"hello\")` although I've found that that it doesn't end with `\\0` by following statement\n\n\n\n```\nprintf(\"%d\", printf(\"hello\"));\n\nOutput: 5\n\n```\n\nbut this seem to be inconsistent, as far I know that variable like strings get stored in main memory and I guess while printing something it might also be stored in main memory, then why the difference?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c"},"answer":"The null byte marks the end of a string. It isn't counted in the length of the string and isn't printed when a string is printed with `printf`. Basically, the null byte tells functions that do string manipulation when to stop.\n\n\nWhere you will see a difference is if you create a `char` array initialized with a string. Using the `sizeof` operator will reflect the size of the array including the null byte. For example:\n\n\n\n```\nchar str[] = \"hello\";\nprintf(\"len=%zu\\n\", strlen(str));     \/\/ prints 5\nprintf(\"size=%zu\\n\", sizeof(str));    \/\/ prints 6\n\n```\n\n"}
{"questionId":"ef41c9d6ce6c44c096437a82b2db4a37","question":"Is this the right way to do dependency injection in Django?\nI'm trying to inject dependencies into my Django view (controller?). Here's some background.\n\n\nNormally, the `urls.py` file is what handles the routing. It is usually something like this:\n\n\n\n```\n urlpatterns = [\n     path(\"\", views.get_all_posts, name=\"get_all_posts\"),\n     path(\"<int:post_id>\", views.get_post, name=\"get_post\"),\n     path(\"create\", views.create_post, name=\"create_post\"),\n ]\n\n```\n\nThe problem with this, is that once you get to `create_post` for instance, you might have a dependency on a service that creates posts:\n\n\n\n```\n# views.py\n...\n\ndef create_post(self):\n    svc = PostCreationService()\n    svc.create_post()\n\n```\n\nThis kind of pattern is difficult to test. While I know python testing libraries have tools to mock this sort of thing, I'd rather inject the dependency into the view. Here's what I came up with.\n\n\nA Controller class that has a static method, `export(deps)` that takes in a list of dependencies and returns a list of url pattern objects:\n\n\n\n```\nclass ApiController(object):\n\n    @staticmethod\n    def export(**deps):\n        ctrl = ApiController(**deps)\n        return [\n            path(\"\", ctrl.get_all_posts, name=\"get_all_posts\"),\n            path(\"<int:post_id>\", ctrl.get_post, name=\"get_post\"),\n            path(\"create\", ctrl.create_post, name=\"create_post\"),\n        ]\n\n    def __init__(self, **deps):\n        self.deps = deps\n\n    def get_all_posts():\n        pass\n    ...\n\n```\n\nThis looks janky, but I'm not aware of any other way to do what I'm trying to do. The controller needs to return a list of url patterns, and it also needs to take in a list of dependencies. Using the above technique, I can do this in `urls.py`:\n\n\n\n```\nurlpatterns = ApiController.export(foo_service=(lambda x: x))\n\n```\n\nI am now free to use `foo_service` in any of the methods of `ApiController`.\n\n\n**Note:**\n\n\nOne alternative would be for the constructor to return the list of urls, but I don't see that as a huge improvement over this. In fact, it strikes me as being more confusing because the class constructor would return a list instead of an instance of the class.\n\n\n**Note 2:**\n\n\nI'm aware that python has mocking tools for mocking class members. Please don't suggest using them. I'd like to use DI as the way to control and manage dependencies.\n\n\nAny ideas on what the best way to do this is?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"Consider injecting using decorators:\n\n\n\n```\nfrom functools import wraps\n\nclass ServiceInjector:\n\n    def __init__(self):\n        self.deps = {}\n\n    def register(self, name=None):\n\n        name = name\n        def decorator(thing):\n            \"\"\"\n            thing here can be class or function or anything really\n            \"\"\"\n\n            if not name:\n                if not hasattr(thing, \"__name__\"):\n                    raise Exception(\"no name\")\n                thing_name = thing.__name__\n            else:\n                thing_name = name\n            self.deps[thing_name] = thing\n            return thing\n\n        return decorator\n\n    def inject(self, func):\n\n        @wraps(func)\n        def decorated(*args, **kwargs):\n            new_args = args + (self.deps, )\n            return func(*new_args, **kwargs)\n\n        return decorated\n\n# usage:\n\n\nsi = ServiceInjector()\n\n# use func.__name__, registering func\n@si.register()\ndef foo(*args):\n    return sum(args)\n\n\n# we can rename what it's been registered as, here, the class is registered \n# with name `UpperCase` instead of the class name `UpperCaseRepresentation`\n@si.register(name=\"UpperCase\")\nclass UpperCaseRepresentation:\n    def __init__(self, value):\n        self.value = value\n\n    def __str__(self):\n        return self.value.upper()\n\n#register float\nsi.register(name=\"PI\")(3.141592653)\n\n\n# inject into functions\n@si.inject \ndef bar(a, b, c, _deps): # the last one in *args would be receiving the dependencies\n    UpperCase, PI, foo = _deps['UpperCase'], _deps['PI'], _deps['foo']\n    print(UpperCase('abc')) # ABC\n    print(PI) # 3.141592653\n    print(foo(a, b, c, 4, 5)) # = 15\n\nbar(1, 2, 3)\n\n# inject into class methods\nclass Foo:\n\n    @si.inject\n    def my_method(self, a, b, _deps, kwarg1=30):\n        return _deps['foo'](a, b, kwarg1)\n\nprint(Foo().my_method(1, 2, kwarg1=50)) # = 53\n\n```\n\n"}
{"questionId":"869d5940aba649e09032b46897bda54f","question":"GRPC Connection Management in Golang\nI am relatively new to GRPC and want to be sure that I am doing connection management correctly with golang. I don't want to have to create a new connection for every call but I also don't want to create bottlenecks as I scale. \n\n\nWhat I did was to create a single connection in the init function:\n\n\n\n```\nvar userConn *grpc.ClientConn\nvar userServiceName string\n\nfunc init() {\n    userServiceName := os.Getenv(\"USER_SERVICE_URL\")\n    if userServiceName == \"\" {\n        userServiceName = \"localhost\"\n    }\n    logging.LogDebug(\"userClient:  Connecting to: \"+userServiceName, \"\")\n    tempConn, err := grpc.Dial(userServiceName, grpc.WithInsecure())\n    if err != nil {\n        logging.LogEmergency(\"account_user_client.Init()  Could not get the connection.  \"+err.Error(), \"\")\n        return\n    }\n    userConn = tempConn\n}\n\n```\n\nAnd then for each function I will use that connection to create a Client: \n\n\n\n```\nc := user.NewUserClient(userConn)\n\/\/ Contact the server and print out its response.\nctx, cancel := context.WithTimeout(context.Background(), time.Second)\ndefer cancel()\nr, err := c.GetUserFromTokenID(ctx, &user.GetUserFromTokenRequest{TransactionID: transactionID, OathToken: *oathToken})\n\/\/Handle Error and Response\n\n```\n\nis this an acceptable way to handle grpc connections? Any recommendations on better ways?\n\n\nThank you very much.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"go"},"answer":"Yes, it's fine to have single GRPC client connection per service. Moreover, I don't see any other options here. GRPC does all the heavy lifting under the hood: for example, you don't need to write your own client connection pool (as you would do for a typical RDBMS), because it won't provide better results than a single GRPC connection.\n\n\nHowever I would suggest you to avoid using global variables and init functions, especially for networking setup. Also you don't need to create GRPC client (`c := user.NewUserClient(userConn)`) every time you post a request to the GRPC service: this is just an extra work for garbage collector, you can create the only instance of client at the time of application startup.\n\n\n**Update**\n\n\nAssuming that you're writing server application (because it can be seen from the method you call on the remote GRPC service), you can simply define a type that will contain all the objects that have the same lifetime as the whole application itself. According to the tradition, these types are usually called \"server context\", though it's a little bit confusing because Go has very important concept of `context` in its standard library.\n\n\n\n```\n   \/\/ this type contains state of the server\n   type serverContext struct {\n       \/\/ client to GRPC service\n       userClient user.UserClient\n\n       \/\/ default timeout\n       timeout time.Duration\n\n       \/\/ some other useful objects, like config \n       \/\/ or logger (to replace global logging)\n       \/\/ (...)       \n   }\n\n   \/\/ constructor for server context\n   func newServerContext(endpoint string) (*serverContext, error) {\n       userConn, err := grpc.Dial(endpoint, grpc.WithInsecure())\n       if err != nil {\n           return nil, err\n       }\n       ctx := &serverContext{\n          userClient: user.NewUserClient(userConn),\n          timeout: time.Second,\n       }\n       return ctx, nil\n   }\n\n   type server struct {\n       context *serverContext\n   }\n\n   func (s *server) Handler(ctx context.Context, request *Request) (*Response, error) {\n       clientCtx, cancel := context.WithTimeout(ctx, time.Second)\n       defer cancel()\n       response, err := c.GetUserFromTokenID(\n          clientCtx, \n          &user.GetUserFromTokenRequest{\n              TransactionID: transactionID,\n              OathToken: *oathToken,\n          },\n       )\n       if err != nil {\n            return nil, err\n       }\n       \/\/ ...\n   }\n\n   func main() {\n       serverCtx, err := newServerContext(os.Getenv(\"USER_SERVICE_URL\"))\n       if err != nil {\n          log.Fatal(err)\n       }\n       s := &server{serverCtx}\n\n       \/\/ listen and serve etc...\n   }\n\n\n```\n\nDetails may change depending on what you're actually working on, but I just wanted to show that it's much more better to encapsulate state of your application in an instance of distinct type instead of infecting global namespace.\n\n\n"}
{"questionId":"6b919d8a00b0490c8a5275790ad89752","question":"Change timezone in laravel\nI want change the default timezone from UTC to Asia\/Tehran\nwhere I can change it? \nI tried by changing this code in app.php but it did not work.\n\n\n\n```\n'timezone' => 'UTC',\n\n```\n\nto\n\n\n\n```\n'timezone' => 'Asia\/Tehran',\n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"php"},"answer":"After update app.php run below command and check\n\n\n\n```\nphp artisan config:cache\nphp artisan cache:clear\n\n```\n\nYou can create below type of route for clear cache in laravel\n\n\n\n```\nRoute::get('\/clear-cache', function() {\n\n    $configCache = Artisan::call('config:cache');\n    $clearCache = Artisan::call('cache:clear');\n    \/\/ return what you want\n});\n\n```\n\n"}
{"questionId":"664ae725749b4530b9cfd406c56630da","question":"Getting \"temporary value dropped while borrowed\" when trying to update an Option<&str> in a loop\nI'm trying to implement a commonly used pattern - using the result of a previous loop iteration in the next loop iteration. For example, to implement pagination where you need to give the id of the last value on the previous page.\n\n\n\n```\nstruct Result {\n    str: String,\n}    \n\nfn main() {\n    let times = 10;\n    let mut last: Option<&str> = None;\n\n    for i in 0..times {\n        let current = do_something(last);\n        last = match current {\n            Some(r) => Some(&r.str.to_owned()),\n            None => None,\n        };\n    }\n}\n\nfn do_something(o: Option<&str>) -> Option<Result> {\n    Some(Result {\n        str: \"whatever string\".to_string(),\n    })\n}\n\n```\n\nHowever, I'm not sure how to actually get the value out of the loop. Currently, the compiler error is `temporary value dropped while borrowed` (at `&r.str.to_owned()`), though I made many other attempts, but to no avail.\n\n\nThe only way I found to actually get it working is to create some sort of local `tmp_str` variable and do a hack like this:\n\n\n\n```\nmatch current {\n    Some(r) => {\n        tmp_str.clone_from(&r.str);\n        last = Some(&tmp_str);\n    }\n    None => {\n        last = None;\n    }\n}\n\n```\n\nBut that doesn't feel like it's the way it's supposed to be done.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"rust"},"answer":"In your code, it remains unclear who the owner of the `String` referenced in `last: Option<&str>` is supposed to be. You could introduce an extra mutable local variable that owns the string. But then you would have two variables: the owner and the reference, which seems redundant. It would be much simpler to just make `last` the owner:\n\n\n\n```\nstruct MyRes {\n    str: String,\n}\n\nfn main() {\n    let times = 10;\n    let mut last: Option<String> = None;\n\n    for _i in 0..times {\n        last = do_something(&last).map(|r| r.str);\n    }\n}\n\nfn do_something(_o: &Option<String>) -> Option<MyRes> {\n    Some(MyRes {\n        str: \"whatever string\".to_string(),\n    })\n}\n\n```\n\nIn `do_something`, you can just pass the whole argument by reference, this seems more likely to be what you wanted. Also note that naming your own struct `Result` is a bad idea, because `Result` is such a pervasive trait built deeply into the compiler (`?`-operator etc).\n\n\n\n\n---\n\n\n**Follow-up question: `Option<&str>` or `Option<String>`?**\n\n\nBoth `Option<&str>` and `Option<String>` have different trade-offs. One is better for passing string literals, other is better for passing owned `String`s. I'd actually propose to use neither, and instead make the function generic over type `S` that implements `AsRef<str>`. Here is a comparison of various methods:\n\n\n\n```\nfn do_something(o: &Option<String>) {\n    let _a: Option<&str> = o.as_ref().map(|r| &**r);\n    let _b: Option<String> = o.clone();\n}\nfn do_something2(o: &Option<&str>) {\n    let _a: Option<&str> = o.clone(); \/\/ do you need it?\n    let _b: Option<String> = o.map(|r| r.to_string());\n}\nfn do_something3<S: AsRef<str>>(o: &Option<S>) {\n    let _a: Option<&str> = o.as_ref().map(|s| s.as_ref());\n    let _b: Option<String> = o.as_ref().map(|r| r.as_ref().to_string());\n}\n\nfn main() {\n    let x: Option<String> = None;\n    let y: Option<&str> = None;\n\n    do_something(&x);                           \/\/ nice\n    do_something(&y.map(|r| r.to_string()));    \/\/ awkward & expensive\n\n    do_something2(&x.as_ref().map(|x| &**x));   \/\/ cheap but awkward\n    do_something2(&y);                          \/\/ nice\n\n    do_something3(&x);                          \/\/ nice\n    do_something3(&y);                          \/\/ nice, in both cases\n}\n\n```\n\nNote that not all of the above combinations are very idiomatic, some are added just for completeness (e.g. asking for `AsRef<str>` and then building an owned `String` out of seems a bit strange).\n\n\n"}
{"questionId":"16dd8bb756174e7b9d2d48a692957b16","question":"How to create a Fat JAR with the Kotlin DSL?\nI'm using Gradle 5.5. I have a Groovy-based build script that I'm trying to migrate to the Kotlin DSL. The `jar` task contains the typical line for copying all dependencies to the JAR file:\n\n\n\n```\nfrom { configurations.compile.collect { it.isDirectory() ? it : zipTree(it) } }\n\n```\n\nI can't find a way to translate this line to the Kotlin DSL.\n\n\nLet me give you some context. This is my original Groovy-based build script:\n\n\n\n```\nplugins {\n    id \"org.jetbrains.kotlin.jvm\" version \"1.3.41\"\n}\n\ngroup = \"com.rhubarb_lip_sync\"\nversion = \"1.0.0\"\n\nrepositories {\n    mavenCentral()\n    jcenter()\n}\n\ndependencies {\n    compile \"com.beust:klaxon:5.0.1\"\n    compile \"org.apache.commons:commons-lang3:3.9\"\n    compile \"no.tornado:tornadofx:1.7.19\"\n}\n\ncompileKotlin {\n    kotlinOptions.jvmTarget = \"1.8\"\n}\ncompileTestKotlin {\n    kotlinOptions.jvmTarget = \"1.8\"\n}\n\njar {\n    manifest {\n        attributes \"Main-Class\": \"com.rhubarb_lip_sync.rhubarb_for_spine.MainKt\"\n    }\n\n    \/\/ This line of code recursively collects and copies all of a project\"s files\n    \/\/ and adds them to the JAR itself. One can extend this task, to skip certain \n    \/\/ files or particular types at will\n    from { configurations.compile.collect { it.isDirectory() ? it : zipTree(it) } }\n}\n\n```\n\nAnd this is my Kotlin-based build script. It's working fine, except for that one line:\n\n\n\n```\nimport org.jetbrains.kotlin.gradle.tasks.KotlinCompile\n\nplugins {\n    kotlin(\"jvm\") version \"1.3.41\"\n}\n\ngroup = \"com.rhubarb_lip_sync\"\nversion = \"1.0.0\"\n\nrepositories {\n    mavenCentral()\n    jcenter()\n}\n\ndependencies {\n    implementation(kotlin(\"stdlib-jdk8\"))\n    implementation(\"com.beust:klaxon:5.0.1\")\n    implementation(\"org.apache.commons:commons-lang3:3.9\")\n    implementation(\"no.tornado:tornadofx:1.7.19\")\n}\n\ntasks.withType<KotlinCompile> {\n    kotlinOptions.jvmTarget = \"1.8\"\n}\n\ntasks.withType<Jar> {\n    manifest {\n        attributes(\"Main-Class\" to \"com.rhubarb_lip_sync.rhubarb_for_spine.MainKt\")\n    }\n\n    \/\/ ?\n}\n\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"`collect()` in groovy is `map()` in Kotlin. \n\n\nThe ternary operator of groovy can be transformed into an `if` in Kotlin. \n\n\nThe main difference is that configurations.compile in Kotlin is not a `Configuration` but a `Provider<Configuration>`. So either you `get` the configuration out of the Provider, or you stay lazy by `map`ping the Provider to another Provider. So I think it should work\n\n\n\n```\nfrom(configurations.compileClasspath.get().map { if (it.isDirectory()) it else zipTree(it) })\n\n```\n\nor\n\n\n\n```\nfrom(configurations.compileClasspath.map { config -> config.map { if (it.isDirectory) it else zipTree(it) } })\n\n```\n\nNote that `compile` is deprecated for a long time now. Since use `implementation` now to declare your dependencies, there's nothing anymore in the compile configuration, and you must get the dependencies out of the `compileClasspath` one to build your uber jar.\n\n\n"}
{"questionId":"17faa26794a04d719acd1777705aa4fa","question":"Check if JSON is Object or Array\nIs there a simple way in Go to check whether given JSON is either an Object `{}` or array `[]`?\n\n\nThe first thing that comes to mind is to `json.Unmarshal()` into an interface, and then see if it becomes a map, or a slice of maps. But that seems quite inefficient.\n\n\nCould I just check if the first byte is a `{` or a `[`? Or is there a better way of doing this that already exists.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"go"},"answer":"Use the following to detect if JSON text in the `[]byte` value `data` is an array or object:\n\n\n\n```\n \/\/ Get slice of data with optional leading whitespace removed.\n \/\/ See RFC 7159, Section 2 for the definition of JSON whitespace.\n x := bytes.TrimLeft(data, \" \\t\\r\\n\")\n\n isArray := len(x) > 0 && x[0] == '['\n isObject := len(x) > 0 && x[0] == '{'\n\n```\n\nThis snippet of code handles optional leading whitespace and is more efficient than unmarshalling the entire value.\n\n\nBecause the top-level value in JSON can also be a number, string, boolean or nil, it's possible that `isArray` and `isObject` both evaluate to false. The values `isArray` and `isObject` can also evaluate to false when the JSON is invalid.\n\n\n"}
{"questionId":"0371b06d66cf42beba839b80d8409f8b","question":"How can I remove a unique constraint from a database column in Rails?\nI created a table using the following migration: \n\n\n\n```\nclass CreateProfilePictures < ActiveRecord::Migration\n  def change\n    create_table :profile_pictures do |t|\n      t.integer :user_id, null: false\n      t.integer :picture_id, null: false\n      t.timestamps null: false\n    end\n\n    add_index :profile_pictures, :user_id, unique: true\n    add_index :profile_pictures, :picture_id, unique: true\n  end\nend\n\n```\n\nI tried to remove the constraint with the following: \n\n\n\n```\nclass FixProfilePic < ActiveRecord::Migration\n  def change\n    change_column :profile_pictures, :picture_id, :integer, unique: false\n  end\nend\n\n```\n\nI still get a unique constraint violation error if I try to use the same picture\\_id in more than one place. What is the proper way to remove the uniqueness constraint from picture\\_id?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"There is a problem with the accepted answer: Rollbacks don't work correctly as the unique index is not restored.\n\n\nYou could try this instead:\n\n\n\n```\nreversible do |dir|\n  dir.up do\n    remove_index :profile_pictures, :picture_id\n    add_index :profile_pictures, :picture_id\n  end\n\n  dir.down do\n    remove_index :profile_pictures, :picture_id\n    add_index :profile_pictures, :picture_id, unique: true\n  end\nend\n\n```\n\n"}
{"questionId":"2fb2d969a53c452596349fd2c28ec05c","question":"What functions is the libm intended for?\nAs far as I know some math functions are contained in libc, while others are in libm. I've discovered that experimentally:\n\n\n\n```\n$ nm --dynamic --defined-only \/lib\/x86_64-linux-gnu\/libm.so.6 | grep -w abs \n$ nm --dynamic --defined-only \/lib\/x86_64-linux-gnu\/libc.so.6 | grep -w abs \nT abs\n\n```\n\nIs there a requirement concerning which mathematical functions must be provided by libm? Does libc and libm together provide all the math functions required by C standard?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"Language standards such as ISO C and ISO C++ do not concern themselves with matters such as linking.\n\n\nPOSIX only requires that the `c99` compiler supports `-lm`, and that the functions declared in the headers `<math.h>`, `<complex.h>` and `<fenv.h>` are available for linking if `-lm` is specified. It is possible to meet this requirement if functions are defined in a library which is linked in by default.\n\n\nWith current glibc, the split of functions is mostly arbitrary, subject to a few limitations in the current implementation. (A long time ago, two threading libraries were supported, so all thread-related functionality had to go into `libpthread`, but this is no longer the case.) Other approaches are possible: musl puts everything into `libc.a` for static linking, and into the dynamic linker for dynamic linking.\n\n\n"}
{"questionId":"39d192aad4504059b66469e6d4fe87bf","question":"Is a C compiler allowed to coalesce sequential assignments to volatile variables?\nI'm having a theoretical (non-deterministic, hard to test, never happened in practice) hardware issue reported by hardware vendor where double-word write to certain memory ranges may corrupt any future bus transfers.\n\n\nWhile I don't have any double-word writes explicitly in C code, I'm worried the compiler is allowed (in current or future implementations) to coalesce multiple adjacent word assignments into a single double-word assignment.\n\n\nThe compiler is not allowed to reorder assignments of volatiles, but it is unclear (to me) whether coalescing counts as reordering. My gut says it is, but I've been corrected by language lawyers before!\n\n\nExample:\n\n\n\n```\ntypedef struct\n{\n   volatile unsigned reg0;\n   volatile unsigned reg1;\n} Module;\n\nvolatile Module* module = (volatile Module*)0xFF000000u;\n\n\/\/ two word stores, or one double-word store?\nmodule->reg0 = 1;\nmodule->reg1 = 2;\n\n```\n\n(I'll ask my compiler vendor about this separately, but I'm curious what the canonical\/community interpretation of the standard is.)\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"The behavior of `volatile` seems to be up to the implementation, partly because of a curious sentence which says: \"What constitutes an access to an object that has volatile-qualified type is implementation-defined\".\n\n\nIn ISO C 99, section 5.1.2.3, there is also:\n\n\n\n> \n> 3 In the abstract machine, all expressions are evaluated as specified by the semantics. An\n> actual implementation need not evaluate part of an expression if it can deduce that its\n> value is not used and that no needed side effects are produced (including any caused by\n> calling a function **or accessing a volatile object**).\n> \n> \n> \n\n\nSo although requirements are given that a `volatile` object must be treated in accordance with the abstract semantics (i.e not optimized), curiously, **the abstract semantics itself** allows for the elimination of dead code and data flows, which are examples of optimizations!\n\n\nI'm afraid that to know what `volatile` will and will not do, you have to go by your compiler's documentation.\n\n\n"}
{"questionId":"7b0fb98fbc404a53a36dfbdbe514afdd","question":"How to fix String field does not implement `Copy`?\nI am building a simple command-line todo app in Rust. If I don't implement the copy trait I get this error: \"move occurs because 'todo' has type 'todo::Todo', which does not implement the 'Copy' trait\". When I try to implement the Copy trait for my Todo struct, I receive the following error: \"field text: String does not implement the Copy trait\". How do I fix this error? My code is below:\n\n\n\n```\npub type todo_type = Vec<Todo>;\n\n#[derive(Copy)]\npub struct Todo {\n    id: usize,\n    text: String,\n    completed: bool,\n}\n\nimpl Todo {\n    pub fn new(text: String, id: usize) -> Todo {\n        Todo {\n            text,\n            id,\n            completed: false,\n        }\n    }\n}\n\npub struct Todos {\n    todos: todo_type,\n}\n\nimpl Todos {\n    pub fn new(todos: todo_type) -> Todos {\n        Todos { todos }\n    }\n\n    pub fn get_all_todos(self) -> todo_type {\n        self.todos\n    }\n\n    pub fn get_single_todo(self, todo_index: usize) -> Todo {\n        unimplemented!()\n    }\n\n    pub fn add_todo(self, text: String) -> Todo {\n        let id: usize = 1;\n\n        if self.todos.len() == 0 {\n            let id = 1;\n        } else {\n            let last_todo = match self.todos.len() {\n                0 => None,\n                n => Some(&self.todos[n - 1]),\n            };\n            let id = last_todo.unwrap().id;\n        }\n\n        let todo = Todo::new(text, id);\n        self.todos.push(todo);\n\n        todo\n    }\n\n    pub fn remove_todo(self, todo_index: usize) -> bool {\n        self.todos.remove(todo_index);\n\n        true\n    }\n}\n\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"rust"},"answer":"Here you need the Clone trait instead of Copy trait. The Copy trait indicates that the variable can be copied bit-for-bit exactly as is, and that the variables of such type do not underly to move semantics.\n\n\nSome limitations apply to implementations of Copy trait. Structs can implement the Copy trait only if none of their components implement the Drop trait. Since String implements the Drop trait, your struct can not implement the Copy trait.\n\n\nIf you are looking to make copies of your struct then you need the Clone trait.\n\n\n"}
{"questionId":"bd377e6e456b4cc5a87621c523186b54","question":"Convert a map into a keyword list in Elixir\nI have a map of the form:\n\n\n\n```\n%{\"browser_name\" => \"Chrome\", \"platform\" => \"linux\"}\n\n```\n\nand I need to convert it to a keyword list:\n\n\n\n```\n[browser_name: \"Chrome\", platform: \"linux\"]\n\n```\n\nWhat's the best way of achieving this?\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"elixir"},"answer":"Wouldn't this work:\n\n\n\n```\n def convert_to_klist(map) do\n   Enum.map(map, fn({key, value}) -> {String.to_existing_atom(key), value} end)\n end\n\n```\n\n"}
{"questionId":"d3f5973f8e35461f941fa41d0a46fb1a","question":"Shutdown or Reboot a WSL session from inside the WSL session\nI would like to be able to reboot WSL sessions. To do so is a little awkward as WSL does not use systemd so we cannot use `reboot`. Within a WSL session, we can run any Windows executable:\n\n\n\n```\nboss@Asus: ~ $ wsl.exe -l -v\n  NAME            STATE           VERSION\n* Ubuntu-20.04    Running         2\n  fedoraremix     Stopped         1\n  Alpine          Stopped         1\n  Ubuntu          Stopped         1\n\n```\n\nTherefore, we can use `wsl.exe` (you have to make sure to always add `.exe` when calling Windows commands or they will not work) to shutdown the currently running WSL session `wsl.exe -t Ubuntu-20.03`, but the problem is that I don't know the session name.\n\n\nWhen we are inside a WSL session, `hostname` is something different, and so I don't know how to find the name of the currently running session that I am inside (maybe a Windows process command that tells me what process I am running from??).\n\n\nIdeally, I would like a command to equate to a reboot. I guess this would have to look something like:\n\n\n- Run an asynchronous command that will initiate a new session 5-10 seconds in the future to allow the previous session to fully shutdown (and that will not terminate when this session is terminated).\n- Terminate the currently running session with `wsl.exe -t <my found name>`.\n- A few seconds later, the new session will start up.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"*Credits to the commenters above.*\n\n\n## To shutdown a session from within a WSL guest, you can run:\n\n\n\n```\nwsl.exe --terminate $WSL_DISTRO_NAME\n\n```\n\n**Rebooting** is also possible, however so far I do not know how to get a new terminal ***inside the same console window***. The following will reboot the WSL guest and open a new console window of it when it has finished:\n\n\n\n```\ncd \/mnt\/c\/ && cmd.exe \/c start \"rebooting WSL\" cmd \/c \"timeout 5 && wsl -d $WSL_DISTRO_NAME\" && wsl.exe --terminate $WSL_DISTRO_NAME\n\n```\n\n**Explanation**:\n\n\n- From the perspective of Windows, WSL systems are mounted as network resources. `cmd` does not support the resulting UNC path formats such as `\\\\wsl$\\Debian\\<...>`. Therefore it may be best to `cd` to a directory it can resolve as a Windows path instead, such as `C:\\`, before it is executed. If ommited, cmd will complain and change its directory to `cmd`'s `%windir%`.\n- `&&` runs another command after the previous one has finished in linux and windows `cmd`.\n- `cmd.exe \/c` starts a `cmd` instance and tells it to execute a command that follows.\n- `start \"<WindowTitle>\" ...` is a `cmd`-internal command to run another program inside its own window, independent of the `cmd` instance. In this case the program is *another* `cmd` window. **It will not be affected when WSL shuts down.**\n- In the original Linux-Terminal, the first `cmd \/c` command has finished, and the third command after `&&` shuts down the guest like above.\n- The second `cmd` window waits for a few seconds, then starts a new WSL session of the same WSL machine.\n\n\n## Creating an Alias\n\n\nYou can make this easier to use by creating an alias. For `bash` users, edit your `~\/.bashrc` file and apply the changes afterwards:\n\n\n\n```\nnano ~\/.bashrc && source ~\/.bashrc\n\n```\n\nAdd either or both of the lines below anywhere in the file.\n\n\nYou can of course choose any name you want. Both `shutdown` and `reboot` exist as systemd commands, but since they do not work in WSL machines, you can replace them with an alias as follows:\n\n\n\n```\nalias shutdown='wsl.exe --terminate $WSL_DISTRO_NAME'\nalias reboot='cd \/mnt\/c\/ && cmd.exe \/c start \"rebooting WSL\" cmd \/c \"timeout 5 && wsl -d $WSL_DISTRO_NAME\" && wsl.exe --terminate $WSL_DISTRO_NAME'\n\n```\n\n"}
{"questionId":"fb1605cd93144e47b81b078c5e34dd77","question":"How do I add a custom CHECK constraint on a MySQL table?\nI am having trouble with this table\n\n\n\n```\nCREATE TABLE `Participants` (\n  `meetid` int(11) NOT NULL,\n  `pid` varchar(15) NOT NULL,\n  `status` char(1) DEFAULT NULL,\n  PRIMARY KEY (`meetid`,`pid`),\n  CONSTRAINT `participants_ibfk_1` FOREIGN KEY (`meetid`) REFERENCES `Meetings` (`meetid`) ON DELETE CASCADE\n  CONSTRAINT `participants_ibfk_2` CHECK (status IN ('a','d','u'))\n  CONSTRAINT `participants_ibfk_3` CHECK (pid IN (SELECT name FROM Rooms) OR pid IN (SELECT userid FROM People))\n);\n\n```\n\nI want to have a foreign key constraint, and that works. Then, I also want to add a constraint to the attribute `status` so it can only take the values 'a', 'd' and 'u'. It is not possible for me to set the field as `Enum` or `set`.\n\n\nCan anyone tell me why this code does not work in MySQL?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"Starting with version 8.0.16, MySQL has added support for CHECK constraints:\n\n\n\n```\nALTER TABLE topic\nADD CONSTRAINT post_content_check\nCHECK (\n    CASE\n        WHEN DTYPE = 'Post'\n        THEN\n            CASE\n                WHEN content IS NOT NULL\n                THEN 1\n                ELSE 0\n            END\n        ELSE 1\n    END = 1\n);\n \nALTER TABLE topic\nADD CONSTRAINT announcement_validUntil_check\nCHECK (\n    CASE\n        WHEN DTYPE = 'Announcement'\n        THEN\n            CASE\n                WHEN validUntil IS NOT NULL\n                THEN 1\n                ELSE 0\n            END\n        ELSE 1\n    END = 1\n);\n\n```\n\nPreviously, this was only available using BEFORE INSERT and BEFORE UPDATE triggers:\n\n\n\n```\nCREATE\nTRIGGER post_content_check BEFORE INSERT\nON topic\nFOR EACH ROW\nBEGIN\n   IF NEW.DTYPE = 'Post'\n   THEN\n       IF NEW.content IS NULL\n       THEN\n           signal sqlstate '45000'\n           set message_text = 'Post content cannot be NULL';\n       END IF;\n   END IF;\nEND;\n \nCREATE\nTRIGGER post_content_update_check BEFORE UPDATE\nON topic\nFOR EACH ROW\nBEGIN\n   IF NEW.DTYPE = 'Post'\n   THEN\n       IF NEW.content IS NULL\n       THEN\n           signal sqlstate '45000'\n           set message_text = 'Post content cannot be NULL';\n       END IF;\n   END IF;\nEND;\n \nCREATE\nTRIGGER announcement_validUntil_check BEFORE INSERT\nON topic\nFOR EACH ROW\nBEGIN\n   IF NEW.DTYPE = 'Announcement'\n   THEN\n       IF NEW.validUntil IS NULL\n       THEN\n           signal sqlstate '45000'\n           set message_text = 'Announcement validUntil cannot be NULL';\n       END IF;\n   END IF;\nEND;\n \nCREATE\nTRIGGER announcement_validUntil_update_check BEFORE UPDATE\nON topic\nFOR EACH ROW\nBEGIN\n   IF NEW.DTYPE = 'Announcement'\n   THEN\n       IF NEW.validUntil IS NULL\n       THEN\n           signal sqlstate '45000'\n           set message_text = 'Announcement validUntil cannot be NULL';\n       END IF;\n   END IF;\nEND;\n\n```\n\n"}
{"questionId":"be81212d52bd4f77b036f5d1bdac416e","question":"I am unable to create a new virtualenv in ubuntu?\nSo, I installed virtualenv in ubuntu terminal. I installed using the following commands:\n\n\n\n```\nsudo apt install python3-virtualenv\npip install virtualenv\n\n```\n\nBut when I try creating a new virtualenv using:\n\n\n\n```\nvirtualenv -p python3 venv\n\n```\n\nI am getting the following error:\n\n\n\n```\nAttributeError: module 'virtualenv.create.via_global_ref.builtin.cpython.mac_os' has no attribute 'CPython2macOsArmFramework'\n\n```\n\nHow can I solve it?\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"python"},"answer":"You don't need to use `virtualenv`. You can use this:\n\n\n\n```\npython3 -m venv .\/some_env\n\n```\n\n"}
{"questionId":"d52f2cb1d5344ba2a3d828f8bce4333e","question":"Why does int i = 10; i \/ 0; compile, but 5 \/ 0 gives CS0020 - Division by constant zero?\nConsider the following snippet:\n\n\n\n```\nint i = 5 \/ 0;\n\n```\n\nThis gives compiler error *CS0020: Division by constant zero*, which is fine. However, the next snippet:\n\n\n\n```\nint i = 10;\ni = i \/ 0;\n\n```\n\nCompiles just fine.\n\n\nDoes someone know why? I see no reason why the compiler allows an integer variable to be divided by a zero integer constant.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"In the general case, the compiler has no reason to disallow division by zero (or any other number) at runtime.\n\n\nYour first example, though, is a *compile time* constant, i.e. it's calculated by the compiler and replaced with the result of the evaluation. This is what the compiler's complaining about as it rightly doesn't know what integer value to put in place of 5\/0.\n\n\n"}
{"questionId":"42fde842cc7a4215ad7fcaf2739654b4","question":"How to make APT assume yes and force yes for all installations in a Bash script\nI'm currently getting into Linux and want to write a Bash script which sets up a new machine just the way I want it to be.\n\n\nIn order to do that I want to install different things on it, etc.\n\n\nWhat I'm trying to achieve here is to have a setting at the top of the Bash script which will make APT accept all [y\/n] questions asked during the execution of the script.\n\n\nA question example I want to automatically accept:\n\n\n `After this operation, 1092 kB of additional disk space will be used. Do you want to continue? [Y\/n]`\n\n\nI just started creating the file, so here is what I have so far:\n\n\n\n```\n#!\/bin\/bash\n\n# Constants\n\n# Set APT to accept all [y\/n] questions\n>> some setting here <<\n\n# Update and upgrade APT\napt update;\napt full-upgrade;\n\n# Install terminator\napt install terminator\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"`apt` is meant to be used interactively. If you want to automate things, look at `apt-get`, and in particular its `-y` option:\n\n\n\n> \n> -y, --yes, --assume-yes\n> \n> \n> Automatic yes to prompts; assume \"yes\" as answer to all prompts and run non-interactively. If an undesirable\n> situation, such as changing a held package, trying to install an\n> unauthenticated package or removing an essential package occurs then\n> apt-get will abort. Configuration Item: APT::Get::Assume-Yes.\n> \n> \n> \n\n\nSee also `man apt-get` for many more options.\n\n\n"}
{"questionId":"e3464daedc2b48d0a121a713b570ca32","question":"CompletableFuture - Run multiple rest calls in parallel and get different result\nI have a rather common or unique requirement. For example, I have the following `AccountDetails` list:\n\n\n`List<AccountDetails>`\n\n\n\n```\nclass AccountDetails {\n    String bankAccountId;\n    String mortgageAccountId;\n    Integer noOfTrans;\n    String addressLine;\n    String externalLink;   \n}\n\n```\n\nAll the above fields, except `bankAccountId` are pulled from external REST service call.\nI want to call all the REST services in parallel and update each object in the list:\n\n\nSo, it looks like below:\n\n\nFor each `accountDetails`\n\n\n- Call mortgage REST service and update `martgageAccountId`field (REST returns MortgageInfo object)\n- Call transaction REST service and update `noOfTrans` field (REST returns `Transactions` object)\n- Call address REST service and update `addressLine` field (REST returns `Address` object)\n- Call link REST service and update `externalLink` field. (REST returns `Links` object)\n\n\nI want all the above calls in parallel, and for each `AcccountDetails` object in the list.\nIf there is an exception, I want do gracefully handle it. Note that each of the above REST service returns different custom object\n\n\nI am confused about how to achieve this with `CompletableFuture` chaining.\nNot sure `allOf` or `thenCombine` (which only takes two), or `thenCompose` should use and how to put all of these together.\n\n\nAny examples\/ideas?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"\n```\nAccountDetails accountDetails = new AccountDetails();\n\nCompletableFuture.allOf(\n                        CompletableFuture.\n                                supplyAsync(() -> \/\/CALL MORTAGE INFO REST, executor).\n                                thenAccept(x -> {\n                                    accountDetails.setMortgageAccountId(x.getReqdField())\n                                }).\n                                handle(\/\/HANDLE GRACEFULLY),\n                        CompletableFuture.\n                                supplyAsync(() -> \/\/CALL SOME OTHER REST, executor).\n                                thenAccept(x -> {\n                                    accountDetails.setNoOfTrans(x.getReqdField())\n                                }).\n                                handle(\/\/HANDLE GRACEFULLY),\n                        CompletableFuture.\n                                supplyAsync(() -> \/\/CALL SOME INFO REST, executor).\n                                thenAccept(x -> {\n                                    accountDetails.setAddressLine(x.getReqdField())\n                                }).\n                                handle(\/\/HANDLE GRACEFULLY),\n                        CompletableFuture.\n                                supplyAsync(() -> \/\/CALL SOME OTHER REST, executor).\n                                thenAccept(x -> {\n                                    accountDetails.setExternalLink(x.getReqdField())\n                                }).\n                                handle(\/\/HANDLE GRACEFULLY),\n                ).join();\n\n```\n\n"}
{"questionId":"20c37a6fd667495bbc8ef5d80a0dfb8f","question":"python sklearn get list of available hyper parameters for model\nI am using python with sklearn, and would like to get a list of available hyper parameters for a model, how can this be done? Thanks\n\n\nThis needs to happen before I initialize the model, when I try to use\n\n\n\n```\nmodel.get_params() \n\n```\n\nI get this\n\n\n\n```\nTypeError: get_params() missing 1 required positional argument: 'self'\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"This should do it: `estimator.get_params()` where `estimator` is the name of your model.\n\n\nTo use it on a model you can do the following:\n\n\n\n```\nreg = RandomForestRegressor()\nparams = reg.get_params()\n# do something...\nreg.set_params(params)\nreg.fit(X,  y)\n\n```\n\n**EDIT:**\n\n\nTo get the model hyperparameters before you instantiate the class:\n\n\n\n```\nimport inspect\nimport sklearn\n\nmodels = [sklearn.ensemble.RandomForestRegressor, sklearn.linear_model.LinearRegression]\n\nfor m in models:\n    hyperparams = inspect.getargspec(m.__init__).args\n    print(hyperparams) # Do something with them here\n\n```\n\nThe model hyperparameters are passed in to the constructor in `sklearn` so we can use the `inspect` model to see what constructor parameters are available, and thus the hyperparameters. You may need to filter out some arguments that aren't specific to the model such as `self` and `n_jobs`. \n\n\n"}
{"questionId":"e76ff7353f454d58b9f727f3f6651255","question":"\"No set method providing array access\" -- why does this happen in Kotlin?\nHere's the code. \n\n\n\n```\nval stack = Array(inputString.length + 1) { \"\" }\nvar current = 0\nfor ((i, char) in inputString.withIndex()) {\n    if (char == '(') {\n        current += 1\n        continue\n    } else if (char == ')') {\n        val enclosedString = stack[current]\n        stack[current - 1] = stack[current - 1] + enclosedString.reversed()\n        current -= 1\n        continue\n    } else {\n        stack[current] +=  char \/\/here's the compile time error \n    }\n}\n\n```\n\nI'm getting an error saying \"No set method providing array access\". I don't understand this.\n\n\nIf I change it to:\n\n\n\n```\nstack[current] = stack[current] + char\n\n```\n\nthings work fine. \n\n\nWhy is this happening?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"kotlin"},"answer":"The cause of the error is the incorrect assignment of a `Char` variable to an `Array<String>`, you need to convert the `Char` to `String` before, and that's what is happening in the statement \n\n\n\n```\nstack[current] = stack[current] + char\n\n```\n\nThe `+` function returns a new `String` concatenating with the string representation of the right side (i.e. it automatically call `toString` in the right operand). In other words, it is converting the `Char` variable `char` to `String` before the concatenation.\n\n\nYou can also convert it yourself.\n\n\n\n```\nstack[current] += char.toString()\n\n```\n\n"}
{"questionId":"dcadc803eae64931a6a9faab285c9738","question":"Unexpected printf output\nI just discovered very weird behavior from the C compiler. It's very simple code. I tried it in many online C compilers, but the result is always the same, which is driving me insane.\n\n\n\n```\n#include <stdio.h>\n\nint main()\n{\n    char Buffer[10] = \"0123456789\";\n    char ID[5] = \"abcde\";\n    printf(\"%s\",ID);\n\n    return 0;\n}\n\n```\n\nTake your time and try predict the result of the `printf` function. If you're a human like me, then I think the most obvious solution is \"abcde\", which is not correct! But if somehow you figured it out \"abcde0123456789\", then you're consuming electricity to live.\n\n\nHow, just how, is that possible? I'm only selecting the `ID` array to be printed, so WHY is the `Buffer` one printed with it too? It doesn't make sense. Even the `ID` array isn't big enough to fit all that data. I'm really losing my mind here.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c"},"answer":"The format specification `%s` expects a pointer to a string: sequence of characters terminated by the zero character `'\\0'`.\n\n\nHowever the both arrays\n\n\n\n```\nchar Buffer[10] = \"0123456789\";\nchar ID[5] = \"abcde\";\n\n```\n\ndo not contain strings. So the call of `printf` invokes undefined behavior.\n\n\nYou should write\n\n\n\n```\nchar Buffer[] = \"0123456789\";\nchar ID[] = \"abcde\";\n\n```\n\nor\n\n\n\n```\nchar Buffer[11] = \"0123456789\";\nchar ID[6] = \"abcde\";\n\n```\n\nPay attention to that string literals are stored as character arrays with addition zero character '\\0'.\n\n\nFor example this declaration\n\n\n\n```\nchar ID[] = \"abcde\";\n\n```\n\nin fact is equivalent to\n\n\n\n```\nchar ID[] = { 'a', 'b', 'c', 'd', 'e', '\\0' };\n\n```\n\nand this declaration\n\n\n\n```\nchar ID[5] = \"abcde\";\n\n```\n\nis equivalent to\n\n\n\n```\nchar ID[5] = { 'a', 'b', 'c', 'd', 'e' };\n\n```\n\nThat is in the last case the zero character `'\\0'` is not used as an initializer of the array `ID`.\n\n\nIf you want to output a character array that does not contain a string you can use the precision field as for example\n\n\n\n```\nprintf( \"%.5s\\n\", ID );\n\n```\n\nor\n\n\n\n```\nprintf( \"%.*s\\n\", 5, ID );\n\n```\n\nor\n\n\n\n```\nprintf( \"%.*s\\n\", ( int )sizeof( ID ), ID );\n\n```\n\nAlso bear in mind that opposite to C in C++ such a declaration like\n\n\n\n```\nchar ID[5] = \"abcde\";\n\n```\n\nis invalid. In C++ you may not ignore the terminating zero character `'\\0'` of a string literal used as an initializer. Otherwise the number of initializers will exceed the number of initialized array elements.\n\n\n"}
{"questionId":"f3deb96ae21d4ac28799b4827ba4f133","question":"Why do these two code snippets have the same effect?\n\n```\ntemplate <typename T1, typename T2>\nauto max (T1 a, T2 b) -> decltype(b<a?a:b);\n\n```\n\n\n```\ntemplate <typename T1, typename T2>\nauto max (T1 a, T2 b) -> decltype(true?a:b);\n\n```\n\nI do not understand why these two code snippets can have the same effect. Plz give me some hint and a underlying explanation.\n\n\nCheers.\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c++"},"answer":"Because the type returned by a ternary operator is decided according the types of the second and third arguments, not according the value of the first.\n\n\nYou can verify this with the following code\n\n\n\n```\n#include <type_traits>\n\nint main ()\n {\n   auto x = true ? 1 : 2l;\n\n   static_assert( std::is_same<decltype(x), long>::value, \"!\" );\n }\n\n```\n\nIsn't important that `true ? 1 : 2l` return ever `1`; the ternary operator return a common type between `1` (`int`) and `2l` (`long`). That is `long`.\n\n\nIn other words: there isn't (at the moment) a `constexpr` ternary operator.\n\n\n"}
{"questionId":"b159a334c45f4e3f92dd3cfd62f63f7a","question":"SwiftUI - If inside ForEach loop\nI am building a List based on my elements in an array I fetched before.\n\n\nI am fetching all the entities.. when the user makes a search in the search bar, I want to filter my List. I am NOT doing a new FetchRequest, I just want to filter my objects.\n\n\nThat is the code I am using at the moment:\n\n\n\n```\nList(selection: $selectedDocument)\n{\n    ForEach(self.documentItems, id: \\.self) { document in\n        HStack(spacing: 0)\n        {\n            if (self.checkSearchString(document: document))\n            {\n                ListRow(document: document).tag(document)\n            }\n    }\n\n```\n\nI am having a List, then my ForEach loop. In that loop, I want to decide if I show that element or not. The problem is, that even if I do not want to show the element, there is still a small view inside my List. I know **why**, it is because I still render that `HStack()`.\nI basically need to drag that `HStack()` inside my `If`, however that is not working for me. I think it is because I need to render a view inside my List. But how can I contiuue my `ForEach` without rendering something.\n\n\nThat is what I want to achieve, BUT it is not working:\n\n\n\n```\n List(selection: $selectedDocument)\n {\n     ForEach(self.documentItems, id: \\.self) { document in\n         if (self.checkSearchString(document: document))\n         {\n             HStack(spacing: 0)\n             {\n                 ListRow(document: document).tag(document)\n             }\n         }\n\n```\n\nThanks in advance!\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"swift"},"answer":"filter your data BEFORE passing it to ForEach constuctor.\n\n\n\n```\nForEach(self.documentItems.filter {self.checkSearchString(document: $0)}, id: \\.self) { document in\n    HStack(spacing: 0)\n        {\n            ListRow(document: document).tag(document)\n        }\n}\n\n```\n\n"}
{"questionId":"0d5580688fff495e8c3e4daf75de0b1c","question":"Give names to Key and Value in C# Dictionary to improve code readability\nIn C# struct, we can know clearly the purpose of a variable by it's name. For example,\n\n\n\n```\npublic struct Book\n{\n    public string title;\n    public string author;\n}\n\n```\n\nThen, i know b.title is a type of string and it's referring to title.\n\n\nHowever in C# dictionary, we can only specify the type\n\n\n\n```\nDictionary<string,string> d\n\n```\n\nHow can i make the code more readable such that the key of the dictionary is type of string **and it is referring to title**, and the value is type of string and it is referring to **author**? That means, other people can easily know that d[\"J.R.R. Tolkien\"] is a wrong use of the dictionary when reading the code.\n\n\n**EDIT**\n@mike z suggested to use a variable name **titleToAuthor** to help readability. But my real issue is that in the code there are nested dictionary. E.g. \n\n\n\n```\nDictionary<string, Dictionary<string, string>>, \nor even 3 levels   \nDictionary<string, Dictionary<string , Dictionary< string , string[] >>>. \n\n```\n\nWe want to keep to convenience of using Dictionary without creating our own class but at the same time we need some way to improve the readability\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"With .NET 6 and C# 10, you can now use `global using` directive to give type aliases in your project.\n\n\nGive built-in types their aliases in one single place, e.g. `GlobalUsings.cs`.\n\n\n\n```\nglobal using Title = System.String;\nglobal using Author = System.String;\n\n```\n\nThen use aliases for better readability in your dictionaries.\n\n\n\n```\nDictionary<string, Dictionary<Title, Author>>\n\n```\n\n"}
{"questionId":"851775c861ed474c827687806c6876be","question":"How to get a property by this name in String?\nI want to access to a property by the name si string format.\n\n\nIf I have a class like that:\n\n\n\n```\nclass PrefsState {\n  String a;\n\n  PrefsState({\n    this.a,\n\n  })\n\n```\n\nHow can I do something like that:\n\n\n\n```\nPrefsState test= PrefsState(a: \"it is a test\");\nString key = \"a\";\n\nprint(test[key]);\n\n\n```\n\nOf course is not working. There is a way to do that in Dart ?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"dart"},"answer":"Unfortunately, you cannot use reflection\/mirrors in flutter.\nWhat you can do, which is tedious, is use maps.\n\n\n\n```\nclass PrefsState { \n   String a; \n   const PrefsState({ this.a, });\n   dynamic getProp(String key) => <String, dynamic>{\n    'a' : a,\n    }[key];\n}\n\n```\n\nIt's probably better to build the map in the constructor, but if you want const constructors then you'll have to settle for this. Likely won't make much of a difference unless you have a million parameters anyway. Then you use it like so:\n\n\n\n```\nPrefsState test= PrefsState(a: \"it is a test\");\nString key = \"a\"; \nprint(test.getProp(key));\n\n```\n\nI don't think there is a less cumbersome way of doing this, but would love to be proven wrong :-)\n\n\n"}
{"questionId":"df191aa1090c4d48a7e7bb933357000c","question":"Clojure \"repeatedly\" makes \"future\" run sequentially\nWhile this snippet \n\n\n\n```\n(dorun \n  (map deref \n    (map #(future \n            (println % (Thread\/currentThread))) \n         (range 10))))\n\n```\n\nprints 10 intermixed lines showing different threads:\n\n\n\n```\n0 #object[java.lang.Thread 0x5f1b4a83 Thread[clojure-agent-send-off-pool-26,5,main]]                                                                                                                           \n2 #object[java.lang.Thread 1 0x79dfba1f #object[Thread[clojure-agent-send-off-pool-28,5,main]java.lang.Thread]                                                                                                 \n3 4 #object[java.lang.Thread #object[java.lang.Thread 0x7ef7224f Thread[clojure-agent-send-off-pool-27,5,main]0x5f1b4a83 ]Thread[clojure-agent-send-off-pool-26,5,main]]                                       \n5                                                                                                                                                                                                              \n67  #object[java.lang.Thread #object[0x79dfba1f java.lang.Thread Thread[clojure-agent-send-off-pool-28,5,main]]0x77526645                                                                                      \n 8 #object[java.lang.Thread #object[java.lang.ThreadThread[clojure-agent-send-off-pool-29,5,main] ]9 #object[java.lang.Thread 0xc143aa5 0x7ef7224f                                                             Thread[clojure-agent-send-off-pool-31,5,main]]Thread[clojure-agent-send-off-pool-27,5,main]]                                                                                                                       \n\n0x1ce8675f 0x379ae862 Thread[clojure-agent-send-off-pool-30,5,main]Thread[clojure-agent-send-off-pool-32,5,main]]]\n\n```\n\nas I would expect, the following snippet:\n\n\n\n```\n(dorun\n  (map deref \n    (map #(future \n            (println % (Thread\/currentThread))) \n         (repeatedly 10 #(identity 42)))))\n\n```\n\nproduces 10 neatly aligned strings with the same thread:\n\n\n\n```\n42 #object[java.lang.Thread 0x1e1b7ffb Thread[clojure-agent-send-off-pool-39,5,main]]                                                                                                                              \n42 #object[java.lang.Thread 0x1e1b7ffb Thread[clojure-agent-send-off-pool-39,5,main]]                                                                                                                              \n42 #object[java.lang.Thread 0x1e1b7ffb Thread[clojure-agent-send-off-pool-39,5,main]]                                                                                                                              \n42 #object[java.lang.Thread 0x1e1b7ffb Thread[clojure-agent-send-off-pool-39,5,main]]                                                                                                                              \n42 #object[java.lang.Thread 0x1e1b7ffb Thread[clojure-agent-send-off-pool-39,5,main]]                                                                                                                              \n42 #object[java.lang.Thread 0x1e1b7ffb Thread[clojure-agent-send-off-pool-39,5,main]]                                                                                                                              \n42 #object[java.lang.Thread 0x1e1b7ffb Thread[clojure-agent-send-off-pool-39,5,main]]                                                                                                                              \n42 #object[java.lang.Thread 0x1e1b7ffb Thread[clojure-agent-send-off-pool-39,5,main]]                                                                                                                              \n42 #object[java.lang.Thread 0x1e1b7ffb Thread[clojure-agent-send-off-pool-39,5,main]]                                                                                                                              \n42 #object[java.lang.Thread 0x1e1b7ffb Thread[clojure-agent-send-off-pool-39,5,main]]                          \n\n```\n\nwhich clearly indicates that the futures are not run in parallel, but each in the same thread.\n\n\nThis happens only with `repeatedly`, even if I realize the sequence with `doall` first, but vectors, `range`s or other sequences all result in parallel execution. \n\n\nWhy is future dispatching to the same thread when `repeatedly` is used?\n\n\nThanks!\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"clojure"},"answer":"This works:\n\n\n\n```\n(dorun (map deref (doall (map #(future (println % (Thread\/currentThread))) (repeatedly 10 #(identity 42))))))\n\n```\n\nThe problem is that `range` produces a **chunked** sequence while `repeatedly` produces an **unchunked** sequence. Map is lazy, so in the `repeatedly` case you're creating a future, then derefing it, then creating the next future, then dereffing it. In the `range` case the sequence is chunked so you're creating all futures and then `deref`ing all of them.\n\n\nHere's another fun way to observe the difference between the behaviour of chunked and unchunked sequences.\n\n\n\n```\n=> (first (map prn (range 10)))\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nnil\n=> (first (map prn (repeatedly 10 #(identity 13))))\n13\nnil\n\n```\n\nThe size of the chunks is usually 32 (but I think that's not guaranteed anywhere), as can be seen if you run `(first (map prn (range 1000)))`.\n\n\nChunking is one of those hidden features of Clojure that you usually learn when it first bites you :)\n\n\n"}
{"questionId":"2cd9363846c740f6bcf8bd3f21050c4a","question":"Jest throws TypeError: (0 , \\_module.myFunction) is not a function, testing exported functions (non default) with Jest\nSo I'm using Jest, Typescript and Create-react-app\n\n\nI have this test:\n\n\n\n```\nimport { defaultHeaders } from '.\/http';\n\ndescribe('http tests', () => {\n  test('defaultHeaders()', () => {\n    const headers = defaultHeaders(undefined);\n    expect(headers).toEqual({ foo: 2 });\n  });\n});\n\n```\n\nThe code is in same subdir in file `http.ts` and looks like this:\n\n\n\n```\nexport function defaultHeaders(headers?: FetchHeaders): FetchHeaders {\n  return { 'Content-Type': 'application\/json' };\n}\n\n```\n\nWhen I run my jest tests it throws:\n\n\n\n```\nTypeError: (0 , _http.defaultHeaders) is not a function\n\n```\n\nThe weird part is that all other tests that are wrapped in a default function or const do work.\n\n\n**Is there any way to test non default exported functions with Jest?**\n\n\n## update:\n\n\n- also tried converting defaultHeaders into an arrow function and that didn't help.\n- and tried changing import as: `import * as http from '.\/http'` that still makes it throw same error\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"# Update with Answer\n\n\nSo the issue was that in `src\/setupTest.ts` I included a mock of the module without the function I was trying to test therefore it was undefined.\n\n\n\n```\n\/\/ mock our fetch wrapper to avoid doing http calls in tests\njest.mock('utilities\/http\/http', () => ({\n  \/\/ NO defaultHeaders() function mocked that was the problem \n  getRequest: function () {\n    return {\n      json: {},\n      status: 200,\n    };\n  },\n  postRequest: function () {\n    return {\n      json: {},\n      status: 200,\n    };\n  },\n  request: function () {\n    return {\n      json: {},\n      status: 200,\n    };\n  },\n}));\n\n```\n\nIf I remove the mocked from setupTest.ts then I can unit test it.\n\n\n"}
{"questionId":"17ff43d6e8434fc3a3cb8d128ca556a2","question":"How to generate a production build of an API done with NESTJS\nI am generating the production version of an API I made using the NESTJS framework and would like to know which files I should upload to the server. When I run the \"npm run start: prod\" compile it generates the \"dist\" folder but I tried to run only with it but it is not enough to run my application. Do I need to upload all files to the server? I did several tests removing the folders I used during development but only managed to run in production mode when I was all the same in dev mode.\n\n\nI looked in the documentation for something about this but found nothing. can anybody help me?\n\n\nThank you\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"Honestly, you should only really need the dist folder as that's the JS 'complied' files. To run your application, commonly you'd use this command `node dist\/main.js`. As to what files you upload it's up to you. Me personally, I use a lot of continuous integration so I would just clone to repo into my container\/server and use `yarn start:prod`. This is so everytime I deploy I'm generating the required files to run in a production environment. \n\n\nLike @Kim Kern mentioned, some node modules are native built using node-gyro; so it's also always best to build your node\\_modules on the server\/container when deploying. Your deployment script should look something like this \n\n\n\n```\ngit clone git@github.com:myuser\/myrepo.git \/var\/www\/\ncd \/var\/www\/\nnode -v && \\\nyarn && \\\nyarn build && \\\nyarn start:prod\n\n```\n\nThe above script should \n\n\n1) pull the required repo into a 'hosted' directory  \n\n2) check the node version  \n\n3) install node\\_modules and build native scripts etc  \n\n4) build the production distribution  \n\n5) run the production JS scripts\n\n\nIf you look in your package.json file you'll notice the different scripts that are run when you use `yarn start`, `yarn start:dev` and `yarn start:prod`. When in dev you'll notice the use of `ts-node` which is a typescript node runner type thing (can't remember the correct phrase). Also the `start:dev` script uses nodemode to restart the `ts-node` script. You'll also see the `start:prod` script uses `node dist\/main.js` and that the `prestart:prod` script runs `rm -rf dist && tsc` which removes the dist folder and 'compiles' the javascript required for a production environment. \n\n\nHowever, the drawback of a typescript application on your server without continuous integration is that there is the possibility of typescript compilation errors which you wouldn't see or know about until running the prod scripts. I would recommend putting a procedure in place to compile the javascipt from typescript before making a deployment as you don't want to delete the current dist build before knowing the next release will build and run! \n\n\n"}
{"questionId":"1f28fc139da54f16961d595de0f1e080","question":"How to properly extend a styled component using TypeScript\nAs per styled-components v4, `.extend` is deprecated, the correct way to extend, or compose components is:\n\n\n\n```\nconst ButtonA = styled('button')`color: ${props => props.color};`\nconst ButtonB = styled(ButtonA)`background: 'white';`\n\n```\n\nHowever I can't find the correct way to do this with TS, as I get some errors, for example:\n\n\n\n```\nimport styled from \"styled-components\";\n\n\/\/ Let's create ButtonA\ntype ButtonAProps = { a: string };\nconst ButtonA = styled<ButtonAProps, \"button\">(\"button\")`\n  color: ${props => props.a};\n`;\n\n\/\/ So, here is what I've tried\n\n\/\/ Fail #1\n\/\/ =======\ntype ButtonBProps = { b: string };\nconst ButtonB = styled<ButtonBProps, ButtonAProps>(ButtonA)`\n  background: ${props => props.b};\n`; \/\/ Here I get autocompletion only for B props :(\nconst Test = () => <ButtonB a=\"something\" \/>; \/\/ And here I get autocompletion only for A props :(\n\n\/\/ Fail #2\n\/\/ =======\ntype ButtonBProps = { b: string } & ButtonAProps;\nconst ButtonB = styled<ButtonBProps, ButtonAProps>(ButtonA)`\n  background: ${props => props.b};\n`; \/\/  Here I get autocompletion for A & B props, good!\nconst Test = () => <ButtonB a=\"something\" \/>; \/\/ Here I still get autocompletion only for A props :(\n\n\/\/ Fail #3\n\/\/ =======\ntype ButtonBProps = { b: string } & ButtonAProps;\nconst ButtonB = styled<ButtonBProps, ButtonBProps>(ButtonA)` \/\/ Property 'b' is missing in type 'ButtonAProps', of course\n  background: ${props => props.b};\n`; \/\/  Here I get \"props has implicitly any type\"\nconst Test = () => <ButtonB \/>; \/\/ Here I don't get any type checking at all\n\n```\n\nSeems to be almost there, but can't figure it out.\n\n\nAny advices? Thank you!\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"As of September 2019, the following code also works:\n\n\n\n```\n\/\/ First extend ButtonAProps with an additional prop\ninterface ButtonBProps extends ButtonAProps {\n  b:string;\n}\n\n\/\/ ButtonB Component\nconst ButtonB = styled(ButtonA)<ButtonBProps>`\n  background: ${props => props.b};\n`;\n\n```\n\n"}
{"questionId":"dd50e4f76ecb4cdf8078e30b9e225768","question":"How to replace certain values in a specific rows and columns with NA in R?\nIn my data frame, I want to replace certain blank cells and cells with values with NA. But the cells I want to replace with NAs has nothing to do with the value that cell stores, but with the combination of row and column it is stored in.\n\n\nHere's a sample data frame DF:\n\n\n\n```\n  Fruits   Price   Weight   Number of pieces\n\n  Apples      20      2          10\n  Oranges     15      4          16\n  Pineapple   40      8           6\n  Avocado     60      5          20\n\n```\n\nI want to replace Pineapple'e weight to NA and Orange's number of pieces to NA. \n\n\n\n```\nDF$Weight[3] <- NA\nDF$`Number of pieces`[2] <- NA  \n\n```\n\nThis replaces any value that's stored in that position and that may change. I want to use specific row and column names to do this replacement so the position of value becomes irrelevant. \n\n\nOutput:\n\n\n\n```\n Fruits   Price   Weight   Number of pieces\n\n  Apples      20      2          10\n  Oranges     15      4          NA\n  Pineapple   40      NA           6\n  Avocado     60      5          20\n\n```\n\nBut if order of the table is changed, this would replace wrong values with NA. \n\n\nHow should I do this?\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"r"},"answer":"Since your data structure is 2 dimensional, you can find the indices of the rows containing a specific value first and then use this information.\n\n\n\n```\nwhich(DF$Fruits == \"Pineapple\")\n[1]  3\nDF$Weight[which(DF$Fruits == \"Pineapple\")] <- NA\n\n```\n\nYou should be aware of that `which` will return a vector, so if you have multiple fruits called \"Pineapple\" then the previous command will return all indices of them.\n\n\n"}
{"questionId":"1a8c40041fca43f6b337bcc40bdc4598","question":"Add newline to Oh My ZSH Theme\nI'm trying to add a newline to my existing Oh My ZSH theme but can't figure out what to add or where it should be added \/ changed. Any ideas?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"I think the proper place to change one's prompt is in the theme itself. On my system, it's in `~\/.oh=my-zsh\/themes\/agnoster.zsh-theme`. I added a `\\n\u279c` there:  \n\nFind this section:\n\n\n\n```\n# End the prompt, closing any open segments\n\nprompt_end() {\n  if [[ -n $CURRENT_BG ]]; then\n    echo -n \" % \n\n{%k%F{$CURRENT_BG}%}$SEGMENT_SEPARATOR\"\n  else\n    echo -n \"%{%k%}\"\n  fi\n  echo -n \"\\n\u279c%{%f%}\"\n  CURRENT_BG=''\n}\n\n```\n\n"}
{"questionId":"74499a4a3e734f01b6d5c315ef12df68","question":"How to fix \"Illuminate\\Database\\QueryException: SQLSTATE[HY000] [1044] Access denied for user\"\nI tried to run: `php artisan migrate`\n\n\nAlso to connect to MySQL using Xampp on Windows.\n\n\nI Got this error:\n\n\n\n```\nIlluminate\\Database\\QueryException  : SQLSTATE[HY000] [1044] Access\ndenied for user ''@'localhost' to database 'homestead' (SQL: select *\nfrom information_schema.tables where table_schema = homestead and\ntable_name = migrations)\n\n  at\nC:\\Users\\harsh\\Laravel1\\vendor\\laravel\\framework\\src\\Illuminate\\Database\\Connection.php:664\n    660|         \/\/ If an exception occurs when attempting to run a query, we'll format the error\n    661|         \/\/ message to include the bindings with SQL, which will make this exception a\n    662|         \/\/ lot more helpful to the developer instead of just the database's errors.\n    663|         catch (Exception $e) {\n  > 664|             throw new QueryException(\n    665|                 $query, $this->prepareBindings($bindings), $e\n    666|             );\n    667|         }\n    668|  Exception trace:\n\n  1   PDOException::(\"SQLSTATE[HY000] [1044] Access denied for user\n''@'localhost' to database 'homestead'\")\n      C:\\Users\\harsh\\Laravel1\\vendor\\laravel\\framework\\src\\Illuminate\\Database\\Connectors\\Connector.php:70\n\n  2  \nPDO::__construct(\"mysql:host=127.0.0.1;port=3306;dbname=homestead\",\n\"homestead\", \"\", [])\n      C:\\Users\\harsh\\Laravel1\\vendor\\laravel\\framework\\src\\Illuminate\\Database\\Connectors\\Connector.php:70\n\n  Please use the argument -v to see more details.\n\n```\n\n**.env file:**\n\n\n\n```\nDB_CONNECTION=mysql \nDB_HOST=127.0.0.1 \nDB_PORT=3306 \nDB_DATABASE=homestead \nDB_USERNAME=homestead \nDB_PASSWORD=\n\n```\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"php"},"answer":"Open the *`.env`* file and edit it. Just set up correct DB credentials: \n\n\n\n```\nDB_CONNECTION=mysql \nDB_HOST=127.0.0.1 \nDB_PORT=3306 \nDB_DATABASE=            \/\/ Your Database Name\nDB_USERNAME=           \/\/ Yout Database Username\nDB_PASSWORD=          \/\/ Your Database Password \n\n```\n\nThe *`DB_USERNAME`* should be set to *`root`* if you do not have a default **username** in the installation of MySQL in **xampp**.\n\n\nIf no password is set on the database, clear it *`DB_PASSWORD`*, **empty space must also be removed**(In the past I've also faceout this problem, window consider blank space as a password)\n\n\nAfter completion of *`.env`* edit please enter this command in your terminal for clear cache:\n\n\n\n```\nphp artisan config:cache\n\n```\n\n"}
{"questionId":"1241fe63567d4cd786c628b5ede68e21","question":"bash - nested EOF\nI\u00b4m trying to create a file using\n\n\n\n```\ncat - << EOF > file.sh\n\n```\n\nBut inside this, I want to write another EOF. Its hard to explain, so here an example:\n\n\n\n```\ncat - << EOF > file1.sh\necho first\ncat - << EOF > file2.sh\necho second\nEOF\necho again first\nEOF\n\n```\n\nBut of course, at line 5 it breaks. It does not create file1.sh with the content line 2-6, but with the content line 2-4.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"Just use a different delimiter on the outer cat, \"EOF\" isn't special in any way to the shell:\n\n\n\n```\ncat - << REALEND > file1.sh\necho first\ncat - << EOF > file2.sh\necho second\nEOF\necho again first\nREALEND\n\n```\n\nResults in this content in `file1.sh`\n\n\n\n```\necho first\ncat - << EOF > file2.sh\necho second\nEOF\necho again first\n\n```\n\n"}
{"questionId":"8f2212a8a85b43e0a23c9506cf6d4872","question":"Load markdown file on a jupyter notebook cell\nI know about the existance of the `%load markdown_file.md` magic command but this will load the content of the file on the first run of the cell.\nIf the file changes, the cell won't be updated.\nDoes anyone know if it is possible to avoid this problem and load the content of the file each time the cell runs?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"If you want to load a markdown every time a cell is run, you can do:\n\n\n\n```\nfrom IPython.display import Markdown, display\n\ndisplay(Markdown(\"markdown_file.md\"))\n\n```\n\n"}
{"questionId":"5b1910f87b034147bd432d726d15878a","question":"how to Abort\/exit\/stop\/terminate in laravel console command using code\nLet's say I'm coding a command. How would I stop it completely in the middle of it running?\n\n\nExample:\n\n\n\n```\npublic function handle()\n{\n    if (!$this->good_times) {\n        $this->error('Bad times');\n        $this->exit();\n    }\n\n    \/\/ continue command stuff\n}\n\n```\n\nI have tried:\n\n\n\n```\nthrow new RuntimeException('Bad times');\n\n```\n\nBut that dumps a bunch of ugliness in the terminal.\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"php"},"answer":"Just use a **`return`** statement instead of throwing an exception. Like...\n\n\n\n```\npublic function handle()\n{\n    if (!$this->good_times) {\n        $this->error('Bad times');\n        \/\/ $this->exit();\n        \/\/ throw new RuntimeException('Bad times');\n        return;\n    }\n\n    \/\/ ...\n}\n\n```\n\n"}
{"questionId":"6e7663b508ae4c72bdd43d183a591042","question":"MutableSharedFlow - difference between replay and extraBufferCapacity\nMutableSharedFlow takes 3 parameters: replay, extraBufferCapacity and onBufferOverflow. What is the difference between replay and extraBufferCapacity?\n\n\nThe documentation mentions the following:\n\n\n\n> \n> replay - the number of values replayed to new subscribers (cannot be negative, defaults to zero).\n> \n> \n> \n\n\n\n> \n> extraBufferCapacity - the number of values buffered in addition to replay. emit does not suspend while there is a buffer space remaining (optional, cannot be negative, defaults to zero).\n> \n> \n> \n\n\nI don't understand exactly the difference between the 2 and when we would need extraBufferCapacity > 0. Is extraBufferCapacity just an additional replay capacity for emitters?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"kotlin"},"answer":"\n> \n> Is extraBufferCapacity just an additional replay capacity for emitters?\n> \n> \n> \n\n\nThe \"replay\" terminology only really makes sense for subscribers, not emitters. The `replay` parameter defines how many past values new subscribers will receive upon subscribing. It obviously implies that those values need to be stored, so the overall buffer needs to be at least this big.\n\n\nHowever, the buffer size (as a whole) impacts emitters. The exact consequence of a full buffer depends on `onBufferOverflow`, but this buffer size can be used to control backpressure on emitters (slowing them down) or how we drop messages. With a larger buffer, you can allow emitters to have bursts of emissions without slowing them down, like any regular buffer.\n\n\nNow, choosing to have a larger buffer shouldn't force you to replay those buffered values to new subscribers, hence the `extraBufferCapacity`. With `extraBufferCapacity > 0`, you can define a buffer of any desired size without also forcing you to replay as many values, simply by using the formula:\n\n\n\n```\nbufferSize = replay + extraBufferCapacity\n\n```\n\nYou could for instance decide to replay no values at all to new subscribers, but still allow bursts from emitters by having *some* (non-replayed) buffer.\n\n\n"}
{"questionId":"e413441e1e0e4f44925a5287c307e1c7","question":"How can I make an object with an interface like a random number generator, but that actually generates a specified sequence?\nI'd like to construct an object that works like a random number generator, but generates numbers in a specified sequence.\n\n\n\n```\n# a random number generator\nrng = lambda : np.random.randint(2,20)\/\/2\n\n# a non-random number generator\ndef nrng():\n    numbers = np.arange(1,10.5,0.5)\n    for i in range(len(numbers)):\n        yield numbers[i]\n\nfor j in range(10):\n    print('random number', rng())\n    print('non-random number', nrng())\n\n```\n\nThe issue with the code above that I cannot call `nrng` in the last line because it is a generator. I know that the most straightforward way to rewrite the code above is to simply loop over the non-random numbers instead of defining the generator. I would prefer getting the example above to work because I am working with a large chunk of code that include a function that accepts a random number generator as an argument, and I would like to add the functionality to pass non-random number sequences without rewriting the entire code.\n\n\nEDIT: I see some confusion in the comments. I am aware that python's random number generators generate pseudo-random numbers. This post is about replacing a pseudo-random-number generator by a number generator that generates numbers from a **non-random, user-specified** sequence (e.g., a generator that generates the number sequence `1,1,2,2,1,0,1` if I want it to).\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"Edit:\n\n\nThe cleanest way to do this would be to use a lambda to wrap your call to `next(nrng)` as per great comment from @GACy20:\n\n\n\n```\ndef nrng_gen():\n    yield from range(10)\n\nnrng = nrng_gen()\n\nnrng_func = lambda: next(nrng)\n\nfor i in range(10):\n    print(nrng_func())\n\n```\n\nOriginal answer:\n\n\nIf you want your object to keep state and look like a function, create a custom class with `__call__` method.\n\n\neg.\n\n\n\n```\nclass NRNG:\n    def __init__(self):\n        self.numbers = range(10)\n        self.state = -1\n    def __call__(self):\n        self.state += 1\n        return self.numbers[self.state]\n        \nnrng = NRNG()\n\n\nfor i in range(10):\n    print(nrng())\n\n```\n\nHowever, I wouldn't recommend this unless absolutely necessary, as it obscures the fact that your nrng keeps a state (although technically, most rngs keep their state internally).\n\n\nIt's best to just use a regular generator with `yield` by calling next on it or to write a custom iterator (also class-based). Those will work with things like for loops and other python tools for iteration (like the excellent itertools package).\n\n\n"}
{"questionId":"abeccacc4675493da88107a64d5810b3","question":"Where to declare Kotlin extension functions in an Android app\nSuppose I have the following code that I want to make as a re-usable component:\n\n\n\n```\nfun <T> MutableList<T>.swap(index1: Int, index2: Int) {\n    val tmp = this[index1] \/\/ 'this' corresponds to the list\n    this[index1] = this[index2]\n    this[index2] = tmp\n}\n\n```\n\nand I want to use it anywhere in my app as follows:\n\n\n\n```\nval l = mutableListOf(1, 2, 3)\nl.swap(0, 2)\n\n```\n\nCorrect me if I'm wrong but I believe that function extension declarations can exist outside of a class. So in an Android app, where would I put this declaration? Or does it even matter? Will the compile just compile the code regardless where the extension is declared and make it re-usable globally or do I have to make it part of a class?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"kotlin"},"answer":"You can create some file, e.g. `ListExt.kt` anywhere you want (for example in package `main_package_name\/util\/ListExt.kt`) and place the extension function there and it will be available in the whole project. So the content of the `ListExt.kt` file can be:\n\n\n\n```\npackage main_package_name.util\n\nfun <T> MutableList<T>.swap(index1: Int, index2: Int) {\n    val tmp = this[index1] \/\/ 'this' corresponds to the list\n    this[index1] = this[index2]\n    this[index2] = tmp\n}\n\n\/\/ other extension functions on the Lists\n\n```\n\n"}
{"questionId":"5a378ca7c3f940aba860f549f0eb42a9","question":"How to perform feature selection with gridsearchcv in sklearn in python\nI am using `recursive feature elimination with cross validation (rfecv)` as a feature selector for `randomforest classifier` as follows. \n\n\n\n```\nX = df[[my_features]] #all my features\ny = df['gold_standard'] #labels\n\nclf = RandomForestClassifier(random_state = 42, class_weight=\"balanced\")\nrfecv = RFECV(estimator=clf, step=1, cv=StratifiedKFold(10), scoring='roc_auc')\nrfecv.fit(X,y)\n\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)\nfeatures=list(X.columns[rfecv.support_])\n\n```\n\nI am also performing `GridSearchCV` as follows to tune the hyperparameters of `RandomForestClassifier` as follows.\n\n\n\n```\nX = df[[my_features]] #all my features\ny = df['gold_standard'] #labels\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nrfc = RandomForestClassifier(random_state=42, class_weight = 'balanced')\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\nk_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= k_fold, scoring = 'roc_auc')\nCV_rfc.fit(x_train, y_train)\nprint(CV_rfc.best_params_)\nprint(CV_rfc.best_score_)\nprint(CV_rfc.best_estimator_)\n\npred = CV_rfc.predict_proba(x_test)[:,1]\nprint(roc_auc_score(y_test, pred))\n\n```\n\nHowever, I am not clear how to merge feature selection (`rfecv`) with `GridSearchCV`.\n\n\n**EDIT:** \n\n\nWhen I run the answer suggested by @Gambit I got the following error: \n\n\n\n```\nValueError: Invalid parameter criterion for estimator RFECV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n   estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n            criterion='gini', max_depth=None, max_features='auto',\n            max_leaf_nodes=None, min_impurity_decrease=0.0,\n            min_impurity_split=None, min_samples_leaf=1,\n            min_samples_split=2, min_weight_fraction_leaf=0.0,\n            n_estimators='warn', n_jobs=None, oob_score=False,\n            random_state=42, verbose=0, warm_start=False),\n   min_features_to_select=1, n_jobs=None, scoring='roc_auc', step=1,\n   verbose=0). Check the list of available parameters with `estimator.get_params().keys()`.\n\n```\n\nI could resolve the above issue by using `estimator__` in the `param_grid` parameter list.\n\n\n\n\n---\n\n\nMy question now is **How to use the selected features and parameters in `x_test` to verify if the model works fine with unseen data. How can I obtain the `best features` and train it with the `optimal hyperparameters`?**\n\n\nI am happy to provide more details if needed.\n\n\n","questionMetadata":{"type":"implementation","level":"advanced","tag":"python"},"answer":"Basically you want to fine tune the hyper parameter of your classifier (with Cross validation) after feature selection using recursive feature elimination (with Cross validation).\n\n\nPipeline object is exactly meant for this purpose of assembling the data transformation and applying estimator.\n\n\nMay be you could use a different model (`GradientBoostingClassifier`, etc. ) for your final classification. It would be possible with the following approach:\n\n\n\n```\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, \n                                                    random_state=42)\n\n\nfrom sklearn.pipeline import Pipeline\n\n#this is the classifier used for feature selection\nclf_featr_sele = RandomForestClassifier(n_estimators=30, \n                                        random_state=42,\n                                        class_weight=\"balanced\") \nrfecv = RFECV(estimator=clf_featr_sele, \n              step=1, \n              cv=5, \n              scoring = 'roc_auc')\n\n#you can have different classifier for your final classifier\nclf = RandomForestClassifier(n_estimators=10, \n                             random_state=42,\n                             class_weight=\"balanced\") \nCV_rfc = GridSearchCV(clf, \n                      param_grid={'max_depth':[2,3]},\n                      cv= 5, scoring = 'roc_auc')\n\npipeline  = Pipeline([('feature_sele',rfecv),\n                      ('clf_cv',CV_rfc)])\n\npipeline.fit(X_train, y_train)\npipeline.predict(X_test)\n\n```\n\nNow, you can apply this pipeline (Including feature selection) for test data.\n\n\n"}
{"questionId":"a548f6acdff34e209365bc44bed8568f","question":"Equivalent of ON CONFLICT DO NOTHING for UPDATE postgres\nI want to update rows in my postgres database if the updated version wouldn't violate the primary key constraint. If it would, I want to leave the row as it is.\n\n\nAssuming the table has primary keys on `col1, col2, col3`, if I run a query like this:\n\n\n\n```\nUPDATE table SET (col1, col2) = ('A', 'B') \n      WHERE col1='D' AND col2='E';\n\n```\n\nThe query will fail and I will get a duplicate key error if there exists two entries:\n\n\n\n```\n'A', 'B', 'C'\n'D', 'E', 'C'\n\n```\n\ni.e `col3` is the same between an existing row and a row to be updated.\n\n\nIf I was `INSERT`ing rows I would use `ON CONFLICT DO NOTHING` but I can't find an implementation of this for `UPDATE`. Does an equivalent exist?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"sql"},"answer":"AFAIK, there is no such equivalent. \n\n\nLet us say you are developing an application that connects to a postgresql database, there are a few things you need to keep in mind, in the context of your question:\n\n\n- It may be counter-intuitive but you should consider errors being thrown by the DB as a **good** thing.  \n\nThis is just about getting a status, it does not mean application crash.\n- For insert, there is an alternative choice of action `on conflict` (update or nothing) so it makes sense to have a syntax to let you decide.  \n\nFor updates, the only thing you can do is ... nothing.  \n\nSo why would SQL let you ask to do something specific since there is no choice? Remember that DB reporting errors is **good**, so let the DB do nothing and tell you why.\n- Last, it is a bad practice to update primary keys.  \n\nThe `ON CONFLICT ...` for inserts is not intended to update the primary key fields. The very opposite in fact: it is intended to update all the fields **except the ones from the primary key** in a single record.\n\n\nWhile I am on that point, please note that there was no need for a conflict on primary key for the query to fail  \n\n1 record with the \"convenient\" `ON UPDATE NO ACTION` foreign key would have made it fail too (which is still better than updating 10M+ records in 50 tables with a `ON UPDATE CASCADE` ...). BTW, did you know Oracle does not even have the `ON UPDATE CASCADE` clause? What do you think is the reason for that?\n\n\n\n\n---\n\n\nWhat can you\/should not do in that situation?\n\n\n1. **Do not update the primary key**, like I said. Your question is still valid for `UNIQUE` constraints but please please please, NEVER update primary keys.\n2. **Do not attempt to see if a conflicting record already exists**. It may take a long time and still be unreliable.  \n\nDo you really want to select millions of records just to avoid the error codes?  \n\nAlso, when you extend to other constraints (`CHECK` or `EXCLUSION`), will you really type the additional code it takes with no error in order to, once again, only avoid an error code?  \n\nLast, if you have implemented row-level security, the conflict may arise from a record you cannot see.\n3. **Handle the error code in your app**. Receiving status is **GOOD**.\n4. **Use save points if you are in the middle of a transaction**.  \n\nThis is the only annoying thing with DB errors: if you get one in the middle of a transaction, you will start getting `current transaction is aborted, commands ignored until end of transaction block` for everything.  \n\nHopefully, you do not need to roll the entire transaction back and redo everything from scratch. You can get away using the following piece of code.\n\n\nHere you go: \n\n\n\n```\nBEGIN;\nSAVEPOINT MySavepoint;\nUPDATE mytable set myuniquefield = 3; \/*2+ records are going to be updated *\/\nrollback to savepoint MySavepoint;\n\/*Insert Some more queries here*\/\nCOMMIT;\n\n```\n\n"}
{"questionId":"e4ed05f6161a4732bb1ca2619e45fb28","question":"How to find, stop and disable a Windows service using Powershell\nI am trying to find a service, stop it and then disable it remotely using Powershell. It can find and stop but cannot disable. For disabling, I have to run the `Set-Service` command separately. Can it be done in one line?\n\n\nThe following code-snippet will stop the **Print Spooler** service, but will not disable it:\n\n\n\n```\n$ip = \"10.10.10.10\"\n$svc_name = \"Spooler\"\nget-service -ComputerName $ip | Where-Object {$_.Name -eq $svc_name} |  Stop-Service | Set-Service -StartupType  Disabled\n\n```\n\nThe following code-snippet will stop and disable the **Print Spooler** service:\n\n\n\n```\n$ip = \"10.10.10.10\"\n$svc_name = \"Spooler\"\nget-service -ComputerName $ip | Where-Object {$_.Name -eq $svc_name} |  Stop-Service\nSet-Service $svc_name -StartupType  Disabled\n\n```\n\nPowershell version is `5.1.14393.2969`.\n\n\n**Edit:**\nThe following line will also find and disable. So, it looks like I can give two instructions with pipeline.\n\n\n\n```\nget-service -ComputerName $ip | Where-Object {$_.Name -eq $svc_name} | Set-Service -StartupType  Disabled\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"You need to use `Set-Service` to set the startup type, as outlined in your question:\n\n\n\n```\nSet-Service -StartupType Disabled $svc_name\n\n```\n\nIf you want to do it in \"one line\", you can use the `-PassThru` argument on `Stop-Service` to return the service object which can then be sent down the pipeline (you also don't need a `Where-Object` clause, `Get-Service` can filter on service name as well):\n\n\n\n```\nGet-Service -ComputerName $ip $svc_name | Stop-Service -PassThru | Set-Service -StartupType Disabled\n\n```\n\nYou had this close in your original question, but it didn't work because you didn't use the  \n`-PassThru` parameter on `Stop-Service`. As a note, many cmdlets that don't return an object by default do include a `-PassThru` parameter to return an object that can further processed if necessary, this isn't limited to `Stop-Service` by any means.\n\n\n"}
{"questionId":"32b80da772c640eda2129bd53b26e776","question":"C# 9 records validation\nWith the new record type of C# 9, how is it possible to **inject a custom parameter validation**\/ null check\/ etc during the construction of the object **without having to re-write the entire constructor**?\n\n\nSomething similar to this:\n\n\n\n```\nrecord Person(Guid Id, string FirstName, string LastName, int Age)\n{\n    override void Validate()\n    {\n        if(FirstName == null)\n            throw new ArgumentException(\"Argument cannot be null.\", nameof(FirstName));\n        if(LastName == null)\n            throw new ArgumentException(\"Argument cannot be null.\", nameof(LastName));\n        if(Age < 0)\n            throw new ArgumentException(\"Argument cannot be negative.\", nameof(Age));\n    }\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"I'm late to the party, but this might still help someone...\n\n\nThere's actually a simple solution (**but please read the warning below before using it**). Define a base record type like this:\n\n\n\n```\npublic abstract record RecordWithValidation\n{\n    protected RecordWithValidation()\n    {\n        Validate();\n    }\n\n    protected virtual void Validate()\n    {\n    }\n}\n\n```\n\nAnd make your actual record inherit `RecordWithValidation` and override `Validate`:\n\n\n\n```\nrecord Person(Guid Id, string FirstName, string LastName, int Age) : RecordWithValidation\n{\n    protected override void Validate()\n    {\n        if (FirstName == null)\n            throw new ArgumentException(\"Argument cannot be null.\", nameof(FirstName));\n        if (LastName == null)\n            throw new ArgumentException(\"Argument cannot be null.\", nameof(LastName));\n        if (Age < 0)\n            throw new ArgumentException(\"Argument cannot be negative.\", nameof(Age));\n    }\n}\n\n```\n\nAs you can see, it's almost exactly the OP's code. It's simple, and it works.\n\n\n**However**, be very careful if you use this: it will only work with properties defined with the \"positional record\" syntax (a.k.a. \"primary constructor\").\n\n\nThe reason for this is that I'm doing something \"bad\" here: I'm calling a virtual method from the base type's constructor. This is usually discouraged, because the base type's constructor runs before the derived type's constructor, so the derived type might not be fully initialized, so the overridden method might not work correctly.\n\n\nBut for positional records, things don't happen in that order: positional properties are initialized first, *then* the base type's constructor is called. So when the `Validate` method is called, the properties are already initialized, so it works as expected.\n\n\nIf you were to change the `Person` record to have an explicit constructor (or init-only properties and no constructor), the call to `Validate` would happen before the properties are set, so it would fail.\n\n\nEDIT: another annoying limitation of this approach is that it won't work with `with` (e.g. `person with { Age = 42 }`). This uses a different (generated) constructor, which doesn't call `Validate`...\n\n\n"}
{"questionId":"c969440218dd4d308876eb2b65492d48","question":"In Haskell, Can Kinds Be Anything Other Than a Sequence of Stars?\nPlease forgive me if this question is dumb.\n\n\nWhile reading about Haskell kinds, I notice a theme:\n\n\n\n```\n*\n* -> *\n* -> * -> *\n\n```\n\nI get the impression that kinds in Haskell ultimately boil down to how many asterisks there are. You might say that the kind of a type is really just the number of types you need to apply to it before it becomes \\*. In other words, you could count all \\*'s but the last one, and define a type's kind by an integer. Say 0, 1, 2, etc.\n\n\nHere's my question:\n\n\nIs this a correct observation about Haskell's type system? Or does it allow something other than \\*'s to go where you typically see \\*'s? For example:\n\n\n\n```\n* -> a -> *\n\n```\n\nI imagine someone might want to do this to constrain type variables to have an instance of a type class, for example.\n\n\n\n```\nFunctor a, Applicative b => * -> a -> b -> *\n\n```\n\nIs that a thing?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"haskell"},"answer":"The most basic form of the kind language contains only `*` (or `Type` in more modern Haskell; I suspect we'll eventually move away from `*`) and `->`.\n\n\nBut there are more things you can build with that language than you can express by just \"counting the number of `*`s\". It's not just the number of `*` or `->` that matter, but how they are nested. For example `* -> * -> *` is the kind of things that take two type arguments to produce a type, but `(* -> *) -> *` is the kind of things that take a single argumemt to produce a type where that argument itself must be a thing that takes a type argument to produce a type. `data ThreeStars a b = Cons a b` makes a type constructor with kind `* -> * -> *`, while `data AlsoThreeStars f = AlsoCons (f Integer)` makes a type constructor with kind `(* -> *) -> *`.\n\n\nThere are several language extensions that add more features to the kind language.\n\n\n`PolyKinds` adds kind *variables* that work exactly the same way type variables work. Now we can have kinds like `forall k. (* -> k) -> k`.\n\n\n`ConstraintKinds` makes constraints (the stuff to the left of the `=>` in type signatures, like `Eq a`) become ordinary type-level entities in a new kind: `Constraint`. Rather than the stuff left of the `=>` being special purpose syntax fairly disconnected from the rest of the language, now what is acceptable there is anything with kind `Constraint`. Classes like `Eq` become type constructors with kind `* -> Constraint`; you apply it to a type like `Eq Bool` to produce a `Constraint`. The advantage is now we can use all of the language features for manipulating type-level entities to manipulate constraints (including `PolyKinds`!).\n\n\n`DataKinds` adds the ability to create new user-defined kinds containing new type-level things, in exactly the same way that in vanilla Haskell we can create new user-defined types containing new term-level things. (**Exactly** the same way; the way `DataKinds` actually works is that it lets you use a `data` declaration as normal and then you can use the resulting type constructor at either the type or the kind level)\n\n\nThere are also kinds used for unboxed\/unlifted types, which must not be ever mixed with \"normal\" Haskell types because they have a different memory layout; they can't contain thunks to implement lazy evaluation, so the runtime has to know never to try to \"enter\" them as a code pointer, or look for additional header bits, etc. They need to be kept separate at the kind level so that ordinary type variables of kind `*` can't be instantiated with these unlifted\/unboxed types (which would allow you to pass these types that need special handling to generic code that doesn't know to provide the special handling). I'm vaguely aware of this stuff but have never actually had to use it, so I won't add any more so I don't get anything wrong. (Anyone who knows what they're talking about enough to write a brief summary paragraph here, please feel free to edit the answer)\n\n\nThere are probably some others I'm forgetting. But certainly the kind language is richer than the OP is imagining just with the basic Haskell features, and there is much more to it once you turn on a few (quite widely used) extensions.\n\n\n"}
{"questionId":"7587b73ec2874514845d6710ebf44700","question":"How to convert DataFrame.append() to pandas.concat()?\nIn pandas 1.4.0: `append()` was deprecated, and the docs say to use `concat()` instead.\n\n\n\n> \n> FutureWarning: The frame.append method is deprecated and will be\n> removed from pandas in a future version. Use pandas.concat instead.\n> \n> \n> \n\n\n**Codeblock in question:**\n\n\n\n```\ndef generate_features(data, num_samples, mask):\n    \"\"\"\n    The main function for generating features to train or evaluate on.\n    Returns a pd.DataFrame()\n    \"\"\"\n    logger.debug(\"Generating features, number of samples\", num_samples)\n    features = pd.DataFrame()\n\n    for count in range(num_samples):\n        row, col = get_pixel_within_mask(data, mask)\n        input_vars = get_pixel_data(data, row, col)\n        features = features.append(input_vars)\n        print_progress(count, num_samples)\n\n    return features\n\n```\n\n**These are the two options I've tried, but did not work:**\n\n\n\n```\nfeatures = pd.concat([features],[input_vars])\n\n```\n\nand\n\n\n\n```\npd.concat([features],[input_vars])\n\n```\n\n**This is the line that is deprecated and throwing the error:**\n\n\n\n```\nfeatures = features.append(input_vars)\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"You can store the DataFrames generated in the loop in a list and concatenate them with `features` once you finish the loop.\n\n\nIn other words, replace the loop:\n\n\n\n```\nfor count in range(num_samples):\n    # .... code to produce `input_vars`\n    features = features.append(input_vars)        # remove this `DataFrame.append`\n\n```\n\nwith the one below:\n\n\n\n```\ntmp = []                                  # initialize list\nfor count in range(num_samples):\n    # .... code to produce `input_vars`\n    tmp.append(input_vars)                        # append to the list, (not DF)\nfeatures = pd.concat(tmp)                         # concatenate after loop\n\n```\n\nYou can certainly concatenate in the loop but it's more efficient to do it only once.\n\n\n"}
{"questionId":"e836ace0e82c405c907599ea80b36cb3","question":"predefined type System.Range is not defined or imported\nI'm using C# 8.0 (beta) in my UWP app with Visual Studio 2019 v16.0.2.\nI was trying to use the new range operator of C# 8 : `str[start..index]` and then two errors showed up:\n\n\n\n> \n> Predefined type `System.Range` is not defined or imported\n> \n> \n> \n\n\n\n> \n> Predefined type `System.Index` is not defined or imported\n> \n> \n> \n\n\nBut there is no such reference that I could use.\n\n\nVisual Studio itself suggested me to use this feature. Is that the feature hasn't made available yet or what?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"This is a part of .NET Core 3 which is not released yet. \n\n\n"}
{"questionId":"6219753c89b94fbfb21d5f53a808c8e0","question":"SwiftUI .onAppear, only running once\nIn a `SwiftUI` app I have code like this:\n\n\n\n```\nvar body: some View {\n    VStack {\n        Spacer()\n        ........\n    }\n    .onAppear {\n      .... I want to have some code here ....\n      .... to run when the view appears ....\n    }\n}\n\n```\n\nMy problem is that I would like to run some code inside the *.onAppear* block, so that it gets run when the app appears on screen, after launching or after being in the background for a while. But it seems like this code is only run once at app launch, and never after. Am I missing something? Or should I use a different strategy to get the result I want?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"swift"},"answer":"You would have to observe the event when the app is entering foreground and publish it using `@Published` to the `ContentView`. Here's how:\n\n\n\n```\nstruct ContentView: View {\n\n    @ObservedObject var observer = Observer()\n\n    var body: some View {\n        VStack {\n            Spacer()\n            \/\/...\n        }\n        .onReceive(self.observer.$enteredForeground) { _ in\n            print(\"App entered foreground!\") \/\/ do stuff here\n        }\n    }\n}\n\nclass Observer: ObservableObject {\n\n    @Published var enteredForeground = true\n\n    init() {\n        if #available(iOS 13.0, *) {\n            NotificationCenter.default.addObserver(self, selector: #selector(willEnterForeground), name: UIScene.willEnterForegroundNotification, object: nil)\n        } else {\n            NotificationCenter.default.addObserver(self, selector: #selector(willEnterForeground), name: UIApplication.willEnterForegroundNotification, object: nil)\n        }\n    }\n\n    @objc func willEnterForeground() {\n        enteredForeground.toggle()\n    }\n\n    deinit {\n        NotificationCenter.default.removeObserver(self)\n    }\n}\n\n```\n\n"}
{"questionId":"7d1fa326d4c04b328741bb4a07ac3520","question":"How to wrap struct initializer in clang-format?\nTake this example before clang-format runs:\n\n\n\n```\nstruct ApplicationState app_state = {\n    .signal = {\n        .use_crash_handler = true,\n        .use_abort_handler = true,\n    },\n    .exit_code_on_error = {\n        .python = 0,\n    }\n};\n\n```\n\nAfter running, clang-format applies as follows:\n\n\n\n```\nstruct ApplicationState app_state = {.signal =\n                                             {\n                                                     .use_crash_handler = true,\n                                                     .use_abort_handler = true,\n                                             },\n                                     .exit_code_on_error = {\n                                             .python = 0,\n                                     }};\n\n```\n\nIs there a way to add a newline after the brace, before the struct member so it's more like the first example and not like the second?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c"},"answer":"Currently `clang-format` doesn't have a useful way of controlling this *(as of version 14.0)*.\n\n\nWhile `BreakBeforeBinaryOperators: All` does force wrapping (see @eric-backus' s answer), it impacts formatting in many other places too, unrelated to struct declaration.\n\n\nYou *can* however, workaround this simply by using a trailing comma.\n\n\n\n\n---\n\n\nBefore:\n\n\n\n```\nstruct ApplicationState app_state = {.signal =\n                                             {\n                                                     .use_crash_handler = true,\n                                                     .use_abort_handler = true,\n                                             },\n                                     .exit_code_on_error = {\n                                             .python = 0,\n                                     }};\n\n```\n\nAfter:\n\n\n\n```\n\nstruct ApplicationState app_state = {\n    .signal = {\n        .use_crash_handler = true,\n        .use_abort_handler = true,\n    },\n    .exit_code_on_error = {\n        .python = 0,\n    },\n};\n\/*   ^ notice trailing comma on the second last line! *\/\n\n```\n\n"}
{"questionId":"8de380883d334b03b6fa325e6f0c195f","question":"Blazor Navigation: Update URL without changing reloading page\nI use URL parameters for page state in my app.\n\n\nHow can i change the URL without actually navigating?\n\n\nThanks!\n\n\n(using blazor server side)\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"You can do this with JS Interop and call `history.pushState(null, '', url)`\n\n\nHere is a simple example\n\n\n**.razor**\n\n\n\n```\n@page \"\/\"\n@inject IJSRuntime jsRuntime\n\n<input\n    @bind=\"url\"\n\/>\n\n<button @onclick=\"ChangeUrl\">\n    Change Url\n<\/button>\n\n<p>@url<\/p>\n\n@code {\n    [Parameter]\n    public string Url { get; set; }\n\n    void ChangeUrl(){\n        \/\/ You can also change it to any url you want\n        jsRuntime.InvokeVoidAsync(\"ChangeUrl\", Url);\n    }\n}\n\n```\n\n**.js**\n\n\n\n```\nwindow.ChangeUrl = function(url){\n    history.pushState(null, '', url);   \n}\n\n```\n\n**Please notice** that this only works for visual purpose, it will only change for the browser while in the server side, you probably won't see the change.\n\n\n"}
{"questionId":"c88eede4f8f249ccaf8c314d0f4a16e9","question":"rm -f equivalent for PowerShell that ignore nonexistent files\n## Background\n\n\nI have a PowerShell script that writes some results into a file.\n\n\n- I want to remove the result file automatically at the start of the script with `Remove-Item`.\n- You can remove the result file manually, so I don't want to show error messages even if the result file doesn't exist.\n- I want to show error messages when the script couldn't remove the result file for another reason, e.g. the file is locked.\n\n\nYou can fulfill all the requirements above with `rm -f` in Unix-like systems.\n\n\n## Problem\n\n\nFirst I have tried `Remove-Item -Force`, but it couldn't ignore nonexistent files (cf. `rm -f` ignores nonexistent files).\n\n\n\n```\nPS C:\\tmp> Remove-Item C:\\tmp\\foo.txt -Force\nRemove-Item : Cannot find path 'C:\\tmp\\foo.txt' because it does not exist.\nAt line:1 char:1\n+ Remove-Item C:\\tmp\\foo.txt -Force\n+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (C:\\tmp\\foo.txt:String) [Remove-Item], ItemNotFoundException\n    + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.RemoveItemCommand\n\n```\n\nNext, I have tried `Remove-Item -ErrorAction Ignore` and `Remove-Item -ErrorAction SilentlyContinue`, but they don't show error messages when they failed to remove the file (cf. `rm -f` shows an error message like `rm: cannot remove 'foo.txt': Operation not permitted` in this situation).\n\n\n\n```\nPS C:\\tmp> $file = [System.IO.File]::Open('C:\\tmp\\foo.txt',[System.IO.FileMode]::Open,[System.IO.FileAccess]::Read,[System.IO.FileShare]::None)\nPS C:\\tmp> Remove-Item C:\\tmp\\foo.txt -ErrorAction Ignore\n# I expected it shows an error because it couldn't remove the file because of the lock, but it showed nothing\n\n```\n\n\n```\nPS C:\\tmp> $file = [System.IO.File]::Open('C:\\tmp\\foo.txt',[System.IO.FileMode]::Open,[System.IO.FileAccess]::Read,[System.IO.FileShare]::None)\nPS C:\\tmp> Remove-Item C:\\tmp\\foo.txt -ErrorAction SilentlyContinue\n# I expected it shows an error because it couldn't remove the file because of the lock, but it showed nothing\n\n```\n\n## Question\n\n\nIs there `rm -f` equivalent in PowerShell that fulfills all the requirements above?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"To me, the simplest solution is:\n\n\n\n```\nif (test-path $file) {\n  remove-item $file\n}\n\n```\n\nThis also occurs to me. $error[0] is always the most recent error.\n\n\n\n```\nremove-item $file -erroraction silentlycontinue\nif ($error[0] -notmatch 'does not exist') {\n  write-error $error[0]  # to standard error\n}\n\n```\n\nI think you can also use try\/catch with specific exceptions. Here's an example. I found the exception through tab completion. But the script will stop with other uncaught exceptions. This error doesn't normally stop.\n\n\n\n```\ntry { remove-item foo -erroraction stop }\ncatch [System.Management.Automation.ItemNotFoundException] { $null }\n'hi'\n\n```\n\n"}
{"questionId":"715c4d991870481fa1abf6ce235b683d","question":"Can I shorten a function name I use repeatedly?\nI have a long formula, like the following:\n\n\n\n```\nfloat a = sin(b)*cos(c)+sin(c+d)*sin(d)....\n\n```\n\nIs there a way to use `s` instead of `sin` in C, to shorten the formula, **without** affecting the running time?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c"},"answer":"There are at least three options for using `s` for `sin`:\n\n\nUse a preprocessor macro:\n\n\n\n```\n#define s(x) (sin(x))\n#define c(x) (cos(x))\nfloat a = s(b)*c(c)+s(c+d)*c(d)....\n#undef c\n#undef s\n\n```\n\nNote that the macros definitions are immediately removed with `#undef` to prevent them from affecting subsequent code. Also, you should be aware of the basics of preprocessor macro substitution, noting the fact that the first `c` in `c(c)` will be expanded but the second `c` will not since the function-like macro `c(x)` is expanded only where `c` is followed by `(`.\n\n\nThis solution will have no effect on run time.\n\n\nUse an inline function:\n\n\n\n```\nstatic inline double s(double x) { return sin(x); }\nstatic inline double c(double x) { return cos(x); }\n\n```\n\nWith a good compiler, this will have no effect on run time, since the compiler should replace a call to `s` or `c` with a direct call to `sin` or `cos`, having the same result as the original code. Unfortunately, in this case, the `c` function will conflict with the `c` object you show in your sample code. You will need to change one of the names.\n\n\nUse function pointers:\n\n\n\n```\nstatic double (* const s)(double) = sin;\nstatic double (* const c)(double) = cos;\n\n```\n\nWith a good compiler, this also will have no effect on run time, although I suspect a few more compilers might fail to optimize code using this solution than than previous solution. Again, you will have the name conflict with `c`. Note that using function pointers creates a direct call to the `sin` and `cos` functions, bypassing any macros that the C implementation might have defined for them. (C implementations are allowed to implement library function using macros as well as functions, and they might do so to support optimizations or certain features. With a good quality compiler, this is usually a minor concern; optimization of a direct call still should be good.)\n\n\n"}
{"questionId":"8adc041d6fed41ac9836c8f68c49a524","question":"When and why to use self.\\_\\_dict\\_\\_ instead of self.variable\nI'm trying to understand some code which is using this class below:\n\n\n\n```\nclass Base(object):\n\n    def __init__(self, **kwargs):\n        self.client = kwargs.get('client')\n        self.request = kwargs.get('request')\n    ...\n\n    def to_dict(self):\n        data = dict()\n\n        for key in iter(self.__dict__): # <------------------------ this\n            if key in ('client', 'request'):\n                continue\n\n            value = self.__dict__[key]\n            if value is not None:\n                if hasattr(value, 'to_dict'):\n                    data[key] = value.to_dict()\n                else:\n                    data[key] = value\n        return data\n\n```\n\nI understand that it gets keyword arguments passed to the `Base` class like for example, `Base(client=\"foo\", request=\"bar\")`.\n\n\nMy confusion is, why is it using `self.__dict__` which turns variables inside `__init__` to a dict (e.g `{\"client\": \"foo\", \"request\": \"bar\"}`) instead of just calling them by `self.client` & `self.request` inside other methods? When and why I should use `self.__dict__` instead?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"python"},"answer":"Almost all of the time, you shouldn't use `self.__dict__`.\n\n\nIf you're accessing an attribute like `self.client`, i.e. the attribute name is known and fixed, then the only difference between that and `self.__dict__['client']` is that the latter won't look up the attribute on the class if it's missing on the instance. There is very rarely any reason to do this, but the difference is demonstrated below:\n\n\n\n```\n>>> class A:\n...     b = 3 # class attribute, not an instance attribute\n... \n>>> A.b # the class has this attribute\n3\n>>> a = A()\n>>> a.b # the instance doesn't have this attribute, fallback to the class\n3\n>>> a.__dict__['b'] # the instance doesn't have this attribute, but no fallback\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nKeyError: 'b'\n\n```\n\nThe main use-case for `self.__dict__` is when you *don't* want to access a fixed, known attribute name. In almost all code, you *always* know which attribute you want to access; and if you do need to look something up dynamically using an unknown string, you should create a dictionary yourself, and write `self.that_dict[key]` instead of `self.__dict__[key]`.\n\n\nSo the only times you should really use `__dict__` is when you are writing code which needs to work regardless of which attributes the instance might have; i.e. you specifically want code which will work even if you change the class's structure or its attribute names, or code which will work across multiple classes with different structures. I'll show one example below.\n\n\n### The `__repr__` method\n\n\nThe `__repr__` method is meant to return a string representing the instance, for the programmer's convenience when using a REPL. For debugging\/testing purposes this string usually contains information about the object's state. Here's a common way to implement it:\n\n\n\n```\nclass Foo:\n    def __init__(self, foo, bar, baz):\n        self.foo = foo\n        self.bar = bar\n        self.baz = baz\n\n    def __repr__(self):\n        return 'Foo({!r}, {!r}, {!r})'.format(self.foo, self.bar, self.baz)\n\n```\n\nThis means if you write `obj = Foo(1, 'y', True)` to create an instance, then `repr(obj)` will be the string `\"Foo(1, 'y', True)\"`, which is convenient because it shows the instance's entire state, and also the string itself is Python code which creates an instance with the same state.\n\n\nBut there are a few issues with the above implementation: we have to change it if the class's attributes change, it won't give useful results for instances of subclasses, and we have to write lots of similar code for different classes with different attributes. If we use `__dict__` instead, we can solve all of those problems:\n\n\n\n```\n    def __repr__(self):\n        return '{}({})'.format(\n            self.__class__.__name__,\n            ', '.join('{}={!r}'.format(k, v) for k, v in self.__dict__.items())\n        )\n\n```\n\nNow `repr(obj)` will be `Foo(foo=1, bar='y', baz=True)`, which also shows the instance's entire state, and is also executable Python code. This generalised `__repr__` method will still work if the structure of `Foo` changes, it can be shared between multiple classes via inheritance, and it returns executable Python code for any class whose attributes are accepted as keyword arguments by `__init__`.\n\n\n"}
{"questionId":"d9fbd3e9bd7b4b5180cde85d2a9a78b2","question":"Can't remove \\ufeff from a string\nThe app basically works like this:\n\n\n1) The user uploads a CSV file.\n\n\n2) The file is catched by PHP via POST.\n\n\n3) I open the file with `fopen()` and read the file with `fgetcsv()`.\n\n\nThe first column it always have the `\\ufeff` char. I know that is called UTF-8 BOM, and it's generated by Microsoft Excel. But, when I want to remove that, I can't.\n\n\nI've tried: `str_replace('\\ufeff', '', $columns[0]);`\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"\n```\n$columns[0] = preg_replace('\/[\\x00-\\x1F\\x80-\\xFF]\/', '', $columns[0]);\n\n```\n\nThe above code helps you remove hidden characters that exist in your document, just like the one you mentioned.\n\n\n"}
{"questionId":"8e2d9020ccd34815a9bbe28050ebb728","question":"Update only year in date field with postgres\nI have a column with the type date and I want to update only the year in the dates while leaving the day and month as they are. I want to set the year to a specific value regardless of what the current value is. The answers currently on stack overflow involve adding or subtracting years from the date which doesn't seem to be what I need.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"sql"},"answer":"You can set a specific year like this:\n\n\n\n```\nUPDATE my_table SET date_column = date_column + \n    MAKE_INTERVAL(YEARS := ***year*** - EXTRACT(YEAR FROM date_column)::INTEGER)\n\n```\n\nwhere `***year***`is the specific year as integer. For instance\n\n\n\n```\nUPDATE my_table SET date_column = date_column + \n    MAKE_INTERVAL(YEARS := 2001 - EXTRACT(YEAR FROM date_column)::INTEGER)\n\n```\n\nwill set the year of all dates to 2001.\n\n\n"}
{"questionId":"cbad7d6202014cd299b1ab3ee68a3389","question":"Assembly build version at runtime in blazor wasm app\nWhat is the best way to get build version number at runtime in web assembly client-side blazor app?\nIn server side version I was able to use `Assembly.GetEntryAssembly().GetCustomAttribute<AssemblyInformationalVersionAttribute>().InformationalVersion.ToString();`\n\n\nCombined with msbump package it was automatically generating new version for me with each new build. Is that possible to achieve at client side blazor too?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"try using `GetExecutingAssembly()`.\n\n\nExample:\n\n\n\n```\nAssembly.GetExecutingAssembly().\n    GetCustomAttribute<AssemblyInformationalVersionAttribute>().\n    InformationalVersion;\n\n```\n\nThe reason you can't use the entry assembly is I believe the entry assembly is not your actual assembly. So if you call out for executing assembly you are guaranteed to get your actual assembly.\n\n\n"}
{"questionId":"bd46e1c1306e48c3b20cfcc441e87126","question":"Is it safe to call placement new on `this` for trivial object?\nI know that this question was asked several times already but I couldn't find an answer for this particular case.\n\n\nLet's say I have a trivial class that doesn't own any resources and has empty destructor and default constructor. It has a handful of member variables with in-class initialization; not one of them is `const`.\n\n\nI want to re-initialize and object of such class it without writing `deInit` method by hand. Is it safe to do it like this?\n\n\n\n```\nvoid A::deInit()\n{\n  new (this)A{};\n}\n\n```\n\nI can't see any problem with it - object is always in valid state, `this` still points to the same address; but it's C++ so I want to be sure.\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c++"},"answer":"Similarly to the legality of `delete this`, placement new to `this` is also allowed as far as I know. Also, regarding whether `this`, or other pre-existing pointers \/ references can be used afterwards, there are a few restrictions:\n\n\n\n> \n> [basic.life]\n> \n> \n> If, after the lifetime of an object has ended and before the storage which the object occupied is reused or released, a new object is created at the storage location which the original object occupied, a pointer that pointed to the original object, a reference that referred to the original object, or the name of the original object will automatically refer to the new object and, once the lifetime of the new object has started, can be used to manipulate the new object, if:\n> \n> \n> - the storage for the new object exactly overlays the storage location which the original object occupied, and\n> - the new object is of the same type as the original object (ignoring the top-level cv-qualifiers), and\n> - the type of the original object is not const-qualified, and, if a class type, does not contain any non-static data member whose type is\n>  const-qualified or a reference type, and\n> - neither the original object nor the new object is a potentially-overlapping subobject ([intro.object]).\n> \n> \n> \n\n\nThe first two are satisfied in this example, but the last two will need to be taken into consideration.\n\n\nRegarding the third point, given that the function is non-const-qualified, it should be fairly safe to assume that the original object is non-const. The fault is on the caller side if the constness has been cast away. Regarding const \/ reference member, I think that can be checked by asserting that this is assignable:\n\n\n\n```\nstatic_assert(std::is_trivial_v<A> && std::is_copy_assignable_v<A>);\n\n```\n\nOf course, since assignability is a requirement, you could instead simply use `*this = {};` which I would expect to produce the same program. A perhaps more interesting use case might be to reuse memory of `*this` for an object of another type (which would fail the requirements for using `this`, at least without reinterpreting + laundering).\n\n\nSimilar to `delete this`, placement new to `this` could hardly be described as \"safe\".\n\n\n"}
{"questionId":"1181546ee1a649cf8cac63bd5b4a6d18","question":"How to silence output of all commands in a Bash script?\nMy Bash script calls a lot of commands, most of them output something. I want to silence them **all**. Right now, I'm adding `&>\/dev\/null` at the end of most command invocations, like this:\n\n\n\n```\nsome_command &>\/dev\/null\nanother_command &>\/dev\/null\ncommand3 &>\/dev\/null\n\n```\n\nSome of the commands have flags like `--quiet` or similar, still, I'd need to work it out for all of them and I'd just rather **silence all of them by default** and only allow output explicitly, e.g., via `echo`.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"You can use the `exec` command to redirect everything for the rest of the script.\n\n\nYou can use `3>&1` to save the old stdout stream on FD 3, so you can redirect output to that if you want to see the output.\n\n\n\n```\nexec 3>&1 &>\/dev\/null\nsome_command\nanother_command\ncommand_you_want_to_see >&3\ncommand3\n\n```\n\n"}
{"questionId":"dc5a6878b7144f77a0fcd78d218137d9","question":"Eclipse not able to open java files -> Unable to make protected final java.lang.Class java.lang.ClassLoader.defineClass\nGetting the following error, after adding `Lombok` lib\n\n\n\n```\nAn error has occurred. See error log for more details.\nUnable to make protected final java.lang.Class java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int) throws java.lang.ClassFormatError accessible: module java.base does not \"opens java.lang\" to unnamed module @1d1c37d5\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"Add the below lines to the end of the `eclipse.ini` file\n\n\n\n```\n--illegal-access=warn\n--add-opens java.base\/java.lang=ALL-UNNAMED\n\n```\n\n"}
{"questionId":"9246454f51b644d899b5ddbb87ecc271","question":"What does \"cannot borrow as immutable because it is also borrowed as mutable\" mean in an nested array index?\nWhat does the error mean in this case:\n\n\n\n```\nfn main() {\n    let mut v: Vec<usize> = vec![1, 2, 3, 4, 5];\n    v[v[1]] = 999;\n}\n\n```\n\n\n```\nerror[E0502]: cannot borrow `v` as immutable because it is also borrowed as mutable\n --> src\/main.rs:3:7\n  |\n3 |     v[v[1]] = 999;\n  |     --^----\n  |     | |\n  |     | immutable borrow occurs here\n  |     mutable borrow occurs here\n  |     mutable borrow later used here\n\n```\n\nI found that indexing is implemented via the `Index` and `IndexMut` traits and that `v[1]` is syntactic sugar for `*v.index(1)`. Equipped with this knowledge, I tried to run the following code:\n\n\n\n```\nuse std::ops::{Index, IndexMut};\n\nfn main() {\n    let mut v: Vec<usize> = vec![1, 2, 3, 4, 5];\n    *v.index_mut(*v.index(1)) = 999;\n}\n\n```\n\nTo my surprise, this works flawlessly! Why doesn't the first snippet work, but the second one does? The way I understand the documentation, they should be equivalent, but this obviously isn't the case.\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"rust"},"answer":"The desugared version is slightly different than what you have. The line\n\n\n\n```\nv[v[1]] = 999;\n\n```\n\nactually desugars to\n\n\n\n```\n*IndexMut::index_mut(&mut v, *Index::index(&v, 1)) = 999;\n\n```\n\nThis results in the same error message, but the annotations give a hint as to what's happening:\n\n\n\n```\nerror[E0502]: cannot borrow `v` as immutable because it is also borrowed as mutable\n --> src\/main.rs:7:48\n  |\n7 |     *IndexMut::index_mut(&mut v, *Index::index(&v, 1)) = 999;\n  |      ------------------- ------                ^^ immutable borrow occurs here\n  |      |                   |\n  |      |                   mutable borrow occurs here\n  |      mutable borrow later used by call\n\n```\n\nThe important difference to your desugared version is the evaluation order. The arguments of a function call are evaluated left to right in the order listed, before actually making the function call. In this case this means that first `&mut v` is evaluated, mutably borrowing `v`. Next, `Index::index(&v, 1)` should be evaluated, but this is not possible \u2013 `v` is already mutably borrowed. Finally, the compiler shows that the mutable reference is still needed for the function call to `index_mut()`, so the mutable reference is still alive when the shared reference is attempted.\n\n\nThe version that actually compiles has a slightly different evaluation order.\n\n\n\n```\n*v.index_mut(*v.index(1)) = 999;\n\n```\n\nFirst, the function arguments to the method calls are evaluated left to right, i.e. `*v.index(1)` is evaluated first. This results in a `usize`, and the temporary shared borrow of `v` can be released again. Then, the receiver of `index_mut()` is evaluated, i.e. `v` is mutably borrowed. This works fine, since the shared borrow has already been finalised, and the whole expression passes the borrow checker.\n\n\nNote that the version that compiles only does so since the introduction of \"non-lexical lifetimes\". In earlier versions of Rust, the shared borrow would live until the end of the expression and result in a similar error.\n\n\nThe cleanest solution in my opinion is to use a temporary variable:\n\n\n\n```\nlet i = v[1];\nv[i] = 999;\n\n```\n\n"}
{"questionId":"8a8acdaa3f7442988c636dca6258af77","question":"Bash compare a command output to string\nOutput is same, and it always echos `need to pull`.\nIf I remove the quotes around `$text` in `if` condition it throws the `too many arguments` error. \n\n\n\n```\nvar=\"$(git status -uno)\" && \n\ntext=\"On branch master Your branch is up-to-date with 'origin\/master'. nothing to commit (use -u to show untracked files)\"; \n\necho  $var; \necho  $text; \nif [ \"$var\" = \"$text\" ]; then\n    echo \"Up-to-date\"\nelse\n    echo \"need to pull\"\nfi\n\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"Better do this, `=~` for *bash regex* :\n\n\n\n```\n#!\/bin\/bash\n\nvar=\"$(git status -uno)\" \n\nif [[ $var =~ \"nothing to commit\" ]]; then\n    echo \"Up-to-date\"\nelse\n    echo \"need to pull\"\nfi\n\n```\n\nor\n\n\n\n```\n#!\/bin\/bash\n\nvar=\"$(git status -uno)\" \n\nif [[ $var == *nothing\\ to\\ commit* ]]; then\n    echo \"Up-to-date\"\nelse\n    echo \"need to pull\"\nfi\n\n```\n\n"}
{"questionId":"b220ed3ef25e438d93ccdb06774f0329","question":"How do I create Java Instant from epoch microseconds or nanoseconds?\nI'm trying to standardize time stamp format for my project, where the source reports in microsecond precision. I'm trying to find out whether there is a clean or minimal approach that does not require using handwritten constants.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"Thanks for the suggestions. This is the cleanest I could come up with:\n\n\n\n```\nstatic Instant getInstantFromMicros(long microsSinceEpoch) {\n    return Instant.ofEpochSecond(\n            TimeUnit.MICROSECONDS.toSeconds(microsSinceEpoch), \n            TimeUnit.MICROSECONDS.toNanos(\n                    Math.floorMod(microsSinceEpoch, TimeUnit.SECONDS.toMicros(1))\n            )\n    );\n}\n\nstatic Instant getInstantFromNanos(long nanosSinceEpoch) {\n    return Instant.ofEpochSecond(0L, nanosSinceEpoch);\n}\n\n```\n\nTest Cases:\n\n\n\n```\nSystem.out.println(getInstantFromMicros(1_500_000_000_123_456L));\n\/\/ 2017-07-14T02:40:00.123456Z\n    \nSystem.out.println(getInstantFromNanos(1_500_000_000_123_456_789L));\n\/\/ 2017-07-14T02:40:00.123456789Z\n\n```\n\n"}
{"questionId":"b2719b1dd5db4e8d997b22e88c6d79b4","question":"Limiting number of go routines running\nI have a list of urls to process, but I want to run a maximum number of goroutines at a time. For example, if I have 30 urls, I only want 10 goroutines working in parallel.\n\n\nMy attempt at this is the following:\n\n\n\n```\nparallel := flag.Int(\"parallel\", 10, \"max parallel requests allowed\")\nflag.Parse()\nurls := flag.Args()\n\nvar wg sync.WaitGroup\nclient := rest.Client{}\n\nresults := make(chan string, *parallel)\n\nfor _, url := range urls {\n    wg.Add(1)\n    go worker(url, client, results, &wg)\n}\n\nfor res := range results {\n    fmt.Println(res)\n}\n\nwg.Wait()\nclose(results)\n\n```\n\nMy understanding is that if I create a buffered channel of size parallel, then the code will block until I read off the results channel, which will unblock my code and allow another goroutine to be spawned.\nHowever, this code doesn't seems to block after processing all the urls. Can someone explain to me how I can use channels to limit the number of goroutines running?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"go"},"answer":"Create the desired number of workers instead of one worker per url:\n\n\n\n```\nparallel := flag.Int(\"parallel\", 10, \"max parallel requests allowed\")\nflag.Parse()\n\n\/\/ Workers get URLs from this channel\nurls := make(chan string) \n\n\/\/ Feed the workers with URLs\ngo func() {\n    for _, u := range flag.Args() {\n        urls <- u\n    }\n    \/\/ Workers will exit from range loop when channel is closed\n    close(urls)\n}()\n\nvar wg sync.WaitGroup\nclient := rest.Client{}\n\nresults := make(chan string)\n\n\/\/ Start the specified number of workers.\nfor i := 0; i < *parallel; i++ {\n    wg.Add(1)\n    go func() {\n        defer wg.Done()\n        for url := range urls {\n            worker(url, client, results)\n        }\n    }()\n}\n\n\/\/ When workers are done, close results so that main will exit.\ngo func() {\n    wg.Wait()\n    close(results)\n}()\n\nfor res := range results {\n    fmt.Println(res)\n}\n\n```\n\n"}
{"questionId":"fd415c0fd11c4a0eb3b31fd165c14599","question":"How to assign multiple lines string in Powershell Console\nWhen I do enter this in powershell Console\n\n\n\n```\n$test=@'\nTest\nTest'@\n\n```\n\nAnd do enter several times, it keeps printing\n\n\n\n```\n>>\n\n```\n\nSo I can never finish command.\n\n\nWhat to do ?\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"bash\/shell"},"answer":"\n```\n$test=@'\nTest\nTest\n'@\n\n```\n\nThe important thing to note is that the delimiters include (invisible) carriage returns. There must be one at the end of the starting tag, and one before the closing tag.\n\n\n"}
{"questionId":"cea37a4eddac4ca7a778d9ad6a971278","question":"static int arr[10] memory address always ends in 060\nI have a c program that looks like this\n\n\nmain.c\n\n\n\n```\n#include <stdio.h>\n#define SOME_VAR 10\n\nstatic int heap[SOME_VAR];\n\n\nint main(void) {\n    printf(\"%p\", heap);\n    return 0;\n}\n\n```\n\nand outputs this when I run the compiled program a few times\n\n\n\n```\n0x58aa7c49060\n0x56555644060\n0x2f8d1f8e060\n0x92f58280060\n0x59551c53060\n0xd474ed6e060\n0x767c4561060\n0xf515aeda060\n0xbe62367e060\n\n```\n\nWhy does it always end in 060? And is the array stored in heap?\n\n\nEdit: I am on Linux and I have ASLR on. I compiled the program using gcc\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"c"},"answer":"The addresses differ because of ASLR (Address space layout ramdomization). Using this, the binary can be mapped at different locations in the virtual address space.\n\n\nThe variable `heap` is - in contrast to it's name - not located on the heap, but on the `bss`. The offset in the address space is therefore constant.\n\n\nPages are mapped at page granularity, which is 4096 bytes (hex: 0x1000) on many platforms. This is the reason, why the last three hex digits of the address is the same.\n\n\nWhen you did the same with a *stack* variable, the address could even vary in the last digits on some platforms (namely linux with recent kernels), because the stack is not only mapped somewhere else but also receives a random offset on startup.\n\n\n"}
{"questionId":"48cb39e17e964bada77e8beff11b24a1","question":"What am I missing in my custom std::ranges iterator?\nI'd like to provide a view for a customer data structure, with it's own iterator. I wrote a small program to test it out, shown below. It I uncomment begin(), then it works. But if I use DummyIter, then I get a compile error.\n\n\nIn my full program, I've implemented a full iterator but for simplicity, I narrowed it down to the necessary functions here.\n\n\n\n```\n#include <iostream>\n#include <ranges>\n#include <vector>\n\ntemplate<class T>\nstruct DummyIter\n{\n  using iterator_category = std::random_access_iterator_tag;\n  using value_type = T;\n  using difference_type = std::ptrdiff_t;\n\n  DummyIter() = default;\n\n  auto operator*() const { T t; return t; }\n\n  auto& operator++() { return *this; }\n\n  auto operator++(int val) { DummyIter tmp = *this; ++*this; return tmp; }\n\n  auto operator==(const DummyIter& iter) const { return true; }\n};\n\ntemplate<class V>\nstruct DummyView : std::ranges::view_interface<DummyView<V>>\n{\n  \/\/auto begin() const { return std::ranges::begin(v); }\n  auto begin() const { return DummyIter<int>(); }\n\n  auto end() const { return std::ranges::end(v); }\n\n  V v;\n};\n\nint main() {\n  auto view = DummyView<std::vector<int>>();\n  view | std::views::filter([](auto i) { return i > 0; });\n}\n\n```\n\nI'm using GCC 11.1.0. What am I missing in my iterator for it to be ranges compliant?\n\n\n\n```\nerror: no match for 'operator|' (operand types are 'DummyView<std::vector<int> >' and 'std::ranges::views::__adaptor::_Partial<std::ranges::views::_Filter, main()::<lambda(auto:15)> >')\n   37 |   view | std::views::filter([](auto i) { return i > 0; });\n      |   ~~~~ ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n      |   |                        |\n      |   |                        std::ranges::views::__adaptor::_Partial<std::ranges::views::_Filter, main()::<lambda(auto:15)> >\n      |   DummyView<std::vector<int> >\n\n```\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"c++"},"answer":"The way to check this kind of thing for ranges is:\n\n\n1. Verify that your iterator is an `input_iterator`.\n2. Verify that your sentinel is a `sentinel_for` your iterator.\n\n\nThose are the checks that will tell you what functionality you're missing.\n\n\nIn this case, that's:\n\n\n\n```\nusing I = DummyIter<int>;\nusing S = std::vector<int>::const_iterator;\n\nstatic_assert(std::input_iterator<I>);   \/\/ ok\nstatic_assert(std::sentinel_for<S, I>);  \/\/ error\n\n```\n\nAnd the issue there is that `S` isn't a `sentinel_for<I>` because it's not equality comparable. You need to know when the iteration stops, and you don't have that operator - your `DummyIter<T>` is comparable to another `DummyIter<T>`, but not to what you're returning from `end()`.\n\n\nSo you either need to add another `operator==` to `DummyIter<T>`, or have `DummyView<V>::end()` return some kind of `DummySentinel<V>` that is comparable to `DummyIter<T>`. It depends on the real problem which is the better approach, there are examples of both in the standard library.\n\n\n"}
{"questionId":"ea904a81f2bc41f28e48d7c6648a4cbc","question":"How to check that a string contains only \u201ca-z\u201d, \u201cA-Z\u201d and \u201c0-9\u201d characters\nI am importing string and trying to check if text contains only \"a-z\", \"A-Z\", and \"0-9\".\n\n\nBut I get only input and it doesn't print success when I enter letters and digits\n\n\n\n```\nimport string\ntext=input(\"Enter: \")\ncorrect = string.ascii_letters + string.digits\nif text in correct:\n    print(\"Success\")\n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"python"},"answer":"You could use regex for this, e.g. check string against following pattern:\n\n\n\n```\nimport re\npattern = re.compile(\"[A-Za-z0-9]+\")\npattern.fullmatch(string)\n\n```\n\n**Explanation:**\n\n\n`[A-Za-z0-9]` matches a character in the range of A-Z, a-z and 0-9, so letters and numbers.\n\n\n`+` means to match 1 or more of the preceeding token.\n\n\nThe `re.fullmatch()` method allows to check if the whole string matches the regular expression pattern. Returns a corresponding match object if match found, else returns `None` if the string does not match the pattern.\n\n\nAll together:\n\n\n\n```\nimport re\n\nif __name__ == '__main__':\n    string = \"YourString123\"\n    pattern = re.compile(\"[A-Za-z0-9]+\")\n\n    # if found match (entire string matches pattern)\n    if pattern.fullmatch(string) is not None:\n        print(\"Found match: \" + string)\n    else:\n        # if not found match\n        print(\"No match\")\n\n```\n\n"}
{"questionId":"121c92ae939248d1a587c1e781d68329","question":"Specify example requests for swagger's \"Try it out\"\nIs there a way to specify example requests for swagger? Maybe even multiple ones? \n\n\nThe `Try it out` button shows only generic values like:\n\n\n\n```\n{\n    \"firstName\": \"string\",\n    \"lastName\": \"string\"\n}\n\n```\n\nfor\n\n\n\n```\npublic class User\n{\n    public string FirstName { get; set; }\n    public string LastName { get; set; }\n}\n\n```\n\nIt becomes very difficult to use with large objects when you have to edit all the values first. I know I could use Postman, and I do too, but being able to create **multiple** good looking and useful examples with swagger would be very nice.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"In .Net5 you can add a SchemaFilter to Swagger in the Startup.cs\n\n\n\n```\npublic override void ConfigureServices(IServiceCollection services)\n{\n    services.AddSwaggerGen(c =>\n    {\n        c.SchemaFilter<ExampleSchemaFilter>();\n    });\n}\n\n```\n\nIn the ExampleSchemaFilter.cs you simply define an OpenApiObject for your specific class:\n\n\n\n```\nusing Microsoft.OpenApi.Any;\nusing Microsoft.OpenApi.Models;\nusing Swashbuckle.AspNetCore.SwaggerGen;\n\npublic class ExampleSchemaFilter : ISchemaFilter\n{\n    public void Apply(OpenApiSchema schema, SchemaFilterContext context)\n    {\n        if (context.Type == typeof(User))\n        {\n            schema.Example = new OpenApiObject()\n            {\n                [\"firstName\"] = new OpenApiString(\"John\"),\n                [\"lastName\"] = new OpenApiString(\"Doe\"),\n            };\n        }\n    }\n}\n\n```\n\n"}
{"questionId":"0ce1c9a0657b4157ad4095dd16cec9aa","question":"How do I check if both variables are both Some?\nI am confused about the `Some(T)` keyword.\n\n\nI want to check for two variables, if the value is defined (not `None`). If that is the case, the value of this variables is processed.\n\n\nI know the `match` pattern which works like this:\n\n\n\n```\nmatch value {\n    Some(val) => println!(\"{}\", val),\n    None => return false,\n}\n\n```\n\nIf I use this pattern, it will get very messy:\n\n\n\n```\nmatch param {\n    Some(par) => {\n        match value {\n            Some(val) => {\n                \/\/process\n            },\n\n            None => return false,\n        }\n    },\n\n    None => return false,\n}\n\n```\n\nThis can't be the right solution.\n\n\nThe is a possibility, to ask if the param and value `is_some()` That would effect code like that:\n\n\n\n```\nif param.is_some() && value.is_some() {\n    \/\/process\n}\n\n```\n\nBut if I do it like that, I always have to unwrap `param` and `value` to access the values.\n\n\nI thought about something like this to avoid that. But this code does not work:\n\n\n\n```\nif param == Some(par) && value == Some(val) {\n    \/\/process\n}\n\n```\n\nThe idea is that the values are accessible by `par` and `val` like they are in the `match` version.\n\n\nIs there any solution to do something like this?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"rust"},"answer":"If I have several `Option` values to match, I match on a tuple of the values:\n\n\n\n```\nenum Color {\n    Red,\n    Blue,\n    Green,\n}\n\nfn foo(a: Option<Color>, b: Option<i32>) {\n    match (a, b) {\n        (Some(Color::Blue), Some(n)) if n > 10 => println!(\"Blue large number\"),\n        (Some(Color::Red), _) => println!(\"Red number\"),\n        _ => (),\n    }\n}\n\nfn main() {\n    foo(Some(Color::Blue), None);\n    foo(Some(Color::Blue), Some(20));\n}         \n\n```\n\nThis allows me to match the combinations that are interesting, and discard the rest (or return false, if that is what you want to do).\n\n\n"}
{"questionId":"16db394e27bf42b792c9de37685a5d7d","question":"React Typescript how to setState\nI am trying to learn typescript with react and I am pretty confused right now, how will I setState of an interface?\nMy code example: \n\n\n\n```\ninterface Props {\n    children: ReactNode | ReactNode[];\n}\n\nexport interface IMovie {\n    title: string;\n    video: boolean;\n    poster_path: string;\n}\n\n```\n\n\n```\n\nexport const MoviesContext = createContext<IMovie[] | undefined>([]);\n\nexport const MoviesProvider = ({ children }: Props): JSX.Element => {\n    const [movies, setMovies] = useState<IMovie[]>();\n\n    return (\n            <MoviesContext.Provider value={[movies, setMovies]}>\n                {children}\n            <\/MoviesContext.Provider>\n    );\n};\n\n```\n\nThe error I get is \"Type (\u0399\u039covie[] | Dispatch> | undefined>> is not assignable to type IMovie[]\"\n\n\nany other suggestions on what to change on my code welcomed. :)\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"The issue here is the value type that your context is defined with is `IMovie`, where as the type being passed to your context during rendering is actually an array: `[movies, setMovies]`.\n\n\nOne solution would be to define and use a prop interface with your provider, that carries both the `movies` state as well as the setter for movies like this:\n\n\n\n```\nexport interface IMovieProviderProps {\n    movies? : IMovie,\n    setMovies : (movies:IMovie) => void,\n}\n\n```\n\nThis `IMovieProviderProps` interface would then be used to define the shape of your context value, which would provide the means for accessing the `movies` state as well as updating it, from outside the provider:\n\n\n\n```\n\/* Define context with new provider props *\/\nexport const MoviesContext = createContext<IMovieProviderProps>(null);\n\nexport const MoviesProvider = ({ children }: Props): JSX.Element => {\n    const [movies, setMovies] = useState<IMovie>();\n\n    \/* Define provider value. Can be done inline, but I've defined\n    it here for clarity *\/\n    const providerValue : IMovieProviderProps = {\n        movies,\n        setMovies\n    }\n\n    \/* Pass providerValue to provider *\/\n    return (<MoviesContext.Provider value={providerValue}>\n        {children}\n    <\/MoviesContext.Provider>);\n};\n\n\/* Define a hook for this provider, that allows the provided value\nto be consumed *\/\nexport const useMovies = () => useContext(MoviesContext);\n\n```\n\nThe `useMovies` hook allows the provider value to be accessed from elsewhere in your project, and can be used like this:\n\n\n\n```\nconst ExampleComponent = () => {\n\n     \/* Imported useMovies hook defined with provider above, and\n     extract the movies state, and movies setter base on the shape\n     of IMovieProviderProps *\/\n     const { movies, setMovies } = useMovies();\n\n     return <>\n     {JSON.stringify(movies)}\n     <button onClick={() => setMovies(null)}>Set Movies<\/button>\n     <\/>\n}\n\n```\n\nA key thing to note now is that the context now exposes an object value with the shape of `IMovieProviderProps` rather than an array value (as your code may have been expecting).\n\n\nHope that helps!\n\n\n"}
{"questionId":"8b4f09b87e77497e9c54751e6d1e1170","question":"MODULE\\_NOT\\_FOUND Nestjs and Swagger\nI'm trying to add Swagger to my Nestjs app. Module not found error is thrown when I'm trying to compile it.\nI use the same code from Nestjs documentation.\nThis is my main.ts:\n\n\n\n```\nasync function bootstrap() {\n  const app = await NestFactory.create(AppModule);\n\n  const config = new DocumentBuilder()\n    .setTitle('Cats example')\n    .setDescription('The cats API description')\n    .setVersion('1.0')\n    .addTag('cats')\n    .build();\n  const document = SwaggerModule.createDocument(app, config);\n  SwaggerModule.setup('api', app, document);\n\n  await app.listen(3000);\n}\nbootstrap();\n\n```\n\nThis is the error:\n\n\n\n```\ninternal\/modules\/cjs\/loader.js:883\n  throw err;\n  ^\n\nError: Cannot find module '@nestjs\/core\/router\/route-path-factory'\nRequire stack:\n- D:\\BK\\solidity\\MVPApp\\blockchain\\back-end-student-wallet-v2\\node_modules\\@nestjs\\swagger\\dist\\swagger-explorer.js\n- D:\\BK\\solidity\\MVPApp\\blockchain\\back-end-student-wallet-v2\\node_modules\\@nestjs\\swagger\\dist\\swagger-scanner.js \n- D:\\BK\\solidity\\MVPApp\\blockchain\\back-end-student-wallet-v2\\node_modules\\@nestjs\\swagger\\dist\\swagger-module.js  \n- D:\\BK\\solidity\\MVPApp\\blockchain\\back-end-student-wallet-v2\\node_modules\\@nestjs\\swagger\\dist\\index.js\n- D:\\BK\\solidity\\MVPApp\\blockchain\\back-end-student-wallet-v2\\node_modules\\@nestjs\\swagger\\index.js\n- D:\\BK\\solidity\\MVPApp\\blockchain\\back-end-student-wallet-v2\\dist\\main.js\n    at Function.Module._resolveFilename (internal\/modules\/cjs\/loader.js:880:15)\n    at Function.Module._load (internal\/modules\/cjs\/loader.js:725:27)\n\n\n```\n\nI'm using Node 14.15.1, @nestjs\/swagger 5.0.0, swagger-ui-express: 4.1.6\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"Update latest version of @nestjs\/platform-express, @nestjs\/common,@nestjs\/core (8.0.0) solve my problem. It seems like nestjs\/cli uses previous version of nestjs\n\n\n"}
{"questionId":"3b0eece657974090ab49efa48896b155","question":"Angular 11: subscribe is deprecated: Use an observer instead?\nMy tslint gone crazy? It gives warning for every subscribtion I made in whole app. It doesn't matter if I use old or new syntax it still says that subscribe is deprecated... How to write a subscription that will not be deprecated?\n\n\nThat's what was ok till today:\n\n\n\n```\nsomething.subscribe((user: User) => {\n        this.userProviderService.setUserId(user.userId);\n        this.proceed = true;\n      });\n\n```\n\nI tried new syntax but makes no change:\n\n\n\n```\nsomething.subscribe({\n        next: (user: User) =>  {\n          this.userProviderService.setUserId(user.userId);\n          this.proceed = true;\n        },\n        complete: () => {},\n        error: () => {}\n      });\n\n```\n\nThis is exactly what it's saying:\n\n\n\n> \n> (method) Observable.subscribe(next?: (value: Object) => void,\n> error?: (error: any) => void, complete?: () => void): Subscription (+4\n> overloads) @deprecated \u2014 Use an observer instead of a complete\n> callback\n> \n> \n> @deprecated \u2014 Use an observer instead of an error callback\n> \n> \n> @deprecated \u2014 Use an observer instead of a complete callback\n> \n> \n> subscribe is deprecated: Use an observer instead of a complete\n> callback (deprecation)tslint(1)\n> \n> \n> \n\n\nSo how do I subscribe to things now?\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"typescript"},"answer":"I just looked up TSLint (v1.3.3) at VS Code extenstions tab. It says:\n\n\n\n> \n> \u2757IMPORTANT: TSLint has been deprecated in favor of ESLint.\n> \n> \n> \n\n\nAs I disabled TSLint and installed ESLint, all warnings related to subscribtions dissapeared.\n\n\nCheers!\n\n\n"}
{"questionId":"ce3fd89459cf453c9dfd918249fb127d","question":"Creating a zod enum from an object\nI have this **object**:\n\n\n\n```\nconst properties = [\n  { value: \"entire_place\", label: \"The entire place\" },\n  { value: \"private_room\", label: \"A private room\" },\n  { value: \"shared_room\", label: \"A shared room\" },\n] as const;\n\n```\n\nI need to use it with zod in order to\n\n\n1. **Validate** and parse my data on the backend\n2. Create a **typescript union type** with those possible values `\"entire_place\" | \"shared_room\" | \"private_room\"`\n\n\nAccording to the **zod documentation**, i can do this:\n\n\n\n```\nconst properties = [\n  { value: \"entire_place\", label: \"The entire place\" },\n  { value: \"private_room\", label: \"A private room\" },\n  { value: \"shared_room\", label: \"A shared room\" },\n] as const;\n\nconst VALUES = [\"entire_place\", \"private_room\", \"shared_room\"] as const;\nconst Property = z.enum(VALUES);\ntype Property = z.infer<typeof Property>;\n\n```\n\nHowever, I don't want to define my data **twice**, **one time with a label** (the label is used for ui purposes), and another without a **label**.\n\n\nI want to define it only once using the `properties` object, without the `VALUES` array, and use it to create a zod object and infer the type from the zod object.\n\n\nAny solutions how?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"In this case, I think I would infer the type for `Property` from `properties` directly. You can avoid repeating yourself with code like:\n\n\n\n```\nimport { z } from \"zod\";\n\nconst properties = [\n  { value: \"entire_place\", label: \"The entire place\" },\n  { value: \"private_room\", label: \"A private room\" },\n  { value: \"shared_room\", label: \"A shared room\" }\n] as const;\n\ntype Property = typeof properties[number][\"value\"];\n\/\/ z.enum expects a non-empty array so to work around that\n\/\/ we pull the first value out explicitly\nconst VALUES: [Property, ...Property[]] = [\n  properties[0].value,\n  \/\/ And then merge in the remaining values from `properties`\n  ...properties.slice(1).map((p) => p.value)\n];\nconst Property = z.enum(VALUES);\n\n```\n\n"}
{"questionId":"78517a165b1b4410a4d326d343e64240","question":"Django, python3, on install I get: \"Parent module 'setuptools' not loaded\"\nI see lots of errors and suggestions about `Parent module '' not loaded, ...`\n\n\nI don't see any about specifically \"out of the box\" django 3.5.\n\n\n\n```\n$ mkvirtualenv foobar -p \/usr\/bin\/python3\nAlready using interpreter \/usr\/bin\/python3\nUsing base prefix '\/usr'\nNew python executable in \/home\/isaac\/.virtualenvs\/foobar\/bin\/python3\nAlso creating executable in \/home\/isaac\/.virtualenvs\/foobar\/bin\/python\nInstalling setuptools, pkg_resources, pip, wheel...done.\n\n[foobar] $ pip install django\nCollecting django\n  Using cached Django-2.2.15-py3-none-any.whl (7.5 MB)\nCollecting pytz\n  Using cached pytz-2020.1-py2.py3-none-any.whl (510 kB)\nCollecting sqlparse>=0.2.2\n  Using cached sqlparse-0.3.1-py2.py3-none-any.whl (40 kB)\nInstalling collected packages: pytz, sqlparse, django\nSuccessfully installed django-2.2.15 pytz-2020.1 sqlparse-0.3.1\n\n[foobar] $ python\nPython 3.5.3 (default, Jul  9 2020, 13:00:10)\n[GCC 6.3.0 20170516] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import django\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"\/home\/isaac\/.virtualenvs\/foobar\/lib\/python3.5\/site-packages\/django\/__init__.py\", line 1, in <module>\n    from django.utils.version import get_version\n  File \"\/home\/isaac\/.virtualenvs\/foobar\/lib\/python3.5\/site-packages\/django\/utils\/version.py\", line 6, in <module>\n    from distutils.version import LooseVersion\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\n  File \"\/home\/isaac\/.virtualenvs\/foobar\/lib\/python3.5\/site-packages\/_distutils_hack\/__init__.py\", line 82, in create_module\n    return importlib.import_module('._distutils', 'setuptools')\n  File \"\/home\/isaac\/.virtualenvs\/foobar\/lib\/python3.5\/importlib\/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 981, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 931, in _sanity_check\nSystemError: Parent module 'setuptools' not loaded, cannot perform relative import\n\n```\n\nAs you can see, I installed django using python3.5.\n\n\nIt seems to work fine with python2.7...\n\n\nIs anyone else aware of a way around this bug, or something silly I did in my environment?\n\n\nI am using debian stretch instead of buster, but I'm not sure if I'm ready to upgrade yet.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"Something happened in version 50 of setuptools.\n\n\nWe could \"solve\" this problem by downgrading setuptools to 49.3.0 (and maybe pip to 20.2.1)\n\n\n`pip install setuptools==49.3.0` and `pip install pip==20.2.1`\n\n\nBe aware though that this should only be a temporary solution!\n\n\n"}
{"questionId":"567bac672fd84333a1d49f5b4d137c08","question":"BigQuery : is it possible to iterate over an array?\nIs it possible to iterate over an array in bigquery in standard sql?\n\n\nBasically declare an array of strings representing table fields. ex :\n\n\n\n```\nDECLARE FIELDS_TO_CHECK ARRAY<STRING>;\nSET FIELDS_TO_CHECK =  ['field1', 'field2', 'field3' ];\n\n```\n\nand then iterate on this array to create requests getting percentage of non null value on this field :\n\n\n\n```\nselect count(FIELD) \/ count(*) from \n'table_name'`\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"sql"},"answer":"Below is example for BigQuery Standard SQL  \n\nI am using here TEMP TABLE `table\\_name` to mimic your data with some simplistic dummy data, but you can just remove that CREATE statement and use your own table\n\n\n\n```\n#standardSQL\nDECLARE FIELDS_TO_CHECK ARRAY<STRING>;\nDECLARE i INT64 DEFAULT 0;\n\nCREATE TEMP TABLE `table_name` AS \n  SELECT 1 field1, NULL field2, 3 field3, 4 field4, 5 field5 UNION ALL\n  SELECT NULL, NULL, 3, NULL, 5 UNION ALL\n  SELECT 1, NULL, 3, 4, 6;\n\nCREATE TEMP TABLE result(field STRING, percentage FLOAT64);  \n  \nSET FIELDS_TO_CHECK =  ['field1', 'field2', 'field3' ];\n\nLOOP\n  SET i = i + 1;\n  IF i > ARRAY_LENGTH(FIELDS_TO_CHECK) THEN \n    LEAVE; \n  END IF;\n  EXECUTE IMMEDIATE '''\n    INSERT result\n    SELECT \"''' || FIELDS_TO_CHECK[ORDINAL(i)] || '''\", COUNT(''' || FIELDS_TO_CHECK[ORDINAL(i)] || ''') \/ COUNT(*) FROM `table_name`\n  ''';\n\nEND LOOP; \n\nSELECT * FROM result;   \n\n```\n\nAbove example returns below output\n\n\n\n```\nRow field   percentage   \n1   field2  0.0  \n2   field1  0.66666666666666663  \n3   field3  1.0  \n\n```\n\n"}
{"questionId":"6125747c6920407ea0502cdbcc778ed9","question":"Comparing a bit to a boolean\nSay I have a set of flags, encoded in a uint16\\_t `flags`. For example, `AMAZING_FLAG = 0x02`.\nNow, I have a function. This function needs to check if I want to change the flag, because if I want to do that, I need to write to flash. And that is expensive. Therefore, I want a check which tells me if `flags & AMAZING_FLAG` is equal to `doSet`. This is the first idea:\n\n\n\n```\nsetAmazingFlag(bool doSet)\n{\n    if ((flags & AMAZING_FLAG) != (doSet ? AMAZING_FLAG : 0)) {\n        \/\/ Really expensive thing\n        \/\/ Update flags\n    }\n}\n\n```\n\nThis is not an intuitive if statement.\nI feel like there should be a better way, something like:\n\n\n\n```\nif ((flags & AMAZING_FLAG) != doSet){\n\n}\n\n```\n\nBut this does not actually work, `true` seems to be equal to `0x01`.\n\n\nSo, is there a neat way to compare a bit to a boolean?\n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"c"},"answer":"To convert any non-zero number to 1 (true), there is an old trick: apply the `!` (not) operator twice.\n\n\n\n```\nif (!!(flags & AMAZING_FLAG) != doSet){\n\n```\n\n"}
{"questionId":"1808052d5a2b49c69925a7ca0a15e942","question":"How to convert multiple list into single list using Java streams?\nI have an class `A` with multiple `List` members.\n\n\n\n```\nclass A {\n    List<X> xList;\n    List<Y> yList;\n    List<Z> zList;\n    \n    \/\/ getters and setters\n}\n\nclass X {\n     String desc;\n     String xtype;\n    \n     \/\/ getters and setters    \n}\n\nclass Y {   \n    String name;\n    String ytype;\n    \n    \/\/getters and setters\n}\n\nclass Z {\n    String description;\n    String ztype;\n    \n    \/\/ getters and setters    \n}\n\n```\n\nAnd a class `B` with just 2 attributes:\n\n\n\n```\nclass B {\n    String name;\n    String type;     \n}\n\n```\n\nI need to iterate through the various lists in class `A` and create class `B` object and add to a list like this:\n\n\n\n```\npublic void convertList(A a) {  \n   List<B> b = new ArrayList<>();\n    \n    if (!a.getXList().isEmpty()) {\n      for (final X x : a.getXList()) {\n           b.add(new B(x.getDesc(), x.getXType()));\n      }\n    }\n    \n    if (!a.getYList().isEmpty()) {\n      for (final Y y : a.getYList()) {\n           b.add(new B(y.getName(), y.getYType()));\n      }\n    }\n    \n    if (!a.getZList().isEmpty()) {\n      for (final Z z : a.getZList()) {\n           b.add(new B(z.getDescription(), z.getZType()));\n      }\n    }    \n}\n\n```\n\nAs the if and for loops are repeated here.\n\n\nHow can I achieve this using Java streams?\n\n\nNote: There is no relation between the classes `X`, `Y` and `Z` and there is no common interface.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"Since your `X`, `Y` and `Z` types don't have a common super-type, you have to convert them into some common type, such as `Map.Entry<String,String>`.\n\n\nYou can create a `Stream` of all pairs of names and types, and then map it to instances of `B`:\n\n\n\n```\nList<B> b =\n    Stream.of(\n        a.getXList().stream().map(x -> new SimpleEntry<>(x.getDesc(),x.getXType())),\n        a.getYList().stream().map(y -> new SimpleEntry<>(y.getName(),y.getYType())),\n        a.getZList().stream().map(z -> new SimpleEntry<>(z.getDescription(),z.getZType())))\n          .flatMap(Function.identity())\n          .map(e -> new B(e.getKey(), e.getValue()))\n          .collect(Collectors.toList());\n\n```\n\nOr directly generate `B` instances:\n\n\n\n```\nList<B> b =\n    Stream.of(\n        a.getXList().stream().map(x -> new B(x.getDesc(),x.getXType())),\n        a.getYList().stream().map(y -> new B(y.getName(),y.getYType())),\n        a.getZList().stream().map(z -> new B(z.getDescription(),z.getZType())))\n          .flatMap(Function.identity())\n          .collect(Collectors.toList());\n\n```\n\n"}
{"questionId":"20fd09128087402e921c79e9eb5a11cb","question":"When passing a class by-value, does the caller or callee call the destructor?\nSuppose that I have the following (trimmed down) code:\n\n\n\n```\nclass P { P(); P(const P&); ~P(); }\n\nvoid foo(P x) {\n  ...\n}\n\nvoid bar() {\n  P p{};\n  foo(p); \/\/ compiler uses P::(const P&) to construct the value for x\n  ...\n  \/\/ compiler calls P::~P() on p\n}\n\n```\n\nThe compiler must create a copy of `p` in order to call `foo`, so the *caller* invokes the copy constructor before the call. My question is, who is in charge of destroying this created object? There seem to be two valid choices:\n\n\n1. The callee (i.e. `foo`) calls the destructor on all of its by-value arguments before it returns and then the caller deallocates the memory (by popping it off the stack).\n2. The callee doesn't do anything, and the caller (i.e. `bar`) calls the destructor on all of the temporaries before the sequence point at the end of the `foo(p)` call.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c++"},"answer":"The standard answers this question in [expr.call]\/4, with a surprising amount of elaboration:\n\n\n\n> \n> ... The initialization and destruction of each parameter occurs within the context of\n>  the calling function. [ *Example:* The access of the constructor, conversion functions or destructor is checked\n>  at the point of call in the calling function. If a constructor or destructor for a function parameter throws an\n>  exception, the search for a handler starts in the scope of the calling function; in particular, if the function\n>  called has a *function-try-block* (Clause 18) with a handler that could handle the exception, this handler is not\n>  considered. *\u2014end example* ]\n> \n> \n> \n\n\nIn other words, the destructor is invoked by the calling function.\n\n\n"}
{"questionId":"31e16c856fbf430784bce35a30ae1686","question":"What are the .hi and .o files generated by GHC (on Linux)?\nI am just getting started with Haskell and functional programming in general. After compiling and running my first Haskell program, I noticed that GHC (on Linux) generates three separate files from a single .hs source file: a .hi file, a .o file, and finally an executable. What is the purpose of each of these files? Practically, when are .hi and .o files actually used?\n\n\nFrom my little experience in C, I think .o is an object file. Searching Google shows that .hi is an \"interface file\".\n\n\n","questionMetadata":{"type":"conceptual","level":"beginner","tag":"haskell"},"answer":"The `.o` is exactly the same as C's object files; the `.hi` file is an \"interface file\"; it contains information about the `.o` that GHC would need, if you compile other modules, to be able to link against that `.o` file (said information cannot be stored in a standard `.o` file).\n\n\nYou could say that the `.hi` file is the equivalent of C's header files (that is, with `.h` extension), only these are generated by GHC from the original Haskell source.\n\n\nThus, the `.hi` is used when GHC compiles other modules, and the `.o` is used when linking all modules together to produce the executable.\n\n\nYou can safely delete the `.hi` and `.o` files once you have successfully generated the executable (or keep them if you want to make some small change and rebuild quickly - it will save time in unneeded recompilations).\n\n\n"}
{"questionId":"8acb148f8aaa45f5818bc8e9ac1dcbf7","question":"How to reuse MongoDB connection in Go\nI would like to connect my server that was written in Go with a MongoDB but I'm not sure how to do it in an efficient way. A couple of examples I found implemented it like shown below.\n\n\n**libs\/mongodb\/client.go**\n\n\n\n```\npackage mongodb\n\nimport (\n    \"context\"\n    \"log\"\n    \"project\/keys\"\n\n    \"go.mongodb.org\/mongo-driver\/mongo\"\n    \"go.mongodb.org\/mongo-driver\/mongo\/options\"\n)\n\nfunc GetClient() *mongo.Database {\n    client, err := mongo.Connect(\n        context.Background(),\n        options.Client().ApplyURI(keys.GetKeys().MONGO_URI),\n    )\n\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    return client.Database(keys.GetKeys().MONGO_DB_NAME)\n}\n\n```\n\n**services\/user\/findOne.go**\n\n\n\n```\npackage userservices\n\nimport (\n    \"context\"\n    \"log\"\n    \"project\/libs\/mongodb\"\n    \"project\/models\"\n\n    \"go.mongodb.org\/mongo-driver\/bson\"\n)\n\nfunc FindOne(filter bson.M) (models.User, error) {\n    var user models.User\n\n    collection := mongodb.GetClient().Collection(\"users\")\n    result := collection.FindOne(context.TODO(), filter)\n\n    if result.Err() != nil {\n        return user, result.Err()\n    }\n\n    if err := result.Decode(&user); err != nil {\n        log.Println(\"Failed to decode user with error:\", err)\n        return user, err\n    }\n\n    return user, nil\n}\n\n```\n\nThe `GetClient` function returns a database instance that is then used throughout the app. This seems to work, but I'm wondering if this really is best practice as it seems to create a new connection every time a new client is requested as shown in the second code snippet or is that assumption incorrect? I also thought about converting `GetClient` to a singleton, that always returns the same database instance but how would a lost connection be handled in that case? Thank you\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"go"},"answer":"I do it this way. Do it once at the service start and then pass the MongoDatastore object around to orchestrator, service layers and repository layers. I am using the \"github.com\/mongodb\/mongo-go-driver\/mongo\" driver for mongo. I think it internally monitors and recycles idle connections. Hence, we don't have to bother about broken connections as long as reference to the mongo.Client object is not lost.\n\n\n\n```\n\nconst CONNECTED = \"Successfully connected to database: %v\"\n\ntype MongoDatastore struct {\n    db      *mongo.Database\n    Session *mongo.Client\n    logger  *logrus.Logger\n}\n\nfunc NewDatastore(config config.GeneralConfig, logger *logrus.Logger) *MongoDatastore {\n\n    var mongoDataStore *MongoDatastore\n    db, session := connect(config, logger)\n    if db != nil && session != nil {\n\n        \/\/ log statements here as well\n\n        mongoDataStore = new(MongoDatastore)\n        mongoDataStore.db = db\n        mongoDataStore.logger = logger\n        mongoDataStore.Session = session\n        return mongoDataStore\n    }\n\n    logger.Fatalf(\"Failed to connect to database: %v\", config.DatabaseName)\n\n    return nil\n}\n\nfunc connect(generalConfig config.GeneralConfig, logger *logrus.Logger) (a *mongo.Database, b *mongo.Client) {\n    var connectOnce sync.Once\n    var db *mongo.Database\n    var session *mongo.Client\n    connectOnce.Do(func() {\n        db, session = connectToMongo(generalConfig, logger)\n    })\n\n    return db, session\n}\n\nfunc connectToMongo(generalConfig config.GeneralConfig, logger *logrus.Logger) (a *mongo.Database, b *mongo.Client) {\n\n    var err error\n    session, err := mongo.NewClient(generalConfig.DatabaseHost)\n    if err != nil {\n        logger.Fatal(err)\n    }\n    session.Connect(context.TODO())\n    if err != nil {\n        logger.Fatal(err)\n    }\n\n    var DB = session.Database(generalConfig.DatabaseName)\n    logger.Info(CONNECTED, generalConfig.DatabaseName)\n\n    return DB, session\n}\n\n\n```\n\nYou may now create your repository as below:-\n\n\n\n```\ntype TestRepository interface{\n    Find(ctx context.Context, filters interface{}) []Document, error\n}\n\ntype testRepository struct {\n    store      *datastore.MongoDatastore\n}\n\nfunc (r *testRepository) Find(ctx context.Context , filters interface{}) []Document, error{\n    cur, err := r.store.GetCollection(\"some_collection_name\").Find(ctx, filters)\n    if err != nil {\n        return nil, err\n    }\n    defer cur.Close(ctx)\n    var result = make([]models.Document, 0)\n    for cur.Next(ctx) {\n        var currDoc models.Document\n        err := cur.Decode(&currDoc)\n        if err != nil {\n            \/\/log here\n            continue\n        }\n        result = append(result, currDoc)\n    }\n    return result, err\n}\n\n\n```\n\n"}
{"questionId":"430e4a977833408eb8458564589c2077","question":"Why my composable not recomposing on changing value for MutableState of HashMap?\nWhy my composable not recomposing on changing value for MutableState of HashMap.\n\n\nViewModel\n\n\n\n```\n val imageList: MutableState<HashMap<Int, Uri>> = mutableStateOf(HashMap())\n    fun setImage(imageUri: Uri) {\n        imageList.value[imagePosition] = imageUri\n    }\n\n```\n\nFragment\n\n\n\n```\nif (viewModel.imageList.value[stepNo] != null && !TextUtils.isEmpty(\n                                            viewModel.imageList.value[stepNo].toString()\n                                        )\n                                    ) {\n                                        Image(\n                                            contentDescription = \"Recipe\",\n                                            painter = painterResource(R.drawable.ic_baseline_fastfood_24),\n                                            modifier = Modifier\n                                                .padding(8.dp)\n                                        )\n                                    }\n\nfun permissionCallbacks() {\n        imagePickerListener =\n            registerForActivityResult(ActivityResultContracts.StartActivityForResult()) { result ->\n                if (result.resultCode == Activity.RESULT_OK) {\n                    \/\/ There are no request codes\n                    val data: Intent? = result.data\n                    val selectedImageUri = data?.data\n                    selectedImageUri?.let { viewModel.setImage(it) }\n                }\n            }\n        requestPermissionLauncher =\n            registerForActivityResult(\n                ActivityResultContracts.RequestPermission()\n            ) { isGranted: Boolean ->\n                if (isGranted) {\n                    val galleryIntent = Intent(\n                        Intent.ACTION_PICK,\n                        MediaStore.Images.Media.EXTERNAL_CONTENT_URI\n                    )\n                    galleryIntent.type = \"image\/*\"\n                    imagePickerListener?.launch(galleryIntent)\n                } else {\n                   \n                }\n            }\n    }\n\n```\n\nI see by debugging that value correctly set inside `setImage()` method but if condition for `viewModel.imageList.value[stepNo]` is not called again when value for hashmap is changed.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"kotlin"},"answer":"`mutableStateOf` can only track when you replace one value with an other one. If you change state of current value no way it can handle it\n\n\nYou can use `mutableStateMapOf` instead of `mutableStateOf(HashMap())`:\n\n\n\n```\nval imageList = mutableStateMapOf<Int, Uri>()\n---\nfun setImage(imageUri: Uri) {\n    imageList[imagePosition] = imageUri\n}\n\n```\n\n"}
{"questionId":"1cdbe8fbbe72459cb40263265e4e8f03","question":"How can I create a registry value and path leading to it in one line using PowerShell?\nCreating a registry value, including the path up to it, and not erroring if the path already exists is easy using old-school reg.exe:\n\n\n\n```\nreg add HKCU\\Software\\Policies\\Microsoft\\Windows\\EdgeUI \/f \/v DisableHelpSticker \/t reg_sz \/d 1\n\n```\n\nThat's nice and concise. The shortest way I found to do it in pure PowerShell is two lines, or three if you don't want to repeat the path:\n\n\n\n```\n$regPath = 'HKCU:\\Software\\Policies\\Microsoft\\Windows\\EdgeUI'\nNew-Item $regPath -Force | Out-Null\nNew-ItemProperty $regPath -Name DisableHelpSticker -Value 1 -Force | Out-Null\n\n```\n\nIs there an easier way using pure PowerShell? And without adding a utility function.\n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"bash\/shell"},"answer":"Another way of simplifying PS registry handling is calling the .Net function **SetValue()** directly:\n\n\n\n```\n[microsoft.win32.registry]::SetValue(\"HKEY_CURRENT_USER\\Software\\Test\", \"Test-String\", \"Testing\")\n[microsoft.win32.registry]::SetValue(\"HKEY_CURRENT_USER\\Software\\Test\", \"Test-DW\", 0xff)\n\n```\n\n"}
{"questionId":"654b7002940d481cac3b6d6bb2129b6d","question":"Spring Boot with Elastic Search causing java.lang.NoSuchFieldError: IGNORE\\_DEPRECATIONS\nI'm new to Elastic search. Started building a Spring boot application with Elastic search.\n\n\nUsing the latest ES version `\"elasticsearch-7.7.1\"` and for integration, I'm using below maven dependency:\n\n\n\n```\n    <dependency>\n        <groupId>org.elasticsearch.client<\/groupId>\n        <artifactId>elasticsearch-rest-high-level-client<\/artifactId>\n        <version>7.7.1<\/version>\n    <\/dependency>\n\n```\n\nAdded below configuration to my spring boot app:\n\n\n\n```\n@Configuration\npublic class ESConfig {\n\n  @Bean(destroyMethod = \"close\")\n  public RestHighLevelClient client() {\n    RestHighLevelClient restHighLevelClient = new RestHighLevelClient(\n        RestClient.builder(new HttpHost(\"localhost\")));\n    return restHighLevelClient;\n  }\n\n}\n\n```\n\nAdded below properties to application.yaml\n\n\n\n```\nelasticsearch:\n  host: localhost\n\n```\n\nGetting below Exception on Application startup:\n\n\n\n```\nCaused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.elasticsearch.client.RestHighLevelClient]: Factory method 'client' threw exception; nested exception is java.lang.NoSuchFieldError: IGNORE_DEPRECATIONS\n    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:185)\n    at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:622)\n    ... 19 common frames omitted\nCaused by: java.lang.NoSuchFieldError: IGNORE_DEPRECATIONS\n    at org.elasticsearch.client.RestHighLevelClient.<clinit>(RestHighLevelClient.java:1902)\n    at com.sbs.communicationcontrol.search.config.ESConfig.client(ESConfig.java:14)\n\n```\n\n**Can anyone please help why this exception occurred?**\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"After some R&D, fixed the issue by adding below two dependencies:\n\n\n\n```\n        <dependency>\n            <groupId>org.elasticsearch.client<\/groupId>\n            <artifactId>elasticsearch-rest-client<\/artifactId>\n            <version>7.7.1<\/version>\n        <\/dependency>\n\n        <dependency>\n            <groupId>org.elasticsearch<\/groupId>\n            <artifactId>elasticsearch<\/artifactId>\n            <version>7.7.1<\/version>\n        <\/dependency>\n\n```\n\n"}
{"questionId":"759aceafde1341f2a0d0867c9c2e6bfd","question":"Why is the result value of the compare operation different from the mathematically same formula?\nI'm having trouble comparing size of a vector and simple constant `-1`\n\n\nI believe both of these are logically the same:\n\n\n- `(index >= (arr.size() - 1))`\n- `((index + 1) >= arr.size())`\n\n\nHowever, the first one returns `1` not `0`.\nWhat's the difference between the two comparisons?\n\n\n\n```\n#include <iostream>\n#include <vector>\n\nusing namespace std;\n\nint main() {\n  int index = -1;\n  vector<char> arr(6);\n  cout << (index >= (arr.size() - 1)) << endl;\n  cout << ((index + 1) >= arr.size()) << endl;\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c++"},"answer":"The `arr.size` method returns an unsigned integer type, so the type of the right-hand side of the comparison is unsigned. This results in the left side being converted to unsigned.\n\n\nWhen the value on the left is -1, this gets converted to a very large unsigned number, resulting in the first comparison being true. In the second case, the value on the left is 0 so it doesn't change when being converted to an unsigned type and the comparison is false.\n\n\nIf you compile with `-Wall -Wextra` it will warn you about a signed \/ unsigned comparison.\n\n\n"}
{"questionId":"16f3afd78ed14be4bcec6b67fc26dc65","question":"Avoiding AVX-SSE (VEX) Transition Penalties\nOur 64-bit application has lots of code (inter alia, in standard libraries) that use xmm0-xmm7 registers in SSE mode.\n\n\nI would like to implement fast memory copy using ymm registers. I cannot modify all the code that uses xmm registers to add VEX prefix, and I also think that this is not practical, since it will increase the size of the code can make it run slower because of the need for the CPU to decode larger instructions.\n\n\nI just want to use two ymm registers (and possibly zmm - the affordable processors supporting zmm are promised to be available this year) for fast memory copy.\n\n\nQuestion is: how to use the ymm registers but avoid the transition penalties?\n\n\nWill the penalty occur when I use just ymm8-ymm15 registers (not ymm0-ymm7)? SSE originally had eight 128-bit registers (xmm0-xmm7), but in 64-bit mode there are (xmm8-xmm15) also available for non-VEX-prefixed instructions. However, I have reviewed our 64-bit application and it only use xmm0-xmm7, since it also has a 32-bit version with almost the same code. Does the penalty only occur when the CPU tries in fact to use an xmm register that had been used before as ymm and has one of higher 128 bits non-zero? Isn't it better to just zeroize the ymm registers that I have used after the fast memory copy? For example, I have used an ymm register once to copy 32 bytes of memory - what is the fastest way to zeroize it? Is \"vpxor ymm15, ymm15, ymm15\" fast enough? (AFAIK, vpxor can be executed on any of the 3 ALU execution ports, p0\/p1\/p5, while vxorpd can only be execute on p5). Wouldn't be the time to zeroize it more than the gain of using it to just copy 32 bytes of memory?\n\n\n","questionMetadata":{"type":"optimization","level":"advanced","tag":"assembly"},"answer":"Another possibility is to use registers zmm16 - zmm31. These regsters have no non-VEX counterpart. There is no state transition and no penalty for mixing zmm16 - zmm31 with non-VEX SSE code. These 512-bit registers are only available in 64 bit mode and only on processors with AVX512.\n\n\n"}
{"questionId":"3fcf3c49b49e44ce80de241101ce721a","question":"C++ threads inside a 'for' loop print wrong values\nI'm trying to understand multithreading in C++, but I\u2019m stuck in this problem: if I launch threads in a *for* loop they print wrong values. This is the code:\n\n\n\n```\n#include <iostream>\n#include <list>\n#include <thread>\n\nvoid print_id(int id){\n    printf(\"Hello from thread %d\\n\", id);\n}\n\nint main() {\n    int n=5;\n    std::list<std::thread> threads={};\n    for(int i=0; i<n; i++ ){\n        threads.emplace_back(std::thread([&](){ print_id(i); }));\n    }\n    for(auto& t: threads){\n        t.join();\n    }\n    return 0;\n}\n\n```\n\nI was expecting to get printed the values 0,1,2,3,4 but I often got the same value twice. This is the output:\n\n\n\n```\nHello from thread 2\nHello from thread 3\nHello from thread 3\nHello from thread 4\nHello from thread 5\n\n```\n\nWhat am I missing?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c++"},"answer":"The `[&]` syntax is causing `i` to be captured *by reference*. So quite often therefore `i` will be further advanced when the thread runs than you might expect. More seriously, the behaviour of your code is *undefined* if `i` goes out of scope before a thread runs.\n\n\nCapturing `i` by value - i.e. `std::thread([i](){ print_id(i); })` is the fix.\n\n\n"}
{"questionId":"5853f94dcd2546f59af34337470c08cf","question":"How to run powershell script from .ps1 file?\nI'm trying to automate the execution of a simple PS script (to delete a certain .txt file). Obviously, I'm new to powershell :)\nWhen I run the code in shell, it works flawless. But when i save the code as a .ps1 and double-click it (or execute it remotely), it just pops up a window and does nothing.\n\n\nI've tried to save the code as a .bat file and execute it on Windows command line, but it behaves the same: Works by coding directly on prompt, but doesn't Works by executing the .bat file.\n\n\n\n```\n$Excel = New-Object -ComObject Excel.Application\n$Workbook = $Excel.Workbooks.Open('H:\\codes\\test1.xlsm')\n$workSheet = $Workbook.Sheets.Item(2)\n$str_name = $WorkSheet.Cells.Item(2,1).Text\nRemove-Item -Path \"H:\\text files\\$str_name.txt\" -Force\n\n```\n\nI expected it to work by double-clicking it, just as it does by running in shell, or in the command line, but i can't figure out why it doesn't.\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"bash\/shell"},"answer":"Create a batch file which points at your .ps1 file. You may be required to run the batch file with elevated permissions, depending on your access levels (the logged in account will be used for execution). \n\n\nE.g.:\n\n\n\n```\nPowershell.exe -executionpolicy remotesigned -File  \"C:\\Path\\script.ps1\"\n\n```\n\nIf this still isn't working, please execute your batch file via CMD (copying the path, wrapped in quotation marks, into CMD) and let me know the response.\n\n\n"}
{"questionId":"b3d442e9e89e42b0952553e0f3553259","question":"Disable animation in viewpager2\nI have viewpager2 and adapter for him that extends FragmentStateAdapter. I want user to go to another page only by clicking on tablayout. I have disabled user input for this viewpager2. But when I click on tab, there is animation of fast swiping between pages. But I want just new fragment to show. Like with FragmentTransaction, but with viewpager2 and tablayout. Does anyone knows ho to do it?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"kotlin"},"answer":"You should use addOnTabSelectedListener like this:\n\n\n\n```\n    tabLayout.addOnTabSelectedListener(object : TabLayout.OnTabSelectedListener {\n        override fun onTabReselected(tab: TabLayout.Tab?) {\n        }\n\n        override fun onTabUnselected(tab: TabLayout.Tab?) {\n        }\n\n        override fun onTabSelected(tab: TabLayout.Tab?) {\n            tab?.position?.let { viewPager?.setCurrentItem(it, false) }\n        }\n\n    })\n\n```\n\nYou already use\n\n\n\n> \n> viewPager.isUserInputEnabled = false\n> \n> \n> \n\n\n**Note :** setCurrentItem(int item, boolean smoothScroll)\n\n\n\n> \n> Set the currently selected page with smooth scroll. If you set smooth\n> scroll is false, you don't see the animation\n> \n> \n> \n\n\n"}
{"questionId":"6507e5d422ac42c781db56b0326bccad","question":"Should $null be on the left side of the equality comparison? (-eq with arrays)\nDiscussion with a colleague, should `$null` be on the left or right side of the check? Any examples of why this is important?\n\n\n\n```\n$abc = $null\n$null -eq $abc\nTrue\n$abc -eq $null\nTrue\n\n```\n\nAll ok\n\n\n\n```\n$abc = 6,7,$null,8,9\n$null -eq $abc\nFalse\n$abc -eq $null\n*No output*\n\n```\n\nCan someone explain what is happening when an array is compared?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"bash\/shell"},"answer":"When performing comparison operations, PowerShell will evaluate the **first** operand the operation to determine if it is a *collection* or a *scalar* value. If it is a scalar, it will compare the operands to each other as whole objects.\n\n\n\n```\n2 -eq 1,2,$null,3\nFalse\n\n$null -eq 1,2,$null,3\nFalse\n\n4 -eq 1,2,$null,3\nFalse\n\n```\n\nIf it is a collection, it will iterate through the collection comparing each element to the second operand and return the value of the second operand if it finds a match.\n\n\n\n```\n1,2,$null,3 -eq 2\n2\n\n1,2,$null,3 -eq $null\n<nothing appears, indicating returned value is $null>\n\n1,2,3 -eq 4\n<nothing appears, indicating returned value is $null>\n\n```\n\nThe key difference between the second and third examples. Unless you **know absolutely** whether the second operand is *always* or *never* `$null`, you can't trust the output of the comparison operation.\n\n\nOn a side note, if both operands are collections, `-eq` returns `$null` in every case, because you can't use `-eq` to compare collections.\n\n\nSo to answer the question, it is generally good practice to put `$null` on the left side because when using `$null` in a comparison operation, *it is assumed* that you want to compare scalar values. If that assumption is wrong, then `$null` may be better placed on the opposite side, but it's not likely.\n\n\nContrarily, I always put `$null` on the right side because I write my code under the aforementioned assumption - that I am **always** comparing scalar values when `$null` is explicitly mentioned in the comparison operation. That way, when I get a non-boolean value returned, I know that I have not accessed my collection object properly in the equation. I have gotten caught in the trap, however, so my coding practices may be better served by changing my ways.\n\n\nIn conclusion, like any coding practice, it's a matter of opinion. Were I to teach PowerShell, I would teach this practice. However, I find it unlikely I will change my seditious ways.\n\n\n"}
{"questionId":"c52342b74040458ea6b86ad7a72da27c","question":"Multiple strings in LIKE condition - Presto SQL\nI want to query a column in my table using a `LIKE` condition and this works fine-\n\n\n\n```\nselect * from my_table where my_column LIKE '%hello%';\n\n```\n\nBut, how do I query this column with multiple strings in my `LIKE` condition? Looking for something like-\n\n\n\n```\nselect * from my_table where my_column LIKE ['%hello%'|'example%'|'%random%'|'%demo'];\n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"sql"},"answer":"Use `regexp_like()`:\n\n\n\n```\nselect *\nfrom my_table\nwhere regexp_like(my_column, 'hello|example|random|demo');\n\n```\n\n"}
{"questionId":"a06a0a32946d403daafa52542da7f872","question":"Elasticsearch: could not find java in bundled jdk at ...\/jdk\/bin\/java\nWhen I try to run .bin\/elasticsearch, I get the following error:\n\n\n\n> \n> could not find java in bundled jdk at \/home\/ubuntu\/Elastic\n> Search\/elasticsearch-7.8.0\/jdk\/bin\/java\n> \n> \n> \n\n\nI have absolutely no idea what's going on. I know this topic has been created before, but I haven't found a way to fix. For `java -version` I get:\n\n\n\n```\nopenjdk version \"1.8.0_265\"\nOpenJDK Runtime Environment (build 1.8.0_265-8u265-b01-0ubuntu2~20.04-b01)\nOpenJDK 64-Bit Server VM (build 25.265-b01, mixed mode)\n\n```\n\nCan anyone help me please? Thank you!\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"\n> \n> **The most probable reason for the above error is that `JAVA_HOME` is not set**\n> \n> \n> \n\n\n1. To display `JAVA_HOME` variable path, run this command `echo $JAVA_HOME`\n\n\nIf nothing appears then follow the below steps:\n\n\n2. To see all the java versions installed in Ubuntu, use this command:\n\n\n`sudo update-alternatives --config java`\n\n\n3. Set your java path using this. - `export JAVA_HOME=<YOUR-JAVA-PATH>`\n\n\n"}
{"questionId":"bec800b2b7364f3cbf24f9b7b16f0589","question":"Benefit of using Java Function rather than normal method?\nThe Function Interface is introduced in Java 8, to implement *functional programming* in Java. It represents a function that takes in one argument and produces a result. It's easy to practise and read, but I am still trying to understand the benefit of it other than just making it look cool. For example,\n\n\n\n```\nFunction<Integer, Double> half = a -> a \/ 2.0;\nFunction<Double, Double> triple = b -> b * 3;\ndouble result = half.andThen(triple).apply(8);\n\n```\n\ncan just be converted as a standard method like\n\n\n\n```\nprivate Double half(int a) {\n    return a \/ 2.0;\n}\nprivate Double triple (int b) {\n    return b * 3;\n}\ndouble result = triple(half(8));\n\n```\n\nSo what's the benefit of using Function? As it refers to *functional programming*, what exactly is functional programming in Java and benefit it could bring? Would it benefit the way like:\n\n\n1. execution of *chaining* functions together (e.g andThen & compose)\n2. usage inside Java `Stream`?\n3. the *access modifier* as function tends to define with private not public, while method can be either?\n\n\nBasically, I'm curious to know, in what circumstances would we prefer using function rather than normal method? Is there any use case that's unable or difficult to use, or converted with a normal method?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"java"},"answer":"One usage of `Function` is in Streams. Everyone uses `map` method these days, I believe:\n\n\nThis `map` method accepts the `Function` as a parameter. This allows writing a pretty elegant code - something that could not be achieved before Java 8:\n\n\n\n```\nStream.of(\"a\", \"b\", \"c\")\n   .map(s -> s.toUpperCase())\n   .collect(Collectors.toList());\n\/\/ List of A, B, C\n\n```\n\nNow its true that there are method references and functional interfaces (one of which is `Function` of course), this lets you using method reference to rewrite the above example as:\n\n\n\n```\nStream.of(\"a\", \"b\", \"c\")\n    .map(String::toUpperCase)\n    .collect(Collectors.toList())\n\n```\n\n... but that's only a syntactic sugar - `map` still accepts the `Function` as a parameter of course.\n\n\nAnother example that uses `Function` from Java itself is `StackWalker`:\nHere is an example:\n\n\n\n```\nList<StackFrame> frames = StackWalker.getInstance().walk(s ->\n    s.dropWhile(f -> f.getClassName().startsWith(\"com.foo.\"))\n     .limit(10)\n     .collect(Collectors.toList()));\n}\n\n```\n\nNote the call to `walk` method - it accepts a function as a parameter.\n\n\nSo bottom line, it's just yet another tool that can help the programmer to express his\/her intentions. Use it wisely wherever appropriate.\n\n\n"}
{"questionId":"c277a46ba02442e799886895bf39c5d9","question":"Golang run on Windows without deal with the Firewall\nI'm working on a Rest API with Go, but everytime I try to run my application with\n\n\n\n```\ngo run main.go\n\n```\n\nthe Windows Firewall tells me that has blocked some features of my app. I would like to know if there's some way to make my executions without have to *Accept* everytime.\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"go"},"answer":"Hi I had the same problem:\nTry that:\n\n\n1. Go to **Windows Defender Firewall**, in Left side menu you saw **Inbound Rules** click there, then Right Side menu you will see **New Rule...** click.\n2. *Choose Port* opened from window -> **Next**\n*Select TCP*, then *define which ports you want* I choose 8080 click **Next** again, Choose *Allow the connection* **Next**, Check All **Next**, *Give any Name* Goland or anything you want and press **Finish**. Thats it\n\n\n"}
{"questionId":"4bda302853064c75b2f33538b2fbe3b0","question":"How to configure kafka topic retention policy during creation with Spring?\nI need to configure retention policy of a particular topic during creation. I tried to look for solution i could only find command level alter command as below\n\n\n\n> \n> .\/bin\/kafka-topics.sh --zookeeper localhost:2181 --alter --topic my-topic --config retention.ms=1680000\n> \n> \n> \n\n\nCan someone let me know a way to configure it during creation, something like xml or properties file configuration in spring-mvc.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"Spring Kafka lets you create new topics by declaring `@Bean`s in your application context. This will require a bean of type `KafkaAdmin` in the application context, which will be created automatically if using Spring Boot. You could define your topic as follows:\n\n\n\n```\n@Bean\npublic NewTopic myTopic() {\n    return TopicBuilder.name(\"my-topic\")\n            .partitions(4)\n            .replicas(3)\n            .config(TopicConfig.RETENTION_MS_CONFIG, \"1680000\")\n            .build();\n}\n\n```\n\nIf you are not using Spring Boot, you'll additionally have to define the `KafkaAdmin` bean:\n\n\n\n```\n@Bean\npublic KafkaAdmin admin() {\n    Map<String, Object> configs = new HashMap<>();\n    configs.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,\"localhost:9092\");\n    return new KafkaAdmin(configs);\n}\n\n```\n\nIf you want to edit the configuration of an existing topic, you'll have to use the `AdminClient`, here's the snippet to change the `retention.ms` at a topic level:\n\n\n\n```\nMap<String, Object> config = new HashMap<>();                \nconfig.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,\"localhost:9092\");\n                         \nAdminClient client = AdminClient.create(config);\n                         \nConfigResource resource = new ConfigResource(ConfigResource.Type.TOPIC, \"new-topic\");\n            \n\/\/ Update the retention.ms value\nConfigEntry retentionEntry = new ConfigEntry(TopicConfig.RETENTION_MS_CONFIG, \"1680000\");\nMap<ConfigResource, Config> updateConfig = new HashMap<>();\nupdateConfig.put(resource, new Config(Collections.singleton(retentionEntry)));\n\nAlterConfigOp op = new AlterConfigOp(retentionEntry, AlterConfigOp.OpType.SET);\nMap<ConfigResource, Collection<AlterConfigOp>> configs = new HashMap<>(1);\nconfigs.put(resource, Arrays.asList(op));\n\nAlterConfigsResult alterConfigsResult = client.incrementalAlterConfigs(configs);\n        alterConfigsResult.all();\n\n```\n\nThe configuration can be set up automatically using this `@PostConstruct` method that takes in `NewTopic` beans.\n\n\n\n```\n\n    @Autowired\n    private Set<NewTopic> topics;\n\n    @PostConstruct\n    public void reconfigureTopics() throws ExecutionException, InterruptedException {\n\n        try (final AdminClient adminClient = AdminClient.create(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaBootstrapServers))) {\n            adminClient.incrementalAlterConfigs(topics.stream()\n                .filter(topic -> topic.configs() != null)\n                .collect(Collectors.toMap(\n                    topic -> new ConfigResource(ConfigResource.Type.TOPIC, topic.name()),\n                    topic -> topic.configs().entrySet()\n                        .stream()\n                        .map(e -> new ConfigEntry(e.getKey(), e.getValue()))\n                        .peek(ce -> log.debug(\"configuring {} {} = {}\", topic.name(), ce.name(), ce.value()))\n                        .map(ce -> new AlterConfigOp(ce, AlterConfigOp.OpType.SET))\n                        .collect(Collectors.toList())\n                )))\n                .all()\n                .get();\n        }\n\n    }\n\n```\n\n"}
{"questionId":"08e8d144743843d1a3c0efa4c68f29df","question":"Cannot select where ip=inet\\_pton($ip)\nI have a unique column in database which is named `ip`\n\n\nIP addresses are stored in this column as `BINARY(16)` (with no collation) after converting them using the PHP function \n\n\n\n```\n$store_ip = inet_pton($ip);\n\n```\n\nWhen I try to insert the same IP twice it works fine and fails because it is unique,\n\n\n**But** when I try to select the IP it doesn't work and always returns FALSE (not found)\n\n\n\n```\n<?php\n\ntry {\n    $ip = inet_pton($_SERVER['REMOTE_ADDR']);\n    $stmt = $db->prepare(\"SELECT * FROM `votes` WHERE ip=?\");\n    $stmt->execute([$ip]);\n    $get = $stmt->fetch();\n\n    if( ! $get){\n        echo 'Not found';\n    }else{\n        echo 'Found';\n    }\n\n    \/\/ close connection\n    $get = null;\n    $stmt = null;\n\n} catch (PDOException $e) {\n    error_log($e->getMessage());\n}\n\n```\n\nThe part where I insert the IP:\n\n\n\n```\n<?php\n\nif( ! filter_var($ip, FILTER_VALIDATE_IP)){\n        return FALSE;\n}\n\n$ip = inet_pton($_SERVER['REMOTE_ADDR']);\n\ntry {\n    $stmt = $db->prepare(\"INSERT INTO votes(ip, answer) VALUES(?,?)\");\n    $stmt->execute([$ip, $answer]);\n    $stmt = null;\n} catch (PDOException $e) {\n    return FALSE;\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"First the fix, which is quite simple:\nIf you want to store both, IPv4 and IPv6 addresses,\nyou should use `VARBINARY(16)` instead of `BINARY(16)`.\n\n\nNow to the problem: Why doesn't it work as expected with `BINARY(16)`?\n\n\nConsider we have a table `ips` with only one column `ip BINARY(16) PRIMARY KEY`.\nWe store the default local IPv4 address with\n\n\n\n```\n$stmt = $db->prepare(\"INSERT INTO ips(ip) VALUES(?)\");\n$stmt->execute([inet_pton('127.0.0.1')]);\n\n```\n\nand find the following value in the database:\n\n\n\n```\n0x7F000001000000000000000000000000\n\n```\n\nAs you see - It's a 4 byte binary value (`0x7F000001`)\nright-padded with zeros to fit the 16 byte fixed-length column.\n\n\nWhen you now try to find it with\n\n\n\n```\n$stmt = $db->prepare(\"SELECT * FROM ips WHERE ip = ?\");\n$stmt->execute([inet_pton('127.0.0.1')]);\n\n```\n\nthe following happens:\nPHP sends the value `0x7F000001` as parameter which is then compared\nwith the stored value `0x7F000001000000000000000000000000`.\nBut since two binary values of different length are never equal,\nthe WHERE condition will always return FALSE.\nYou can try it with\n\n\n\n```\nSELECT 0x00 = 0x0000\n\n```\n\nwhich will return `0` (FALSE).\n\n\nNote: The behavior is different for fixed length non binary strings (`CHAR(N)`).\n\n\nWe could use explicit casting as a workaround:\n\n\n\n```\n$stmt = $db->prepare(\"SELECT * FROM ips WHERE ip = CAST(? as BINARY(16))\");\n$stmt->execute([inet_pton('127.0.0.1')]);\n\n```\n\nand it will find the row. But if we look at what we get\n\n\n\n```\nvar_dump(inet_ntop($stmt->fetch(PDO::FETCH_OBJ)->ip));\n\n```\n\nwe will see\n\n\n\n```\nstring(8) \"7f00:1::\"\n\n```\n\nBut that is not (really) what we have tried to store.\nAnd when we now try to store `7f00:1::`,\nwe will get a *duplicate key error*,\nthough we have never stored any IPv6 address yet.\n\n\nSo once again: Use `VARBINARY(16)`, and you can keep your code untouched.\nYou will even save some storage, if you store many IPv4 addresses.\n\n\n"}
{"questionId":"833a1d3a4a0e40fdb84dad1569f2fc3e","question":"Using casts with accessor in laravel 5.3\nI want to use laravel (5.3) eloquent's `$casts` attribute like `protected $casts = [\"example\" => \"object\"]` with `getExampleAttribute` accessor, but as it seems accessor discards the `$casts` behaviour. It's crucial for me since I want to store JSON object in database and have a default value for it, e.g:\n\n\n\n```\npublic function getExampleAttribute($value) {\n    if($value === NULL) \n        return new \\stdclass();\n    return $value\n}\n\n```\n\nso I would never get NULL values in my views. Is there a way to do it easier than just implementing casts logic within accessor and mutator explicitly?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"php"},"answer":"If you want the field to explicitly follow the `$casts` definition, the following solution works. You just need to manually call the cast function from inside an accessor mutator:\n\n\n\n```\npublic function getExampleAttribute($value)\n{\n    \/\/ force the cast defined by $this->casts because\n    \/\/ this accessor will cause it to be ignored\n    $example = $this->castAttribute('example', $value);\n\n    \/** set defaults for $example **\/\n\n    return $example;\n}\n\n```\n\nThis method sort of assumes that you might change the cast in the future, but if you know it's always going to be an array\/json field then you can substitute the `castAttribute()` call like this:\n\n\n\n```\npublic function getExampleAttribute($value)\n{\n    \/\/ translate object from Json storage\n    $example = $this->fromJson($value, true)\n\n    \/** set defaults for $example **\/\n\n    return $example;\n}\n\n```\n\n"}
{"questionId":"049cf6f166c747d4bd5b9fc763506578","question":"SQLAlchemy best practices: when \/ how to configure a scoped\\_session?\nI am trying to figure out the right approach to use SQLAlchemy scoped sessions the \"right way\" while keeping the logic of defining a session separate from configuration and separate from using the session. I have been told a number of times a good aproeach would be to have some global scoped\\_session factory where I can use everywhere: \n\n\n\n```\n\"\"\"myapp\/db.py\n\"\"\"\n\nfrom sqlalchemy.orm import sessionmaker, scoped_session\n\nSession = scoped_session(sessionmaker())\n\n```\n\nThen when I want to use it:\n\n\n\n```\n\"\"\"myapp\/service\/dosomething.py\n\"\"\"\n\nfrom myapp.db import Session\n\ndef do_something(data): \n    \"\"\"Do something with data\n    \"\"\"\n    session = Session()\n    bars = session.query(Bar).all()\n    for bar in bars:\n        bar.data = data\n    session.commit()\n\n```\n\nThis seems right, but my problem is that in all examples I have seen, `sessionmaker` would also set some parameters of the session, namely and most importantly bind an engine. This makes no sense to me, as the actual DB engine will be created from configuration not known at the global scope during the import of the `myapp.db` module. \n\n\nWhat I have looked at doing is to set everything up in my app's \"main\" (or in a thread's main function), and then just assume that the session is configured in other places (such as when used by `do_something()` above):\n\n\n\n```\n\"\"\"myapp\/main.py\n\"\"\"\n\nfrom sqlalchemy import create_engine\nfrom myapp.db import Session\nfrom myapp.service.dosomething import do_something\n\ndef main(): \n    config = load_config_from_file()\n    engine = create_engine(**config['db'])\n    Session.configure(bind=engine)\n\n    do_something(['foo', 'bar'])\n\n```\n\nDoes this seem like a correct approach? I have not found any good examples of such a flow yet most other examples I find seem either over-simplified or framework specific. \n\n\n","questionMetadata":{"type":"implementation","level":"advanced","tag":"python"},"answer":"This is old and I've never accepted any of the answers below, but flowing @univerio's comment and 3+ years of continued usage in SQLAlchemy in various projects, my selected approach now is to keep doing exactly what I suggested in the OP:\n\n\n1. Create a `myapp.db` module which defines `Session = ScopedSession(sessionmaker())`\n2. Import `from myapp.db import Session` everywhere it is needed\n3. In my app's `main` or in the relevant initialization code, do:\n\n\n\n```\ndef main(): \n     config = load_config_from_file()\n     engine = create_engine(**config['db'])\n     Session.configure(bind=engine)\n\n     do_something(['foo', 'bar'])\n\n```\n\nI've used this pattern successfully in Web apps, command line tools and long-running backend processes, and never had to change it so far. Ot is simple, reusable and works great, and I'd recommend it to anyone stumbling here because they've asked themselves the same question I did 3 years ago. \n\n\n"}
{"questionId":"a2914b49fb864eceb7fee0f8c932aeb3","question":"How to correctly specify type hints with AsyncGenerator and AsyncContextManager\nConsider the following code\n\n\n\n```\nimport contextlib\nimport abc\nimport asyncio\n\nfrom typing import AsyncContextManager, AsyncGenerator, AsyncIterator\n\n\nclass Base:\n\n    @abc.abstractmethod\n    async def subscribe(self) -> AsyncContextManager[AsyncGenerator[int, None]]:\n        pass\n\nclass Impl1(Base):\n\n    @contextlib.asynccontextmanager\n    async def subscribe(self) ->  AsyncIterator[ AsyncGenerator[int, None] ]: <-- mypy error here\n\n        async def _generator():\n            for i in range(5):\n                await asyncio.sleep(1)\n                yield i\n                    \n        yield _generator()\n\n```\n\nFor `Impl1.subscribe` mypy gives the error\n\n\n\n```\nSignature of \"subscribe\" incompatible with supertype \"Base\"\n\n```\n\nWhat is the correct way to specify type hints in the above case? Or is mypy wrong here?\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"python"},"answer":"I just happened to come up with the same problem and found this question on the very same day, but also figured out the answer quickly.\n\n\nYou need to remove `async` from the abstract method.\n\n\nTo explain why, I'll simplify the case to a simple async iterator:\n\n\n\n```\n@abc.abstractmethod\nasync def foo(self) -> AsyncIterator[int]:\n    pass\n\nasync def v1(self) -> AsyncIterator[int]:\n    yield 0\n\nasync def v2(self) -> AsyncIterator[int]:\n    return v1()\n\n```\n\nIf you compare v1 and v2, you'll see that the function signature looks the same, but they actually do very different things. v2 is compatible with the abstract method, v1 is not.\n\n\nWhen you add the `async` keyword, mypy infers the return type of the function to be a `Coroutine`. But, if you also put a `yield` in, it then infers the return type to be `AsyncIterator`:\n\n\n\n```\nreveal_type(foo)\n# -> typing.Coroutine[Any, Any, typing.AsyncIterator[builtins.int]]\nreveal_type(v1)\n# -> typing.AsyncIterator[builtins.int]\nreveal_type(v2)\n# -> typing.Coroutine[Any, Any, typing.AsyncIterator[builtins.int]]\n\n```\n\nAs you can see, the lack of a `yield` in the abstract method means that this is inferred as a `Coroutine[..., AsyncIterator[int]]`. In other words, a function used like `async for i in await v2():`.\n\n\nBy removing the `async`:\n\n\n\n```\n@abc.abstractmethod\ndef foo(self) -> AsyncIterator[int]:\n    pass\nreveal_type(foo)\n# -> typing.AsyncIterator[builtins.int]\n\n```\n\nWe see that the return type is now `AsyncIterator` and is now compatible with v1, rather than v2. In other words, a function used like `async for i in v1():`\n\n\nYou can also see that this is fundamentally the same thing as v1:\n\n\n\n```\ndef v3(self) -> AsyncIterator[int]:\n    return v1()\n\n```\n\nWhile the syntax is different, both v3 and v1 are functions which will return an `AsyncIterator` when called, which should be obvious given that we are literally returning the result of `v1()`.\n\n\n"}
{"questionId":"6e4bcc81cbc74b4083de1b11b810de59","question":"Correctly Injecting Serilog into .net core classes as Microsoft.Extentions.Logging.ILogger - Not ASP .Net Core\nSo I have a .Net Core Console application and a bunch of .Net core Libraries.\n\n\nMost of the classes in the libraries have constructors like this.\n\n\n\n```\npublic class ReportingManager\n{\n   private ILogger _logger;\n   Public ReportingManager(ILogger logger)\n   {\n      _logger = logger;\n   }\n}\n\n```\n\nwith `ILogger` being of type `Microsoft.Extentions.Logging.ILogger`\n\n\nIn my Console app I have this.\n\n\n\n```\nclass Program\n{\n    static void Main(string[] args)\n    {           \n            var serilog = new LoggerConfiguration()\n                .MinimumLevel.Debug()\n                .MinimumLevel.Override(\"Microsoft\", Serilog.Events.LogEventLevel.Warning)\n                .Enrich.FromLogContext()\n                .WriteTo.Console(theme: AnsiConsoleTheme.Code)\n                .CreateLogger();\n\n            ReportingManager manager = new ReportingManager(serilog);\n\n    }\n}\n\n```\n\nI have an intellisense error at 'ReportingManager manager = new ReportingManager(serilog);'\n\n\n\n> \n> cannot convert from 'Serilog.Core.Logger' to 'Microsoft.Extensions.Logging.ILogger'\n> \n> \n> \n\n\nSo my question is, how do I pass in Serilog correctly?\n\n\nHad a look online and here but most talk about ASP.Net Core.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"You need to wrap the Serilog logger into `Microsoft.Extensions.Logging.LoggerFactory`. This is exactly what happens when same is used in DI in ASP.NET.\n\n\nLike this : \n\n\n\n```\nSerilog.Core.Logger serilog = ...;\n\nvar loggerFactory = new LoggerFactory()\n    .AddSerilog(serilog);\n\nMicrosoft.Extensions.Logging.ILogger logger = loggerFactory.CreateLogger(\"Logger\");\n\n```\n\nThis needs `Serilog.Extensions.Logging` NuGet package.\n\n\n"}
{"questionId":"19ed198fec6945899607c35c26f9a95a","question":"How to solve zsh compinit: insecure directories issue on MacOS (other solutions failed)\nI'm aware there are many copies of this question here, but all of their answers recommend adding\n\n\n\n```\nZSH_DISABLE_COMPFIX=\"true\"\n\n```\n\nto the top of my ~\/.zshrc file. I have done this and still every time I open zsh I am greeted with\n\n\n\n```\nzsh compinit: insecure directories, run compaudit for list.\nIgnore insecure directories and continue [y] or abort compinit [n]?\n\n```\n\nIt seems that others asking this question didn't have the quotes around the true in the first sample, but I have added that. I have also run\nsource ~\/.zshrc\nWhich as far as I can tell reloads the zshrc configuration. This still gives me the above warning. I'm not sure if any of these details could be relevant but I'll include them:\n\n\n- This is a new zsh installation on an M1 Macbook running Big Sur\n- I also have Oh My Zsh installed on top of zsh\n- I earlier ran several export commands to set my nvm directory but I don't think that would be relevant\n\n\nAny idea how to resolve this permissions issue? Thanks\n\n\nEdit:\n\n\ncompaudit returns\n\n\n\n```\n\/usr\/local\/share\/zsh\/site-functions\n\/usr\/local\/share\/zsh\n\n```\n\nAlso, here are the other nonstandard entries in my ~\/.zshrc file (in order, but there is some built-in stuff inbetween):\n\n\n\n```\nZSH_DISABLE_COMPFIX=\"true\"\nexport NVM_DIR=~\/.nvm\nsource $(brew --prefix nvm)\/nvm.sh\nexport PATH=\"\/usr\/local\/opt\/icu4c\/bin:$PATH\"\nexport PATH=\"\/usr\/local\/opt\/icu4c\/sbin:$PATH\"\nexport PATH=$HOME\/bin:\/usr\/local\/bin:$PATH\nplugins=(git)\nsource $ZSH\/oh-my-zsh.sh\nzstyle :compinstall filename '\/Users\/jonahsaltzman\/.zshrc'\n# End of lines configured by zsh-newuser-install\n# The following lines were added by compinstall\nautoload -Uz compinit\ncompinit\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"First of all, one problem here is that you\u2019re running `compinit` *twice:* Once through OMZ (Oh My Zsh) \u2013 when you do `source $ZSH\/oh-my-zsh.sh` \u2013 and once manually. You have two options to fix this:\n\n\n- If you want to keep using OMZ, then you should remove the bottom 3 lines from your `.zshrc` file.\n- If you want to *stop* using OMZ, then instead, you should remove both `plugins=(git)` and `source $ZSH\/oh-my-zsh.sh`\n\n\nSecondly, note that `$ZSH_DISABLE_COMPFIX` is specific to OMZ and is not used by `compinit` itself. It has no effect when you call `compinit` manually. You can remove it from your `.zshrc`\n\n\nFinally, `compinit` doesn\u2019t show that warning for nothing. Rather than suppress it, you should instead do `chmod g-w,o-w` on the directories listed by `compaudit`. That will fix the problem and make the warning go away.\n\n\n"}
{"questionId":"1f8732e2e44b482582761e8a68222b69","question":"Difference between type alias and NewType\nWhat is the difference between this:\n\n\n\n```\nINPUT_FORMAT_TYPE  = NewType('INPUT_FORMAT_TYPE', Tuple[str, str, str])\n\n```\n\nand this\n\n\n\n```\nINPUT_FORMAT_TYPE  = Tuple[str, str, str]\n\n```\n\nFunctionally, both work but IDEs like PyCharm flag code like this:\n\n\n\n```\nreturn cast(INPUT_FORMAT_TYPE, (\"*\", \"*\", \"All\"))\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"python"},"answer":"`InputFormat` (renamed it to keep type notation consistent) can be a subtype or alias of `Tuple[str, str, str]`. Having it be a subtype (your first example) instead of an alias (your second example) is useful for a situation where you want to statically verify (through something like `mypy`) that all `InputFormat`s were made in a certain way. For example:\n\n\n\n```\ndef new_input_format(a: str) -> InputFormat:\n    return InputFormat((a, a * 2, a * 4))\n\ndef print_input_format(input_format: InputFormat):\n    print(input_format)\n\nprint_input_format(new_input_format(\"a\")) # Statement 1\nprint_input_format((\"a\", \"aa\", \"aaa\"))    # Statement 2\n\n```\n\nIf `InputFormat` is declared as an alias (through `InputFormat = Tuple[str, str, str]`), both statements will statically verify. If `InputFormat` is declared as a subtype (through `InputFormat = NewType('InputFormat', Tuple[str, str, str])`), only the first statement will statically verify.\n\n\nNow this isn't foolproof. A third statement such as:\n\n\n\n```\nprint_input_format(InputFormat((\"a\", \"aa\", \"aaa\")))\n\n```\n\nwill statically verify, yet it bypasses our careful `InputFormat` creator called `new_input_format`. However, by making `InputFormat` a subtype here we were forced to explicitly acknowledge that we're creating an input format through having to wrap the `tuple` in an `InputFormat`, which makes it easier to maintain this type of code and spot potential bugs in input format constructions.\n\n\nAnother example where `NewType` is beneficial over a type alias:\n\n\nLet's say you had a database which we expose two functions for:\n\n\n\n```\ndef read_user_id_from_session_id(session_id: str) -> Optional[str]:\n    ...\n\ndef read_user(user_id: str) -> User:\n    ...\n\n```\n\nintended to be called like this (exhibit A):\n\n\n\n```\nuser_id = read_user_id_by_session_id(session_id)\n\nif user_id:\n    user = read_user(user_id)\n\n    # Do something with `user`.\nelse:\n    print(\"User not found!\")\n\n```\n\nForget about the fact that we can use a join here to make this only one query instead of two. Anyways, we want to only allow a return value of `read_user_id_from_session_id` to be used in `read_user` (since in our system, a user ID can only come from a session). We don't want to allow any value, reason being that it's probably a mistake. Imagine we did this (exhibit B):\n\n\n\n```\nuser = read_user(session_id)\n\n```\n\nTo a quick reader, it may appear correct. They'd probably think a `select * from users where session_id = $1` is happening. However, this is actually treating a `session_id` as a `user_id`, and with our current type hints it passes despite causing unintended behavior at runtime. Instead, we can change the type hints to this:\n\n\n\n```\nUserID = NewType(\"UserID\", str)\n\ndef read_user_id_from_session_id(session_id: str) -> Optional[UserID]:\n    ...\n\ndef read_user(user_id: UserID) -> User:\n    ...\n\n```\n\nExhibit A expressed above would still work, because the flow of data is correct. But we'd have to turn Exhibit B into\n\n\n\n```\nread_user(UserID(session_id))\n\n```\n\nwhich quickly points out the problem of converting a `session_id` to a `user_id` without going through the required function.\n\n\nIn other programming languages with better type systems, this can be taken a step further. You can actually prohibit explicit construction like `UserID(...)` in all but one place, causing everyone to have to go through that one place in order to obtain a piece of data of that type. In Python, you can bypass the intended flow of data by explicitly doing `YourNewType(...)` anywhere. While `NewType` is beneficial over simply type aliases, it leaves this feature to be desired.\n\n\n"}
{"questionId":"022bf9267a4c4e1898e81df22e2254b2","question":"Github actions: set environment variable for Windows build with PowerShell\nI define `GENERATOR_PLATFORM` as an empty environment variable, and then I want\nto set it to something for my Windows build. But, the variable never gets set:\n\n\n\n```\nenv:\n  GENERATOR_PLATFORM:\n\n steps:\n    - name: windows-dependencies\n      if: startsWith(matrix.os, 'windows')\n      run: |\n         $generator= \"-DCMAKE_GENERATOR_PLATFORM=x64\"\n        echo \"Generator: ${generator}\"\n        echo \"GENERATOR_PLATFORM=$generator\" >> $GITHUB_ENV\n\n   - name: Configure CMake\n      shell: bash\n      working-directory: ${{github.workspace}}\/build\n      run: cmake $GITHUB_WORKSPACE $GENERATOR_PLATFORM\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"If you are using a Windows\/PowerShell environment, you have to use `$env:GITHUB_ENV` instead of `$GITHUB_ENV`:\n\n\n\n```\n    echo \"GENERATOR_PLATFORM=$generator\" >> $env:GITHUB_ENV\n\n```\n\nThis way, you can access your env var through `$env:GENERATOR_PLATFORM`, eg:\n\n\n\n```\n    run: echo $env:GENERATOR_PLATFORM\n\n```\n\n"}
{"questionId":"c4b52b9f31144f13ba2cf8440a803955","question":"mockk exception - no answer found for\nusing mockk for testing kotlin function.\n\n\n\n```\nprivate val serviceObject = mockk<Service>()\nprivate val serviceToBeTested = ServiceToBeTestd(Service)\n        \nfun test(){\n    when(serviceObject.function1(argument1,argument1))\n        .thenReturn(<something>)\n}\n\n```\n\nWhen i try to run it, i get this error:\n\n\n\n```\nio.mockk.MockKException: no answer found for: Service(#1).function1(argument1, argument2)\n\n```\n\nAny idea why ?\n\n\nServiceToBeTestd is the service to be tested, Service is wired in it:\n\n\n\n```\nopen class ServiceToBeTestd\n    constructor(private val service: Service)\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"kotlin"},"answer":"You are using mockito syntax.\n\n\nBelow is correct syntax for mockk.\n\n\n\n```\nval car = mockk<Car>()\n\nevery { car.drive(Direction.NORTH) } returns Outcome.OK\n\n```\n\nPlease update your syntax.\n\n\n"}
{"questionId":"2e631733e49247c389a9c1d97721c935","question":"TypeScript add custom Request header in Express\nI am trying to add a **custom header** to my request, but it must be modified\/implemented in the interface. \n\n\nThe default `Request` interface references `IncomingHttpHeaders`. So I am attempting to extend this interface with my own custom token header. \n\n\n\n```\nimport { IncomingHttpHeaders } from 'http';\n\ndeclare module 'express-serve-static-core' {\n    interface IncomingHttpHeaders {\n        \"XYZ-Token\"?: string\n    }\n}\n\n```\n\nI have updated my `.tsconfig` file to read the `.\/types` folder. The name of my file is `index.d.ts`\n\n\nI can successfully compile the code if I do not use my custom header, but when I try to reference the token header in the code I get the following compilation error: \n\n\n**Error**\n\n\n\n```\nerror TS2538: Type 'string[]' cannot be used as an index type.\n\n    req.headers['XYZ-Token']\n\n\n```\n\nIf I use any of the values of the original interface everything works fine.\n\n\n**Example:**\n\n\n\n```\n    req.headers['user-agent']\n\n```\n\n**Additional information**: I am using NestJS, which uses Fastify\/Express under the hood. I can confirm that the Request interface being used is from Express. Fastify is backwards compatible with all Express modules. Mainly using Fastify because it's faster.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"Looks like the wrong module name is in the declaration. \n\n\nEven though the `IncomingHttpHeaders` interface is being attached to the `Request` object from `express-serve-static-core`, the original source of the `IncomingHttpHeaders` interface is actually part of the `http` package.\n\n\nThe following allowed the custom header to be accessible in the code and ts compiled correctly.\n\n\n\n```\nimport { IncomingHttpHeaders } from 'http';\n\ndeclare module 'http' {\n    interface IncomingHttpHeaders {\n        \"XYZ-Token\"?: string\n    }\n}\n\n```\n\n"}
{"questionId":"492bc6844dc041ffb8faae4732343012","question":"How to solve \"RuntimeError: CUDA error: invalid device ordinal\"?\nI'm trying to run this code. I don't know what is wrong with it, but this code is not running. and I don't know how to solve this problem.\n\n\n\n```\nimport cv2\nfrom facial_emotion_recognition import EmotionRecognition\n\nemotion_detector = EmotionRecognition(device='gpu', gpu_id=1)\ncamera = cv2.VideoCapture(0)\n\nwhile True:\n    image = camera.read()[1]\n    image = emotion_detector.recognise_emotion(image, return_type='BGR')\n    cv2.imshow('Camera', image)\n\n    key = cv2.waitKey(1)\n    if key == 27:\n        break\n\ncamera.release()\ncv2.destroyAllWindows()\n\n```\n\nbut I'm getting this error:\n\n\n\n```\nTraceback (most recent call last):\n  File \"\/home\/fahim\/Documents\/Python_projects\/Python tutorials\/pantech AI Master\/Computer_Vision\/Day 8 Face emotion recognition\/emotion.py\", line 4, in <module>\n    emotion_detector = EmotionRecognition(device='gpu', gpu_id=1)\n  File \"\/home\/fahim\/anaconda3\/envs\/Computer_Vision\/lib\/python3.7\/site-packages\/facial_emotion_recognition\/facial_emotion_recognition.py\", line 25, in __init__\n    self.network = NetworkV2(in_c=1, nl=32, out_f=7).to(self.device)\n  File \"\/home\/fahim\/anaconda3\/envs\/Computer_Vision\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 607, in to\n    return self._apply(convert)\n  File \"\/home\/fahim\/anaconda3\/envs\/Computer_Vision\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 354, in _apply\n    module._apply(fn)\n  File \"\/home\/fahim\/anaconda3\/envs\/Computer_Vision\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 354, in _apply\n    module._apply(fn)\n  File \"\/home\/fahim\/anaconda3\/envs\/Computer_Vision\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 376, in _apply\n    param_applied = fn(param)\n  File \"\/home\/fahim\/anaconda3\/envs\/Computer_Vision\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 605, in convert\n    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\nRuntimeError: CUDA error: invalid device ordinal\n\nProcess finished with exit code 1\n\n```\n\nThis is my the configuration of my computer:\nGPU: NVIDIA GeForce MX130\nCPU: Intel i5-10210U (8) @ 4.200GHz\nHelp me to solve this please.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"Try changing:\n\n\n\n```\nemotion_detector = EmotionRecognition(device='gpu', gpu_id=1)\n\n```\n\nTo:\n\n\n\n```\nemotion_detector = EmotionRecognition(device='gpu', gpu_id=0)\n\n```\n\n`gpu_id` is only effective when more than one GPU is detected, you only seem to have one GPU, so it throws an error since you tell the function to get GPU 2 (since we count from 0).\n\n\n"}
{"questionId":"7550e32783d245eda2390f5d15db0da8","question":"How can I cross compile Rust code into Intel assembly on an ARM M1 Apple Silicon Mac?\nI've been comparing the assembly code generated by C and Rust for x86 and ARM.\n\n\nI have an M1 Mac and I found how to cross-compile C with Clang, but so far I can't find how to cross-compile Rust.\n\n\nHow can I generate an x86\\_64 binary from Rust on an M1 Mac?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"rust"},"answer":"Cross-compilation is built in, just use `rustup` to install the target support:\n\n\n\n```\n$ rustup target install x86_64-apple-darwin\n\n```\n\nand build your crate like this:\n\n\n\n```\n$ cargo build --target x86_64-apple-darwin\n\n```\n\nThanks to Rosetta you can even run it like this:\n\n\n\n```\n$ cargo run --target x86_64-apple-darwin\n\n```\n\n"}
{"questionId":"fce95456abeb4a40942c52deea0c1d54","question":"Associate a function with a type in Haskell\nSuppose you have a serializer\/deserializer type class\n\n\n\n```\nclass SerDes a where\n    ser :: a -> ByteString\n    des :: ByteString -> a\n\n```\n\nand it turns out that it's crucial to have a special helper function for each type `a`, e.g.\n\n\n\n```\ncompress :: ByteString -> ByteString     -- actually varies with the original type\n\n```\n\nI see `compress` as a function that I would like to associate with each `a` that is a `SerDes`. (The word \"associate\" is probably a bad choice, and the reason why internet searches yield nothing.)\n\n\nThe example is not as contrived as it looks, for example when `decompress` is an optional\nfeature of the serializer\/deserializer. (Yes, the helper could be avoided by augmenting\n`ser` with a switch that controls the compression, `ser:: a -> Bool -> ByteString`, or better use a `Config` record. But let's stick with the example.)\n\n\nOne way to do this is a 'dummy' class, a singleton:\n\n\n\n```\ndata For a = For\n\n```\n\nThen this will work:\n\n\n\n```\nclass SerDes a where\n    ser      :: a -> ByteString\n    des      :: ByteString -> a\n    compress :: For a -> ByteString -> ByteString\n\n```\n\nand a `compress` for `a` would be instantiated as\n\n\n\n```\ncompress (For :: For MyType) input = ...\n\n```\n\nAnother way, somewhat unusual, would be to stick all the functions in a record.\n\n\n\n```\ndata SerDes a = SerDes { ser      :: a -> ByteString\n                       , des      :: ByteString -> a\n                       , compress :: ByteString -> ByteString \n                       }\n\n```\n\nAre there any other ways to \"associate\" the `compress` function with the type `a`?\n\n\n","questionMetadata":{"type":"implementation","level":"advanced","tag":"haskell"},"answer":"Your `For a` type is known as `Proxy a` in the libraries.\n\n\n\n```\nimport Data.Proxy\n\nclass SerDes a where\n    ser      :: a -> ByteString\n    des      :: ByteString -> a\n    compress :: Proxy a -> ByteString -> ByteString\n\n```\n\nSometimes this is generalized to a generic `proxy` type variable.\n\n\n\n```\nclass SerDes a where\n    ser      :: a -> ByteString\n    des      :: ByteString -> a\n    compress :: proxy a -> ByteString -> ByteString\n\n```\n\n\n\n---\n\n\nThere is another option, similar to proxies. Instead of forcibly adding `a` to the arguments, one can add `a` to the result type using `Tagged`:\n\n\n\n```\nimport Data.Tagged\n\nclass SerDes a where\n    ser      :: a -> ByteString\n    des      :: ByteString -> a\n    compress :: ByteString -> Tagged a ByteString\n\n```\n\nThis needs to be used as `unTagged (compress someByteString :: Tagged T ByteString)` to tell the compiler we want the `compress` function for `T`.\n\n\n\n\n---\n\n\nPersonally, I'm not a fan of proxies and tags. They were needed in the past when GHC did not allow another simpler solution, but right now they should no longer be used.\n\n\nThe modern approach is to turn on the harmless extensions `AllowAmbiguousTypes` and `TypeApplications` and simply write your wanted class\n\n\n\n```\nclass SerDes a where\n    ser      :: a -> ByteString\n    des      :: ByteString -> a\n    compress :: ByteString -> ByteString\n\n```\n\nIn this approach, instead of calling `compress (Proxy :: Proxy T) someByteString` we will need to use the shorter `compress @T someByteString` where we explicitly \"pass the type `a` we want\" (`T` in this case), so to select the wanted `compress`.\n\n\nFull example:\n\n\n\n```\n{-# LANGUAGE AllowAmbiguousTypes, TypeApplications, OverloadedStrings #-}\n\nimport Data.ByteString as BS\n\nclass SerDes a where\n    ser      :: a -> ByteString\n    des      :: ByteString -> a\n    compress :: ByteString -> ByteString\n\n-- bogus implementation to show everything type checks\ninstance SerDes Int where\n   ser _ = \"int\"\n   des _ = 42\n   compress bs = BS.tail bs\n\n-- bogus implementation to show everything type checks\ninstance SerDes Bool where\n   ser _ = \"bool\"\n   des _ = True\n   compress bs = bs <> bs\n\nmain :: IO ()\nmain = BS.putStrLn (compress @Int \"hello\" <> compress @Bool \"world\")\n-- output: elloworldworld\n\n```\n\n"}
{"questionId":"fc5551d81f4949a6895470704400cd58","question":"How to create a unique index containing multiple fields where one is a foreign key\nI am trying to create an index with multiple fields, one of the field is a foriegn key to another table. However i get the following error:\n\n\n\n> \n> Error: Index \"player\\_id\\_UNIQUE\" contains column that is missing in the\n>  entity (Earning): player\\_id\n> \n> \n> \n\n\nGiven that player\\_id is a foriegn key that im joining how do i handle this\n\n\n\n```\nimport { Column, Entity, Index, JoinColumn, ManyToOne, PrimaryColumn } from \"typeorm\";\nimport { PersonPlayer } from \".\/PersonPlayer\";\nimport { Team } from \".\/Team\";\n\n    @Entity()\n    @Index(\"player_id_UNIQUE\", [\"player_id\", \"period\", \"year\"], { unique: true })\n    export class Earning {\n\n        @PrimaryColumn({length: 36})\n        id: string;\n\n        @Column({nullable: true})\n        year: number;\n\n        @Column({type: 'decimal', nullable: true})\n        amount: number;\n\n        @Column({nullable: true, length: 45})\n        period: string;\n\n        @ManyToOne(() => Team, {nullable: true})\n        @JoinColumn({name: 'team_id'})\n        team: Team;\n\n\n        @ManyToOne(() => PersonPlayer, {nullable: true})\n        @JoinColumn({name: 'player_id'})\n        player: PersonPlayer;\n\n        @Column({nullable: true, length: 45})\n        dtype: string;\n\n    }\n\n```\n\nWhen i generate this entity and create the sql table (without the index) i see `player_id` as one of the columns. But it appears that typeorm is not able to recognize right now with the index that player\\_id exists in the entity through the joincolumn relationship.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"This is completely undocumented so it took some playing around until I stumbled upon the correct syntax. You can actually use sub-properties in index definitions:\n\n\n\n```\n@Index(\"player_id_UNIQUE\", [\"player.id\", \"period\", \"year\"], { unique: true })\n\n```\n\nThat way, `player.id` is automatically mapped to `player_id` in the resulting SQL:\n\n\n\n```\nCREATE UNIQUE INDEX \"player_id_UNIQUE\" ON \"user_earning\" (\"player_id\", \"period\", \"year\")\n\n```\n\n"}
{"questionId":"dbfe97deab544fa2af9160e470b86a64","question":"Measure execution time in C++ OpenMP code\nI am running a .cpp code (i) in sequential style and (ii) using OpenMP statements. I am trying to see the time difference. For calculating time, I use this:\n\n\n\n```\n#include <time.h>\n.....\nmain()\n{\n\n  clock_t start, finish;\n  start = clock();\n  .\n  .\n  .\n  finish = clock();\n\n  processing time = (double(finish-start)\/CLOCKS_PER_SEC);\n\n }\n\n```\n\nThe time is pretty accurate in sequential (above) run of the code. It takes about 8 seconds to run this. When I insert OpenMP statements in the code and thereafter calculate the time I get a reduction in time, but the time displayed is about 8-9 seconds on the console, when actually its just 3-4 seconds in real time!\n\n\nHere is how my code looks abstractly:\n\n\n\n```\n#include <time.h>\n.....\nmain()\n{\n\n  clock_t start, finish;\n  start = clock();\n  .\n  .\n  #pragma omp parallel for\n  for( ... )\n     for( ... )\n       for (...)\n    {           \n      ...;      \n    }\n  .\n  .\n  finish = clock();\n\n  processing time = (double(finish-start)\/CLOCKS_PER_SEC);\n\n }\n\n```\n\nWhen I run the above code, I get the reduction in time but the time displayed is not accurate in terms of real time. It seems to me as though the clock () function is calculating each thread's individual time and adding up them up and displaying them.\n\n\nCan someone tell the reason for this or suggest me any other timing function to use to measure the time in OpenMP programs?\n\n\nThanks.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c++"},"answer":"You could use the built in `omp_get_wtime` function in omp library itself. Following is an example code snippet to find out execution time.\n\n\n\n```\n#include <stdio.h>\n#include <omp.h>\n\nint main(){\n\n    double itime, ftime, exec_time;\n    itime = omp_get_wtime();\n\n    \/\/ Required code for which execution time needs to be computed\n    \n    ftime = omp_get_wtime();\n    exec_time = ftime - itime;\n    printf(\"\\n\\nTime taken is %f\", exec_time);\n\n}\n\n```\n\n"}
{"questionId":"48113217906044978eeee15ddd90fa81","question":"How to use \"else if\" with the preprocessor #ifdef?\nIn my project the program can do one thing of two, but never both, so I decided that the best i can do for one class is to define it depending of a #define preprocessor variable. The next code can show you my idea, but you can guess that it does not work:\n\n\n\n```\n#ifdef CALC_MODE\ntypedef MyCalcClass ChosenClass;\n#elifdef USER_MODE\ntypedef MyUserClass ChosenClass;\n#else\nstatic_assert(false, \"Define CALC_MODE or USER_MODE\");\n#endif\n\n```\n\nSo i can do\n\n\n\n```\n#define CALC_MODE\n\n```\n\nright before this.\n\n\nI can resign the use of `static_assert` if needed. How to do this?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c++"},"answer":"Here's a suggestion, based largely on comments posted to your question:\n\n\n\n```\n#if defined(CALC_MODE)\n    typedef MyCalcClass ChosenClass;\n#elif defined(USER_MODE)\n    typedef MyUserClass ChosenClass;\n#else\n    #error \"Define CALC_MODE or USER_MODE\"\n#endif\n\n```\n\n"}
{"questionId":"a8c68f94e85446dc87d98aa396204e56","question":"C# Switch expressions returning different result\nI've switched to C# 8 on one of my projects. And I've been moving all of my `switch` statements to expressions. However I found out that my project started working differently and I've found out that it was because of the `switch` expression. Lets get this code for example\n\n\n\n```\nclass Program\n{\n    public enum DataType\n    {\n        Single,\n        Double,\n        UInt16,\n        UInt32,\n        UInt64,\n        Int16,\n        Int32,\n        Int64,\n        Byte\n    }\n\n    static void Main(string[] args)\n    {\n        dynamic value1 = 5;\n        dynamic value2 = 6;\n\n        var casted = CastToType(value1, DataType.Int16);\n        var casted1 = CastToTypeExpression(value2, DataType.Int16);\n\n\n        var type = casted.GetType(); \/\/ Int16\n        var type1 = casted1.GetType(); \/\/ Double\n        var bytes = BitConverter.GetBytes(casted); \/\/ byte arr with 2 el => [5, 0] <- expected behavior \n        var bytes1 = BitConverter.GetBytes(casted1); \/\/ byte arr with 8 el => [0, 0, 0, 0, 0, 0, 24, 64]\n    }\n\n    public static dynamic CastToType(dynamic value, DataType type)\n    {\n        switch (type)\n        {\n            case DataType.Byte:\n                return (byte)value;\n            case DataType.Double:\n                return (double)value;\n            case DataType.Int16:\n                return (short)value;\n            case DataType.Int32:\n                return (int)value;\n            case DataType.Int64:\n                return (long)value;\n            case DataType.Single:\n                return (float)value;\n            case DataType.UInt16:\n                return (ushort)value;\n            case DataType.UInt32:\n                return (uint)value;\n            case DataType.UInt64:\n                return (ulong)value;\n            default: throw new InvalidCastException();\n        }\n    }\n\n    public static dynamic CastToTypeExpression(dynamic value, DataType type)\n    {\n        return type switch\n        {\n            DataType.Byte => (byte)value,\n            DataType.Double => (double)value,\n            DataType.Int16 => (short)value,\n            DataType.Int32 => (int)value,\n            DataType.Int64 => (long)value,\n            DataType.Single => (float)value,\n            DataType.UInt16 => (ushort)value,\n            DataType.UInt32 => (uint)value,\n            DataType.UInt64 => (ulong)value,\n            _ => throw new InvalidCastException(),\n        };\n    }\n}\n\n```\n\nI've wrote the result as a comment, but tl;dr when classic switch is used casting the value returns the value in the expected Type, but when switch expression is used it returns type of \"Double\", resulting to different `byte[]` when getting the bytes of the value.\n\n\nWhat's the difference between the two? What do I miss?\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"c#"},"answer":"In your switch *statement* form, each arm is returning a value directly. It's converting from the numeric type directly to `object`, as that's effectively the return type of the method.\n\n\nYour switch *expression* form is slightly different. It first extracts a result from the switch expression, then converts that result to the declared return type. So what's the type of the switch expression? It's the \"best\" type from all of the types of the individual expressions in the arms of the switch expression.\n\n\nAll of those types can be implicitly converted to `double` (which is one of the types itself), so that's the best type. So your switch expression method is equivalent to:\n\n\n\n```\npublic static dynamic CastToTypeExpression(dynamic value, DataType type)\n{\n    double result = type switch\n    {\n        DataType.Byte => (byte)value,\n        DataType.Double => (double)value,\n        DataType.Int16 => (short)value,\n        DataType.Int32 => (int)value,\n        DataType.Int64 => (long)value,\n        DataType.Single => (float)value,\n        DataType.UInt16 => (ushort)value,\n        DataType.UInt32 => (uint)value,\n        DataType.UInt64 => (ulong)value,\n        _ => throw new InvalidCastException(),\n    };\n    return result;\n}\n\n```\n\nYou can see this \"best type\" without using a switch expression, using implicitly typed arrays:\n\n\n\n```\nvar array = new[]\n{\n    (byte) 0, 0.0, (short) 0, 0,\n    0L, 0f, (ushort) 0, 0U, 0UL\n};\n\n```\n\nHere the type of `array` is inferred to be `double[]`.\n\n\n"}
{"questionId":"cf2d703083c34940b7a9cb6f8a892073","question":"Validation 30000 No Type Specified for the Decimal Column\nWhat's the best way of specifying a decimal precision without using attributes.\nI just need to set it in one place for all decimal's in my Data.Models. Its tedious specifying attributes for every decimal.\n\n\n\n```\npublic class Customer\n{\n    public int customerId { get; set; }\n\n    [Column(TypeName = \"decimal(18,2)\")]\n    public decimal AvailableAmount { get; set; }\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"Add following to the `OnModelCreating` method in the dbcontext:\n\n\n\n```\nprotected override void OnModelCreating(ModelBuilder builder)\n{         \n   foreach (var property in builder.Model.GetEntityTypes()\n                .SelectMany(t => t.GetProperties())\n                .Where(p => p.ClrType == typeof(decimal) || p.ClrType == typeof(decimal?)))\n            {   \n              property.Relational().ColumnType = \"decimal(18,2)\";   \n            }\n}\n\n```\n\n"}
{"questionId":"13864ade2fcf4c5f984195535098f874","question":"How do I hide some fields of struct in C?\nI'm trying to implement a struct person, and I need to hide some fields or make them constant.\n*A trick for create private fields.*\n\n\nHeader:\n\n\n\n```\n#pragma once\n\n#define NAME_MAX_LEN 20\n\ntypedef struct _person {\n    float wage;\n    int groupid;\n} Person;\n\nconst char const *getName (Person *p);\nint getId (Person *p);\n\n\/\/\/ OTHER FUNCTIONS\n\n```\n\nSource\n\n\n\n```\n#include \"person.h\"\n\n\nstruct _person\n{\n    int id;\n\n    float wage;\n    int groupid;\n\n    char name[NAME_MAX_LEN];\n};\n\n\/\/\/ FUNCTIONS\n\n```\n\nGCC says that `person.c:7:8: error: redefinition a 'struct _person' struct _person`\n\n\nI can write this in a header, but after it, I can't use fields of a struct.\n\n\n\n```\ntypedef struct _person Person;\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c"},"answer":"C has no mechanism for hiding individual members of a structure type. However, by operating only in terms of *pointers* to such a type, and not providing a definition, you can make the whole type opaque. Users would then have to use the functions you provide to manipulate instances in any way. This is a thing that is sometimes done.\n\n\nTo some extent, you may be able to achieve something like what you describe with a hidden context. For example, consider this:\n\n\n**header.h**\n\n\n\n```\ntypedef struct _person {\n    float wage;\n    int groupid;\n} Person;\n\n```\n\n**implementation.c**\n\n\n\n```\nstruct _person_real {\n    Person person;  \/\/ must be first, and is a structure, not a pointer.\n    int id;\n    char name[NAME_MAX_LEN];\n};\n\n```\n\nNow you can do this:\n\n\n\n```\nPerson *create_person(char name[]) {\n    struct _person_real *pr = malloc(sizeof(*pr));\n\n    if (pr) {\n        pr->person.wage = DEFAULT_WAGE;\n        pr->person.groupid = DEFAULT_GROUPID;\n        pr->id = generate_id();\n        strncpy(pr->name, name, sizeof(pr->name));\n        pr->name[sizeof(pr->name) - 1] = '\\0';\n\n        return &pr->person;  \/\/ <-- NOTE WELL\n    } else {\n        return NULL;\n    }\n}\n\n```\n\nA pointer to the first member of a structure always points also to the whole structure, too, so if the client passes a pointer obtained from that function back to you, you can\n\n\n\n```\nstruct _person_real *pr = (struct _person_real *) Person_pointer;\n\n```\n\nand work on the members from the larger context.\n\n\nBe well aware, however, that such a scheme is risky. Nothing prevents a user from creating a `Person` *without* the larger context, and passing a pointer to it to a function that expects the context object to be present. There are other issues.\n\n\nOverall, C APIs generally either take the opaque structure approach or just carefully document what clients are permitted to do with the data they have access to, or even just document how everything works, so that users can make their own choices. These, especially the latter, are well aligned with overall C approaches and idioms -- C does not hold your hand, or protect you from doing harm. It trusts you to know what you're doing, and to do only what you intend to do.\n\n\n"}
{"questionId":"278206b00a054d1c8abd8894445380c7","question":"Are `x = &v` and `\\*x = v` equivalent?\n\n```\nint * x;\nint v = 7;\n\n```\n\nGiven this code, what is the difference between \n1. `x = &v` , and \n2. `*x = v` ? \nI understand that in both cases, `*x` contains `7` but does `x` contain memory location of `v` in both cases? If not, what does `x` contain in cases 1 and 2, and is this the only significant difference between the two?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c++"},"answer":"Given the statement:\n\n\n\n```\nint v = 7; \n\n```\n\n`v` has some location in memory. Doing:\n\n\n\n```\nx = &v;\n\n```\n\nwill \"point\" `x` to the memory location of `v`, and indeed `*x` will have the value `7`.\n\n\nHowever, in this statement:\n\n\n\n```\n*x = v;\n\n```\n\nyou are storing the value of `v` at the address pointed at by `x`. But `x` is *not* pointing at a valid memory address, and so this statement invokes undefined behavior.\n\n\nSo to answer your question, *no*, the 2 statements are not equivalent.\n\n\n"}
{"questionId":"04345b3b1b7e482685bb7d6575dab75e","question":"Why am I getting this unexpected keyword argument TypeError?\nI'm testing some code with `*args` and `**kwargs`, and I wrote a dictionary for the `**kwargs`. For some reason, I'm getting\n\n\n\n```\ndef func(*args, **kwargs):\n    if args:\n        second_test(*args)\n    elif kwargs:\n        second_test(**kwargs)\n\ndef second_test(stringa, integera, floata):\n    print(\"Name: %s, Problems Correct: %d, Points: %f\" % (stringa, integera, floata))\n\nprofile_1 = [\"David\", 21, 132.00]\nfunc(*profile_1)\n\nprofile_1a = {'Name': 'David', 'Problems Correct': 21, 'Points': 132.00}\nfunc(**profile_1a)\n\n```\n\nThe code starts from `line 44` and ends at `line 57`. This is the error I'm getting:\n\n\n\n```\nTypeError: second_test() got an unexpected keyword argument 'Name'\n\n```\n\n*I've googled \"unexpected keyword argument\", but I can never find a definition; only other stackoverflow articles.* What is wrong with my code?\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"python"},"answer":"When passing `kwargs` into a function, it expects to find the exact variable name in the list. If instead your dictionary keys were `stringa`, `integera`, and `floata` the function would work without problem.\n\n\nSo you *either need to change your function variable names* or *change the key names in your dictionary to get this to work*\n\n\n"}
{"questionId":"c7b05516f89c4bc483fd26384fa32dbd","question":"Difference between Process.run() and Process.start()\nI am struggling to understand the difference between `run()` and `start()`. According to the documentation, `run()` method invokes the callable object passed to the object's constructor, while `start()` method starts the process and can be called only once.\n\n\nI tried an example below:\n\n\n\n```\ndef get_process_id(process_name):\n    print process_name, os.getpid()\n\np1 = multiprocessing.Process(target=get_process_id, args=('process_1',))\np2 = multiprocessing.Process(target=get_process_id, args=('process_2',))\n\np1.run()\np2.run()\np1.start()\np2.start()\n\n```\n\nThe results are below:\n\n\n\n```\nprocess_1 35138\nprocess_2 35138\nprocess_1 35141\nprocess_2 35142\n\n```\n\nWhen I use `run()`, it shows that `p1` and `p2` uses the same process. But when I use `start()`, they give the two difference ones. Is it because calling `run()` doesn't have anything to do with the process that calls it but just calling the function (which is `get_process_id` in this example)?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"python"},"answer":"You are not supposed to call `process.run()` explicitly. It's the method which invokes your specified `target` function unless you override it when you subclass `Process`. It normally gets called within the new child while it bootstraps. It does nothing else than calling the target function.\n\n\n\n```\n# multiprocessing.process.BaseProcess\n\ndef run(self):\n    '''\n    Method to be run in sub-process; can be overridden in sub-class\n    '''\n    if self._target:\n        self._target(*self._args, **self._kwargs)\n\n```\n\nWhen you call it in your parent process, it gets executed in your parent process like any other method.\n\n\n`process.start()` is the method which you're supposed to call in your parent to create the new process in the first place.\n\n\n"}
{"questionId":"e43ae14b8a164ef3bd599a6b6df47063","question":"How to format a bash array as a JSON array\nI have a bash array\n\n\n\n```\nX=(\"hello world\" \"goodnight moon\")\n\n```\n\nThat I want to turn into a json array\n\n\n\n```\n[\"hello world\", \"goodnight moon\"]\n\n```\n\nIs there a good way for me to turn this into a json array of strings without looping over the keys in a subshell?\n\n\n\n```\n(for x in \"${X[@]}\"; do; echo $x | sed 's|.*|\"&\"|'; done) | jq -s '.'\n\n```\n\nThis clearly doesn't work\n\n\n\n```\necho \"${X[@]}\" | jq -s -R '.'\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"Since jq 1.6 you can do this:\n\n\n\n```\njq --compact-output --null-input '$ARGS.positional' --args -- \"${X[@]}\"\n\n```\n\ngiving:\n\n\n\n```\n[\"hello world\",\"goodnight moon\"]\n\n```\n\nThis has the benefit that no escaping is required at all. It handles strings containing newlines, tabs, double quotes, backslashes and other control characters. (Well, it doesn't handle NUL characters but you can't have them in a bash array in the first place.)\n\n\n"}
{"questionId":"0fd911d9987f44df9c852eaaf96d9e83","question":"Logging in .NET Core without DI?\nIt seems that Microsoft are really trying to shove DI down your throat with .NET Core, and I'm not sure why, but frankly my console app is small and simple and I just don't want to build a whole DI container just to do some simple logging. How can I do logging in .NET Core without using DI? Everything I've read assumed you're going to use .NET Core's built-in logging architecture which obviously requires DI, but there must be a way to just do it without DI using a static variable on the class?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"As per Ilyas and bokibegs comments in Scotts answer here is the currently working code for .NET 5.0:\n\n\n\n```\nusing Microsoft.Extensions.Logging;\n\n```\n\n\n```\nvar factory = LoggerFactory.Create(builder => {\n    builder.AddConsole();\n});\n\nvar logger = factory.CreateLogger<T>();\n\n```\n\nThis requires the `Microsoft.Extensions.Logging` and `Microsoft.Extensions.Logging.Console` nuget packages.\n\n\n"}
{"questionId":"b4ce5ecf53da416bb9724d0412c8ca5d","question":"Cannot access 'java.io.Serializable' which is a supertype of 'kotlin.String'. Check your module classpath for missing or conflicting dependencies\nI have a new Kotlin project in Eclipse (I installed the kotlin plugin).\nHowever this code:\n\n\n\n```\n   val a: Int = 10000\n   println(\"Your Int Value is \"+a);\n\n```\n\ncauses this error:\n\n\n\n```\nERROR: Cannot access 'java.io.Serializable' which is a supertype of 'kotlin.String'. Check your module classpath for missing or conflicting dependencies.\n\n```\n\nWhat is the source of this error? How can I fix it?\n\n\n**I have seen the other posts on stackoverflow but they all talk about intellij and not Eclipse.**\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"kotlin"},"answer":"I got this error while using kotlin in eclipse. In my system, JDK was not installed previously. I installed JDK 15 in the PC and set the path in kotlin plugin settings in eclipse (path: Window -> preferences -> Kotlin -> Compiler -> JDK Home)\n\n\n"}
{"questionId":"ffa0fcdb84934756948b92b3d8f2d309","question":"Laravel sanctum csrf cookie every request?\nI'm using Laravel sanctum (former Airlock) and have a question about it. I read in the docs:\n\n\n\n> \n> To authenticate your SPA, your SPA's login page should first make a\n>  request to the \/sanctum\/csrf-cookie route to initialize CSRF\n>  protection for the application:\n> \n> \n> \n\n\n\n```\naxios.get('\/sanctum\/csrf-cookie').then(response => {\n\/\/ Login... }); \n\n```\n\n\n> \n> Once CSRF protection has been initialized, you should make a POST request to the typical Laravel \/login route. This\n>  \/login route may be provided by the laravel\/ui authentication\n>  scaffolding package.\n> \n> \n> \n\n\nDoes this mean that for every request I make, I should first check if the cookie has already been set? Because let's say I have a user that registers. Before making the POST request to register a user I should first make a GET request to get the CSRF-Cookie from my backend and then make the POST request to register the user.\n\n\nNow the user gets redirected to the login webpage and is asked to login. Does the frontend then first have to check if there's a CSRF-Cookie, and if there isn't should it first again make the GET request to get the cookie?\n\n\nThis last bit also confuses me, because when calling the register method a user doesn't actually get logged in so the user has to be redirect to the login page to log in with the credentials the user just filled in to register which for me seems like a bad user experience, right?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"php"},"answer":"I know it's been a while since this question was asked but just for anyone searching out there, No. You don't have to call `\/sanctum\/csrf-cookie` with every request. Before you make a `post | put | delete...` request, you can check to see if the `XSRF-TOKEN` cookie is set. If it is not, make a call to the `\/sanctum\/csrf-cookie` route (or whatever you have configured it to be). After the request has completed, (the `XSRF-TOKEN` cookie would have been set by your browser automatically) you can now proceed with the initial request.\n\n\nThe best place to do this is in an interceptor (if your http library supports it). I'm going to assume you are using axios.\n\n\n\n```\n\/\/ Install with 'npm i js-cookie'. A library that helps you manage cookies \n\/\/ (or just build your own).\nimport Cookies from 'js-cookie';\n\n\/\/ Create axios instance with base url and credentials support\nexport const axiosInstance = axios.create({\n    baseURL: '\/api',\n    withCredentials: true,\n});\n\n\/\/ Request interceptor. Runs before your request reaches the server\nconst onRequest = (config) => {\n    \/\/ If http method is `post | put | delete` and XSRF-TOKEN cookie is \n    \/\/ not present, call '\/sanctum\/csrf-cookie' to set CSRF token, then \n    \/\/ proceed with the initial response\n    if ((\n            config.method == 'post' || \n            config.method == 'put' || \n            config.method == 'delete',\n            \/* other methods you want to add here *\/\n        ) &&\n        !Cookies.get('XSRF-TOKEN')) {\n        return setCSRFToken()\n            .then(response => config);\n    }\n    return config;\n}\n\n\/\/ A function that calls '\/api\/csrf-cookie' to set the CSRF cookies. The \n\/\/ default is 'sanctum\/csrf-cookie' but you can configure it to be anything.\nconst setCSRFToken = () => {\n    return axiosInstance.get('\/csrf-cookie'); \/\/ resolves to '\/api\/csrf-cookie'.\n}\n\n\/\/ attach your interceptor\naxiosInstance.interceptors.request.use(onRequest, null);\n\nexport default axiosInstance;\n\n```\n\nThe XSRF-TOKEN cookie comes with a time of expiry. After that time, the browser deletes it. So as long as you can find the cookie, it is safe to make a request without calling `\/sanctum\/csrf-cookie` or whatever you have configured it to be.\n\n\n"}
{"questionId":"aaa8111921bf453d9e5a04652eaf0bae","question":"Convert Stream to String in Java\nI want to convert a Stream of a Map<> into a String, to append it to a textArea. I tried some methods, the last with StringBuilder, but they don't work.\n\n\n\n```\npublic <K, V extends Comparable<? super V>> String sortByAscendentValue(Map<K, V> map, int maxSize) {\n\n    StringBuilder sBuilder = new StringBuilder();\n\n    Stream<Map.Entry<K,V>> sorted =\n            map.entrySet().stream()\n               .sorted(Collections.reverseOrder(Map.Entry.comparingByValue()));\n\n    BufferedReader br = new BufferedReader(new InputStreamReader((InputStream) sorted));\n    String read;\n\n    try {\n        while ((read=br.readLine()) != null) {\n            \/\/System.out.println(read);\n            sBuilder.append(read);\n        }\n    } catch (IOException e) {\n        \/\/ TODO Auto-generated catch block\n        e.printStackTrace();\n    }\n\n    sorted.limit(maxSize).forEach(System.out::println);\n\n    return sBuilder.toString();\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"You can collect the entries into a single `String` as follows:\n\n\n\n```\n  String sorted =\n        map.entrySet().stream()\n           .sorted(Collections.reverseOrder(Map.Entry.comparingByValue()))\n           .map(e-> e.getKey().toString() + \"=\" + e.getValue().toString())\n           .collect(Collectors.joining (\",\"));\n\n```\n\n"}
{"questionId":"4327202bb63f436d8494f6535fdc3b62","question":"Install Tensorflow 2.0 in conda enviroment\nI would like to know if anyone knows how can I install tensorflow==2.0.0-alpha0 in a conda enviroment using python 3.7. Is it possible to use python 3.7 or do I have to downgrade to 3.6. Either way what is the command I need to use because the following don't find any package\n\n\n\n```\nconda install tensorflow==2.0.0-alpha0\nconda install tensorflow \nconda install tensorflow=2.0.0-alpha0\n\n```\n\nI am using fedora 29 and conda 4.6.8\nThanks!\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"python"},"answer":"It could be the case that the package version you want is not available in conda-forge. What you could do is install packages with pip in your conda environment.\n\n\n\n```\npip install tensorflow==2.0.0-alpha0 \n\n```\n\nAlso the requirements don't state python 3.7, you can try your luck or downgrade to python 3.6. \n\n\n"}
{"questionId":"f85c108228274a1c9b890d5684137ad0","question":"Function on a variable declaration\nI'm reading the C Programming Language (chapter 5), and I'm confused by this example:\n\n\n\n```\nint n, array[SIZE], getint(int *);\n\n```\n\nWhy is this function call in here like that? Is this just some tricky example and invalid code?\n\n\n","questionMetadata":{"type":"conceptual","level":"beginner","tag":"c"},"answer":"It's not calling the function; it's declaring its prototype.\nIt's equivalent to:\n\n\n\n```\nint n;\nint array[SIZE];\nint getint(int*);\n\n```\n\n"}
{"questionId":"fd62780249ca45b1a30af6eb810462f2","question":"How to ignore semicolons with \"tslint:recomended\"\nI want my tslint to ignore semicolons.\n\n\nI would like to keep the rule `\"extends\": \"tslint:recommended\"`. Right now, I just can\u00b4t follow this rule, which forces me to use semicolon always, or use this other one `\"semicolon\": [true, \"never\"]`, which forces me to delete all semicolons.\nI just want to ignore them. I can do it by deleting the `\"extends\": \"tslint:recommended\"` but I would like to keep this rule and just ignore semicolons.\n\n\ntslint documentation just gives the option to keep them always or delete always, but not ignore them.\n\n\nCan someone help me?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"As in the previous response, you can suppress `tslint` for a file or the next line. But if you want to edit the rules for your entire directory, check the `tslint.json` file, this is your global configuration for the project you're on.\n\n\nYou could probably find this file in the root folder of your app. If not, try pressing `cmd + P` (mac) or `ctrl + P` (windows) and enter `tslint.json`.\n\n\nOnce you're there, add this entry to the rules list:\n\n\n\n```\n{\n  ...\n  \"rules\": {\n    ...\n    \"semicolon\":false\n  }\n}\n\n```\n\nHope it helps!\n\n\n"}
{"questionId":"9cf4613ecee34160bf71657bb4557713","question":"react\/prop-types eslint error in typescript react component\nI am trying to set up a `typescript-react-eslint` project and can't get past eslint error for this boilerplate component:\n\n\n\n```\nimport * as React from \"react\";\n\ninterface ButtonProps {\n  children?: React.ReactNode,\n  onClick?: (e: any) => void,\n}\n\nconst styles = {\n  border: \"1px solid #eee\",\n  borderRadius: 3,\n  backgroundColor: \"#FFFFFF\",\n  cursor: \"pointer\",\n  fontSize: 15,\n  padding: \"3px 10px\",\n  margin: 10\n};\n\nconst Button: React.FunctionComponent<ButtonProps> = props => (\n  <button onClick={props.onClick} style={styles} type=\"button\">\n    {props.children}\n  <\/button>\n);\n\nButton.defaultProps = {\n  children: null,\n  onClick: () => {}\n};\nexport default Button;\n\n```\n\nThe error is:\n\n\n\n```\n  19:26  error  'onClick' is missing in props validation   react\/prop-types\n  20:12  error  'children' is missing in props validation  react\/prop-types\n\n```\n\nIt seems like it is complaining about interface for html `<button>` is not defined?\nOtherwise it might be the `Button` component itself but should it not get type information from `<ButtonProps>` interface I pass there?\n\n\nI tried explicitly setting `children` and `onClick` like this:\n\n\n\n```\nButton.propTypes = {\n  children?: React.ReactNode,\n  onClick?: (e: any) => void\n};\n\n```\n\nit bypasses the eslint error but the component itself stops working.\nWhat am I doing wrong?\n\n\nP.S. This is my `.eslintrc.json`\n\n\n\n```\n{\n    \"env\": {\n        \"browser\": true,\n        \"commonjs\": true,\n        \"es6\": true\n    },\n    \"extends\": [\n        \"eslint:recommended\",\n        \"plugin:react\/recommended\",\n        \"plugin:@typescript-eslint\/eslint-recommended\"\n    ],\n    \"globals\": {\n        \"Atomics\": \"readonly\",\n        \"SharedArrayBuffer\": \"readonly\"\n    },\n    \"settings\": {\n        \"react\": {\n            \"pragma\": \"React\",\n            \"version\": \"detect\"\n        }\n    },\n    \"parser\": \"@typescript-eslint\/parser\",\n    \"parserOptions\": {\n        \"ecmaFeatures\": {\n            \"jsx\": true\n        },\n        \"ecmaVersion\": 2018,\n        \"sourceType\": \"module\"\n    },\n    \"plugins\": [\n        \"react\",\n        \"@typescript-eslint\"\n    ],\n    \"rules\": {\n        \"indent\": [\n            \"error\",\n            2\n        ],\n        \"linebreak-style\": [\n            \"error\",\n            \"unix\"\n        ],\n        \"quotes\": [\n            \"error\",\n            \"double\"\n        ],\n        \"semi\": [\n            \"error\",\n            \"always\"\n        ]\n    }\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"I ended up rewriting the component as:\n\n\n\n```\nconst Button = ({ children, onClick }: ButtonProps) => {\n  return <button onClick={onClick} style={styles} type=\"button\">\n    {children}\n  <\/button>;\n};\n\n```\n\nThe `: React.FC<ButtonProps>` part was ignored by eslint so I decided to provide prop types in a more straightforward way \n\n\n"}
{"questionId":"58d5a41b9e944a3eb6c3108919c1c016","question":"Category shorthand not allowed in this regular expression dialect in TypeScript\nI tried to use a regular expression in TypeScript:\n\n\n\n```\nconst pass = \/^[\\pL\\pM\\pN_-]+$\/u.test(control.value) || !control.value;\n\n```\n\nbut I got this error:\n\n\n\n> \n> Category shorthand not allowed in this regular expression dialect in Typescript\n> \n> \n> \n\n\nWhy am I getting this error, and how can I fix it?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"That regex shorthand (`\\pL`) isn't allowed.\n\n\nYou'll need to use the full versions (`\\p{L}`), instead of the shorthand:\n\n\n\n```\nconst pass = \/^[\\p{L}\\p{M}\\p{N}_-]+$\/u.test(control.value) || !control.value;\n\n```\n\n"}
{"questionId":"e2194262950c4d0e8aa4978d36b57ae4","question":"Can I match on all enum variants that have the same shape of their values?\nI have an enum:\n\n\n\n```\nenum MyEnum {\n    A(i32),\n    B(i32),\n    C,\n    D,\n    \/\/ ...\n}\n\n```\n\nCan I match on `A` and `B` simultaneously with something like this:\n\n\n\n```\nlet a = MyEnum::A(1);\nmatch a {\n    _(n) => { \/\/ do something for all variants which have one i32 value\n    }\n    _ => { \/\/ ignore the rest\n    }\n};\n\n```\n\nIs there any way to accomplish this? Do I have to match all the variants and apply the same \"body\" for each?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"rust"},"answer":"**No**, your desired syntax is not possible; I don't know how that syntax could work if you had multiple variants with the same count of fields with differing types:\n\n\n\n```\nenum MyEnum {\n    A(bool),\n    B(String),\n}\n\n```\n\n\n> \n> Do I have to match all the variants and apply the same \"body\" for each?\n> \n> \n> \n\n\nYes, but you can use patterns to match them in a single match arm:\n\n\n\n```\nmatch a {\n    MyEnum::A(n) | MyEnum::B(n) => {\n        \/\/ use `n`\n    }\n    _ => {}\n};\n\n```\n\nOr the equivalent `if let`:\n\n\n\n```\nif let MyEnum::A(n) | MyEnum::B(n) = a {\n    \/\/ use `n`\n}\n\n```\n\n"}
{"questionId":"95cd1c45dd164cea9b6ddea04fd9c549","question":"How do I display a messagebox with unicode characters in VBA?\nI have a string containing unicode characters in VBA.\n\n\nI want to display that string in a message box containing it.\n\n\nHowever, instead of the string, the message box only contains a questionmark.\n\n\nMCVE:\n\n\n\n```\nDim s As String\ns = ChrW(5123)\nMsgBox s\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"vba"},"answer":"`MsgBox` is not compatible with non-ANSI unicode characters.\n\n\nWe can display message boxes with the WinAPI `MessageBoxW` function, however, and that is .\n\n\nLet's declare that function, and then create a wrapper for it that's nearly identical to the VBA `MsgBox` function:\n\n\n\n```\nPrivate Declare PtrSafe Function MessageBoxW Lib \"User32\" (ByVal hWnd As LongPtr, ByVal lpText As LongPtr, ByVal lpCaption As LongPtr, ByVal uType As Long) As Long\n\nPublic Function MsgBoxW(Prompt As String, Optional Buttons As VbMsgBoxStyle = vbOKOnly, Optional Title As String = \"Microsoft Access\") As VbMsgBoxResult\n    MsgBoxW = MessageBoxW(Application.hWndAccessApp, StrPtr(Prompt), StrPtr(Title), Buttons)\nEnd Function\n\n```\n\nThis function is only compatible with Microsoft Access. However, for Excel you can swap `Application.hWndAccessApp` with `Application.hWnd` to make it work. For other VBA compatible applications, you'll have to find the appropriate way to get the hWnd.\n\n\nYou can use it like `MsgBox`, as long as you don't use the context-dependent help functionality:\n\n\n\n```\nDim s As String\ns = ChrW(5123)\nMsgBoxW s\n\n```\n\n"}
{"questionId":"58fb0a4162b3413fa7d2800959da6196","question":"AttributeError: 'SMOTE' object has no attribute '\\_validate\\_data'\nI'm resampling my data (multiclass) by using SMOTE. \n\n\n\n```\nsm = SMOTE(random_state=1)\nX_res, Y_res = sm.fit_resample(X_train, Y_train)\n\n```\n\nHowever, I'm getting this attribute error. Can anyone help?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"**Short answer**\n\n\nYou need to upgrade `scikit-learn` to version 0.23.1.\n\n\n**Long answer**\n\n\nThe newest version 0.7.0 of `imbalanced-learn` seems to have an undocumented dependency on `scikit-learn` v0.23.1. It would give you `AttributeError: 'SMOTE' object has no attribute '_validate_data'` if your `scikit-learn`is 0.22 or below.\n\n\nIf you are using `Anaconda`, installing `scikit-learn` version 0.23.1 might be tricky. `conda update scikit-learn` might not update `scikit-learn` version 0.23 or higher because the newest `scikit-learn` version Conda has at this point of time is 0.22.1. If you try to install it using `conda install scikit-learn=0.23.1` or `pip install scikit-learn==0.23.1`, you will get tons of compatibility checks and installation might not be quick. Therefore the easiest way to install `scikit-learn` version 0.23.1 in Anaconda is to create a new virtual environment with minimum packages so that there are less or no conflict issues. Then, in the new virtual environment install `scikit-learn` version 0.23.1 followed by version 0.7.0 of `imbalanced-learn`.\n\n\n\n```\nconda create -n test python=3.7.6\nconda activate test\npip install scikit-learn==0.23.1\npip install imbalanced-learn==0.7.0\n\n```\n\nFinally, you need to reinstall your IDE in the new virtual environment in order to use these packages.\n\n\nHowever, once `scikit-learn` version 0.23.1 becomes available in Conda and there are no compatibility issues, you can install it in the base environment directly.\n\n\n"}
{"questionId":"6b1ddf703dd7408b9c61aa066a5841b2","question":"How to convert sealed trait case objects to string using circe\nI am using Scala and Circe. I have the following sealed trait.\n\n\n\n```\n  sealed trait Mode\n  case object Authentication extends Mode\n  case object Ocr extends Mode\n\n```\n\nThe output of this case object when called `SessionModel.Authentication` is the following:\n\n\n\n```\n\"Authentication\":{}\n\n```\n\nI need to convert this to a string so it outputs `\"authentication\"`\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"scala"},"answer":"As Andriy Plokhotnyuk notes above, you can use circe-generic-extras:\n\n\n\n```\nimport io.circe.Codec\nimport io.circe.generic.extras.Configuration\nimport io.circe.generic.extras.semiauto.deriveEnumerationCodec\n\nsealed trait Mode\ncase object Authentication extends Mode\ncase object Ocr extends Mode\n\nobject Mode {\n  private implicit val config: Configuration =\n    Configuration.default.copy(transformConstructorNames = _.toLowerCase)\n\n  implicit val modeCodec: Codec[Mode] = deriveEnumerationCodec[Mode]\n}\n\n```\n\nAnd then:\n\n\n\n```\nscala> import io.circe.syntax._\nimport io.circe.syntax._\n\nscala> (Authentication: Mode).asJson\nres1: io.circe.Json = \"authentication\"\n\nscala> io.circe.Decoder[Mode].decodeJson(res1)\nres2: io.circe.Decoder.Result[Mode] = Right(Authentication)\n\n```\n\n(Note that `Codec` is new in 0.12\u2014for earlier versions you'll have to write out both instances as in Andriy's comment.)\n\n\nUnless you have a lot of these to maintain, though, I personally think writing the instances out by hand is often better than using circe-generic-extras, and in this case it's not even much more verbose:\n\n\n\n```\nimport io.circe.{Decoder, Encoder}\n\nsealed trait Mode\ncase object Authentication extends Mode\ncase object Ocr extends Mode\n\nobject Mode {\n  implicit val decodeMode: Decoder[Mode] = Decoder[String].emap {\n    case \"authentication\" => Right(Authentication)\n    case \"ocr\"            => Right(Ocr)\n    case other            => Left(s\"Invalid mode: $other\")\n  }\n\n  implicit val encodeMode: Encoder[Mode] = Encoder[String].contramap {\n    case Authentication => \"authentication\"\n    case Ocr            => \"ocr\"\n  }\n}\n\n```\n\nWhich works exactly the same as the `deriveEnumerationCodec` version but doesn't require anything but circe-core, is less magical, compiles much faster, etc. Generic derivation can be great for simple case classes with straightforward mappings, but I think people too often try to stretch it to cover all cases when writing instances manually wouldn't be much of a burden and might even be clearer.\n\n\n"}
{"questionId":"0947e9a4b89948a3a4279a394cbbe52f","question":"Defining a function which returns a function pointer which also returns a function pointer without typedefs\nI am trying to really understand function pointers without using `typedef` but cannot seem to get this. I do not understand what signature is needed to convey that I return a pointer to a pointer to a function.\n\n\n\n```\n#include <stdio.h>\n\nvoid odd()  { printf(\"odd!\\n\");  }\nvoid even() { printf(\"even!\\n\"); }\n\nvoid (*get_pfn(int i))()\n{\n    return i % 2 == 0 ? &even : &odd;\n}\n\n__SIGNATURE__\n{\n    return &get_pfn;\n}\n\nint main()\n{\n    get_pfn_pfn()(1)();\n    get_pfn_pfn()(2)();\n    return 0;\n}\n\n```\n\nTo make this work, what does `__SIGNATURE__` have to be?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"It has to return a function pointer to a function that takes an `int` and returns a function pointer:\n\n\n\n```\nvoid (*(*get_pfn_pfn(void))(int))(void) {\n    return &get_pfn;\n}\n\n```\n\nmore lines:\n\n\n\n```\nvoid (*\n        (*\n             get_pfn_pfn(void)  \/\/ this is our function\n        )(int i) \/\/ this is get_pfn(int)\n     )(void)  \/\/ this is odd() or even()\n{\n    return &get_pfn;\n}\n\n```\n\nThe `void`s can be omitted, in which case the function pointer points to a function that takes unknown number of parameters. Which is not what you want. To declare a function pointer to a function that takes no arguments, you should add `void` inside function parameter list. The same way it's best to change `get_pfn` to `void (*get_pfn(int i))(void)`. For example try calling from `get_pfn(1)(\"some arg\", \"some other arg\");`. A C compiler will not give a warning, as empty `()` denote *unknown* arguments. To say that function takes no arguments, you have to `(void)`.\n\n\nFor many the sequences of braces, especially `))(`, in function pointers are hard to parse. That's why many prefer typedefs for function pointers or types:\n\n\n\n```\ntypedef void get_pfn_func_t(void);    \nget_pfn_func_t *get_pfn(int i) {\n    return i % 2 == 0 ? &even : &odd;\n}\n\ntypedef get_pfn_func_t *get_pfn_pfn_func_t(int i);\nget_pfn_pfn_func_t *get_pfn_pfn(void) {\n    return &get_pfn;\n}\n\n```\n\n"}
{"questionId":"b592995cc46047f9aa0a975dfb41445e","question":"Update jsonb object in postgres\nOne of my column is jsonb and have value in the format. The value of a single row of column is below.\n\n\n\n```\n{\n    \"835\": {\n        \"cost\": 0, \n        \"name\": \"FACEBOOK_FB1_6JAN2020\", \n        \"email\": \"test.user@silverpush.co\", \n        \"views\": 0, \n        \"clicks\": 0, \n        \"impressions\": 0, \n        \"campaign_state\": \"paused\", \n        \"processed\":\"in_progress\", \n        \"modes\":[\"obj1\",\"obj2\"]\n    }, \n    \"876\": {\n        \"cost\": 0, \n        \"name\": \"MARVEL_BLACK_WIDOW_4DEC2019\", \n        \"email\": \"test.user@silverpush.co\", \n        \"views\": 0, \n        \"clicks\": 0, \n        \"impressions\": 0, \n        \"campaign_state\": \"paused\", \n        \"processed\":\"in_progress\", \n        \"modes\":[\"obj1\",\"obj2\"]\n    }\n}\n\n```\n\nI want to update campaign\\_info(column name) column's the inner key \"processed\" and \"models\" of the campaign\\_id is \"876\". \n\n\nI have tried this query:\n\n\n\n```\nupdate safe_vid_info \nset campaign_info -> '835' --> 'processed'='completed' \nwhere cid = 'kiywgh'; \n\n```\n\nBut it didn't work.\n\n\nAny help is appreciated. Thanks.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"Is this what you want?\n\n\n\n```\njsonb_set(campaign_info, '{876,processed}', '\"completed\"')\n\n```\n\nThis updates the value at path `\"876\" > \"processed\"` with value `'completed'`.\n\n\nIn your update query:\n\n\n\n```\nupdate safe_vid_info \nset campaign_info = jsonb_set(campaign_info, '{876,processed}', '\"completed\"')\nwhere cid = 'kiywgh'; \n\n```\n\n"}
{"questionId":"e08e4da609d54fe89010fe912cf99a26","question":"Is safe to use va\\_start macro with this as parameter?\nI have to use IAR compiller in embedded application (it does not have namespaces, exceptions, multiple\/virtual inheritance, templates are bit limited and only C++03 is supported). \nI can't use parameter pack so I tried to create member function with variadic parameter. \nI know variadic parameters are generally unsafe. But is safe to use `this` pointer in `va_start` macro?\n\n\nIf I use ordinary variadic function it would need a dummy parameter before `...` to be able to access remaining parameters. I know variadic macro would not need parameter before `...` but I would prefer not to use it.\nIf I use member function it has hidden `this` parameter before `...` so I tried it.:\n\n\n\n```\nstruct VariadicTestBase{\n  virtual void DO(...)=0;\n};\n\nstruct VariadicTest: public VariadicTestBase{\n  virtual void DO(...){\n    va_list args;\n    va_start(args, this);\n    vprintf(\"%d%d%d\\n\",args);\n    va_end(args);\n  }\n};\n\n\/\/Now I can do\nVariadicTestBase *tst = new VariadicTest;\ntst->DO(1,2,3);\n\n```\n\n`tst->DO(1,2,3);` prints 123 as expected. But I am not sure if it is not just some random\/undefined behavior. I know `tst->DO(1,2);` would crash just like normal prinf would. I do not mind it.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c++"},"answer":"Nothing specifies that behaviour in the standard, so this construct just invokes formal Undefined Behaviour. That means that it can work fine in your implementation and cause compilation error or unexpected results in a different implementation.\n\n\nThe fact that non static methods have to pass the hidden `this` pointer cannot guarantee that `va_start` can use it. It probably works that way because in the early times, C++ compilers were just pre-processors that converted C++ source to C source and the hidden `this` parameter was added by the pre-processor to be available to the C compiler. And it has probably be maintained for *compatibility* reasons. But I would try hard to avoid that in mission critical code...\n\n\n"}
{"questionId":"7a87cac6a83345fdbc31b5df171346c6","question":"Redirecting in blazor with parameter\nHello how can you redirect to another page in `Blazor` with a parameter?\n\n\n\n```\n@page \"\/auth\"\n@using Microsoft.AspNetCore.Blazor.Services;\n@inject AuthService auth\n@inject IUriHelper urihelper;\n\n<input type=\"text\" bind=\"@Username\" \/>\n<button onclick=\"@AuthAsync\">Authenticate<\/button>\n\n\n\n@functions{\n\n    public string Username { get; set; }\n    public string url = \"\/home\";\n\n    public async Task AuthAsync()\n    {\n        var ticket=await this.auth.AuthenticateAsync(Username);\n        urihelper.NavigateTo(url); \/\/i see no overload that accepts parameters\n    }\n}\n\n```\n\nIn this case i want to navigate to the `\/home` page giving it a string as parameter.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"Do this:\n\n\n- Create a home.cshtml file page like this:\nNote that two @page directive are employed since optional parameters are not supported yet.\nThe first permits navigation to the component without a parameter. The second @page directive \ntakes the {username} route parameter and assigns the value to the Username property.\n\n\n## Pages\/home.cshtml\n\n\n\n```\n@page \"\/home\"\n@page \"\/home\/{username}\"\n\n<h1>@Username is authenticated!<\/h1>\n\n@functions {\n    \/\/ Define a property to contain the parameter passed from the auth page\n    [Parameter]\n    private string Username { get; set; };\n}\n\n```\n\n- Do this in your auth.cshtml\n\n\n\n```\n    @functions{\n\n        public string Username { get; set; }\n        public string url = \"\/home\";\n\n        public async Task AuthAsync()\n        {\n            var ticket=await this.auth.AuthenticateAsync(Username);\n            \/\/ Attach the parameter to the url\n            urihelper.NavigateTo(url + \"\/\" + Username); \n        }\n    }\n\n```\n\nHope this helps...\n\n\n"}
{"questionId":"1fbaa2ab8a78443a83d176de6dbb4069","question":"How to fill NaT and NaN values separately\nMy dataframe contains both NaT and NaN values\n\n\n\n```\n    Date\/Time_entry      Entry      Date\/Time_exit       Exit   \n0   2015-11-11 10:52:00  19.9900    2015-11-11 11:30:00  20.350 \n1   2015-11-11 11:36:00  20.4300    2015-11-11 11:38:00  20.565 \n2   2015-11-11 11:44:00  21.0000    NaT                  NaN        \n3   2009-04-20 10:28:00  13.7788    2009-04-20 10:46:00  13.700\n\n```\n\nI want to fill NaT with dates and NaN with numbers. Fillna(4) method replaces both NaT and NaN with 4. Is it possible to differentiate between NaT and NaN somehow?\n\n\nMy current workaround is to df[column].fillna()\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"Since NaTs pertain to datetime columns, you can exclude them when applying your filling operation.\n\n\n\n```\nu = df.select_dtypes(exclude=['datetime'])\ndf[u.columns] = u.fillna(4)\ndf\n\n      Date\/Time_entry    Entry      Date\/Time_exit    Exit\n0 2015-11-11 10:52:00  19.9900 2015-11-11 11:30:00  20.350\n1 2015-11-11 11:36:00  20.4300 2015-11-11 11:38:00  20.565\n2 2015-11-11 11:44:00  21.0000                 NaT   4.000\n3 2009-04-20 10:28:00  13.7788 2009-04-20 10:46:00  13.700\n\n```\n\n\n\n---\n\n\nSimilarly, to fill NaT values only, change \"exclude\" to \"include\" in the code above.\n\n\n\n```\nu = df.select_dtypes(include=['datetime'])\ndf[u.columns] = u.fillna(pd.to_datetime('today'))\ndf\n\n      Date\/Time_entry    Entry             Date\/Time_exit    Exit\n0 2015-11-11 10:52:00  19.9900 2015-11-11 11:30:00.000000  20.350\n1 2015-11-11 11:36:00  20.4300 2015-11-11 11:38:00.000000  20.565\n2 2015-11-11 11:44:00  21.0000 2019-02-17 16:11:09.407466   4.000\n3 2009-04-20 10:28:00  13.7788 2009-04-20 10:46:00.000000  13.700\n\n```\n\n"}
{"questionId":"24145cc059a04f62b7f03e86f5858528","question":"How to perform an action only if the Mono is empty and throw an error if not empty\nI'm trying to convert a project to use Spring WebFlux and am running into a problem getting some basic business logic working. I have a repository layer that is responsible for retrieving \/ persisting records and a service layer that is responsible for the business rules of the application. What I want to do (in the service) layer is check if a user already exists for the given username. If so, I want to respond with an error. If not, I want to allow the insert to happen.\n\n\nI call a method on the repository layer that will find a user by username and if not found it will return an empty Mono. This is working as expected; however, I have tried various combinations of flatMap and (defaultIfEmpty and swithIfEmpty) but am unable to get it to compile \/ build.\n\n\n\n```\n    public Mono<User> insertUser(User user) {\n        return userRepository.findByUsername(user.username())\n            .flatMap(__ -> Mono.error(new DuplicateResourceException(\"User already exists with username [\" + user.username() + \"]\")))\n            .switchIfEmpty(userRepository.insertUser(user));\n    }\n\n```\n\nThe error that I'm getting is that `Mono<Object> cannot be converted to Mono<User>`, so the `swithIfEmpty` doesn't seem to reflect the appropriate type and casting doesn't seem to work either.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"After additional testing, and taking into consideration the responses from my fellow developers, I have landed on the following solution:\n\n\n\n```\n    public Mono<User> insertUser(User user) {\n        return userRepository.findByUsername(user.username())\n            .flatMap(__ -> Mono.error(new DuplicateResourceException(\"User already exists with username [\" + user.username() + \"]\")))\n            .switchIfEmpty(Mono.defer(() -> userRepository.insertUser(user)))\n            .cast(User.class);\n    }\n\n```\n\nAs Thomas stated, the compiler was getting confused. My assumption is because the `flatMap` was returning a Mono with an error and the `switchIfEmpty` was returning a Mono with a User so it reverts to a Mono with an Object (hence the additional `.cast` operator to get it to compile).\n\n\nThe other addition was to add the `Mono.defer` in the `switchMap`. Otherwise, the `switchIfEmpty` was always firing.\n\n\nI'm still open to other suggestions \/ alternatives (since this seems like it would be a fairly common need \/ pattern).\n\n\n"}
{"questionId":"e87ecee334d34592ab05a3baa1823aff","question":"What is the purpose of libc\\_nonshared.a?\nWhy does `libc_nonshared.a` exist? What purpose does it serve? I haven't been able to find a good answer for its existence online.\n\n\nAs far as I can tell it provides certain symbols (`stat`, `lstat`, `fstat`, `atexit`, etc.). If someone uses one of these functions in their code, it will get linked into the final executable from this archive. These functions are part of the POSIX standard and are pretty common so I don't see why they wouldn't just be put in the shared or static `libc.so.6` or `libc.a`, respectively.\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"It was a legacy mistake in glibc's implementing extensibility for the definition of `struct stat` before better mechanisms (symbol redirection or versioning) were thought of. The definitions of the `stat`-family functions in `libc_nonshared.a` cause the version of the structure to bind at link-time, and the definitions there call the `__xstat`-family functions in the real shared libc, which take an extra argument indicating the desired structure version. This implementation is **non-conforming** to the standard since each shared library ends up gettings its own copy of the `stat`-family functions with their own addresses, breaking the requirement that pointers to the same function evaluate equal.\n\n\n"}
{"questionId":"28b753b76be14e709233098a4e99152f","question":"Valgrind showing error calling pr\\_set\\_ptracer, vgdb might block\nI am using Valgrind to find memory leaks of my C program and although it looks like it is running fine and showing allocated and freed memory. But, I want to know why it is throwing this error and what are its consequences.\n\n\nHere the snippet of error:\n\n\n\n```\n==483== Memcheck, a memory error detector\n==483== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.\n==483== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info\n==483== Command: .\/main ..\/old\\ projects\n==483== \n==483== error calling PR_SET_PTRACER, vgdb might block\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c"},"answer":"The vgdb executable (part of valgrind) is used to 'connect' to valgrind\nto either launch monitor commands or make the link between gdb and your\nprocess running under valgrind.\nIf your process is blocked in a syscall, vgdb needs to 'wake up' your process\nand for this must be able to 'ptrace' your process.\nDepending on how the security is configured on your system, valgrind\nmight have to tell the kernel that ptrace itself is ok.\nThis is done using the syscall prctl (PR\\_SET\\_PTRACER).\nIf this syscall fails, then you see this message.\nThe consequence is that vgdb cannot connect to your process as long\nas your process is blocked in a syscall.\nSo, unless you have a critical need to debug your process when blocked in\na syscall, or launch a monitor command when blocked in a syscall, there is\nno consequences.\n\n\nThis error is however not expected.\nSo, please report a bug on valgrind bugzilla.\nAttach the output of: cat \/proc\/sys\/kernel\/yama\/ptrace\\_scope\nAlso, it would be nice if you could use strace -f valgrind \nand extract the reason why the syscall prctl (PR\\_SET\\_PTRACER) is failing\n(in particular the errno).\n\n\n"}
{"questionId":"6213435e7b5e4caab024a41052136feb","question":"No module named 'virtualenv.activation.xonsh'\nI triyed to execute pipenv shell in a new environtment and I got the following error:\n\n\n\n```\nLoading .env environment variables\u2026\nCreating a virtualenv for this project\u2026\nUsing \/home\/user\/.pyenv\/shims\/python3.9 (3.9.7) to create virtualenv\u2026\n\u280bModuleNotFoundError: No module named 'virtualenv.activation.xonsh'\nError while trying to remove the \/home\/user\/.local\/share\/virtualenvs\/7t env: \nNo such file or directory\n\nVirtualenv location: \nWarning: Your Pipfile requires python_version 3.9, but you are using None (\/bin\/python).\n  $ pipenv check will surely fail.\nSpawning environment shell (\/usr\/bin\/zsh). Use 'exit' to leave.\n\n```\n\nI tried to remove pipenv, install python with pienv create an alias to python, but anything works.\n\n\nAny idea, I got the same error in existing environment, I tried to remove all environments folder but nothing.\n\n\nThanks.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"By github issue, the solution that works was the following:\n\n\n\n```\nsudo apt-get remove python3-virtualenv\n\n```\n\n"}
{"questionId":"ae2c6da1284c41f9a5cae93431926da5","question":"Why would BigQuery suddenly throw data format error\nThe following query has suddenly stopped working:\n\n\n\n```\nSELECT\n  CAST((SELECT up.value.string_value FROM UNNEST(user_properties) up WHERE key = \"user_id\") AS INT64) AS user_id,\n  (TIMESTAMP_MILLIS(CAST(event_timestamp\/1000 AS INT64))) AS event_date\n`firebase.dataset.events_*`\n\n```\n\nThe error is as follows:\n\n\n\n```\nbad int64 value:\n\n```\n\nThe query has been working fine for months and something happened today to cause this error. Did anyone have the same issue? Is it likely the issue is in our data? Could it be a temporary issue on the GCP side? Thanks!\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"use safe\\_cast() instead cast\n\n\n\n```\nSELECT\n  safe_cast((SELECT up.value.string_value FROM UNNEST(user_properties) up WHERE key = \"user_id\") AS INT64) AS user_id,\n  (TIMESTAMP_MILLIS(CAST(event_timestamp\/1000 AS INT64))) AS event_date\n\n```\n\n"}
{"questionId":"91b38bec18994e60beb2806a64124e7d","question":"Return the last record in a One to many Eloquent Relation using Laravel\nAssuming there existed a `One To Many` relation where a User has Many Jobs, and the last record in the `job` table is the current job of the user. What is a better way of returning the users with their last jobs?\n\n\nHere is what I have tried.\n\n\n`User Class`\n\n\n\n```\npublic function ejob(){\n   return $this->hasMany(Ejob::class);\n}\n\n```\n\n`Ejob Class`\n\n\n\n```\npublic function user(){\n   return $this->belongsTo(User::class);\n}\n\n```\n\n`API Controller Method`\n\n\n\n```\npublic function index()\n{\n  return UserResource::collection(( \n  User::with(\n          $this->particulars() \/\/ I want the last record from this line\n        )->orderBy('id', 'desc')->get() ));\n}\n\n```\n\n`Particulars Method`\n\n\n\n```\n\/\/ I want the last record from this\nprivate function particulars(){\n        return \n        [\n            'ejob.company:id,name', \n            'ejob.job:id,title', \n            'ejob.department:id,name',\n            'ejob.reporting:id,surname,first_name,other_name',\n            'ejob.employmentstatus:id,name', \n            'country:id,name', \n            'gender:id,name', \n            'state:id,name'\n        ];\n}\n\n```\n\n`User Resource`\n\n\n\n```\npublic function toArray($request)\n    {\n        \/\/return parent::toArray($request);\n        return [\n            'data' => [\n                'id' => $this->id,\n                'surname' => $this->surname,\n                'first_name' => $this->first_name,\n                'other_name' => $this->other_name,\n                'email' => $this->email,\n                'phone_number' => $this->phone_number,\n                'birthday' => $this->birthday->format('d-m-Y'),\n                'age'=> $this->birthday->age,\n                'ejob' => $this->whenLoaded('ejob'),\n        ];\n    }\n\n```\n\nCurrently, this returns a user with all related records from the `ejobs` table but I want just the last job.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"php"},"answer":"You could define another relationship method for the same relationship but define it as a Has One instead of a Has Many:\n\n\n\n```\npublic function currentJob()\n{\n   return $this->hasOne(Ejob::class, ...)->latest();\n   \/\/ order by by how ever you need it ordered to get the latest\n}\n\n```\n\nThen you could eager load that instead of the `ejob` relationship where needed.\n\n\n"}
{"questionId":"cab2e2d6e3e740c6905a396bb48fc185","question":"Find version of a module\nWe are working with Go modules. I want in a CLI to get the specific version of a module. Is it possible?\n\n\nIf you are curious, the reason is that I want to add the following `generate` command:\n\n\n\n```\n\/\/go:generate go run github.com\/golang\/mock\/mockgen -source=\"$GOPATH\/pkg\/mod\/mymodules.com\/mymodule@${VERSION}\/module.go\" -destination=module_mock.go\n\n```\n\nSo I need to somehow get the version\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"go"},"answer":"**Basics**:\n\n\n`go list -m all` \u2014 View final versions that will be used in a build for all direct and indirect dependencies\n\n\n`go list -u -m all` \u2014 View available minor and patch upgrades for all direct and indirect dependencies\n\n\n**Example**:\n\n\nTo get the version of a specific module, let's say `golang.org\/x\/text`\n\n\n\n```\ngo list -m all | grep golang.org\/x\/text | awk '{print $2}'\n\n```\n\nor\n\n\n\n```\ngo list -u -m all | grep golang.org\/x\/text | awk '{print $2}'\n\n```\n\n**So, the general way**:\n\n\n\n```\ngo list -u -m all | grep <module-name> | awk '{print $2}'\n\n```\n\n"}
{"questionId":"cfb2253101ef4ea3a568fc5e31cd4d54","question":"Triple exclamation marks on R\nI've been reading a book on feature engineering, and a piece of code has a triple exclamation mark which I don't understand:\n\n\n\n```\nvc_pred <- \n  recipe(Stroke ~ ., data = stroke_train %>% dplyr::select(Stroke, !!!VC_preds)) %>% \n  step_YeoJohnson(all_predictors()) %>% \n  prep(stroke_train %>% dplyr::select(Stroke, !!!VC_preds)) %>% \n  juice() %>% \n  gather(Predictor, value, -Stroke)\n\n```\n\nVC\\_preds is a vector containing the variable names of continuous predictors. I understand all the code except by the `!!!` mark. One `!` is supposed to be a negation, but what does it mean `!!!`?\n\n\nAny help provided will be greatly appreciated. Thank you.\n\n\nRegards,\n\n\nAlexis\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"r"},"answer":"`!!!` is usually used to evaluate a list of expressions. \n\n\n\n```\nlibrary(dplyr)\nlibrary(rlang)\n\nVC_preds <- c('mpg', 'cyl')\nmtcars %>% select(!!!VC_preds) %>% head\n\n#                   mpg Cyl\n#Mazda RX4         21.0   6\n#Mazda RX4 Wag     21.0   6\n#Datsun 710        22.8   4\n#Hornet 4 Drive    21.4   6\n#Hornet Sportabout 18.7   8\n#Valiant           18.1   6\n\n```\n\nIf `VC_preds` is a vector as in your example, `!!` should work as well. \n\n\n\n```\nmtcars %>% select(!!VC_preds) %>% head\n\n```\n\nHelp page of `?\"!!!\"` gives a better example to understand the difference. \n\n\n\n```\nvars <- syms(c(\"height\", \"mass\"))\nvars\n#[[1]]\n#height\n\n#[[2]]\n#mass\n\nstarwars %>% select(!!!vars)\n# A tibble: 87 x 2\n#   height  mass\n#    <int> <dbl>\n# 1    172    77\n# 2    167    75\n# 3     96    32\n# 4    202   136\n# 5    150    49\n# 6    178   120\n# 7    165    75\n# 8     97    32\n# 9    183    84\n#10    182    77\n# \u2026 with 77 more rows\n\n```\n\n"}
{"questionId":"4d2185c1f4f44f129f7c0874e1f94e92","question":"Unable to resolve service for type 'Microsoft.AspNetCore.Mvc.IUrlHelper' while attempting to activate\nI am trying to separate code from controller to service that I created. What I did is to create a User Service with interface IUserService.\n\n\nMoved RegisterUser code from directly controller to UserService, the next challenge I get was that Url which works directly with Controller doesn't work with Service.\n\n\nThis code from Controller to Service has been changed like this:\n\n\n**if in Controller**\n\n\n\n```\nvar callbackUrl = Url.EmailConfirmationLink(user.Email, token, model.contactno, Request.Scheme);\n\n```\n\n**Changed from Controller to Service:**\n\n\n\n```\n  private IUrlHelper _urlHelper;\n\n    public UserService (IUrlHelper urlHelper, HttpRequest request) {\n_urlHelper = urlHelper;\n}\n\n```\n\nthis was constructor,\n\n\nin Method I am calling it like this:\n\n\n\n```\n        var callbackUrl = _urlHelper.EmailConfirmationLink (user.Email, token, U.Email, _request.Scheme);\n\n```\n\nI mentioned in DI in Startup.cs like this:\n\n\n\n```\nservices.AddScoped<IUserService, UserService>();\n\n```\n\nThere is no Excpetion at compile time. At run time it is throwing below excption:\n\n\n\n```\nMicrosoft.AspNetCore.Diagnostics.ExceptionHandlerMiddleware[1]\n      An unhandled exception has occurred while executing the request.\nSystem.InvalidOperationException: Unable to resolve service for type 'Microsoft.AspNetCore.Mvc.IUrlHelper' while attempting to activate 'erp.Services.UserService'.\n\n```\n\nNot very clear what exactly has to be done.\n\n\nhere is full stack trace to see the error more closely. well I can't say the exact line as I am not debugging but just copying from the logs:\n\n\n\n```\nExecuted endpoint 'erp.Controllers.AccountController.Register (erp)'\nfail: Microsoft.AspNetCore.Diagnostics.ExceptionHandlerMiddleware[1]\n      An unhandled exception has occurred while executing the request.\nSystem.InvalidOperationException: Unable to resolve service for type 'Microsoft.AspNetCore.Http.HttpRequest' while attempting to activate 'erp.Services.UserService'.\n   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteFactory.CreateArgumentCallSites(Type serviceType, Type implementationType, CallSiteChain callSiteChain, ParameterInfo[] parameters, Boolean throwIfCallSiteNotFound)\n   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteFactory.CreateConstructorCallSite(Type serviceType, Type implementationType, CallSiteChain callSiteChain)\n   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteFactory.TryCreateExact(ServiceDescriptor descriptor, Type serviceType, CallSiteChain callSiteChain)\n   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteFactory.TryCreateExact(Type serviceType, CallSiteChain callSiteChain)\n   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteFactory.CreateCallSite(Type serviceType, CallSiteChain callSiteChain)\n   at Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceProviderEngine.CreateServiceAccessor(Type serviceType)\n   at System.Collections.Concurrent.ConcurrentDictionary`2.GetOrAdd(TKey key, Func`2 valueFactory)\n   at Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceProviderEngine.GetService(Type serviceType, ServiceProviderEngineScope serviceProviderEngineScope)\n   at Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceProviderEngineScope.GetService(Type serviceType)\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"For `Object reference not set to an instance of an object`, it is caused by that you did not register `IActionContextAccessor`. \n\n\nTry follow steps below: \n\n\n1. UserService \n\n\n\n```\npublic interface IUserService\n{\n    void RegisterUser();\n}\npublic class UserService : IUserService\n{\n    private IUrlHelper _urlHelper;\n    private HttpRequest _request;\n\n    public UserService(IUrlHelper urlHelper, IHttpContextAccessor httpContextAccessor)\n    {\n        _urlHelper = urlHelper;\n        _request = httpContextAccessor.HttpContext.Request;\n    }\n    public void RegisterUser()\n    {\n        var callbackUrl = _urlHelper.EmailConfirmationLink(\"user.Email\", \"token\", _request.Scheme);\n        \/\/throw new NotImplementedException();\n    }\n}\n\n```\n2. Register \n\n\n\n```\nservices.TryAddSingleton<IHttpContextAccessor, HttpContextAccessor>();\nservices.AddSingleton<IActionContextAccessor, ActionContextAccessor>();\nservices.AddScoped<IUrlHelper>(x => {\n    var actionContext = x.GetRequiredService<IActionContextAccessor>().ActionContext;\n    var factory = x.GetRequiredService<IUrlHelperFactory>();\n    return factory.GetUrlHelper(actionContext);\n});\n\nservices.AddScoped<IUserService, UserService>();\n\n```\n\n\n"}
{"questionId":"4808c2e5134541c79abdf2195c93a591","question":"How can I start spring boot application in docker with profile?\nI have a simple spring-boot project:\n\n\n\n```\n-resources\n -application.yaml\n -application-test.yaml\n\n```\n\nAnd I have this `Dockerfile`:\n\n\n\n```\nFROM openjdk:8-jdk-alpine\nEXPOSE 8080\nADD micro-boot.jar micro-boot.jar\nENTRYPOINT [\"java\",\"-Dspring.profiles.active=test\" \"-jar\",\"\/micro-boot.jar\"]\n\n```\n\n1) I build image - `C:\\micro-boot>docker build -f Dockerfile -t micro-boot .`\n\n\n2) show all images - `C:\\micro-boot>docker image ls -a`\n\n\n\n```\nmicro-boot   latest  ccc9a75ebc24  4 seconds ago 112MB\n\n```\n\n3) try to start `C:\\micro-boot>docker image ls -a`\n\n\nAnd I get an error:\n\n\n\n```\n\/bin\/sh: [java,-Dspring.profiles.active=test: not found\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"There's a typo here\n\n\n`ENTRYPOINT [\"java\",\"-Dspring.profiles.active=test\" comma missing here \"-jar\",\"\/micro-boot.jar\"`]\n\n\n"}
{"questionId":"690d5d2240cc455b8722391a17dbfc34","question":"Why do I need to do export const useAppDispatch = () => useDispatch() when using Typescript with React\nI've worked a bit with React using JS, but now I'm creating a new project to learn React with Typescript. When I was using JS and needed to use `dispatch`, I just imported useDispatch from react-redux:\n\n\n\n```\nimport { useDispatch, useSelector } from 'react-redux';\nconst AuthAppBar = () => {\n  const dispatch = useDispatch();\n  const isUserLogged = useSelector(authSelector.isUserLogged);\n  const { event } = useGoogleAnalytics();\n\n  const userLogout = () => {\n    const userManager = authManager.getUserManager();\n    dispatch(authActions.setLoggingOut(true));\n    userManager.signoutRedirect({ state: { callbackUrl: routes.home.path } });\n\n    event('Account', 'Logout');\n  };\n\n  return <><\/>;\n};\n\n```\n\nBut now in this Typescript project the docs says that I need to do like this:\n\n\n\n```\n\/\/ hooks.ts\nimport { TypedUseSelectorHook, useDispatch, useSelector } from 'react-redux';\nimport type { RootState, AppDispatch } from '.\/store';\n\n\/\/ Use throughout your app instead of plain `useDispatch` and `useSelector`\nexport const useAppDispatch = () => useDispatch<AppDispatch>();\n\n\n\/\/ useGetDeviceById.ts\nimport { useEffect } from 'react';\nimport { useHistory, useParams } from 'react-router-dom';\nimport { useAppDispatch, useAppSelector } from 'src\/hooks';\n\nconst useGetDeviceById = () => {\n  const dispatch = useAppDispatch();\n  \/\/ ...\n}\n\n```\n\nWhy do I need to do it this way?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"typescript"},"answer":"You aren't *required* to do this, but it's a nice convenience factor, and can prevent some errors later.\n\n\nNormally, you'd have to do this in every component file:\n\n\n\n```\n\/\/ import the RootState type\nimport { RootState, AppDispatch } from \"app\/store\";\nimport { useSelector, useDispatch } from \"react-redux\";\n\nfunction MyComponent() {\n  \/\/ Specifically mark the `state` arg as being of type RootState\n  const todos = useSelector( (state: RootState) => state.todos);\n\n  \/\/ Specifically mark `dispatch` as being a type that understands thunks\n  const dispatch : AppDispatch = useDispatch();\n}\n\n```\n\nNot a huge cost, but it can be annoying to repeat that. In addition, one of the most common problems we see is people not using the store-specific version of the `Dispatch` type, and having TS tell them they can't dispatch thunks because those aren't plain action objects.\n\n\nSo, for consistency, we recommend that users always create pre-typed versions of the hooks and use them, so they don't accidentally forget to use the right types:\n\n\n\n```\nimport { useAppSelector, useAppDispatch } from \"app\/hooks\";\n\nfunction MyComponent() {\n  \/\/ Already knows the state is `RootState`\n  const todos = useAppSelector(state => state.todos);\n\n  \/\/ Already knows that `dispatch` can accept a thunk\n  const dispatch = useAppDispatch();\n}\n\n```\n\n"}
{"questionId":"e1510e80e74c4f85859e595336649267","question":"How to fix config.headers.Authorization \"Object is possibly undefined\" when using axios interceptors\nI got the following code :\n\n\n\n```\nloggedInAxios.interceptors.request.use(\n  async (config) => {\n    if (isTokenExpired('access_token')) {\n      const response = await getRefreshToken();\n      await refreshAccessToken(response);\n    }\n    const accessToken = localStorage.getItem('access_token');\n    config.headers.Authorization = `Bearer ${accessToken}`;\n    return config;\n  },\n  (error) => error\n);\n\n```\n\nBut typescript is complaining that config.headers.Authorization object is possibly undefined.\n\n\nI found a way by adding the following:\n\n\n\n```\nif (!config) {\n config = {};\n}\nif (!config.headers) {\n  config.headers = {};\n}\n\n```\n\nBut I do not think that this is the best way to do it...\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"`config` is of type `AxiosRequestConfig`, thus cannot be undefined.\n\n\nOn the other hand, `config.header` can indeed be.\nAs it's a `Record` (`export type AxiosRequestHeaders = Record<string, string>;`), you can indeed default it with an empty object:\n\n\n\n```\nloggedInAxios.interceptors.request.use(\n  async (config: AxiosRequestConfig) => {\n    config.headers = config.headers ?? {};\n\n    \/\/ Now config.headers can be safely used\n    config.headers.Authorization = `...`\n\n    return config;\n  },\n  (error) => error\n);\n\n```\n\n"}
{"questionId":"5cdad85c13f14b06ab5d9668995cb06c","question":"NestJS\/Mongoose - Create an Array of Object with reference to another Schema\nI'm building the back-end side of a personal application, and I have two particular models\/schemas. One if for Products an another for Orders. I want to do the following:\n\n\nThe Orders need to have the following array with this structure:\n\n\n\n```\nproducts: [\n  {\n    product: string;\n    quantity: number;\n  }\n]\n\n```\n\nThe product should be an `ObjectId` of mongo, and this needs a reference for a 'Product' model.\n\n\nHow I can reach this? I don't really know how to \"type\" this with the `@Prop()` decorator.\n\n\n\n```\n@Prop({\n    \/\/ HOW I DO THIS?\n})\nproducts: [{ quantity: number; product: mongoose.Types.ObjectId }];\n\n```\n\nThis is my Order Schema:\n\n\n\n```\nimport { Prop, Schema, SchemaFactory } from '@nestjs\/mongoose';\nimport * as mongoose from 'mongoose';\nimport { Document } from 'mongoose';\n\nexport type OrderDocument = Order & Document;\n\n@Schema()\nexport class Order {\n  @Prop({ type: String, required: true })\n  name: string;\n\n  @Prop({ type: Number, min: 0, required: true })\n  total: number;\n\n  @Prop({\n    type: String,\n    default: 'PENDING',\n    enum: ['PENDING', 'IN PROGRESS', 'COMPLETED'],\n  })\n  status: string;\n\n  @Prop({\n    \/\/ HOW I DO THIS?\n  })\n  products: [{ quantity: number; product: mongoose.Types.ObjectId }];\n\n  @Prop({\n    type: mongoose.Schema.Types.ObjectId,\n    ref: 'Customer',\n    required: true,\n  })\n  customer: mongoose.Types.ObjectId;\n\n  @Prop({\n    type: mongoose.Schema.Types.ObjectId,\n    ref: 'User',\n    required: true,\n  })\n  owner: mongoose.Types.ObjectId;\n\n  @Prop({ type: Date, default: Date.now() })\n  createdAt: Date;\n}\n\nexport const OrderSchema = SchemaFactory.createForClass(Order);\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"\n```\n@Prop({\n    type:[{quantity:{type:Number}, product:{type:Schema.Types.ObjectId}}]\n  })\n  products: { quantity: number; product: Product }[];\n\n```\n\n"}
{"questionId":"73818f6f5a364612b2c26ce73aeea44f","question":"React Redux: How to handle errors in RTK Queries\/Mutation Typescript?\nI'm using Typescript with RTK mutation everything is working good but if I send any error from backend in specific JSON format like:\n\n\n\n```\n{ \n   status: \"Error\",\n   message \"Something went wrong\"\n}\n\n```\n\nWhen I check on my browser network tab its showing me the correct error response like:\n\n\n\n```\n{\n   data: { \n      status: \"Error\",\n      message \"Something went wrong\"\n    }\n}\n\n```\n\nI'm getting error in the mutation hook:\n\n\n\n```\nconst [createCategory, {isLoading, error }] = useCreateCategoryMutation();\n\n```\n\nbut I can't access `error.data.message` in my react it is giving me types error like:\n\n\n\n> \n> Property 'data' does not exist on type 'FetchBaseQueryError | SerializedError'.\n> \n> \n> \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"At this point, it could be an error from the server (`FetchBaseQueryError`) or just any error thrown by code you wrote (`SerializedError`, e.g. in `query`, `queryFn`, `transformResponse` etc.) - and that could have a completely different shape.\n\n\nTo make sure it's a `FetchBaseQueryError`, just do\n\n\n\n```\nif ('data' in error) {\n  \/\/ TypeScript will handle it as `FetchBaseQueryError` from now on.\n}\n\n```\n\n"}
{"questionId":"1a39493e8d4f472d8901e9c7ae3bf190","question":"How to obtain container id base on docker image name via command line ?\nIf I ran `sudo doccker ps` I got this \n\n\n\n```\n[user@vm1 ~]$ sudo docker ps \n\nCONTAINER ID        IMAGE                          COMMAND                  CREATED                  STATUS                  PORTS                        NAMES\ne8ff73dec1d5        portal-mhn:latest         \"nginx -g 'daemon of\u2026\"   43 minutes ago           Up 43 minutes  portal-mhn_portal-mhn.1.4rsfv94wy97gb333q3kfyxz32\n62a7cf09d7bf        portal-admin:latest       \"nginx -g 'daemon of\u2026\"   43 minutes ago           Up 43 minutes  portal-admin_portal-admin.1.s62iep4gl5g5oj2hrap14kz1t  \n\n```\n\nI'm trying to grab the container ID base on ImageName.\n\n\nEx. Is there away to grab the container id of portal-mhn:latest via a command line ? which is `e8ff73dec1d5`\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"If you want to get the container id based on the image name this should work:\n\n\n\n```\n$ docker ps | grep '<image_name>' | awk '{ print $1 }'\n\n```\n\nOr even:\n\n\n\n```\n$ docker ps | awk '\/<image_name>\/ { print $1 }'\n\n```\n\nAs others have suggested you can also directly filter by the image name using the `ancestor` filter:\n\n\n\n```\n$ docker ps -aqf \"ancestor=<image_name>\"\n\n```\n\nThanks to @kevin-cui and @yu-chen.\n\n\n"}
{"questionId":"24803043dc5c4a36af065755cd17e1de","question":"Unit test function that is opening and reading a file\nI am working on learning go with a simple program that is doing some file reading and am working on adding unit testing to my program. I have ran into an issue\/question while doing this. I want to unit test the function below and my question is that the function takes a name of the file which is then opened and processed. During testing I do not want to actually pass it a real file. I am wondering is this something I can somehow mock so that I can just pass it a \"fake\" file and have it process that instead? Thanks! \n\n\n\n```\nfunc openAndReadFile(fileName string) [][]string {\n    file, err := os.Open(fileName)\n    if err != nil {\n        fmt.Printf(\"Failed to read file: %s\", fileName)\n    }\n    r := csv.NewReader(file)\n    lines, err := r.ReadAll()\n    if err != nil {\n        log.Fatal(err)\n    }\n    return lines\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"go"},"answer":"You need to refactor your code and make more suitable for testing.\n\n\nHere is how I would do it:\n\n\n\n```\nfunc openAndReadFile(fileName string) [][]string {\n    file, err := os.Open(fileName)\n    if err != nil {\n        fmt.Printf(\"Failed to open file: %s\", fileName)\n    }\n    lines, err := readFile(file)\n    if err != nil {\n        fmt.Printf(\"Failed to read file: %s\", fileName)\n    }\n    return lines\n}\n\nfunc readFile(reader io.Reader) ([][]string, error) {\n    r := csv.NewReader(reader)\n    lines, err := r.ReadAll()\n    if err != nil {\n        log.Fatal(err)\n    }\n    return lines, err\n}\n\n```\n\nThen for testing you can simply use any data structure that implements the `io.reader` interface. For example, I use a bytes buffer, but you can choose a network connection:\n\n\n\n```\nfunc TestReadFile(t *testing.T) {\n    var buffer bytes.Buffer\n    buffer.WriteString(\"fake, csv, data\")\n    content, err := readFile(&buffer)\n    if err != nil {\n        t.Error(\"Failed to read csv data\")\n    }\n    fmt.Print(content)\n}\n\n```\n\n"}
{"questionId":"7a02300e0b7645d193f63ec4b685c0f4","question":"How to mock result from KafkaTemplate\nI have a method for sending kafka message like this: \n\n\n\n```\n@Async\npublic void sendMessage(String topicName, Message message) {\n    ListenableFuture<SendResult<String, Message >> future = kafkaTemplate.send(topicName, message);\n\n    future.addCallback(new ListenableFutureCallback<>() {\n\n        @Override\n        public void onSuccess(SendResult<String, Message > result) {\n            \/\/do nothing\n        }\n\n        @Override\n        public void onFailure(Throwable ex) {\n            log.error(\"something wrong happened\"!);\n        }\n    });\n}\n\n```\n\nAnd now I am writing unit tests for it. I would like to test also the two callback methods `onSuccess` and `onFailure` methods, so my I idea is to mock the KafkaTemplate, something like : \n\n\n\n```\nKafkaTemplate kafkaTemplate = Mockito.mock(KafkaTemplate.class);\n\n```\n\nBut now I am getting stuck on the mocking result for these two cases:\n\n\n\n```\nwhen(kafkaTemplate.send(anyString(), any(Message.class))).thenReturn(????);\n\n```\n\nwhat should I put in the `thenReturn` method for the case success and for the case failure? Does anyone have an idea please? Thank you very much!\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"You can mock the template but it's better to mock the interface.\n\n\n\n```\n    Sender sender = new Sender();\n    KafkaOperations template = mock(KafkaOperations.class);\n    SettableListenableFuture<SendResult<String, String>> future = new SettableListenableFuture<>();\n    when(template.send(anyString(), any(Message.class))).thenReturn(future);\n    sender.setTemplate(template);\n    sender.send(...);\n\n    future.set(new SendResult<>(...));\n    \n    ...or...\n\n    future.setException(...\n\n```\n\n**EDIT**\n\n\nUpdated to `CompletableFuture` (Spring for Apache Kafka 3.0.x and later)...\n\n\n\n```\npublic class Sender {\n\n    private  KafkaOperations<String, String> template;\n\n    public void setTemplate(KafkaOperations<String, String> template) {\n        this.template = template;\n    }\n\n    public void send(String topic, Message<?> data) {\n        CompletableFuture<SendResult<String, String>> future = this.template.send(data);\n        future.whenComplete((result, ex) -> {\n            if (ex == null) {\n                System.out.println(result);\n            }\n            else {\n                System.out.println(ex.getClass().getSimpleName() + \"(\" + ex.getMessage() + \")\");\n            }\n        });\n    }\n\n}\n\n@ExtendWith(OutputCaptureExtension.class)\npublic class So57475464ApplicationTests {\n\n    @Test\n    public void test(CapturedOutput captureOutput) {\n        Message message = new GenericMessage<>(\"foo\");\n        Sender sender = new Sender();\n        KafkaOperations template = mock(KafkaOperations.class);\n        CompletableFuture<SendResult<String, String>> future = new CompletableFuture<>();\n        given(template.send(any(Message.class))).willReturn(future);\n        sender.setTemplate(template);\n        sender.send(\"foo\", message);\n        future.completeExceptionally(new RuntimeException(\"foo\"));\n        assertThat(captureOutput).contains(\"RuntimeException(foo)\");\n    }\n\n}\n\n```\n\n"}
{"questionId":"3b780c8f0eea4e48937b1c787d0d07fd","question":"How to save to disk \/ export a lightgbm LGBMRegressor model trained in python?\nHi I am unable to find a way to save a `lightgbm.LGBMRegressor` model to a file for later re-use.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"For Python 3.7 and `lightgbm==2.3.1`, I found that the previous answers were insufficient to correctly save and load a model. The following worked:\n\n\n\n```\nlgbr = lightgbm.LGBMRegressor(num_estimators = 200, max_depth=5)\nlgbr.fit(train[num_columns], train[\"prep_time_seconds\"])\npreds = lgbr.predict(predict[num_columns])\nlgbr.booster_.save_model('lgbr_base.txt')\n\n```\n\nFinally, we can validated that this worked via:\n\n\n\n```\nmodel = lightgbm.Booster(model_file='lgbr_base.txt')\nmodel.predict(predict[num_columns])\n\n\n```\n\nWithout the above, I was getting the error: `AttributeError: 'LGBMRegressor' object has no attribute 'save_model'`\n\n\n"}
{"questionId":"17f18ff7b56c404c907dd520965e4295","question":"Why are std::vector and std::valarray initializing constructors different?\nI have just been burned by the following.\n\n\nIf I want to initialize a `std::vector` of `n` elements with a constant `X` I do this:\n\n\n\n```\nstd::vector<double> v(n, X);\n\n```\n\nBut if I need to initialize a `std::valarray` of `n` elements with a constant `X` I need to swap the size and initializing constant:\n\n\n\n```\nstd::valarray<double> va(X, n);\n\n```\n\nThis seems to me like an arbitrary 'gotcha'.\n\n\nIs there a technical reason or some design rationale provided by the standards committee when deciding about the order of the fill constructor's parameters when `std::vector` and `std::valarray` were standardized?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c++"},"answer":"Because they don't come from the same place: `vector` comes from the STL library, and `valarray` doesn't (I haven't been able to find out where it comes from, but there seem to be strong connections to Fortran).\n\n\nQuoting Bjarne himself:\n\n\n\n> \n> Most of the time, work on each of these components progressed in\n> isolation from work on the others. There was no overall design or\n> design philosophy.  \n> \n> [...]  \n> \n> Basically, the committee failed to contain \u201cdesign by committee\u201d so\n> whereas the STL reflects a clear philosophy and coherent style, most\n> of the other components suffered. Each represents its own style and\n> philosophy, and some (such as `string`) manage simultaneously to present several.\n> \n> \n> \n\n\n(From \"Evolving a language in and for the real world: C++ 1991-2006\".)\n\n\nSo I would say that the rationale is the traditional C++ one, \"things are the way they are, and changing them for the sake of Standardization would break a lot of things, so let's leave well enough alone\".\n\n\n"}
{"questionId":"1bde09b346bc4a498cf8e40c9deb992f","question":"Count the number of characters, words and lines in PowerShell\nIn Linux we have the \"wc\" command which allows us to count the number of characters, words and lines in a file.\n\n\nBut do we have a similar cmdlet in PowerShell. The Measure-Object cmdlet I tried could only count the number of lines but not the characters and words.\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"bash\/shell"},"answer":"\n```\nGet-Content [FILENAME] | Measure-Object -Character\n\n```\n\nIt counts the number of characters in the file.\n\n\n\n```\nGet-Content [FILENAME] | Measure-Object -Word\n\n```\n\nIt counts the number of words in the file.\n\n\n\n```\nGet-Content [FILENAME] | Measure-Object -Line\n\n```\n\nIt counts the number of lines in the file.\n\n\n"}
{"questionId":"107c86599e5b4f13949134233ee9aa29","question":"How to sort integer array in ascending and descending order using lambda only in java\n\n```\nint[] arr2 = new int[] {54, 432, 53, 21, 43};\n\n```\n\nI am using this to sort but it is giving an error.\n\n\n\n```\nArrays.sort(arr2, (a, b) -> a - b);\n\n```\n\nThis is also giving an error.\n\n\n\n```\narr2.sort((a, b) -> a - b);\n\n```\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"java"},"answer":"You could sort the input of type `Integer[]` as :\n\n\n\n```\nInteger[] arr2 = new Integer[] {54,432,53,21,43};\nArrays.sort(arr2, Comparator.reverseOrder());\n\n```\n\nor possibly with primitive types as :\n\n\n\n```\nint[] arr2 = new int[]{54, 432, 53, 21, 43};\nint[] sortedArray = Arrays.stream(arr2)\n        .boxed()\n        .sorted(Comparator.reverseOrder()) \/\/ just use 'sorted()' for ascending order\n        .mapToInt(Integer::intValue)\n        .toArray();\n\n```\n\nor further using a trick from one of the existing answers (do note that it should be cautiously used with boundary values though) :\n\n\n\n```\nint[] sortedArray = Arrays.stream(arr2)\n        .map(i -> -i).sorted().map(i -> -i) \/\/ just use 'sorted()' for ascending order\n\/\/ Edit - use map(i -> ~i).sorted().map(i -> ~i) to be safe from the issue with Integer.MIN_VALUE\n        .toArray();\n\n```\n\n***Edit***: For an in-place ascending order sort, you just need to perform :\n\n\n\n```\nint[] arr2 = new int[]{54, 432, 53, 21, 43};\nArrays.sort(arr2);\n\n```\n\n"}
{"questionId":"55c5eddc2fd546b0beec9ed23aaeef25","question":"IntelliJ-Idea disable inspection: Actual value of parameter is always\nIs there a way to disable this inspection? I know, this maybe be a bad design, but I still would like to disable it. \n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"Find an instance of the inspection, and hit `ALT`+`ENTER` to open the content menu. You'll see a suggestion there that looks like \"Inline value 'XYZ' for parameter 'myParameter'\". Click the right arrow next to it to open a second context menu, and there you'll find options to edit the inspection setting in order to tune the conditions that will produce it, suppress it for the method\/class\/parameter, or even disable it altogether.\n\n\n"}
{"questionId":"b217f1408e164dfbbd92a63a4111d059","question":"How to retrieve all the content of calls made to a mock?\nI'm writing a unit test for a function that takes an array of dictionaries and ends up saving it in a CSV. I'm trying to mock it with pytest as usual:\n\n\n\n```\ncsv_output = (\n    \"Name\\tSurname\\r\\n\"\n    \"Eve\\tFirst\\r\\n\"\n)\nwith patch(\"builtins.open\", mock_open()) as m:\n    export_csv_func(array_of_dicts)\n\nassert m.assert_called_once_with('myfile.csv', 'wb') is None\n[and here I want to gather all output sent to the mock \"m\" and assert it against \"csv_output\"]\n\n```\n\nI cannot get in any simple way **all the data** sent to the mock during the *open()* phase by `csv` to do the comparison in bulk, instead of line by line. To simplify things, I verified that the following code mimics the operations that `export_csv_func()` does to the mock:\n\n\n\n```\nwith patch(\"builtins.open\", mock_open()) as m:\n  with open(\"myfile.csv\", \"wb\") as f:\n    f.write(\"Name\\tSurname\\r\\n\")\n    f.write(\"Eve\\tFirst\\r\\n\")\n\n```\n\nWhen I dig into the mock, I see:\n\n\n\n```\n>>> m\n<MagicMock name='open' spec='builtin_function_or_method' id='4380173840'>\n>>> m.mock_calls\n[call('myfile.csv', 'wb'),\n call().__enter__(),\n call().write('Name\\tSurname\\r\\n'),\n call().write('Eve\\tFirst\\r\\n'),\n call().__exit__(None, None, None)]\n>>> m().write.mock_calls\n[call('Name\\tSurname\\r\\n'), call('Eve\\tFirst\\r\\n')]\n>>> dir(m().write.mock_calls[0])\n['__add__'...(many methods), '_mock_from_kall', '_mock_name', '_mock_parent', 'call_list', 'count', 'index']\n\n```\n\nI don't see anything in the MagickMock interface where I can gather all the input that the mock has received. \n\n\nI also tried calling `m().write.call_args` but it only returns the last call (the last element of the `mock_calls` attribute, i.e. `call('Eve\\tFirst\\r\\n')`).\n\n\nIs there any way of doing what I want?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"You can create your own `mock.call` objects and compare them with what you have in the `.call_args_list`.\n\n\n\n```\nfrom unittest.mock import patch, mock_open, call\n\nwith patch(\"builtins.open\", mock_open()) as m:\n    with open(\"myfile.csv\", \"wb\") as f:\n        f.write(\"Name\\tSurname\\r\\n\")\n        f.write(\"Eve\\tFirst\\r\\n\")\n\n# Create your array of expected strings\nexpected_strings = [\"Name\\tSurname\\r\\n\", \"Eve\\tFirst\\r\\n\"]\nwrite_calls = m().write.call_args_list\nfor expected_str in expected_strings:\n    # assert that a mock.call(expected_str) exists in the write calls\n    assert call(expected_str) in write_calls\n\n```\n\nNote that you can use the assert call of your choice. If you're in a unittest.TestCase subclass, prefer to use `self.assertIn`.\n\n\nAdditionally, if you just want the arg values you can unpack a `mock.call` object as tuples. Index 0 is the \\*args. For example:\n\n\n\n```\nfor write_call in write_calls:\n    print('args: {}'.format(write_call[0]))\n    print('kwargs: {}'.format(write_call[1]))\n\n```\n\n"}
{"questionId":"a7ac24ff698e4b4f8d031d35bad7b06a","question":"Given an integer N. What is the smallest integer greater than N that only has 0 or 1 as its digits?\nI have an integer N. I have to find the smallest integer greater than N that doesn't contain any digit other than 0 or 1. For example: If `N = 12` then the answer is `100`. \nI have coded a brute force approach in C++. \n\n\n\n```\nint main() {\n    long long n;\n    cin >> n;\n\n    for (long long i = n + 1; ; i++) {\n        long long temp = i;\n        bool ok = true;\n        while (temp != 0) {\n            if ( (temp % 10) != 0 && (temp % 10) != 1) {\n                ok = false;\n                break;\n            }\n            temp \/= 10;\n        }\n        if (ok == true) {\n            cout << i << endl;\n            break;\n        }\n    }\n}\n\n```\n\nThe problem is, my approach is too slow. I believe there is a very efficient approach to solve this. How can I solve this problem efficiently?\n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"c++"},"answer":"1. Increment N,\n2. Starting from the left, scan until you find a digit above 1. Increment the partial number before it and zero out the rest.\n\n\nE.g.\n\n\n\n```\n12 -> 13 -> 1|3 -> 10|0\n101 -> 102 -> 10|2 -> 11|0\n109 -> 110 -> 111|\n111 -> 112 -> 11|2 -> 100|0\n198 -> 199 -> 1|99 -> 10|00\n1098 -> 1099 -> 10|99 -> 11|00\n10203 -> 10204 -> 10|204 -> 11|000\n111234 -> 111235 -> 111|235 -> 1000|000\n...\n\n```\n\n\n\n---\n\n\n**Proof:**\n\n\nThe requested number must be at least N+1, this is why we increment. We are now looking for a number greater or equal.\n\n\nLet us call the *prefix* the initial 0\/1 digits and *suffix* what comes after. We must replace the first digit of the suffix by a zero and set a larger prefix. The smallest prefix that fits is the current prefix plus one. And the smallest suffix that fits is all zeroes.\n\n\n\n\n---\n\n\n**Update:**\n\n\nI forgot to specify that the prefix must be incremented *as a binary number*, otherwise forbidden digits could appear.\n\n\n"}
{"questionId":"b125d838d86f45888b95ed2def2317e5","question":"Add one additional bean to \"@WebMvcTest\"\nI have a controller and a test using @WebMvcTest and its running fine. Now i needed to add a little validation logic and for this i `@Autowired` an additional bean (a `@Component`, a MapstructMapper).\n\n\nAs expected now the test is failing due to `@WebMvcTest`. (No components are discovered)\n\n\nIs there a way to add one bean to the context created?\n\n\nSince i am using `@MockBeans` to mock service layer: is there a way to delegate all mock calls to a real object? With this i could mock the mapper and delegate to real mapper?!\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"A simple way of getting additional beans in the context is via using nested configuration classes within test classes\n\n\n\n```\n@TestConfiguration\nstatic class AdditionalConfig {\n    @Bean\n    public SomeBean getSomeBean() {\n        return new SomeBean());\n    }\n}\n\n```\n\n*Example:*\n\n\nScenario - If you have some Controller say *ProductController* and you have the corresponding slice-test for the class say *ProductionControllerTest*\n\n\n\n```\n@RestController\npublic class ProductController {\n\n    @Autowired\n    private IProductService productService;\n\n    @Autowired\n    private IProductValidator productValidator;\n\n\n    @GetMapping(\"product\")\n    public Product getProduct(@RequestParam Long id) {\n\n        Product product = productService.getProduct(id); \/\/ you are using mockBean of productService\n\n        productValidator.validateProduct(product); \/\/ you need real bean of productValidator\n        return product;\n    }\n}\n\n```\n\nCorresponding slide test class with an additional bean configuration\n\n\n\n```\n@RunWith(SpringRunner.class)\n@WebMvcTest\npublic class ProductControllerSliceTest {\n\n    @Autowired\n    private MockMvc mockMvc;\n\n    @MockBean\n    private IProductService productService;\n\n    @Autowired\n    private ApplicationContext applicationContext;\n\n    @TestConfiguration\n    static class AdditionalConfig {\n        @Bean\n        public IProductValidator productValidator() {\n            return new ProductValidator();\n        }\n    }\n\n\n    @Test\n    public void testProductGetById() throws Exception {\n        Product testProductWithID1L = new Product(1L, \"testProduct\");\n        when(productService.getProduct(anyLong())).thenReturn(testProductWithID1L);\n\n        mockMvc.perform(get(\"\/product\")\n                .param(\"id\", \"1\")).andDo(print())\n                .andExpect(status().isOk())\n                .andExpect(jsonPath(\"name\")\n                        .value(\"testProduct\"))\n                .andExpect(MockMvcResultMatchers.jsonPath(\"id\")\n                        .value(\"1\"));\n    }\n}\n\n```\n\n  **Additional thoughts on your scenario:**  If you really intend to do the unit testing of the controller class then ideally you should mock all the additional dependencies of class that you are testing. \nIdeally, the intention of the unit test is to only test the behavior of the object\/class under test. All the dependent classes behavior or external calls should be mocked.   \n\nWhen you start testing several classes together under one test, you are moving more towards a component test or integration test\n\n\n"}
{"questionId":"018a09ef345541ebb42e861acff7e53f","question":"Split\/Expand Dataframe based on column values\nI have a DataFrame like the below one, with identifiers as a column on top of an existing dateindex.\n\n\n\n```\npd.DataFrame(index = [pd.to_datetime('2021-01-01'), pd.to_datetime('2021-01-01'),pd.to_datetime('2021-01-02'),pd.to_datetime('2021-01-02'), pd.to_datetime('2021-01-03'),pd.to_datetime('2021-01-03')], columns=['id','A', 'B'], data=[['foo',1,5],['bar',8,12],['foo',7,1], ['bar',5,1], ['foo',4,3],['bar',7,1]])\n\nOut[6]: \n             id  A   B\n2021-01-01  foo  1   5\n2021-01-01  bar  8  12\n2021-01-02  foo  7   1\n2021-01-02  bar  5   1\n2021-01-03  foo  4   3\n2021-01-03  bar  7   1\n\n```\n\nMy goal is to create a new sub-dataframes for each of the columns (A and B) except id, with dateIndex as single Index, and id (foo, bar) as column names. The expected output is shown below:\n\n\n\n```\nA\nOut[9]: \n            foo  bar\n2021-01-01    1    8\n2021-01-02    7    5\n2021-01-03    4    7\n\nB\nOut[11]: \n            foo  bar\n2021-01-01    5   12\n2021-01-02    1    1\n2021-01-03    3    1\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"\n```\nA, B = map(df.set_index('id', append=True).unstack().get, ['A', 'B'])\n\nprint(A)\n\nid          bar  foo\n2021-01-01    8    1\n2021-01-02    5    7\n2021-01-03    7    4\n\nprint(B)\n\nid          bar  foo\n2021-01-01   12    5\n2021-01-02    1    1\n2021-01-03    1    3\n\n```\n\n"}
{"questionId":"063fe848ce7147abaa129d46069c52ce","question":"Bash associative array with list as value\nI have to work with an output of a Java tool, which returns a map data structure that looks like `HashMap<String, ArrayList<String>`. I have to work with BASH and i tried to declare it as an associative array, what is very similar to a map. The declaration of the associative array in bash should be in one line, i try to do this as following. \n\n\n\n```\nARRAY=([\"sem1\"]=(\"first name\" \"second name\") [\"sem2\"]=(\"third name\") [\"sem3]=OTHER_LITS)\n\n```\n\nBut this creates the following error:\n\n\n\n```\nbash: syntax error near unexpected token `('\n\n```\n\nI can define this line by line, but i want to have it in one line. How can i define a assoviative array in bash in only one line? \n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"BTW, associative array, dictionary or map - all come into the one abstract data type (let's call it a *dictionary*).\n\n\nSo, here is the solution for storing *array* as values in the *dictionary* of Bash (4+ version).\n\n\nNote, that *array* in Bash is a space delimited list of strings (so no any spaces inside the element, i.e. string), so we could write a quoted list:\n\n\n`\"firstname middlename secondname\"`\n\n\nas a value of the `s1` key in our `X` dictionary:\n\n\n\n```\ndeclare -A X=(\n  ['s1']=\"firstname middlename secondname\"\n  ['s2']=\"surname nickname\"\n  ['s3']=\"other\"\n)\n\n```\n\nNow we can get the value of the `s1` key as array:\n\n\n`declare -a names=(${X[s1]})`\n\n\nVariable `names` now contains *array*:\n\n\n\n```\n> echo $names\nfirstname\n\n> echo ${names[1]}\nmiddlename\n\n> echo ${#names[@]}\n3\n\n```\n\nFinally, your question part where the strings with spaces were shown:\n\n\n`\"first name\"`, `\"second name\"`\n\n\nLet's do a trick - represent a *space* as a special symbol sequence (it could be just one symbol), for example, double underscores:\n\n\n`\"first__name\"`, `\"second__name\"`\n\n\nDeclare our *dictionary* again, but with \"escaped\" spaces inside array elements:\n\n\n\n```\ndeclare -A X=(\n  ['s1']=\"first__name middle__name second__name\"\n  ['s2']=\"surname nickname\"\n  ['s3']=\"other\"\n)\n\n```\n\nIn this case after we get the value of the `s1` key as array:\n\n\n`declare -a names=(${X[s1]})`\n\n\nWe need to post process our array elements to remove `__` the space-replacements to the actual *space* symbols. To do this we simply use replace commands of Bash strings:\n\n\n\n```\n> echo ${names\/__\/ }\nfirst name\n\n> echo ${names[1]\/__\/ }\nmiddle name\n\n> echo ${#names[@]}\n3\n\n```\n\n"}
{"questionId":"b319ddf8948b4dce895947271c021107","question":"md5 in bigquery\nIn BigQuery, I'm using md5 function as: \n\n\n\n```\nselect md5('<<some string>>') as hashed\n\n```\n\nwhich always returns \"==\" in the last of the letter like: \n\n\n\n```\nR7zlx09Yn0hn29V+nKn4CA==    \n\n```\n\nWhy does '==' always come with it? \n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"sql"},"answer":"You need to use TO\\_HEX to get the representation you want as md5 returns `BYTES` and you need strings: \n\n\n\n> \n> TO\\_HEX: Converts a sequence of BYTES into a hexadecimal STRING.\n>  Converts each byte in the STRING as two hexadecimal characters in the\n>  range (0..9, a..f).\n> \n> \n> \n\n\n\n```\nselect TO_HEX(md5('123456')) as hashed\n\n```\n\nreturns:\n\n\n\n```\ne10adc3949ba59abbe56e057f20f883e\n\n```\n\n"}
{"questionId":"466195d0867645e68559090df3549482","question":"How do I make a Rust program which can be executed without using `cargo run`?\nHow can I make a program in Rust which can be executed from anywhere without using `cargo run`, by just clicking on the file?\n\n\nIs there any crate? I have written code for snake game and I want to run it by just clicking on a file.\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"rust"},"answer":"If you compile a Rust application with:\n\n\n\n```\ncargo build --release\n\n```\n\nit will place a binary file in `.\/target\/release`. So if your application is called `snake_game`, you can run it with `.\/target\/release\/snake_game`, or by double-clicking on that file.\n\n\nThis binary is completely self-contained, so you can move or copy it to somewhere else on your computer.\n\n\n"}
{"questionId":"5b4d77d332fa448bb0700cf4d9f1b096","question":"What is '@{}' meaning in PowerShell\nI have line of scripts for review here, I noticed variable declaration with a value:\n\n\n\n```\nfunction readConfig {\n    Param([string]$fileName)\n    $config = @{}\n    Get-Content $fileName | Where-Object {\n        $_ -like '*=*'\n    } | ForEach-Object {\n        $key, $value = $_ -split '\\s*=\\s*', 2\n        $config[$key] = $value\n    }\n    return $config\n}\n\n```\n\nI wonder what `@{}` means in `$config = @{}`?\n\n\n","questionMetadata":{"type":"conceptual","level":"beginner","tag":"bash\/shell"},"answer":"`@{}` in PowerShell defines a hashtable, a data structure for mapping unique keys to values (in other languages this data structure is called \"dictionary\" or \"associative array\").\n\n\n`@{}` on its own defines an empty hashtable, that can then be filled with values, e.g. like this:\n\n\n\n```\n$h = @{}\n$h['a'] = 'foo'\n$h['b'] = 'bar'\n\n```\n\nHashtables can also be defined with their content already present:\n\n\n\n```\n$h = @{\n    'a' = 'foo'\n    'b' = 'bar'\n}\n\n```\n\nNote, however, that when you see similar notation in PowerShell *output*, e.g. like this:\n\n\n\n```\n\nabc: 23\ndef: @{\"a\"=\"foo\";\"b\"=\"bar\"}\n\n```\n\nthat is usually *not* a hashtable, but the string representation of a custom object.\n\n\n"}
{"questionId":"1a0473f9204e4e5d8dc7d13eb284b957","question":"Understanding how `lw` and `sw` actually work in a MIPS program\nI'm having bit of a difficulty understanding what `sw` and `lw` do in a MIPS program. My understanding of the topic is that we use `lw` to transfer data from the memory into the register and vice-versa for `sw`. But how is this exactly accomplished?\n\n\nLet's say we have the following line of code:\n\n\n\n```\nlw Reg.Dest, Offset(Reg.Source)\nsw Reg.Source, Offset(Reg.Dest)\n\n```\n\nIf we concentrate on `lw` it's essentially storing the data from the memory, `Reg.Source` and multiplying the address of that data with the `Offset`, always a multiple of $4$ because the registers deal with $32$ bits and the memory uses $8$ bits, into a specific address in the register which is equal to `Offset + Reg.Source` - so if we say that `Offset = 16, Reg.Source = $s1 = 12` then the register will store the data from the memory into the address $28$ in the register.\n\n\nAssuming that my understanding of `lw` is correct, my question is then how does `sw` work? \n\n\nPS: It would be great if the answers could also contain just a one liner example such as `sw $t0, 32($s3)`.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"assembly"},"answer":"lw (load word) loads a word from memory to a register.\n\n\n\n```\nlw $2, 4($4) # $2 <- mem($4+4)\n\n```\n\n$2 is the destination register and $4 the address register. And the source of information is the memory.\n\n\n4 is an offset that is added (not multiplied) to the address register. This kind of memory access is called based addressing and is it quite useful in many situations. For instance, if $4 hold the address of a struct, the offset allows to pick the different fields of the struct. Or the next or previous element in a array, etc. offset does not have to be a multiple of 4, but (address register + offset) must and most of the time both are.\n\n\nSw is similar, but stores a register into memory.\n\n\n\n```\n sw $5, 8($7) # mem[$7+8] <- $5\n\n```\n\nAgain $7 is the register holding the memory address, 8 an offset and $5 is the source of the information that will be written in memory.\n\n\nNote that contrary to others MIPS instructions, the first operand is the source, not the destination. Probably this is to enforce the fact that the address register plays a similar role in both instruction, while in lw it is used to compute the memory address of the source of data and in sw the memory destination address.\n\n\n"}
{"questionId":"805390ca6eb14e7ba5ec7650d36a8f0b","question":"TypeError: can't convert np.ndarray of type numpy.object\\_\nHow to convert a numpy array of `dtype=object` to torch `Tensor`?\n\n\n\n```\narray([\n   array([0.5, 1.0, 2.0], dtype=float16),\n   array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"It is difficult to answer properly since you do not show us how you try to do it. From your error message I can see that you try to convert a numpy array containing objects to a torch tensor. This does not work, you will need a numeric data type:\n\n\n\n```\nimport torch\nimport numpy as np\n\n# Your test array without 'dtype=object'\na = np.array([\n    np.array([0.5, 1.0, 2.0], dtype=np.float16),\n    np.array([4.0, 6.0, 8.0], dtype=np.float16),\n])\n\nb = torch.from_numpy(a)\n\nprint(a.dtype) # This should not be 'object'\nprint(b)\n\n```\n\n**Output**\n\n\n\n```\nfloat16\ntensor([[0.5000, 1.0000, 2.0000],\n        [4.0000, 6.0000, 8.0000]], dtype=torch.float16)\n\n```\n\n"}
{"questionId":"bb0708fc876d4db08b2ceacdc0bd21c8","question":"How to empty an array in bash script\nI am trying to get specific information from a bunch of files.\nIterating over a list of files,`grep`ing for what I need. I know for sure that each `grep` will give more than 1 result and I want to store that result in an array. After finishing the work specific to file, I want to erase everything from arrays and start afresh for new file.\n\n\n\n```\nfiles_list=`ls`\n\nfor f in $files_list\ndo\n        echo $f\n        arr1=`cat $f | grep \"abc\" | grep \"xyz\"`\n        arr2=`cat $f | grep \"pqr\" | grep \"mno\"`\n        arr3=`cat $f | grep \"df\"`\n        for ((i=0; i<${#arr1[@]}; ++i)) \n        do\n            printf \"%s  %s %s\\n\" \"${arr1[i]}\" \"${arr2[i]}\" \"${arr3[i]}\"\n        done\n        unset $arr1\n        unset $arr2\n        unset $arr3\ndone\n\n```\n\nSo I used `unset` to empty the array but it's giving error. \n\n\n\n```\nline 49: unset: `x': not a valid identifier\n\n```\n\nI don't want to delete a particular member\/index of array but entire array itself. Can anyone tell me how to do it?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"`unset` works with variable names, not the values they keep. So:\n\n\n\n```\nunset arr1\n\n```\n\nor, if you want to empty it:\n\n\n\n```\narr1=()\n\n```\n\n"}
{"questionId":"e697a14b24e34a69baf0a6b69470077e","question":"Type ... is not assignable to type 'never'.(2322)\nThe following code snippet reports an error `Type 'string' is not assignable to type 'never'.(2322)` in the line `obj[prop] = value` and I struggle understanding why?\n\n\n\n```\ninterface fooType {\n    s: string,\n    n: number,\n}\n\nfunction bar(value: string, obj: fooType, prop: keyof fooType) {\n    if (typeof value === 'string') {\n        obj[prop] = value;\n    }\n}\n\nconst foo = {s: '', n: 0};\n\nbar('s', foo, 's');\n\nconsole.log(foo);\n\n\n```\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"typescript"},"answer":"`prop: keyof fooType` means that prop could be `\"s\"` or `\"n\"`. But those two property values have incompatible types. So typescript tries to infer a value that could be assignable to both by finding the intersection of the types of the properties `.s` *and* `.n`. So it does: `string & number`, which have no overlap, so the type is `never`.\n\n\nIf you want to type an argument as a union (like `\"s\" | \"n\"`) *and* you need to know which member of that union it is, then you need generics.\n\n\nFor generic parameters you need at least two here. One for the property name (we'll use `K` for key) you wish to update, and one for the value to make sure the it matches the property's type (we'll use `V` for value).\n\n\nYou can now drop the `typeof value === 'string'` because typescript will ensure the value is the right type for the property.\n\n\n\n```\nfunction bar<\n    K extends keyof fooType,\n    V extends fooType[K]\n>(\n    value: V,\n    obj: fooType,\n    prop: K\n) {\n    obj[prop] = value;\n}\n\nconst foo = {s: '', n: 0};\n\nbar('string', foo, 's'); \/\/ works\nbar(123, foo, 'n'); \/\/ works\n\n\n\/\/ Should be type errors:\nbar('string', foo, 'n') \/\/ '\"string\"' is not assignable to parameter of type 'number'\nbar(123, foo, 's') \/\/ '123' is not assignable to parameter of type 'string'.\n\n```\n\n"}
{"questionId":"abb1185283d54e7cbf5c62bee3dccaec","question":"Using multer and express with typescript\n# Background\n\n\nI'm making a simple website in which a user can upload an image.\nI'm using Node\/React\/Multer\/**Typescript**.\n\n\n# Problem\n\n\n\n```\napp.post('\/admin\/uploads', async (req, res) => {\n  uploadHandler(req, res, (err) => {\n    ...\n    if ( req.files.companyImage !== undefined ) {\n      ...\n    }\n    res.sendStatus(201);\n  });\n});\n\n```\n\ntypescript intellisense shows error like below.\n\n\n\n```\nProperty 'companyImage' does not exist on type '{ [fieldname: string]: File[]; } | File[]'.\nProperty 'companyImage' does not exist on type '{ [fieldname: string]: File[]; }'.ts(2339)\n\n```\n\nBut, I cannot understand why this is error. I think `files` object has type `{ [fieldname: string]: File[]; }`. This means `files` object can have property which is string.\n\n\nSo I test with simple example.\n\n\n\n```\ntype myType = {\n  [fieldName: string]: number\n}\n\nlet req: myType = {\n  a: 333,\n  b: 344\n}\n\nconsole.log(req.a);\nconsole.log(req.c); \/\/ undefined but intellisense don't show error\n\n```\n\nI don't know why `files` object cannot have `companyImage` property.\n\n\nCheck please.\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"typescript"},"answer":"I don't know if you successfully resolved your issue, but I had the same and had to explicitly tell TypeScript the type of my `req.files` property like this:\n\n\n\n```\nconst files = req.files as { [fieldname: string]: Express.Multer.File[] };\n\n```\n\nNote that, since I'm using `upload.fields(...)`, I'm not specifying that `req.files` can also be a simple `Express.Multer.File[]`.\n\n\n"}
{"questionId":"94edfec602a04febb7b4ea8c18bfcbb3","question":"Koa + TypeScript: Property 'body' does not exist on type Request\nI wanted to use koa & koa-bodyparser with TypeScript but whenever I access `ctx.request.**body**` I get an error that `**body**` doesn't exist on type `Request`\n\n\n\n```\nimport Koa from 'koa'\nimport Router from 'koa-router'\nimport bodyparser from 'koa-bodyparser'\n\nconst app = new Koa()\nconst router = new Router()\n\nconst data = ['lorem', 'ipsum', 'dolor', 'sit', 'amet']\n\napp.use(bodyparser())\nrouter.post('\/', (ctx, next) => {\n  const phrase = ctx.request.body; \/\/ Property 'body' does not exist on type Request\n  if (typeof phrase === 'string') {\n    ctx.response.body = data.filter(element => element.includes(phrase))\n  }\n})\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"Run `npm install --save-dev @types\/koa-bodyparser` in a terminal while in the directory where your package.json is\n\n\nThis package contains types introduced by koa-bodyparser (such as request.body)\n\n\n"}
{"questionId":"c279cdd80cad4b0baf25b6da0915f2fa","question":"Argument of type 'MonoTypeOperatorFunction' is not assignable to parameter of type 'UnaryFunction, Observable>'\ni am trying to migrate from `rxjs` 5 to 6 but i am having difficulties. when i try this\n\n\n\n```\nthis._connectivity.isOnline().pipe(first()).subscribe((state) => {\n  this.syncCheck(user.uid);\n});\n\n```\n\ni am getting this error\n\n\n\n```\nArgument of type 'MonoTypeOperatorFunction<any>' is not assignable to parameter of type 'UnaryFunction<Observable<any>, Observable<any>>'.\n  Types of parameters 'source' and 'source' are incompatible.\n    Type 'import(\"\/home\/User\/Desktop\/projectname\/node_modules\/rxjs\/Observable\").Observable<any>' is not assignable to type 'import(\"\/home\/User\/Desktop\/projectname\/node_modules\/rxjs\/internal\/Observable\").Observable<a...'.\n      Property 'map' is missing in type 'Observable<any>'.\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"I found the same error with my code:\n\n\n\n```\nlet source = of([1, 2, 3, 45, 56, 7, 7])\n    .pipe(\n        filter((x: number) => x % 2 == 0)\n    );\n\n```\n\nTS2345: Argument of type 'MonoTypeOperatorFunction' is not assignable to parameter of type 'OperatorFunction'.\n\n\nTo fix this, remove type from filter function\n\n\n\n```\n filter(x => x % 2 == 0)\n\n```\n\nNow you have error\n\n\nThe left-hand side of an arithmetic operation must be of type 'any', 'number', so make sure, that this annoying filter gets correct data type\n\n\n\n```\nfilter(x => Number(x) % 2 == 0) \/\/ parse element to number\n\n```\n\nBut now code stops work. Finally, to fix this, change of to from, at the beginning.\n\n\n\n```\nlet source = from([1, 2, 3, 45, 56, 7, 7])\n    .pipe(\n        filter((x: number) => Number(x) % 2 === 0)\n    )\n\n\n```\n\nor\n\n\n\n```\nlet source = of(1, 2, 3, 45, 56, 7, 7)\n.pipe(\n        filter((x: number) => Number(x) % 2 === 0)\n    )\n\n\n```\n\nSo, cause of error was my initial data structure.\n\n\nI think, that my example can help you with dealing similar problems.\n\n\n"}
{"questionId":"257d11e5650c4cd6ad8f5c2610e262c2","question":"How to make specific props non-nullable in Typescript?\nI have a type:\n\n\n\n```\ntype Item = {\n  cost: number | null\n  name: string\n  date: string | null\n  rating: number | null\n}\n\n```\n\nIs there a way in TS to create a type based on `Item`, which would have `date` and `rating` `NonNullable`? I could do something like this:\n\n\n\n```\n  type FullItem = Omit<Item, 'date' | 'rating'> & {\n    date: NonNullable<Item['date']>\n    rating: NonNullable<Item['rating']>\n  }\n\n```\n\nbut it seems a bit cumbersome. \n\n\n","questionMetadata":{"type":"implementation","level":"advanced","tag":"typescript"},"answer":"Ok, I just came up with a solution it seems:\n\n\n\n```\ntype RequiredNotNull<T> = {\n  [P in keyof T]: NonNullable<T[P]>\n}\n\ntype Ensure<T, K extends keyof T> = T & RequiredNotNull<Pick<T, K>>\n\ntype Item = {\n  cost: number | null\n  name: string\n  date: string | null\n  rating: number | null\n}\n\ntype FullItem = Ensure<Item, 'rating' | 'date'>\n\n```\n\n"}
{"questionId":"8f8a5259b37f4b128363cdf3dc22bc56","question":"Java Reactor Flux\/Mono, when does doOnNext get triggered before or after element is emitted?\nI have this confusion, when does doOnNext is triggered before or after of element emission by Publisher (Flux\/Mono). \n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"java"},"answer":"It's after the *publication* of the element - and it *has* to be after by definition, otherwise the `Consumer` passed to `doOnNext` wouldn't have access to the element emitted.\n\n\nHowever, `doOnNext()` is called *before* the subscriber. For example:\n\n\n\n```\nFlux.just(\"first\", \"second\")\n        .doOnNext(x -> System.out.println(x + \" onNext\"))\n        .subscribe(System.out::println);\n\n```\n\n...would output:\n\n\n\n```\nfirst onNext\nfirst\nsecond onNext\nsecond\n\n```\n\n"}
{"questionId":"c67f5537b4ca4f8f92bc1dabc45717df","question":"Use is\\_bitstring or is\\_binary in Elixir Guard for String?\nLet's take this simple function:\n\n\n\n```\n  @spec laBionda(String.t()) :: String.t()\n  def laBionda(name \\\\ \"you\") when is_bitstring(name) do\n    \"One for #{name}, one for me\"\n  end\n\n```\n\nI only want to define the function for String inputs.\n\n\nShould I use `is_bitstring` or `is_binary` on the Guard? Are there any differences? Both seem to be fine in that case.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"elixir"},"answer":"\nYou should use `is_binary\/1`.\n\n\nStrings in Elixir are represented as binaries. Elixir binaries are sequences of bytes, whereas bitstrings are sequences of bits. While all binaries are bitstrings, not all bitstrings are binaries.\n\n\n`is_bitstring\/1` can return `true` for some bitstrings that cannot be represented as a binary, for example, the single bit:\n\n\n\n```\niex(1)> is_binary(<<1::1>>)\nfalse\niex(2)> is_bitstring(<<1::1>>)\ntrue\n\n```\n\nYou expect strings only. Bitstrings that are not binaries are never desired, so the more specific `is_binary\/1` is the better choice.\n\n\n"}
{"questionId":"310585c972584f639449878072319020","question":"How to search if dictionary value contains certain string with Python\nI have a dictionary with key-value pair. My value contains strings. How can I search if a specific string exists in the dictionary and return the key that correspond to the key that contains the value.\n\n\nLet's say I want to search if the string 'Mary' exists in the dictionary value and get the key that contains it. This is what I tried but obviously it doesn't work that way.\n\n\n\n```\n#Just an example how the dictionary may look like\nmyDict = {'age': ['12'], 'address': ['34 Main Street, 212 First Avenue'],\n          'firstName': ['Alan', 'Mary-Ann'], 'lastName': ['Stone', 'Lee']}\n\n#Checking if string 'Mary' exists in dictionary value\nprint 'Mary' in myDict.values()\n\n```\n\nIs there a better way to do this since I may want to look for a substring of the value stored ('Mary' is a substring of the value 'Mary-Ann').\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"I am a bit late, but another way is to use list comprehension and the `any` function, that takes an iterable and returns `True` whenever one element is `True` :\n\n\n\n```\n# Checking if string 'Mary' exists in the lists of the dictionary values\nprint any(any('Mary' in s for s in subList) for subList in myDict.values())\n\n```\n\nIf you wanna count the number of element that have \"Mary\" in them, you can use `sum()`:\n\n\n\n```\n# Number of sublists containing 'Mary'\nprint sum(any('Mary' in s for s in subList) for subList in myDict.values())\n\n# Number of strings containing 'Mary'\nprint sum(sum('Mary' in s for s in subList) for subList in myDict.values())\n\n```\n\nFrom these methods, we can easily make functions to check which are the keys or values matching.\n\n\nTo get the keys containing 'Mary':\n\n\n\n```\ndef matchingKeys(dictionary, searchString):\n    return [key for key,val in dictionary.items() if any(searchString in s for s in val)]\n\n```\n\nTo get the sublists:\n\n\n\n```\ndef matchingValues(dictionary, searchString):\n    return [val for val in dictionary.values() if any(searchString in s for s in val)]\n\n```\n\nTo get the strings:\n\n\n\n```\ndef matchingValues(dictionary, searchString):\n    return [s for s i for val in dictionary.values() if any(searchString in s for s in val)]\n\n```\n\nTo get both:\n\n\n\n```\ndef matchingElements(dictionary, searchString):\n    return {key:val for key,val in dictionary.items() if any(searchString in s for s in val)}\n\n```\n\nAnd if you want to get only the strings containing \"Mary\", you can do a double list comprehension :\n\n\n\n```\ndef matchingStrings(dictionary, searchString):\n    return [s for val in dictionary.values() for s in val if searchString in s]\n\n```\n\n"}
{"questionId":"0d9341ac0307445a951a559f4b23e034","question":"Unable to resolve service for type 'Microsoft.Extensions.Logging.ILogger' while attempting to activate 'Controller'\nI am trying to implement Application Insights logging.\n\n\nHere is my startup\n\n\nUnder configureservices\n\n\n\n```\nservices.AddApplicationInsightsTelemetry(Configuration[\"ApplicationInsights:InstrumentationKey\"]);\n\n```\n\nHere is my controller\n\n\n\n```\n    private readonly My_DevContext _context;\n    private Task t;\n    private readonly IKipReport _kipReport;\n    private readonly ILogger _logger;\n    public ReportExtractionController(ILogger logger,My_DevContext context, IKipReport kipReport)\n    {\n        _context = context;\n        _kipReport = kipReport;\n        _logger = logger;\n    }\n    \/\/ GET: api\/<RepprtExtractionController>\n    [HttpGet]\n    public async Task<IActionResult> Get()\n    {\n        _logger.LogError(\"Trial\");\n    }\n\n```\n\nWhen trying to call the API its saying **500,internal server error**\n\n\n\n```\nSystem.InvalidOperationException: Unable to resolve service for type 'Microsoft.Extensions.Logging.ILogger' while attempting to activate 'Toolset_API.Controllers.ReportExtractionController'.\n   at Microsoft.Extensions.DependencyInjection.ActivatorUtilities.GetService(IServiceProvider sp, Type type, Type requiredBy, Boolean isDefaultParameterRequired)\n   at lambda_method10(Closure , IServiceProvider , Object[] )\n   at Microsoft.AspNetCore.Mvc.Controllers.ControllerActivatorProvider.<>c__DisplayClass4_0.<CreateActivator>b__0(ControllerContext controllerContext)\n   at Microsoft.AspNetCore.Mvc.Controllers.ControllerFactoryProvider.<>c__DisplayClass5_0.<CreateControllerFactory>g__CreateController|0(ControllerContext controllerContext)\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ControllerActionInvoker.Next(State& next, Scope& scope, Object& state, Boolean& isCompleted)\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ControllerActionInvoker.InvokeInnerFilterAsync()\n--- End of stack trace from previous location ---\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ResourceInvoker.<InvokeNextResourceFilter>g__Awaited|24_0(ResourceInvoker invoker, Task lastTask, State next, Scope scope, Object state, Boolean isCompleted)\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ResourceInvoker.Rethrow(ResourceExecutedContextSealed context)\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ResourceInvoker.Next(State& next, Scope& scope, Object& state, Boolean& isCompleted)\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ResourceInvoker.InvokeFilterPipelineAsync()\n--- End of stack trace from previous location ---\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ResourceInvoker.<InvokeAsync>g__Logged|17_1(ResourceInvoker invoker)\n   at Microsoft.AspNetCore.Routing.EndpointMiddleware.<Invoke>g__AwaitRequestTask|6_0(Endpoint endpoint, Task requestTask, ILogger logger)\n   at Microsoft.AspNetCore.Authorization.AuthorizationMiddleware.Invoke(HttpContext context)\n   at Microsoft.AspNetCore.Authentication.AuthenticationMiddleware.Invoke(HttpContext context)\n   at Swashbuckle.AspNetCore.SwaggerUI.SwaggerUIMiddleware.Invoke(HttpContext httpContext)\n   at Swashbuckle.AspNetCore.Swagger.SwaggerMiddleware.Invoke(HttpContext httpContext, ISwaggerProvider swaggerProvider)\n   at Microsoft.AspNetCore.Diagnostics.DeveloperExceptionPageMiddleware.Invoke(HttpContext context)\n\n```\n\nI am working in .NET 5.0\n\n\nI didnt get what I did wrong.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"Change your code like below:\n\n\n\n```\nprivate readonly ILogger _logger;\npublic ReportExtractionController(ILogger<ReportExtractionController> logger)\n{\n    _logger = logger;\n}\n\n```\n\n"}
{"questionId":"4ef1a549c7a6472fb2b196b44bef3379","question":"Python difference between yaml.load and yaml.safe\\_load\nI am seeing that PyYaml, truncates zero's while loading from yaml file, if one uses:\n`yaml.safe_load(stream)`.\n\n\nIt can be fixed, if one uses `yaml.load(stream, Loader=yaml.BaseLoader)`, but is that advisable?\n\n\nIt works with `yaml.load` and zeros are not truncated.\n\n\nI want to understand that would it be safe to switch to `yaml.load` instead of `yaml.safe_load`?\n\n\nExample:\n\n\nTest yaml content:\n\n\n\n```\n$cat test.yml\nnumber: 5.10\n\n```\n\nCode:\n\n\n\n```\n$python -c 'import yaml, sys; content = yaml.safe_load(sys.stdin); \nprint(content) ' < test.yml\n{'number': 5.1}\n\n```\n\n<< It truncates the 0 at the end. But that is due to floating point value >>\n\n\nwhereas what I want is the exact number as is.\n\n\n\n```\n$python -c 'import yaml, sys; content = yaml.load(sys.stdin, \nLoader=yaml.BaseLoader); print(content) ' < test.yml\n{u'number': u'5.10'}\n\n```\n\nIs that the correct approach to change it to yaml.load ?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"python"},"answer":"`yaml.safe_load(sys.stdin)` just does `yaml.load(sys.stdin, Loader=yaml.SafeLoader)`.\n\n\nThe facilities to execute arbitrary Python code (which makes loading unsafe) are implemented in `yaml.Loader` which is used by default. `yaml.BaseLoader` does not contain them. Therefore, if you use `yaml.BaseLoader`, loading will not execute arbitrary Python code (that is, unless you yourself register custom constructors with `yaml.BaseLoader`).\n\n\n"}
{"questionId":"1ea3377f419d44c98aa0150188182949","question":"generate source based on other assembly classes (c# source generator)\nI want to generate a static class that should have a method depending on other classes in specific **reference assembly**.\n\n\na simplified example:\n\n\n\n```\n\/\/ Generator.csproj\n[Generator]\n   public class MyGenerator : ISourceGenerator\n   {\n      public void Initialize(GeneratorInitializationContext context)\n      {\n          \/\/ Register a factory that can create our custom syntax receiver\n          context.RegisterForSyntaxNotifications(() => new MySyntaxReceiver());\n      }\n\n      public void Execute(GeneratorExecutionContext context)\n      {\n          \/\/ var syntaxReceiver = (MySyntaxReceiver)context.SyntaxReceiver;\n      }\n   }\n\n    private class MySyntaxReceiver : ISyntaxReceiver\n    {\n       ....\n    }\n\n```\n\n\n```\n\/\/ Core.csproj\n\/\/ namespace Core.Entities\nclass Entity1 : IAccessControl {}\nclass Entity2  {}\nclass Entity3 : IAccessControl {}\n\n```\n\n\n```\n\/\/ Persistence.csproj => has a reference to Core project and the Generator\n\/\/ this class should be generated ...\nstatic class GeneratedClass\n{\n   public static void DoSomethingEntity1()\n   public static void DoSomethingEntity3()\n}\n\n```\n\nI want to find the `Entity` classes in the `Core` project and generate a class in the `Persistence` project,\nThe problem is my `Core` project is not accessible and it is already compiled before `Persistence`. should I use reflection or manually read the Core Entities ? or is there a better way to access SyntaxTree in the `Core` project?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"Because the `Core` project is already compiled we can not access SyntaxTree but we can go through the compilation to get the referenced assemblies, then look through those assemblies and find the **symbols**.\n\n\n\n```\npublic void Execute(GeneratorExecutionContext context)\n{\n\/\/ finding Core reference assembly Symbols\n IAssemblySymbol assemblySymbol = \ncontext.Compilation.SourceModule.ReferencedAssemblySymbols.First(q => q.Name == \"Core\");\n\n\/\/ use assembly symbol to get namespace and type symbols\n\/\/ all members in namespace Core.Entities\nvar members = assemblySymbol.GlobalNamespace.\n                             GetNamespaceMembers().First(q => q.Name == \"Core\")                                       \n                            .GetNamespaceMembers().First(q => q.Name == \"Entities\")\n                            .GetTypeMembers().ToList();\n\nvar targets = new HashSet<INamedTypeSymbol>();\n\n\/\/ find classes that implemented IAccessControl\nforeach (var member in members.Where(m => m.AllInterfaces.Any(i => i.Name == \"IAccessControl\")))\n{\n   targets.Add(member); \/\/ Entity1 Entity3\n}\n\n\n\/\/ generate source using targets ...\n\/\/ context.AddSource(\"GeneratedClass\", source);\n}\n\n\n```\n\nhope this example help others.\n\n\n"}
{"questionId":"5c65c1b9e45745c09a68983936bdfdec","question":"How do I get python2.7 and 3.7 both installed in an alpine docker image\nI am trying to figure out a way that I can install both python2.7 and 3.7 in a lightweight alpine docker image.\n\n\nCurrently I am using the \n\n\n\n```\nFROM python:3.7-alpine3.9\n\n```\n\nBase image but would like to know how to get python2 installed as well.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"Use something like:\n\n\n\n```\nRUN apk add --no-cache python2\n\n```\n\nThis will install the latest version of Python 2 as `python2` or `python2.7`. Python 3.7.3 will still be available using `python3`, or simply `python`.\n\n\n"}
{"questionId":"bc43e75dfaa5404abcc2fd8ff6dcdb62","question":"Can volatile variables be read multiple times between sequence points?\nI'm making my own C compiler to try to learn as much details as possible about C. I'm now trying to understand exactly how `volatile` objects work.\n\n\nWhat is confusing is that, every read access in the code must strictly be executed (C11, 6.7.3p7):\n\n\n\n> \n> An object that has volatile-qualified type may be modified in ways unknown to the implementation or have other unknown side effects. Therefore any expression referring to such an object shall be evaluated strictly according to the rules of the abstract machine, as described in 5.1.2.3. Furthermore, at every sequence point the value last stored in the object shall agree with that prescribed by the abstract machine, except as modified by the unknown factors mentioned previously.134) What constitutes an access to an object that has volatile-qualified type is implementation-defined.\n> \n> \n> \n\n\n*Example : in `a = volatile_var - volatile_var;`, the volatile variable must be read twice and thus the compiler can't optimise to `a = 0;`*\n\n\nAt the same time, the order of evaluation between sequence point is undetermined (C11, 6.5p3):\n\n\n\n> \n> The grouping of operators and operands is indicated by the syntax. Except as specified later, side effects and value computations of subexpressions are unsequenced.\n> \n> \n> \n\n\n*Example : in `b = (c + d) - (e + f)` the order in which the additions are evaluated is unspecified as they are unsequenced.*\n\n\nBut evaluations of unsequenced objects where this evaluation creates a side effect (with `volatile` for instance), the behaviour is undefined (C11, 6.5p2):\n\n\n\n> \n> If a side effect on a scalar object is unsequenced relative to either a different side effect on the same scalar object or a value computation using the value of the same scalar object, the behavior is undefined. If there are multiple allowable orderings of the subexpressions of an expression, the behavior is undefined if such an unsequenced side effect occurs in any of the orderings.\n> \n> \n> \n\n\nDoes this mean the expressions like `x = volatile_var - (volatile_var + volatile_var)` is undefined ? Should my compiler throw an warning if this occurs ?\n\n\nI've tried to see what CLANG and GCC do. Neither thow an error nor a warning. The outputed asm shows that the variables are NOT read in the execution order, but left to right instead as show in the asm risc-v asm below :\n\n\n\n```\nconst int volatile thingy = 0;\nint main()\n{\n    int new_thing = thingy - (thingy + thingy);\n    return new_thing;\n}\n\n```\n\n\n```\nmain:\n        lui     a4,%hi(thingy)\n        lw      a0,%lo(thingy)(a4)\n        lw      a5,%lo(thingy)(a4)\n        lw      a4,%lo(thingy)(a4)\n        add     a5,a5,a4\n        sub     a0,a0,a5\n        ret\n\n```\n\nEdit: I am not asking \"Why do compilers accept it\", I am asking \"Is it undefined behavior if we strictly follow the C11 standard\". The standard seems to state that it is undefined behaviour, but I need more precision about it to correctly interpret that\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"Reading the (ISO 9899:2018) standard literally, then it is undefined behavior.\n\n\nC17 5.1.2.3\/2 - definition of side effects:\n\n\n\n> \n> Accessing a `volatile` object, modifying an object, modifying a file, or calling a function that does any of those operations are all *side effects*\n> \n> \n> \n\n\nC17 6.5\/2 - sequencing of operands:\n\n\n\n> \n> If a side effect on a scalar object is unsequenced relative to either a different side effect on the same scalar object or a value computation using the value of the same scalar object, the behavior is undefined. If there are multiple allowable orderings of the subexpressions of an expression, the behavior is undefined if such an unsequenced side effect occurs in any of the orderings.\n> \n> \n> \n\n\nThus when reading the standard literally, `volatile_var - volatile_var` is definitely undefined behavior. Twice in a row UB actually, since both of the quoted sentences apply.\n\n\n\n\n---\n\n\nPlease also note that this text changed quite a bit in C11. Previously C99 said, 6.5\/2:\n\n\n\n> \n> Between the previous and next sequence point an object shall have its stored value modified at most once by the evaluation of an expression. Furthermore, the prior value shall be read only to determine the value to be stored.\n> \n> \n> \n\n\nThat is, the behaviour was previously unspecified in C99 (unspecified order of evaluation) but was made undefined by the changes in C11.\n\n\n\n\n---\n\n\nThat being said, other than re-ordering the evaluation as it pleases, a compiler doesn't really have any reason to do wild and crazy things with this expression since there isn't much that can be optimized, given `volatile`.\n\n\nAs a quality of implementation, mainstream compilers seem to maintain the previous \"merely unspecified\" behavior from C99.\n\n\n"}
{"questionId":"9fc8ff21e16541518c8d672bb60f3e65","question":"updating column with value from another column in postgres table\nI have a postgres table which has 2 columns\n\n\nusername and email\nI have hundreds of rows in the table such as\n\n\n\n```\nusername | email\nusername1 | test@abc.com\nusername2 | test@abc.com\nusername3 | test@abc.com\nusername4 | test@abc.com\n\n```\n\nThe way this was setup all emails are the same, and now I need to make them unique. I am trying to update the email column to be like this\n\n\n\n```\nusername | email\nusername1 | username1_test@abc.com\nusername2 | username2_test@abc.com\nusername3 | username3_test@abc.com\nusername4 | username4_test@abc.com\n\n```\n\nbasically copy over the value from the username column and add it to the email column.\nI tried using the coalesce function but that will replace the value completely and not update it.\n\n\nPlease can you help me understand how to achieve this.\n\n\nThanks\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"sql"},"answer":"An update with basic concatenation should work here:\n\n\n\n```\nUPDATE yourTable\nSET email = username || '_' || email;\n\n```\n\n"}
{"questionId":"ec5a23e42fda4ccc91071642817a2716","question":"floor and ceil with number of decimals\nI need to floor a float number with an specific number of decimals.\n\n\nSo:\n\n\n\n```\n2.1235 with 2 decimals --> 2.12\n2.1276 with 2 decimals --> 2.12  (round would give 2.13 which is not what I need)\n\n```\n\nThe function `np.round` accepts a `decimals` parameter but it appears that the functions `ceil` and `floor` don't accept a number of decimals and always return a number with zero decimals.\n\n\nOf course I can multiply the number by `10^ndecimals`, then apply floor and finally divide by `10^ndecimals`\n\n\n\n```\nnew_value = np.floor(old_value * 10**ndecimals) \/ 10**ndecimals\n\n```\n\nBut I'm wondering if there's a built-in function that does this without having to do the operations.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"Neither Python built-in nor numpy's version of ceil\/floor support precision.\n\n\nOne hint though is to reuse round instead of multiplication + division (should be much faster):\n\n\n\n```\ndef my_ceil(a, precision=0):\n    return np.round(a + 0.5 * 10**(-precision), precision)\n\ndef my_floor(a, precision=0):\n    return np.round(a - 0.5 * 10**(-precision), precision)\n\n```\n\n**UPD:**\nAs pointed out by @aschipfl, for whole values `np.round` will round to the nearest even, which will lead to unexpected results, e.g. `my_ceil(11)` will return 12. Here is an updated solution, free of this problem:\n\n\n\n```\ndef my_ceil(a, precision=0):\n    return np.true_divide(np.ceil(a * 10**precision), 10**precision)\n\ndef my_floor(a, precision=0):\n    return np.true_divide(np.floor(a * 10**precision), 10**precision)\n\n```\n\n"}
{"questionId":"e49c3b459a2d46ab9348b6d96c075794","question":"Go package with multiple files, how to structure\nGo noob, I cannot seem to figure out how to structure my project with packages. What I want is this: \n\n\n- I want to create a package, lets say its called Dart.\n- I have a file called dart.go with package main and the main function in my project directory.\n- I have another file, lets call it functions.go in my project directory with 'package dart' as the first line.\n- I just want to call functions from functions.go in main, but cannot figure out how to name the packages to get it to build.\n- If I put package dart at the top of functions.go it wont build because it finds packages main and dart. I dont want functions.go \nto be part of another package, I just want one package and the ability to split the functions in this package into multiple files.\n- Is this possible in go, or do I have to make multiple packages?\n\n\ndart.go\n\n\n\n```\npackage main \n\nimport (\n  ...\n)  \n\nfunc main () {\n  ...\n  \/\/ call functions declared in functions.go\n  ...\n}\n\n```\n\nfunctions.go\n\n\n\n```\npackage dart  \n\nimport (\n  ...\n)\n\nfunc Function1() {\n  ... \n}\n\nfunc Function2() {\n  ...\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"go"},"answer":"if all you want to do is access functions in a different file, have functions.go also start with `package main` instead of `package dart`. That way, you are working within a single package but with your code divided into multiple files. Make sure they're in the same directory so that they're considered in the same package.\n\n\n"}
{"questionId":"5ed4a0a7f042441a9d22feb7a06617d1","question":"how can I fix this WARNING in Xgboost?\nI have an imbalanced dataset with 53987 rows, 32columns and 8 classes. I'm trying to perform multiclass classification. This is my code and the corresponding output:\n\n\n\n```\nfrom sklearn.metrics import classification_report, accuracy_score\nimport xgboost\nxgb_model = xgboost.XGBClassifier(num_class=7, learning_rate=0.1, num_iterations=1000, max_depth=10, feature_fraction=0.7, \n                              scale_pos_weight=1.5, boosting='gbdt', metric='multiclass')\nhr_pred = xgb_model.fit(x_train, y_train).predict(x_test)\nprint(classification_report(y_test, hr_pred))\n\n\n[10:03:13] WARNING: C:\/Users\/Administrator\/workspace\/xgboost-win64_release_1.3.0\/src\/learner.cc:541: \nParameters: { boosting, feature_fraction, metric, num_iterations, scale_pos_weight } might not be used.\n\nThis may not be accurate due to some parameters are only used in language bindings but\npassed down to XGBoost core.  Or some parameters are not used but slip through this verification. Please open an issue if you find above cases.\n\n[10:03:13] WARNING: C:\/Users\/Administrator\/workspace\/xgboost-win64_release_1.3.0\/src\/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n          precision    recall  f1-score   support\n\n     1.0       0.84      0.92      0.88      8783\n     2.0       0.78      0.80      0.79      4588\n     3.0       0.73      0.59      0.65      2109\n     4.0       1.00      0.33      0.50         3\n     5.0       0.42      0.06      0.11       205\n     6.0       0.60      0.12      0.20       197\n     7.0       0.79      0.44      0.57       143\n     8.0       0.74      0.30      0.42       169\n\naccuracy                           0.81     16197\nmacro avg       0.74      0.45      0.52     16197\nweighted avg       0.80      0.81      0.80     16197\n\n```\n\nand\n\n\n\n```\nmax_depth_list = [3,5,7,9,10,15,20,25,30]\n\nfor max_depth in max_depth_list:\n    xgb_model = xgboost.XGBClassifier(max_depth=max_depth, seed=777)\n    xgb_pred = xgb_model.fit(x_train, y_train).predict(x_test)\n    xgb_f1_score_micro = f1_score(y_test, xgb_pred, average='micro')\n\n    xgb_df = pd.DataFrame({'tree depth':max_depth_list,             \n                            'accuracy':xgb_f1_score_micro})\n    xgb_df\n\nWARNING: C:\/Users\/Administrator\/workspace\/xgboost-win64_release_1.3.0\/src\/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\n```\n\nHow can I fix these warnings?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"If you don't want to change any behavior, just set the `eval_metric='mlogloss'` as the following.\n\n\n\n```\nxgb_model = xgboost.XGBClassifier(num_class=7,\n                                  learning_rate=0.1,\n                                  num_iterations=1000,\n                                  max_depth=10,\n                                  feature_fraction=0.7, \n                                  scale_pos_weight=1.5,\n                                  boosting='gbdt',\n                                  metric='multiclass',\n                                  eval_metric='mlogloss')\n\n```\n\nFrom the warning log, you will know what `eval_metric` algorithm to set to remove the warning. Mostly either `mlogloss` or `logloss`.\n\n\n"}
{"questionId":"9ec6795a399c46a38e2344f0c303fa52","question":"Is there a way to setup a shortcut to \"re-run\" the Delphi LSP instances?\nAs long as the Delphi LSP implementation has its flaws, it would come in handy if we could restart the LSP processes with an easy to reach shortcut. How can such a shortcut be added?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"delphi"},"answer":"Under *Tools - Configure Tools* add a new entry named *Kill LSP* with the following settings:\n\n\n\n```\nProgram: taskkill\nParameters: \/IM DelphiLSP.exe \/F\n\n```\n\nThis will add a new menu entry *Kill LSP* under the *Tools* menu.\n\n\n"}
{"questionId":"ca9747e52ea54a109989a0861411317c","question":"First element of array by condition\nI am looking for an elegant way to get the first (and only the first) element of an array that satisfies a given condition.\n\n\n**Simple example:**\n\n\nInput:\n\n\n\n```\n[\n    ['value' => 100, 'tag' => 'a'],\n    ['value' => 200, 'tag' => 'b'],\n    ['value' => 300, 'tag' => 'a'], \n ]\n\n```\n\nCondition: `$element['value'] > 199`\n\n\nExpected output:\n\n\n\n```\n['value' => 200, 'tag' => 'b']\n\n```\n\nI came up with several solutions myself:\n\n\n1. Iterate over the array, check for the condition and break when found\n2. Use array\\_filter to apply condition and take first value of filtered:\n\n\n\n```\narray_values(\n    array_filter(\n        $input, \n        function($e){\n            return $e['value'] >= 200;\n        }\n    )\n)[0];\n\n```\n\n\nBoth seems a little cumbersome. Does anyone have a cleaner solution? Am i missing a built-in php function? \n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"php"},"answer":"There's **no need** to use all above mentioned functions like `array_filter`. Because `array_filter` **filters** array. And **filtering** is not the same as **find first value**. So, just do this:\n\n\n\n```\nforeach ($array as $key => $value) {\n    if (meetsCondition($value)) {\n        $result = $value;\n        break;\n        \/\/ or: return $value; if in function\n    }\n}\n\n```\n\n`array_filter` will filter **whole** array. So if your required value is first, and array has 100 or more elements, `array_filter` will still check all these elements. So, do you really need 100 iterations instead of 1? The asnwer is clear - **no**.\n\n\n"}
{"questionId":"183254974732458a8e2c969991147f68","question":"Fatal error: cannot execute 'as': execvp: no such file or directory\nWhenever I try to compile c\/cpp files it gives this error:\n\n\n\n```\ngcc: fatal error: cannot execute \u2018as\u2019: execvp: No such file or directory\ncompilation terminated.\n\n```\n\nI have also tried to include full path of file while compiling but same error occured.\nJust to be sure of version mismatch I looked for both gcc and g++ version but both are same,\ngcc\/g++ version: 9.1.0.\n\n\nHow can I fix this?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c"},"answer":"`as` command is from `binutils`. Have you installed this package?\n\n\n"}
{"questionId":"373746cce03a4104ad87adc402765688","question":"C++, function that can take either ifstream or istringstream\nI have a function `do_something` that reads unsigned characters from a stream.\n\n\nThe stream can be created from a file given the file name. Or it can be created from the given string by considering it as data. I would like to reuse the function in both cases.\n\n\nThe code below gives an error in the second case: *\"error C2664: 'do\\_something: cannot convert argument 1 from 'std::basic\\_istringstream' to 'std::basic\\_istream'\"*.\n\n\nWhat is the proper way to do this?\n\n\n\n```\nstatic void do_something(std::basic_istream<unsigned char>& in)\n{\n   in.get();\n}\n\nstatic void string_read(unsigned char* in)\n{\n   std::basic_ifstream<unsigned char> file(std::string(\"filename\"));\n   do_something(file);\n\n   std::basic_istringstream<unsigned char> str(std::basic_string<unsigned char>(in));\n   do_something(str);\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c++"},"answer":"Your code is experiencing something called a vexing parse. The line:\n\n\n\n```\nstd::basic_istringstream<unsigned char> str(std::basic_string<unsigned char>(in));\n\n```\n\nis interpreted as a function declaration. `str` here is a function that returns a `std::istringstream` and takes as its parameter a variable of type `std::string` called `in`. So when you pass it into the function there's an obvious type mismatch.\n\n\nTo change it into a variable declaration you can use curly braces:\n\n\n\n```\nstd::basic_istringstream<unsigned char> str{std::basic_string<unsigned char>(in)};\n\n```\n\n"}
{"questionId":"24f870b620cd4b2291ffc1a6c644adef","question":"Call to undefined method Illuminate\\Routing\\RouteFileRegistrar::get() - Error after upgrading from Laravel 5.7 to 5.8\nI have a running app written on Laravel 5.7. I tried to change the record in `composer.json` to match \"5.8.\\*\" and ran `composer update`. On my local (win10\/WAMP) machine it went fine, but on the staging server (Debian 9\/nginx) the update command changed the vendor contents and failed at the end.\nSince then anything I do with the app on the server I get this error and I can't find any information anywhere.\n\n\n\n```\nCall to undefined method Illuminate\\Routing\\RouteFileRegistrar::get()\n\n```\n\nAnd this is the line that fails:\n\n\n\n```\n$this->get('login', 'Auth\\LoginController@showLoginForm')->name('login');\n\n```\n\nThanks in advance!\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"remove \"$this\" from your routes and use \"Route::\"\n\n\n"}
{"questionId":"7e98773c2f6a44f78ecd096f84439a00","question":"How can I fix Edit cancelled, no changes made in shell\nI run\n\n\n\n```\nkubectl edit deployment\n\n```\n\nto change the version of one of my pods (this commands opens a temp file in my text editor and then I usually edit and close this temp file) and even before I *close* this temp file in my text editor I can see the following note in my bash.\n\n\n\n```\nEdit cancelled, no changes made.\n\n```\n\nIt was OK before I installed *fish* and I tried to switch to *bash* but it doesn't help either.\n\n\nHow can I fix it?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"Things like this are most likely caused by it opening an editor that forks off instead of staying.\n\n\nThat means you'll want to set $EDITOR to an editor that does wait. E.g. `nano`, `vim` or `emacs` should work, and e.g. if you use sublime text you'll have to use `subl -w` to explicitly tell it to wait.\n\n\nIt's not quite clear which shell you're running at the moment. If it's bash, run `export EDITOR=\"subl -w\"`, in fish run `set -gx EDITOR subl -w` (or `\"subl -w\"` if you use fish < 3.0).\n\n\n"}
{"questionId":"a10c1d2f12704e049d235ac44ed4364d","question":"cannot return reference to temporary value\nI am trying to learn Rust, and while doing so I wanted to try converting a struct object to a byte array, but I am running into issues doing so.\n\n\nSo I have this:\n\n\n\n```\nstruct Node<'a> {\n    id: u8,\n    name: &'a str,\n    data: &'a str,\n}\nimpl<'a> Node<'a> {\n    fn new() -> Node<'a> {\n        return Node {\n            id: 1,\n            name: \"superName\",\n            data: \"some random desc2\",\n        };\n    }\n\n    fn to_buffer(&mut self) -> &'a [u8] {\n        let mut size = mem::size_of::<Node>();\n        size = size + self.name.len() * mem::size_of::<char>();\n        size = size + self.data.len() * mem::size_of::<char>();\n        println!(\"size {}\", size);\n        return &[self.id];\n    }\n}\n\n```\n\nbut I am just getting the error \"cannot return reference to temporary value\"\nAnd I am not 100% sure that I understand the error message to begin with... is it because `self.id` is only scoped to this function and would be removed from the stack when it is returned?\n\n\nAnd is there any way around this?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"rust"},"answer":"Anything starting with `&` in Rust is a reference to a value, rather than a value itself. A `&[u8]` is a reference to a value which needs to exist elsewhere.\n\n\nBecause you create the value it's a reference to *within* the function, when the function returns, the value the reference points to no longer exists. (`id` still exists, but the slice containing `id` does not).\n\n\nInstead, you can return an *owned* value, rather than a reference to one:\n\n\n\n```\n   fn to_buffer(&mut self) -> Vec<u8> {\n        let mut size = mem::size_of::<Node>();\n        size = size + self.name.len() * mem::size_of::<char>();\n        size = size + self.data.len() * mem::size_of::<char>();\n        println!(\"size {}\", size);\n        return vec![self.id];\n    }\n\n```\n\nwhich the caller can then take a reference of if that's what they need.\n\n\n"}
{"questionId":"c72de87798504f07aee1aef07195fea0","question":"SwiftUI: Animate changes in List without animating content changes\nI have a simple app in SwiftUI that shows a `List`, and each item is a `VStack` with two `Text` elements:\n\n\n\n```\nvar body: some View {\n    List(elements) { item in\n        NavigationLink(destination: DetailView(item: item)) {\n            VStack(alignment: .leading) {\n                Text(item.name) \n                Text(self.distanceString(for: item.distance))\n            }\n        }\n    }\n    .animation(.default)\n}\n\n```\n\nThe `.animate()` is in there because I want to animate changes to the list when the `elements` array changes. Unfortunately, SwiftUI also animates any changes to content, leading to weird behaviour. For example, the second `Text` in each item updates quite frequently, and an update will now shortly show the label truncated (with `...` at the end) before updating to the new content. \n\n\nSo how can I prevent this weird behaviour when I update the list's content, but keep animations when the elements in the list change? \n\n\nIn case it's relevant, I'm creating a watchOS app.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"swift"},"answer":"The following should disable animations for row internals\n\n\n\n```\nVStack(alignment: .leading) {\n    Text(item.name) \n    Text(self.distanceString(for: item.distance))\n}\n.animation(nil)\n\n```\n\n"}
{"questionId":"ffecaa8b85874ac8b5a70d7140698474","question":"How to get list of Active Directory group names beginning with \"ABC\\_\" in PowerShell?\nI am new to PowerShell and I am trying to get a list of Active Directory items that start with the same naming convention for example I have a number of groups beginning with \"ABC\\_Group1\", \"ABC\\_Group2\", \"ABC\\_Group3\". \n\n\nI know that:\n\n\n\n```\nget-adgroup \"ABC_Group1\"\n\n```\n\nwill list that specific group \n\n\n\n```\n'get-adgroup -filter * | sort name | select Name' \n\n```\n\nwill list all the groups but I don't know how to filter to find just the specific groups starts with \"ABC\\_\" \n\n\nI then want to list it's members.\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"bash\/shell"},"answer":"You can use Wildcard search with Where condition. In the newer PS version, the where clause can be used as `Filter`\n\n\n\n```\nImport-Module ActiveDirectory\n\nGet-ADGroup -Filter {Name -like 'ABC_*'}  -Properties * | select -property SamAccountName,Name,Description,DistinguishedName,CanonicalName,GroupCategory,GroupScope,whenCreated\n\n```\n\nSince the OP asked to get the members of the group as well, here is the piece of code which will help you:\n\n\n\n```\nGet-ADGroup -Filter {Name -like 'ABC_*'} -SearchBase \"DC=YourDC\" | Get-ADGroupMember -Partition \"DC=YourDC\"\n\n```\n\n**OR**\n\n\n\n```\nGet-ADGroup 'Group Name' -Properties Member | Select-Object -ExpandProperty Member\n\n```\n\n**OR use Dot notation:**\n\n\n\n```\n(Get-ADGroup 'Group Name' -Properties Member).Member\n\n```\n\nHope this helps.\n\n\n"}
{"questionId":"3a918368153e41a69172177104c46c77","question":"Docker exec quoting variables\nI'd like to know if there's a way to do this\n\n\nLet's say the `dockerfile` contains this line, that specifies path of an executable\n\n\n`ENV CLI \/usr\/local\/bin\/myprogram`\n\n\nI'd like to be able to call this program using ENV variable name through `exec` command.\n\n\nFor example\n`docker exec -it <my container> 'echo something-${CLI}`\nExpecting \n`something-\/usr\/local\/bin\/myprogram`\n\n\nHowever that returns:\n\n\n`OCI runtime exec failed: exec failed: container_linux.go:348: starting container process caused \"exec: \\\"${CLI} do something\\\": executable file not found in $PATH\": unknown`\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"Ok, I found a way to do it, all you need to do is evaluate command with bash\n\n\n`docker exec -it <container id> bash -c 'echo something-${CLI}'`\n\n\nreturns `something-\/usr\/local\/bin\/myprogram`\n\n\nIf the `CLI` environment variable is not already set in the container, you can also pass it in such as:\n\n\n`docker exec -it -e CLI=\/usr\/local\/bin\/myprogram <container id> bash -c 'echo something-${CLI}'`\n\n\nSee the help file:\n\n\n\n```\n docker exec --help\n\n Usage: docker exec [OPTIONS] CONTAINER COMMAND [ARG...]\n\n Run a command in a running container\n\nOptions:\n-d, --detach               Detached mode: run command in the background\n-e, --env list             Set environment variables\n....\n\n```\n\n"}
{"questionId":"fec8125d854642cfb5875efedf448ba9","question":"How to fix \"Type 'string' is not assignable to type 'never'\"?\nI'm trying to get a new object\/type from the other one, but get a `Type 'string' is not assignable to type 'never'` error\n\n\n\n```\nconst Flags = {\n  ONE: () => 'one',\n  TWO: () => 'two',\n  \/\/ ...\n};\n\ntype createEnumType<T> = {[K in keyof T]: K};\ntype FlagsObject = createEnumType<typeof Flags>;\ntype FlagsKeys = keyof FlagsObject;\n\nconst keys = Object.keys(Flags) as FlagsKeys[];\nlet result = {} as FlagsObject;\nfor (const key of keys) {\n  result[key] = key;\/\/ Type 'string' is not assignable to type 'never'\n}\n\n\/*\nExpected value and type:\nresult = {\n  ONE: \"ONE\";\n  TWO: \"TWO\";\n}\n*\/\n\n```\n\nI set the type to the `result`, so what am I doing wrong? Why is `never` here?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"the problem is that the expression `result[key]` has type `never`, which is a type with no values. Let's see why it happens:\n\n\n- `key` has type `'ONE'|'TWO'`\n- `result` has type `{ 'ONE': 'ONE'; 'TWO': 'TWO' }` hence:\n\t- `result['ONE']` has type `'ONE'`\n\t- `result['TWO']` has type `'TWO'`\n- `result[key]` may sometime be `result['ONE']` and sometimes be `result['TWO']`. If you want to assign something to it you have to choose a value that can be both. No value can be at the same time `'ONE'` and `'TWO'`.\n\n\nTo do what you expect you have to type `key` more strongly but you cannot do that since you have to loop. I suggest you override the typing for `result` inside the loop so that the type is still strong outside:\n\n\n\n```\n(result as Record<typeof key, typeof key>)[key] = key;\n\n```\n\n"}
{"questionId":"444d46e3b72845a18a5b3c01687cbab4","question":"tsc --build + specify tsconfig.json\nI want to run\n\n\n\n```\ntsc --build\n\n```\n\nBut I want to have tsc use a different tsconfig.json file.\n\n\n\n```\nnpx tsc --build --project tsconfig-abc.json\n\n```\n\nBut I get this error\n\n\n\n```\nerror TS5094: Compiler option '--project' may not be used with '--build'.\n\n```\n\nI've tried adding the build option to tsconfig but that doesn't seem to work?\n\n\n\n```\n  \"buildOptions\": {\n    \"build\" : true\n  },\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"The answer is simply to provide the file name at the end - no param name.\n\n\n\n```\ntsc --build tsconfig-simple.json\n\n```\n\n"}
{"questionId":"ceb3aefd679a43b6904c1b3a0288273b","question":"How to fix \"Type '{}' is missing in the following properties...\" error in Typescript?\nI'm new to Typescript and therefore having a problem about it.\nI'm using Ant Design and followed how to use Form in Typescript but with `FunctionComponent`; however, I'm getting an error thrown by Typescript:\n\n\n\n> \n> `TypeScript error: Type '{}' is missing the following properties from type 'Readonly<RcBaseFormProps & Pick<SetupFormProps, \"username\" | \"email\" | \"password\" | \"confirm_password\" | \"first_name\" | \"last_name\">>': username, email, password, confirm_password, and 2 more. TS2740`\n> \n> \n> \n\n\nHere's the code:\n\n\n\n```\nimport React, { useState } from 'react';\nimport { Form, Input, Row, Col } from 'antd';\nimport { FormComponentProps } from 'antd\/lib\/form';\n\n\ninterface SetupFormProps extends FormComponentProps {\n  username: string;\n  email: string;\n  password: string;\n  confirm_password: string;\n  first_name: string;\n  last_name: string;\n}\n\nconst SetupForm: React.FC<SetupFormProps> = ({ form }) => {\n  ...\n  return (\n    <Form id=\"setup-form\" layout=\"vertical\" onSubmit={handleSubmit}>...<\/Form>\n  )\n}\n\nexport default Form.create<SetupFormProps>({ name: 'register' })(SetupForm);\n\n```\n\nand in my other component, I'm accessing it this way:\n\n\n\n```\nimport SetupForm from '.\/form';\n\n<SetupForm \/>\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"All the props in your props interface are required (they can't be undefined)\n\n\n\n```\ninterface SetupFormProps extends FormComponentProps {\n  username: string;\n  email: string;\n  password: string;\n  confirm_password: string;\n  first_name: string;\n  last_name: string;\n}\n\n```\n\nBut you are using your component without specifying the props from the interface \n\n\n\n```\n<SetupForm \/>\n\n```\n\nSo you should either specify the props from the interface (SetupFormProps)\n\n\n\n```\n<SetupForm username=\"myUserName\" ...etc \/>\n\n```\n\nor make the props optional\n\n\n\n```\ninterface SetupFormProps extends FormComponentProps {\n  username?: string;\n  email?: string;\n  password?: string;\n  confirm_password?: string;\n  first_name?: string;\n  last_name?: string;\n}\n\n```\n\n"}
{"questionId":"bf901931f19549c98925e4f0d43b86b7","question":"Update PHP to 7.4 macOS Catalina with brew\nI try to update my PHP version to `7.4` on macOS Catalina with brew.\n\n\nI did `brew install php@7.4`\n\n\nIf I check my version `php -v`, I still see the old version `PHP 7.3.11`?\n\n\nWhat do I have to do?\n\n\n**Update:**\n\n\nAfter `brew doctor` I get:\n\n\n\n> \n> Warning: Homebrew's sbin was not found in your PATH but you have\n> installed formulae that put executables in \/usr\/local\/sbin. Consider\n> setting the PATH for example like so: echo 'export\n> PATH=\"\/usr\/local\/sbin:$PATH\"' >> ~\/.profile\n> \n> \n> \n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"php"},"answer":"try:\n\n\n\n```\nbrew update\nbrew upgrade php\nphp -v\nbrew services start php\n\n```\n\nor\n\n\n\n```\nbrew services restart php\n\n```\n\nif you use apache server:\n\n\n\n```\nsudo apachectl restart\n\n```\n\nif you use ngnix\n\n\n\n```\nsudo nginx -s reload\n\n```\n\n**Edit:**\n\n\n\n```\nbrew unlink php@7.3\nbrew link php@7.4\n\n```\n\n"}
{"questionId":"b94af5998c44404e90284ad68975cca3","question":"How to merge Flask login with a Dash application?\nI have to design a web-app that provides Flask services and Dash services. For example I would like to create a login in Flask, combined with a Dash application. The problem is that I can't bind the flask login with dash. I would need a method like '@require\\_login' that filters access to even Dash services.\nThe code is as follows:\n\n\n\n```\napp_flask = Flask(__name__)\n\napp_flask.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:\/\/\/\/login.db'\napp_flask.config['SECRET_KEY'] = 'thisissecret'\n\ndb = SQLAlchemy(app_flask)\nlogin_manager = LoginManager()\nlogin_manager.init_app(app_flask)\n\nclass User(UserMixin, db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(30), unique=True)\n\n@login_manager.user_loader\ndef load_user(user_id):\n    return User.query.get(int(user_id))\n\n@app_flask.route('\/')\ndef index():\n    user = User.query.filter_by(username='admin').first()\n    login_user(user)\n    return 'You are now logged in!'\n\n@app_flask.route('\/logout')\n@login_required\ndef logout():\n    logout_user()\n    return 'You are now logged out!'\n\n@app_flask.route('\/home')\n@login_required\ndef home():\n    return 'The current FLASK user is ' + current_user.username\n\n# TODO how to add login_required for dash? \napp_dash = Dash(server=app_flask, url_base_pathname='\/dash\/')\napp_dash.layout = html.H1('MY DASH APP')\n\n\nif __name__ == '__main__':\n    app_dash.run_server(debug=True)\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"This line, `app_dash = Dash(server=app_flask, url_base_pathname='\/dash\/')`, creates new `view_functions` in `app_flask` identified by its `url_base_pathname`.\n\n\nYou can debug and inspect the value of `app_flask.view_functions` before and after the creation of `app_dash`.\n\n\nNow that we know which `view_functions` are created by `app_dash`, we can apply `login_required` to them manually.\n\n\n\n```\nfor view_func in app_flask.view_functions:\n    if view_func.startswith(app_dash.url_base_pathname):\n        app_flask.view_functions[view_func] = login_required(app_flask.view_functions[view_func])\n\n```\n\nThe `app\\_dash` endpoints will now be protected.\n"}
{"questionId":"531cf123abf34c9fa90c78918a733b64","question":"What is the difference between ConcurrencyLimit and PrefetchCount?\nWhat is the difference between ConcurrencyLimit and PrefetchCount in masstransit? and what is the optimize configuration for them.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c#"},"answer":"`PrefetchCount` is a broker-level setting. It indicates to RabbitMQ (or Azure Service Bus) how many messages should be pushed to the client application so that they're ready for processing.\n\n\n\n> \n> In addition, if a RabbitMQ consumer has prefetch space available, published messages are immediately written to the consumer, reducing overall message latency. Because of this, having prefetch space available on a consumer can improve overall message throughput.\n> \n> \n> \n\n\n`ConcurrentMessageLimit` is a client-level thing, that indicates the maximum number of messages that will be consumed concurrently. This may be due to resource limits, or to avoid database overloading, etc.\n\n\nIn cases where messages process very quickly, but cannot be processed *too* concurrently, a limit may be set using ConcurrentMessageLimit to avoid overloading the CPU. However, super fast message consumption increases the sensitivity to the time it takes to request more messages from the broker. So a higher prefetch count is recommended for fast message consumers.\n\n\nFor slow consumers, such as those that make external calls, where the consumer duration is more dependent on slow external systems, a higher concurrency limit can increase overall throughput. In this case, a higher prefetch count doesn't add much, but it should at least be as high as the concurrency limit.\n\n\nIf you're scaling out (competing consumer), then it's a tuning exercise to figure out how many instances, concurrent consumers, and prefetched messages make sense.\n\n\nFor example, we have a database consumer, that can run up to 100 concurrent transactions on the SQL server before it starts to block, so we run a concurrency limit of 100 with a prefetch of 110.\n\n\n"}
{"questionId":"eca1d79aa3634a49bb8dbe6c49a39339","question":"Setting X-Frame-Options in PHP\nHow can I set X-Frame-Options in my PHP code so that it will be there in all the web pages from my server. Basically, I am trying to avoid iframe loading of my web app.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"php"},"answer":"Use below in your php file which outputs response to client side.\n\n\n\n```\nheader(\"X-Frame-Options: DENY\");\n\n```\n\nDENY will fully block. You may try SAMEORIGIN option also.\n\n\n\n```\nheader(\"X-Frame-Options: SAMEORIGIN\");\n\n```\n\nIf you are using apache web server, you can directly set in httpd.conf also.\n\n\n\n```\n<Directory \/>\n    ...\n    Header always set X-Frame-Options \"SAMEORIGIN\"\n<\/Directory>\n\n```\n\n"}
{"questionId":"a4a1859fdbba45a995f509ecaa1729ce","question":"Use env variables in Dockerfile\nI have a Dockerfile in a php project where I need to pass a user and a password to download a library during the execution.\n\n\nThe user and password must be hidden in production or in the local .env files. At the moment I'm just trying the local option and the user and password come empty.\n\n\nI have used `\"${USER}\"` and `${USER}`, but not only the login fails, but when I print the variables they come empty. Also I've tried putting the variables hardcoded and it works fine, so the problem is that the variables are not retrieve from the .env file.\n\n\nThe docker-compose starts as follows\n\n\n\n```\nversion: '3'\n\nservices:\n  server:\n    build:\n      context: .\n      dockerfile: docker\/Server\/Dockerfile\n    container_name: \"server\"\n    ports:\n      - 80:80\n      - 8888:8888\n    networks:\n      - network\n    env_file:\n      - .env\n    command: sh \/start-workers.sh\n\n```\n\nAnd the Dockerfile:\n\n\n\n```\nFROM php:7.3-cli-alpine3.10\nRUN apk add --update\n\n#\n# Dependencies\n#\nRUN apk add --no-cache --no-progress \\\n    libzip-dev zip php7-openssl pkgconfig \\\n    php-pear php7-dev openssl-dev bash \\\n    build-base composer\n\n#\n# Enable PHP extensions\n#\nRUN docker-php-ext-install bcmath sockets pcntl\n\n#\n# Server Dependencies\n#\nRUN echo '' | pecl install swoole \\\n    && echo \"extension=swoole.so\" >> \/usr\/local\/etc\/php\/conf.d\/swoole.ini\n\n#\n# installation\n#\nWORKDIR \/var\/www\/service\nCOPY . .\n\nRUN echo \"START\"\n\nRUN echo \"${USER}\"\n\nRUN echo \"${PASSWORD}\"\n\nRUN echo \"END\"\n\n\nRUN composer config http.libraries.com \"${USER}\" \"${PASSWORD}\" --global \\\n    && composer install -n -o --prefer-dist --no-dev --no-progress --no-suggest \\\n    && composer clear-cache \\\n    && mv docker\/Server\/start-workers.sh \/\nEXPOSE 80\n\n```\n\nThe .env starts and ends as follows:\n\n\n\n```\nAPP_ENV=dev\nAPP_SECRET=666666666666666\n.\n.\n.\nUSER=user\nPASSWORD=password\n\n```\n\nAt the moment if I execute `docker-compose up --build` the output as follows\n\n\n\n```\nStep 10\/15 : RUN echo \"START\"\n ---> Running in 329b1707c2ab\nSTART\nRemoving intermediate container 329b1707c2ab\n ---> 362b915ef616\nStep 11\/15 : RUN echo \"${USER}\"\n ---> Running in e052e7ee686a\n\nRemoving intermediate container e052e7ee686a\n ---> 3c9cfd43a4df\nStep 12\/15 : RUN echo \"${PASSWORD}\"\n ---> Running in b568e7b8d9b4\n\nRemoving intermediate container b568e7b8d9b4\n ---> 26a727ba6842\nStep 13\/15 : RUN echo \"END\"\n ---> Running in 726898b3eb42\nEND\n\n```\n\nI'd like the user and the password to be printed, so I know I'm receiving the .env data and I can use it.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"You could use `args` to meet your requirement.\n\n\nAnd one notice here is: you should not use `USER` in `.env` as a keyword, as it will be override by bash's default environment `USER` which will make your dockerfile not get the correct value. \n\n\nA full workable minimal example as follows, FYI:\n\n\n**docker\/Server\/Dockerfile:**\n\n\n\n```\nFROM php:7.3-cli-alpine3.10\n\nARG USER\nARG PASSWORD\n\nRUN echo ${USER}\nRUN echo ${PASSWORD}\n\n```\n\n**.env (NOTE: you had to use USR, not USER here):**\n\n\n\n```\nUSR=user\nPASSWORD=password\n\n```\n\n**docker-compose.yaml:**\n\n\n\n```\nversion: '3'\n\nservices:\n  server:\n    build:\n      context: .\n      dockerfile: docker\/Server\/Dockerfile\n      args:\n         - USER=${USR}\n         - PASSWORD=${PASSWORD}\n\n```\n\n**Execute:**\n\n\n\n```\n$ docker-compose build --no-cache\nBuilding server\nStep 1\/5 : FROM php:7.3-cli-alpine3.10\n ---> 84d7ac5a44d4\nStep 2\/5 : ARG USER\n ---> Running in 86b35f6903e2\nRemoving intermediate container 86b35f6903e2\n ---> ee6a0e84c76a\nStep 3\/5 : ARG PASSWORD\n ---> Running in 92480327a820\nRemoving intermediate container 92480327a820\n ---> 1f886e8f6fbb\nStep 4\/5 : RUN echo ${USER}\n ---> Running in 8c207c7e6080\nuser\nRemoving intermediate container 8c207c7e6080\n ---> cf97b2cc0317\nStep 5\/5 : RUN echo ${PASSWORD}\n ---> Running in 7cbdd909826d\npassword\nRemoving intermediate container 7cbdd909826d\n ---> 6ab7987e080a\nSuccessfully built 6ab7987e080a\nSuccessfully tagged 987_server:latest\n\n```\n\n"}
{"questionId":"2ededb611d6742d889b78d017fd82210","question":"Pandas FutureWarning: Columnar iteration over characters will be deprecated in future releases\nI have an existing solution to split a dataframe with one column into 2 columns.\n\n\n\n```\ndf['A'], df['B'] = df['AB'].str.split(' ', 1).str\n\n```\n\nRecently, I got the following warning `FutureWarning: Columnar iteration over characters will be deprecated in future releases.`\n\n\nHow to fix this warning?\n\n\nI'm using python 3.7\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"That's not entirely correct, plus the trailing `.str` does not make sense. Since `split` with `expand` returns a DataFrame, this is easier:\n\n\n\n```\ndf[['A', 'B']] = df['AB'].str.split(' ', n=1, expand=True)\n\n```\n\n\n\n---\n\n\nYour existing method without `expand` returns a single Series with a list of columns. I'm not sure what version of pandas used to work with your code but AFAIK you'll need to make some tweaks for this to work with pandas (>= 1.0) today. Assignment in this way is tedious but still possible.\n\n\n\n```\ns = df['AB'].str.split(' ', n=1)\ndf['A'], df['B'] = s.str[0], s.str[1]\n\n```\n\nI prefer the `expand` solution as it's a line shorter.\n\n\n"}
{"questionId":"e7fecebd7e694969991417167de1f5bd","question":"git-svn Can't locate SVN\/Core.pm after fresh installation of macOS Catalina 10.15.4\nRecently I reinstalled `macOS Catalina 10.15.4`. After I installed `Command_Line_Tools_11.4.1`, it told me svn is no longer working with Xcode. Then I installed both git and svn by brew, hopefully git-svn would work. Unfortunately got this error message:\n\n\n\n> \n> Can't locate SVN\/Core.pm in @INC (you may need to install the SVN::Core module) (@INC contains: \/usr\/local\/Cellar\/git\/2.26.2\/share\/perl5 \/Applications\/Xcode.app\/Contents\/Developer\/Library\/Perl\/5.18\/darwin-thread-multi-2level \/Library\/Developer\/CommandLineTools\/Library\/Perl\/5.18\/darwin-thread-multi-2level \/Library\/Perl\/5.18\/darwin-thread-multi-2level \/Library\/Perl\/5.18 \/Network\/Library\/Perl\/5.18\/darwin-thread-multi-2level \/Network\/Library\/Perl\/5.18 \/Library\/Perl\/Updates\/5.18.4 \/System\/Library\/Perl\/5.18\/darwin-thread-multi-2level \/System\/Library\/Perl\/5.18 \/System\/Library\/Perl\/Extras\/5.18\/darwin-thread-multi-2level \/System\/Library\/Perl\/Extras\/5.18 .) at \/usr\/local\/Cellar\/git\/2.26.2\/share\/perl5\/Git\/SVN\/Utils.pm line 6.\n> \n> \n> \n\n\nBoth git & svn are the latest version.\n\n\nPlease help me out.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"perl"},"answer":"Finally I figure it out!!!\n\n\nOpen the `git-svn` file (perl script, in my case file location is `\/usr\/local\/opt\/git\/libexec\/git-core\/git-svn`), change the first line `#!\/usr\/bin\/perl` to `#!\/usr\/local\/bin\/perl`\n\n\nAnd it works!\n\n\nI guess system perl doesn't include perl SVN\/Core, but brew perl has it (dependency of brew svn).\n\n\n"}
{"questionId":"0bb7ed5e0d254dc3af01a199d31ace65","question":"AddressSanitizer, What do these terms mean?\nSo I'm using the AddressSanitizer. But it uses some dense terms when describing the problem.\n\n\n\n```\nShadow bytes around the buggy address:\n  0x0c067fff7fb0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x0c067fff7fc0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x0c067fff7fd0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x0c067fff7fe0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x0c067fff7ff0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n=>0x0c067fff8000: fa fa 00 00 00 00[fa]fa 00 00 00 fa fa fa 00 00\n  0x0c067fff8010: 00 fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c067fff8020: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c067fff8030: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c067fff8040: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c067fff8050: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\nShadow byte legend (one shadow byte represents 8 application bytes):\n  Addressable:           00\n  Partially addressable: 01 02 03 04 05 06 07 \n  Heap left redzone:       fa\n  Freed heap region:       fd\n  Stack left redzone:      f1\n  Stack mid redzone:       f2\n  Stack right redzone:     f3\n  Stack after return:      f5\n  Stack use after scope:   f8\n  Global redzone:          f9\n  Global init order:       f6\n  Poisoned by user:        f7\n  Container overflow:      fc\n  Array cookie:            ac\n  Intra object redzone:    bb\n  ASan internal:           fe\n  Left alloca redzone:     ca\n  Right alloca redzone:    cb\n==7320==ABORTING\n\n```\n\nWhat does `Heap left redzone` mean? (and the others but I'm mostly interested in the `fa` as there is one `[fa]` which indicates the problem probably?)\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c"},"answer":"\n> \n> What does Heap left redzone mean?\n> \n> \n> \n\n\nWhen AddressSanitizer heap interposer allocates heap memory in response to something like:\n\n\n\n```\nchar *p = malloc(5);\n\n```\n\nit allocates more memory than you asked for. Let's say it allocates 32 bytes at address `q`. It then would mark the first 16 bytes (region `[q, q+15]`) as inaccessible heap left red zone (`fa`), the next 5 bytes as addressable (`0`), and the next 11 bytes as heap right red zone (`fb`).\n\n\nFinally it would return the `q+16` to the application (assigned to `p`).\n\n\nNow if the application attempts to read or write from `p-1`, `p-2`, ... `p-15`, all such attempts would be detected because they will all land on the left red zone. This is heap underflow.\n\n\nSimilarly, attempts to access `p+5`, `p+6`, ... `p+10` (heap overflow) would be detected because they will all land on the right red zone.\n\n\nWhy would an application ever have heap underflow? Consider this code:\n\n\n\n```\nint idx = get_valid_index(...);  \/\/ return -1 on failure\n...\nif (p[idx] == ...) {   \/\/ BUG: forgot to check idx!=-1\n\n```\n\nThis actually happens more often that you'd think, and appears to have happened to you.\n\n\n"}
{"questionId":"e316813123aa4f01bd5724aea115ee70","question":"How to zoom between two Google map markers in flutter\nI'm using the google\\_maps\\_flutter package and I'm trying to figure a way to zoom the camera between two placed markers with known positions. Any pointers or sample code would be appreciated.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"dart"},"answer":"To zoom between two Lat Lng bounds in google map, you can do as below:\n\n\nFirst of all import below library in pubspec.yaml otherwise with the older version, you might not be able to see \"getVisibleRegion()\" method with google\nmap controller.\n\n\ngoogle\\_maps\\_flutter: ^0.5.12\n\n\n\n```\nimport 'dart:async';\n\nimport 'package:flutter\/material.dart';\nimport 'package:google_maps_flutter\/google_maps_flutter.dart';\n\nvoid main() => runApp(MyApp());\n\nclass MyApp extends StatefulWidget {\n  @override\n  _MyAppState createState() => _MyAppState();\n}\n\nclass _MyAppState extends State<MyApp> {\n\n  Completer<GoogleMapController> _controller = Completer();\n  GoogleMapController mapController;\n  LatLng _lastMapPosition = _center;\n\n  static const LatLng _center = const LatLng(45.521563, -122.677433);\n\n  final Set<Marker> _markers = {};\n\n  void _onMapCreated(GoogleMapController controller) {\n    mapController = controller;\n    _controller.complete(controller);\n\n    LatLng latLng_1 = LatLng(40.416775, -3.70379);\n    LatLng latLng_2 = LatLng(41.385064, 2.173403);\n    LatLngBounds bound = LatLngBounds(southwest: latLng_1, northeast: latLng_2);\n\n    setState(() {\n      _markers.clear();\n      addMarker(latLng_1, \"Madrid\", \"5 Star Rating\");\n      addMarker(latLng_2, \"Barcelona\", \"7 Star Rating\");\n    });\n\n    CameraUpdate u2 = CameraUpdate.newLatLngBounds(bound, 50);\n    this.mapController.animateCamera(u2).then((void v){\n      check(u2,this.mapController);\n    });\n\n  }\n\n  void addMarker(LatLng mLatLng, String mTitle, String mDescription){\n    _markers.add(Marker(\n      \/\/ This marker id can be anything that uniquely identifies each marker.\n      markerId: MarkerId((mTitle + \"_\" + _markers.length.toString()).toString()),\n      position: mLatLng,\n      infoWindow: InfoWindow(\n        title: mTitle,\n        snippet: mDescription,\n      ),\n      icon: BitmapDescriptor.defaultMarker,\n    ));\n  }\n\n  void check(CameraUpdate u, GoogleMapController c) async {\n    c.animateCamera(u);\n    mapController.animateCamera(u);\n    LatLngBounds l1=await c.getVisibleRegion();\n    LatLngBounds l2=await c.getVisibleRegion();\n    print(l1.toString());\n    print(l2.toString());\n    if(l1.southwest.latitude==-90 ||l2.southwest.latitude==-90)\n      check(u, c);\n  }\n\n\n  void _onCameraMove(CameraPosition position) {\n    _lastMapPosition = position.target;\n  }\n\n  @override\n  Widget build(BuildContext context) {\n    return MaterialApp(\n      home: Scaffold(\n        appBar: AppBar(\n          title: Text('Maps Sample App'),\n          backgroundColor: Colors.green[700],\n        ),\n        body: GoogleMap(\n          markers: _markers,\n          onMapCreated: _onMapCreated,\n          initialCameraPosition: CameraPosition(\n            target: _center,\n            zoom: 11.0,\n          ),\n          onCameraMove: _onCameraMove,\n        ),\n      ),\n    );\n  }\n}\n\n```\n\n"}
{"questionId":"5d3a925fbaa74f2b9762377cf9bf2dcb","question":"No overload matches this call. Overload 1 of 2,\nI have `store.js` file\n\n\n\n```\nimport { createStore, combineReducers } from \"redux\";\nimport reducerADD from \"..\/reducer\/reducerADD\"\n\nexport const store = createStore(combineReducers({ reducerADD}));\n\n```\n\nWhen I change format in `store.tsx` I'm getting an Error:\n\n\n\n```\nNo overload matches this call.\n  Overload 1 of 2, '(reducers: ReducersMapObject<{ reducerADD: { lastName: string; firstName: string; password: string; email: string; }[]; }, any>): Reducer<{ reducerADD: { lastName: string; firstName: string; password: string; email: string; }[]; }, AnyAction>', gave the following error.\n    Type '(state: never[] | undefined, action: action) => { lastName: string; firstName: string; password: string; email: string; }[]' is not assignable to type 'Reducer<{ lastName: string; firstName: string; password: string; email: string; }[], any>'.\n      Types of parameters 'state' and 'state' are incompatible.\n        Type '{ lastName: string; firstName: string; password: string; email: string; }[] | undefined' is not assignable to type 'never[] | undefined'.\n          Type '{ lastName: string; firstName: string; password: string; email: string; }[]' is not assignable to type 'never[]'.\n            Type '{ lastName: string; firstName: string; password: string; email: string; }' is not assignable to type 'never'.\n  Overload 2 of 2, '(reducers: ReducersMapObject<{ reducerADD: { lastName: string; firstName: string; password: string; email: string; }[]; }, AnyAction>): Reducer<{ reducerADD: { lastName: string; firstName: string; password: string; email: string; }[]; }, AnyAction>', gave the following error.\n    Type '(state: never[] | undefined, action: action) => { lastName: string; firstName: string; password: string; email: string; }[]' is not assignable to type 'Reducer<{ lastName: string; firstName: string; password: string; email: string; }[], AnyAction>'.\n      Types of parameters 'state' and 'state' are incompatible.\n        Type '{ lastName: string; firstName: string; password: string; email: string; }[] | undefined' is not assignable to type 'never[] | undefined'.\n          Type '{ lastName: string; firstName: string; password: string; email: string; }[]' is not assignable to type 'never[]'.\n\n```\n\nWhat is the reason for this?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"The state needs to be abit more strongly typed. Its trying to cast your initial state from type `any` to type `never`.\n\n\nWhen creating the default state for the data you should define an interface for your state, here's an example for a todo list array:\n\n\n\n```\ninterface IAppState {\n  toDoList: any[];\n}\nconst initialState: IAppState = {\n  toDoList: []\n};\n\nexport default function reducer(state = initialState, action) {}\n\n```\n\n"}
{"questionId":"b13e0f67b1a7467eb5cd7bc8a500c99b","question":"React + TypeScript error: No overload matches this call\nI am receiving a massive error when mapping through an array like:\n\n\n\n```\n\/\/ Home.tsx\n\n  render() {\n    const { inputs, outputs, expectedOutputs } = this.state;\n    return (\n        <ContentContainer>\n          {inputs.map((input, i) => {\n            return (\n              <TestRow\n                key={i}\n                rowNumber={i}\n                xml={inputs[i].xml}\n                desc={inputs[i].desc}\n                output={outputs[i]}\n                expectedOutput={expectedOutputs[i]}\n                handleTextAreaUpdate={this.handleTextAreaUpdate}\n              \/>\n            );\n          })}\n        <\/ContentContainer>\n    );\n\n```\n\n\n```\n\/\/ TestRow.tsx\n\ninterface TestRowProps {\n  xml: string;\n  desc: string;\n  output: string;\n  expectedOutput: string;\n  rowNumber: number;\n}\n\nclass TestRow extends Component<TestRowProps, {}> {\n  textArea: any;\n\n```\n\nthe error is:\n\n\nNo overload matches this call.\n Overload 1 of 2, '(props: Readonly): TestRow', gave the following error.\n\n\n\n```\n    Type '{ key: number; rowNumber: number; xml: string; desc: string; output: never; expectedOutput: string; handleTextAreaUpdate: (e: { target: { value: string; }; }, column: number, rowNumber: number) => void; }' is not assignable to type 'IntrinsicAttributes & IntrinsicClassAttributes<TestRow> & Readonly<TestRowProps> & Readonly<{ children?: ReactNode; }>'.\n      Property 'handleTextAreaUpdate' does not exist on type 'IntrinsicAttributes & IntrinsicClassAttributes<TestRow> & Readonly<TestRowProps> & Readonly<{ children?: ReactNode; }>'.\n  Overload 2 of 2, '(props: TestRowProps, context?: any): TestRow', gave the following error.\n    Type '{ key: number; rowNumber: number; xml: string; desc: string; output: never; expectedOutput: string; handleTextAreaUpdate: (e: { target: { value: string; }; }, column: number, rowNumber: number) => void; }' is not assignable to type 'IntrinsicAttributes & IntrinsicClassAttributes<TestRow> & Readonly<TestRowProps> & Readonly<{ children?: ReactNode; }>'.\n      Property 'handleTextAreaUpdate' does not exist on type 'IntrinsicAttributes & IntrinsicClassAttributes<TestRow> & Readonly<DiagramProps> & Readonly<{ children?: ReactNode; }>'\n\n```\n\n. TS2769\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"in the error message, you can find the problem. `Property 'handleTextAreaUpdate' does not exist on type` means that you should define a property for `handleTextAreaUpdate` in the `TestRowProps` interface.\n\n\n"}
{"questionId":"39fede2e568a4a97b000be2c178cbfb4","question":"How to add a helper method to a typeORM entity?\nI'm trying to add a helper method to one of my Entity classes but I'm getting an error message. My entity:\n\n\n\n```\nimport { Entity, PrimaryColumn, Column } from 'typeorm'\n\n@Entity('accounts')\nclass Account {\n  @PrimaryColumn()\n  username: string\n\n  @Column({ name: 'firstname' })\n  firstName: string\n\n  @Column({ name: 'lastname' })\n  lastName: string\n\n  public fullName() : string {\n    return `${this.firstName} ${this.lastName}`\n  }\n}\n\n```\n\nWhen I try to call `account.fullName()` I get the following error message:\n\n\n`\"account.fullName\" is not a function`\n\n\nWhat am I getting wrong?\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"typescript"},"answer":"Add the `get` keyword and call it using property syntax. \n\n\n\n```\nimport { Entity, PrimaryColumn, Column } from 'typeorm'\n\n@Entity('accounts')\nclass Account {\n  @PrimaryColumn()\n  username: string\n\n  @Column({ name: 'firstname' })\n  firstName: string\n\n  @Column({ name: 'lastname' })\n  lastName: string\n\n  public get fullName() : string {\n    return `${this.firstName} ${this.lastName}`\n  }\n}\n\n```\n\n"}
{"questionId":"52f0768af6424b548f6f2e68e6d4535a","question":"Automatically trim white spaces with Yup and Formik\nI am using a Formik React Form and Yup validation defined on a schema:\n\n\n\n```\nexport const Contact = yup.object<IContact>().shape({\n  contactName: yup\n    .string()\n    .trim('The contact name cannot include leading and trailing spaces')\n    .strict(true)\n    .min(1, 'The contact name needs to be at least 1 char')\n    .max(512, 'The contact name cannot exceed 512 char')\n    .required('The contact Name is required'),\n});\n\n```\n\nIs there a way to have Yup trim white spaces without showing a message? So automatically trimming the spaces when a form is submitted?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"\n> \n> Is there a way to have Yup trim white spaces without showing a message\n> \n> \n> \n\n\nNot in a single transform. The yup transform used by formik is only for validation. \nYou can create a seperate transform to use before passing the data, but its simpler to just `valueToUse = userValue.trim()` yourself. \n\n\n"}
{"questionId":"50c24ec1d53d4ee983ec48c48b0b4070","question":"How to use watch function in typescript language Vuejs?\nI need to change this script from js to ts but i need syntax of watchers\n\n\n\n```\nexport default {\n    props: ['branch_id'],\n    watch: {}\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"First you have to declare your variable, like `myProperty` in this case, and then you have to make a `@Watch('myProperty')` to make the actual watcher for that specific variable.\n\n\n\n```\n@Component\nexport default class App extends Vue {\n  myProperty: string\n\n  @Watch('myProperty')\n  onPropertyChanged(value: string, oldValue: string) {\n    \/\/ Do stuff with the watcher here.\n  }\n}\n\n```\n\n"}
{"questionId":"372d7a01813e4d67bff04155f9220044","question":"Simplest way to return literal JSON using gin gonic\nI am learning Go by building a simple API interface for a web server. I want to return a simple message in JSON, when a default route is hit.\n\n\nSo far, reading online, this is the easiest way to return a literal JSON string, and encode it and send it to the user.\n\n\n\n```\nfunc GetDefault(c *gin.Context)  {\n    jsonData := []byte(`{\"msg\":\"this worked\"}`)\n\n    var v interface{}\n    json.Unmarshal(jsonData, &v)\n    data := v.(map[string]interface{})  \n\n    c.JSON(http.StatusOK,data)\n}\n\n```\n\nIs this the most efficient \/ fastest way to do it?\n\n\nin node.js and express, I would do something like:\n\n\n\n```\nreturn res.status(200).json({\"msg\":\"this worked\"});\n\n```\n\nWhats the best way to do this in Go + Gin?\n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"go"},"answer":"you can use the `gin.H` struct on you response:\n\n\n\n```\nc.JSON(http.StatusOK, gin.H{\"msg\":\"this worked\"})\n\n```\n\n"}
{"questionId":"84945aca28ae45d1a5f43d5bbc1e52d3","question":"Why can a destructor change the state of a constant object?\nI know that it is not allowed to modify the state of a constant object but why can the destructor change this state?\n\n\n\n```\nclass A\n{\npublic:\n    ~A()\n    {\n        i = 2; \/\/?\n    }\n\n    void test() const\n    {\n        \/\/i = 1; \/\/Not allowed\n    }\n\n    int i = 0;\n};\n\nint main()\n{\n    const A o;\n    o.test();\n}\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c++"},"answer":"\n> \n> why can the destructor change this state?\n> \n> \n> \n\n\nBecause it may be useful to be able to change the state of objects in the destructor whether they were const or not.\n\n\nAnd because it doesn't matter for encapsulation. The lifetime has ended, so no-one can see the object in the modified state anyway.\n\n\nAnd because the standard (quoting from the draft) says so:\n\n\n\n> \n> [class.dtor]\n> \n> \n> The address of a destructor shall not be taken.\n> A destructor can be invoked for a const, volatile or const volatile object.\n> const and volatile semantics ([dcl.type.cv]) are not applied on an object under destruction.\n> They stop being in effect when the destructor for the most derived object ([intro.object]) starts.\n> \n> \n> \n\n\n"}
{"questionId":"0a07570c102d4cc9a6cd00acb551f113","question":"Permissions Dialog not showing in Android 11\nIn my app I need access to 4 permissions (which are already declared in the manifest)\n\n\n\n```\n<uses-permission android:name=\"android.permission.ACCESS_COARSE_LOCATION\" \/>\n    <uses-permission android:name=\"android.permission.ACCESS_FINE_LOCATION\" \/>\n    <uses-permission android:name=\"android.permission.ACCESS_BACKGROUND_LOCATION\" \/>\n    <uses-permission android:name=\"android.permission.FOREGROUND_SERVICE\" \/>\n\n```\n\nIn my code I check If I already have permissions, and if NOT:\n\n\n\n```\nif (Build.VERSION.SDK_INT >= Build.VERSION_CODES.M) {\n                requestPermissions(\n                    arrayOf(\n                          Manifest.permission.ACCESS_FINE_LOCATION,\n                          Manifest.permission.ACCESS_COARSE_LOCATION,\n                          Manifest.permission.ACCESS_BACKGROUND_LOCATION), 1)\n            } else {\n                ActivityCompat.requestPermissions(this, arrayOf(\n                  Manifest.permission.ACCESS_FINE_LOCATION,\n                  Manifest.permission.ACCESS_COARSE_LOCATION,\n                  Manifest.permission.ACCESS_BACKGROUND_LOCATION), 1)\n            }\n\n```\n\nThis code works in my emulator, which has a 29 API. But on my own device, the permissions popup won't show. Do you guys know why?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"kotlin"},"answer":"Any Android apps targeting API 30 are now no longer allowed to ask for BACKGROUND\\_PERMISSION at the same time as regular location permission. You have to split it into 2 seperate asks:\n\n\n1. Ask for regular foreground location permission,\nonce granted,\n2. Ask for BACKGROUND\\_LOCATION permission on a new ask\n\n\n"}
{"questionId":"7e55fda66ee84e9183b707efe3ad8285","question":"What is std::false\\_type or std::true\\_type?\nI saw its usage as below \n\n\n\n```\ntemplate <typename T>\nstruct DependentFalse : std::false_type\n{};\n\n```\n\nThen, it is used here \n\n\n\n```\ntemplate <typename T>\nclass RadarSensor\n{\n    static_assert(DependentFalse<T>::value, \"RadarSensor must be created using Identifier template\");\n};\n\n```\n\nI do not have idea what is it used for?\n\n\nWhat is DependentFalse structure?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c++"},"answer":"`std::false_type` is used as a building block in type traits and is defined as `std::integral_constant<bool, false>` (which I will skip over here). It's definition boils down to something like this (simplified):\n\n\n\n```\nstruct false_type {\n    static constexpr bool value = false;\n    constexpr operator bool() const noexcept { return value; }\n    \/\/ There is more here, but it doesn't really matter for your question\n};\n\n```\n\nSimilarly:\n\n\n\n```\nstruct true_type {\n    static constexpr bool value = true;\n    constexpr operator bool() const noexcept { return value; }\n    \/\/ There is more here, but it doesn't really matter for your question\n};\n\n```\n\nIt is used to represent the *values* `false` and `true` as *types*. This is useful in type traits where you let a class template inherit from either `std::false_type` or `std::true_type` for different (partial) specializations, depending on some condition met by the template argument. Doing so allows one to test whether a given type satisfies the condition of the type trait and to obtain a compile time constant *value* indicating the result through access to the static `value` member which is inherited from either `std::false_type` or `std::true_type` or alternative through conversion of an instance of the type trait using the conversion operator.\n\n\nWhat you are showing here is a simple type trait which always (for all `T`) evaluates to `std::false_type`. It is used in `static_asserts` that should always fail when the template they are located in is instantiated. This is necessary, because a `static_assert` that does not dependent on a template parameter is triggered already at the point of definition, rather than the point of instantiation, therefore making every program containing something like `static_assert(false);` ill-formed.\n\n\n"}
{"questionId":"c046d399f82a4910b0634d63bac4a050","question":"How to compare two instants in Java?\n**I want comparison two `Instant`s to see if equals both or greater than**, but i can\u2019t.\ni dont know. how i can compare instants?\n\n\n\n```\n  private Instant expiration;\n\n if(expiration()==Instant.now()||expiration()>Instant.now())\n  {\n     valid+=\"the Expire date is invalid check it . \";\n  }\n\n```\n\ni try this way but i have compile error.\nbut i think cant comparison in this way\nit must change instant to String and compare after it\nbut i dont know how to format to string\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"java"},"answer":"Don't convert them to Strings. Instant, like every other `Comparable` type, has the `compareTo` method. Use it like this:\n\n\n\n```\nif (expiration.compareTo(Instant.now()) >= 0) {\n  ...\n}\n\n```\n\n"}
{"questionId":"6ae950f1e6b74cacad4ef477e94200d7","question":"Xcode 12 issue - Could not find module 'FrameworkName' for target 'arm64-apple-ios-simulator'; found: x86\\_64-apple-ios-simulator, x86\\_64\nAfter updating to Xcode 12 the project gives me this error when building on simulator:\n\n\n\n```\nCould not find module 'FrameworkName' for target 'arm64-apple-ios-simulator'; found: x86_64-apple-ios-simulator, x86_64\n\n```\n\nThe framework is installed with cocoapods.\nIt works with Xcode 11. Building on \"Any iOS Device\" or real iPhone with Xcode 12 also works just fine.\nWhat's different in Xcode 12?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"swift"},"answer":"I fixed this by ensuring the build setting `VALID_ARCHS` (now appearing at the bottom of Build Settings in Xcode 12) contains \"`x86_64`\".\n\n\nThat is:\n\n\n- Before I had: `VALID_ARCHS = arm64, arm64e`\n- After fixing: `VALID_ARCHS = arm64, arm64e, x86_64`\n\n\n(A little counterintuitive, as the error message says it couldn't find the module for `arm64`-apple-ios-simulator, :shrug:)\n\n\n"}
{"questionId":"89951f63e6dc43a3b28eb1a7226635f9","question":"How to filter s3 objects by last modified date with Boto3\nIs there a way to filter s3 objects by last modified date in boto3? I've constructed a large text file list of all the contents in a bucket. Some time has passed and I'd like to list only objects that were added after the last time I looped through the entire bucket. \n\n\nI know I can use the `Marker` property to start from a certain object name,so I could give it the last object I processed in the text file but that does not guarantee a new object wasn't added before that object name. e.g. if the last file in the text file was oak.txt and a new file called apple.txt was added, it would not pick that up. \n\n\n\n```\ns3_resource = boto3.resource('s3')\nclient = boto3.client('s3')\n\ndef list_rasters(bucket):\n\n    bucket = s3_resource.Bucket(bucket)\n\n    for bucket_obj in bucket.objects.filter(Prefix=\"testing_folder\/\"):\n        print bucket_obj.key\n        print bucket_obj.last_modified\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"The following code snippet gets all objects under specific folder and check if the file last modified is created after the time you specify :\n\n\nReplace `YEAR,MONTH, DAY` with your values.\n\n\n\n```\nimport boto3\nimport datetime\n#bucket Name\nbucket_name = 'BUCKET NAME'\n#folder Name\nfolder_name = 'FOLDER NAME'\n#bucket Resource\ns3 = boto3.resource('s3')\nbucket = s3.Bucket(bucket_name)    \ndef lambda_handler(event, context):\n     for file in bucket.objects.filter(Prefix= folder_name):\n         #compare dates \n         if file.last_modified.replace(tzinfo = None) > datetime.datetime(YEAR,MONTH, DAY,tzinfo = None):\n             #print results\n             print('File Name: %s ---- Date: %s' % (file.key,file.last_modified))\n\n```\n\n"}
{"questionId":"39e1ab0c87514b958ccc3e881a2a902c","question":"FirefoxWebdriver No data is available for encoding 437\nI want to create a FirefoxWebdriver but get the following error\n\n\n\n```\n  Message: \n    Initialization method Sma.Ldx.Systemtest.Ui.Tests.IbaTest.TestInitialize\n threw exception. System.TypeInitializationException: The type initializer for \n'System.IO.Compression.ZipStorer' threw an exception. ---> \nSystem.NotSupportedException: No data is available for encoding 437. For \ninformation on defining a custom encoding, see the documentation for the \nEncoding.RegisterProvider method..\n\n```\n\nit is a netstandard2.0 lib and runs on dotnet core 2.2\nCan anybody help?\n\n\nI tried to import System.Text.Encoding.CodePages and try to use System.Text.Encoding.RegisterProvider(System.Text.CodePagesEncodingProvider.Instance) but this is not supported in dotnetcore2.2\n\n\n\n```\nprivate static IWebDriver InitializeFirefoxDriver(bool headless, bool remote, Uri seleniumHubUri, PlatformType platform, string locale, string webDriverPath)\n        {\n            var options = new FirefoxOptions()\n            {\n                Profile = new FirefoxProfile()\n                {\n                    AcceptUntrustedCertificates = true,\n                    AssumeUntrustedCertificateIssuer = true\n                },\n            };\n            options.AddArgument($\"--lang={locale}\");\n            if (headless || remote)\n            {\n                options.AddArgument(\"-headless\");\n            }\n            options.PlatformName = platform.ToString();\n            FirefoxDriverService service = FirefoxDriverService.CreateDefaultService(webDriverPath, \"geckodriver.exe\");\n            service.Start();\n            return remote ? new RemoteWebDriver(seleniumHubUri, options) : new FirefoxDriver(service, options);\n        }\n\n```\n\nI except the Firefox Browser to start but get an encoding error.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"Add NuGet Package `System.Text.Encoding.CodePages`\n\n\nBefore Creating the `FirefoxDriver` object, do this:\n\n\n\n```\n CodePagesEncodingProvider.Instance.GetEncoding(437);\n Encoding.RegisterProvider(CodePagesEncodingProvider.Instance);\n\n```\n\n"}
{"questionId":"faaa648b318245ec83d762f4bff4d7bd","question":"Why there is an unbound variable error warning by IDE in this simple python function\nVery simple question, but I can't find the answer to it. My IDE vs code (pylance) give me the warning\/hint for `a` being possibly unbound. Why is this? How do I fix it?\n\n\n\n```\ndef f():\n    for i in range(4):\n        a = 1\n        print(a)\n\n    return a\n\n```\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"python"},"answer":"Because `range(4)` might be something empty (if you overwrite the built-in `range`), in which case the loop body will never run and `a` will not get assigned. Which is a problem when it's supposed to get returned.\n\n\nMaybe you can tell your IDE to ignore this and not show the warning. Or assign some meaningful default to `a` before the loop.\n\n\n"}
{"questionId":"bd772b8d4dda4f0286aa69dd49e8a525","question":"How do I debug Go dependency packages?\nSay my Go project depends on package `example.com\/foo`. I am using Go `1.12`, so the dependency is automatically pulled in by Go modules. That dependency has some problems\/bugs, I want to add logs in the source code.\n\n\nI can find the source code of the dependency on GitHub but I don't know how to make it into my project for debugging purpose.\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"go"},"answer":"First fetch all the dependency packages into the `vendor` folder.\n\n\n\n```\ngo mod vendor\n\n```\n\nThen, change the source code in that and build your project by specifying to look into `vendor` folder.\n\n\n\n```\ngo build -mod=vendor\n\n```\n\nor\n\n\n\n```\ngo run -mod=vendor myapp.go\n\n```\n\n"}
{"questionId":"ea8564235f2146b2a073a0d751ddcd83","question":"Sklearn transform error: Expected 2D array, got 1D array instead\nI use sklearn to transform data with this code.\n\n\n\n```\nsc = MinMaxScaler()\n\ntest= df['outcome']\ny = sc.fit_transform(test) \n\n```\n\nIt show error like this.\n\n\n\n```\nValueError: Expected 2D array, got 1D array instead:\narray=[ 21000. 36000.  5000. ...  7000.  12000.  11000.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n\n```\n\nHow to fix it?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"If I remember correctly, `MinMaxScalar` can accept a `pandas dataframe` but not a `series`, so just do `test = df[['outcome']]` (dataframe with one column) instead of `test = df['outcome']` (a series).\n\n\n"}
{"questionId":"f55455934e7940d990086455dc1cd213","question":"How do I print STDOUT and get STDIN on the same line in Rust?\nThe macros `println!()` and `print!()` allow you to print strings and variables with and without a trailing newline, respectively. Additionally, the `stdin()` function provides a function to read a line of user input from STDIN (`stdin().read_line(&mut string)`).\n\n\nIt should be safe to assume that if the `print` macro and the `read_line` function were used consecutively, you should be able to write output and get input on the same line. However, the segments are executed in reverse order when this happens (STDIN is read first, then the statement is printed).\n\n\nHere is an example of what I am trying to accomplish:\n\n\n\n```\nuse std::io;\n\nfn main() {\n    let mut input = String::new();\n    print!(\"Enter a string >> \");\n    io::stdin().read_line(&mut input).expect(\"Error reading from STDIN\");\n}\n\n```\n\nThe desired output would be (`STDIN` represents the point where the user is asked for input, it is not actually printed):\n\n\n\n```\nEnter a string >> STDIN\n\n```\n\nThe actual output is:\n\n\n\n```\nSTDIN\nEnter a string >> \n\n```\n\nOn the other hand, the `println` macro does not reverse the order, although there is still the issue of the trailing newline:\n\n\n\n```\nEnter a string >> \nSTDIN\n\n```\n\nIn Python (3.x), this can be accomplished with a single line, because the `input` function allows for a string argument that precedes the STDIN prompt: `variable = input(\"Output string\")`\n\n\nI separated the task into the `print` macro and the `read_line` function after failing to find a solution in the Rust documentation that would allow something similar to the Python example.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"rust"},"answer":"`stdout` gets flushed on newlines. Since your `print!` statement does not contain nor end in a newline it will not get flushed. You need to do it manually using `std::io::stdout().flush()`\n\n\nFor example\n\n\n\n```\nuse std::io::{self, Write};\n\nfn main() {\n    let mut input = String::new();\n    print!(\"Enter a string >> \");\n    let _ = io::stdout().flush();\n    io::stdin().read_line(&mut input).expect(\"Error reading from STDIN\");\n}\n\n```\n\n"}
{"questionId":"6d29497607734f94b9c4153c20c44384","question":"Consecutive arrays\nI'm curious about a sentence in the C18 standard:\n\n\n\n> \n> Two pointers compare equal if and only if both are null pointers, both\n> are pointers to the same object (including a pointer to an object and\n> a subobject at its beginning) or function, both are pointers to one\n> past the last element of the same array object, or one is a pointer to\n> one past the end of one array object and the other is a pointer to the\n> start of a **different array object** that happens to **immediately\n> follow the first** array object in the address space. \u00a7 6.5.9 6\n> \n> \n> \n\n\nWhy does the object following the array have to be necessarily another array? Couldn't simply be an object of the same type as the array base type (like an `int` immediately following an `int[]`)?\n\n\nNo wonder I've tried this code:\n\n\n\n```\n#include <stdio.h>\n\nstruct test { int arr[10]; int i; };\n\nint main() {\n    struct test t;\n    int *p, *q;\n    p = t.arr + 10;\n    q = &t.i;\n    if (p == q)\n        printf(\"Equal pointers.\");\n    return 0;\n}\n\n```\n\nAnd it yields equal pointers. Is this behaviour not guaranteed at all, just an implementation-defined coincidence?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"\n> \n> OP: Why the object following the array has to be necessarily another array?\n> \n> \n> \n\n\nIt does not. \"... start of a different *array* ...\" is a simplification. The next spec is:\n\n\n\n> \n> For the purposes of these operators, a pointer to an object that is not an element of an array behaves the same as a pointer to the first element of an array of length one with the type of the object as its element type. C17dr \u00a7 6.5.9 7\n> \n> \n> \n\n\n\n\n---\n\n\n\n> \n> OP: Couldn't simply be an object which type was the same as the array base type (like an `int` immediately following an `int[]`)?\n> \n> \n> \n\n\nYes.\n\n\n"}
{"questionId":"c1c6a802089f48d1a2f1a7dc22d72b7e","question":"Given the alias, get the HostName from ssh config\nI have an SSH config file like:\n\n\n\n```\nHost myAlias\n  HostName the.actual.host.name.com\n\n```\n\nIs it possible to **resolve the hostname from the alias**, from the shell and without connecting to the host? I'm aiming for something like:\n\n\n\n```\n$ <something> myAlias\nthe.actual.host.name.com\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"While it is possible to parse the ~\/.ssh\/config with a simple `awk` script, it might not work for arbitrary config file, which may have various blocks, etc. Consider instead using the 'ssh -G', which will dump the parameters of an ssh session, then extract the hostname attribute\n\n\n\n```\nssh -G myAlias | awk '$1 == \"hostname\" { print $2 }'\n\n```\n\nNote that this has the advantage of supporting all ssh configuration sources (local config, command line, global config, environment variables, etc).\n\n\nJust for completeness, quick and dirty `awk` solution\n\n\n\n```\nawk -v H=\"myAlias\" '\ntolower($1) == \"host\" { m=$2 == H }\ntolower($1) == \"hostname\" && m{ print $2 }\n' ~\/.ssh\/config\n\n```\n\n"}
{"questionId":"368ecf27244942819c0dd5af8b3a64bc","question":"Initializer list inside std::pair\nThis code:\n\n\n\n```\n#include <iostream>\n#include <string>\n\nstd::pair<std::initializer_list<std::string>, int> groups{ { \"A\", \"B\" }, 0 };\n\nint main()\n{\n    for (const auto& i : groups.first)\n    {\n        std::cout << i << '\\n';\n    }\n    return 0;\n}\n\n```\n\ncompiles but returns segfault. Why?\n\n\nTested on gcc 8.3.0 and on online compilers.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c++"},"answer":"`std::initializer_list` is not meant to be stored, it is just meant for ... well initialization. Internally it just stores a pointer to the first element and the size. In your code the `std::string` objects are temporaries and the `initializer_list` neither takes ownership of them, neither extends their life, neither copies them (because it's not a container) so they go out of scope immediately after creation, but your `initializer_list` still holds a pointer to them. That is why you get segmentation fault.\n\n\nFor storing you should use a container, like `std::vector` or `std::array`.\n\n\n"}
{"questionId":"5d5d9a3ad3c64cfb919069934dc45211","question":"How can I define a concept that is satisfied by an arbitrary std::vector?\nI would like to have a `concept` requiring an arbitrary vector as the return type:\n\n\n\n```\ntemplate<typename T>\nconcept HasVector = requires (T t) {\n    { T.vec() } -> std::same_as<std::vector<int>>; \/\/works\n    { T.vec() } -> std::same_as<std::vector<foo>>; \/\/want to put something arbitrary in here\n}\n\n```\n\nSuch that we would have something like the following:\n\n\n\n```\nclass A {\nstd::vector<int> vec() { \/* ... *\/}\n}\n\nclass B {\nstd::vector<double> vec() { \/* ... *\/}\n}\n\nstatic_assert(HasVector<A>);\nstatic_assert(HasVector<B>);\n\n```\n\nMoreover, it would be even nicer to require a vector as the return type whose value type satisfies some other concept, i.e.\n\n\n\n```\n\ntemplate<typename T>\nconcept Arithmetic = \/\/ as in the standard\n\ntemplate<typename T>\nconcept HasArithmeticVector = requires (T t ) {\n    { T. vec() } -> std::same_as<std::vector<Arithmetic>>;\n\n```\n\nIs there such a way to put this in names of concepts?\n\n\n","questionMetadata":{"type":"implementation","level":"advanced","tag":"c++"},"answer":"We start by writing a variable template to check if a type specializes a template:\n\n\n\n```\ntemplate <typename T, template <typename...> class Z>\ninline constexpr bool is_specialization_of = false;\n\ntemplate <template <typename...> class Z, class... Args>\ninline constexpr bool is_specialization_of<Z<Args...>, Z> = true;\n\n```\n\nWhich we can turn into a concept:\n\n\n\n```\ntemplate <typename T, template <typename...> class Z>\nconcept Specializes = is_specialization_of<T, Z>;\n\n```\n\nWhich we can then use to implement another concept:\n\n\n\n```\ntemplate<typename T>\nconcept HasVector = requires (T t) {\n    { t.vec() } -> Specializes<std::vector>;\n};\n\n```\n\nIf you want to then do further checking, that's just adding more requirements.\n\n\n\n```\ntemplate<typename T>\nconcept HasVector = requires (T t) {\n    { t.vec() } -> Specializes<std::vector>;\n\n    \/\/ or something along these lines\n    requires Arithmetic<decay_t<decltype(t.vec()[0])>>;\n    requires Arithmetic<range_value_t<decltype(t.vec())>>;\n    \/\/ etc.\n};\n\n```\n\n"}
{"questionId":"53fcea3a60594422b18eac61274845f5","question":"On Windows what is the difference between Git Bash vs Windows Power Shell vs Command prompt\nI am a Mac guy who is used to Mac's Terminal. Now I am using Windows.\n\n\n- Whats the diff between those CLI options?\n- When should I use one over the other?\n- Are there more CLI options that I should consider?\n- What CLI would you use if you were a Mac person trying to adapt to Windows?\n\n\nThe reason I am trying to use Windows, is that I want to ensure the CLI of my Docker projects work for Windows users, that I can write files coming from my container to Windows and ensure my README files have instructions for Windows users. And basically test everything I do on Windows too, like Python.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"bash\/shell"},"answer":"Git bash is `bash`, which is IIRC also the default shell on MacOS. It is not the default shell on Windows, although several implementations exist (CygWin, MinGW, ...).  \n\nGit is bundled with a number of POSIX (UNIX\/Linux\/etc.) utilities in addition to `bash`; in order to avoid \"collisions\" with similarly named Windows commands, the most common installation option is to install `bash` in such a way that the other POSIX commands are only available when running `bash`. The Git installer will create a shortcut to launch this \"private\" version of `bash`, hence \"git bash\".\n\n\nThe Windows command prompt runs the default Windows shell, CMD.EXE, which is a derivative of the old MS-DOS command shell, COMMAND.COM. It is much less capable than most POSIX shells; for example, it did not until relatively recently support an if\/then\/else construct, and it does not support shell functions or aliases (although there are some workarounds for these limitations).\n\n\nPowerShell is more of a scripting environment. I'd compare it to Perl on UNIX\/Linux systems -- much more powerful than the standard shell, but not necessarily something I'd want to use at the command line.  \n\nOne thing to be aware of is that some of the nicer PowerShell features may require you to update your version of PowerShell -- the version bundled with Windows is typically a few years old. And updating PowerShell usually requires admin privilege; depending on the version, you may also need to update the .NET framework.\n\n\n\n\n---\n\n\nIf I were a Mac person trying to adapt to Windows ... it depends. In the short term it would be easier to use something familiar like `bash`. But long term, you -- and more importantly, your potential users -- may not want to be dependent on a third party tool, especially since for Windows users that will typically present an additional learning curve.\n\n\nAs to which to use when ... it really depends on what you're trying to accomplish -- both in terms of technical functionality and the interface you want to present to your users. As noted above, I'd consider PowerShell more appropriate for scripting than the CLI, unless you just need to run a cmdlet (either a built-in or one you've created yourself).\n\n\n"}
{"questionId":"e7a8bd1b7b254f6888956906e16b46f8","question":"Print in bytes with \"du\"\nI am using `du -sh` to see the size of directories. If I check a 1KiB directory, I will see:\n\n\n\n```\n1.0K    .\n\n```\n\nHowever, I want the output in bytes, and *only* the bytecount.\n\n\nFor example:\n\n\n\n```\n$ du -sh .\n1024\n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"bash\/shell"},"answer":"To get size in bytes you should use command on this way:\n\n\n\n```\ndu -sb\n\n```\n\n(this `b` mean bytes)\n\n\nfor the `du` which do not work properly with `-b` you can use\n\n\n\n```\ndu -s --block-size=1\n\n```\n\n"}
{"questionId":"b18f78e95be24c6aba6c7ac71809ffb3","question":"TS: how to get constant Array element type\nI'm using TS 3.4.5 with the const assertion. How can I retrieve the type of the elements of the declared constant array variable?\n\n\n\n```\nexport type GetArrayElementType<T extends Array<any>> = T extends (infer U)[] ? U : never;\n\nexport const MyConstArray = [\n  'item1',\n  'item2',\n  'item3',\n] as const;\n\nexport type MyConstArrayItem = GetArrayElementType<typeof MyConstArray>;\n\n```\n\nI'd like to have as output:\n\n\n\n```\nexport type MyConstArrayItem = \"item1\" | \"item2\" | \"item3\"\n\n```\n\nI'm not totally sure how to extract the type information of the items because due to the const assertion, my array is not an array type anymore but is a constant tuple, so `GetArrayElementType` can't be applied on it.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"If you want to use a conditional type you have to keep in mind that `as const` generates readonly arrays. So this should work as you expect it to:\n\n\n\n```\nexport type GetArrayElementType<T extends readonly any[]> = T extends readonly (infer U)[] ? U : never;\n\nexport const MyConstArray = [\n  'item1',\n  'item2',\n  'item3',\n  'item4',\n] as const;\n\nexport type MyConstArrayItem = GetArrayElementType<typeof MyConstArray>;\n\n```\n\nBut the simpler solution is to not use a conditional type. Type index queries work better here:\n\n\n\n```\nexport const MyConstArray = [\n  'item1',\n  'item2',\n  'item3',\n  'item4',\n] as const;\n\nexport type MyConstArrayItem = typeof MyConstArray[number];\n\n```\n\n"}
{"questionId":"aadc6177584449b79d49482c1b309a79","question":"What's the difference between DetailedHTMLProps and HTMLAttributes?\nLet's say I have a component interface that should extend the interface of a standard `<div>` element. What's the difference in writing this:\n\n\n\n```\ninterface ComponentProps extends React.DetailedHTMLProps<React.HTMLAttributes<HTMLDivElement>, HTMLDivElement> { ... }\n\n```\n\nversus this:\n\n\n\n```\ninterface ComponentProps extends React.HTMLAttributes<HTMLDivElement> { ... }\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"typescript"},"answer":"lets create `ComponentProps` types:\n\n\n\n```\ninterface ComponentProps1 extends React.DetailedHTMLProps<React.HTMLAttributes<HTMLDivElement>, HTMLDivElement> {}\n\ninterface ComponentProps2 extends React.HTMLAttributes<HTMLDivElement> {}\n\n```\n\nthen we can check the difference between them quite easily:\n\n\n\n```\ntype Dif = Omit<ComponentProps1, keyof ComponentProps2>;\n\n```\n\nthe `Dif` type is:\n\n\n\n```\ntype Dif = {\n    ref?: LegacyRef<HTMLDivElement>;\n    key?: string | number;\n}\n\n```\n\n"}
{"questionId":"e3794f0d30fc41a3a16e7994916173fb","question":"what is the use and when to use @classmethod in python?\nI have never used @classmethod and I do not think of any examples to use it, I know how it works but I do not know when it's time to use it for example\n\n\n\n```\nclass Example:\n    def __init__(self,param1,param2):\n        self.param1 = param1\n        self.param2 = param2\n    @classmethod\n    def my_method(cls,param1,param2):\n        return cls(param1,param2)\n\nexample = Example.my_method(1,2)\nprint(example)\n\n```\n\noutput:\n\n\n\n```\n<__main__.Example object at 0x02EC57D0>\n\n```\n\nBut why not do this?\n\n\n\n```\nclass Example:\n    def __init__(self,param1,param2):\n        self.param1 = param1\n        self.param2 = param2\n\n    def my_method(self,param1,param2):\n        return Example(param1,param2)\n\nexample = Example(1,2)\nmethod = example.my_method(3,4)\nprint(method)\n\n```\n\noutput:\n\n\n\n```\n<__main__.Example object at 0x02EC57D0>\n\n```\n\nIt's the same result but it does not come to mind when I could use classmethod\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"python"},"answer":"There are 3 kinds of methods in python:\n\n\n- Instance method\n- Class method\n- Static Method\n\n\n\n```\nclass Person():\n    species='homo_sapiens' # This is class variable\n    def __init__(self, name, age):\n        self.name = name # This is instance variable\n        self.age = age\n\n    def show(self):\n        print('Name: {}, age: {}.'.format(self.name, date.today().year - self.age))\n\n    @classmethod\n    def create_with_birth_year(cls, name, birth_year):\n        return cls(name, date.today().year - birth_year)\n\n    @classmethod\n    def print_species(cls):\n        print('species: {}'.format(cls.species))\n\n    @staticmethod\n    def get_birth_year(age):\n        return date.today().year - age\n\n\nclass Teacher(Person):\n    pass\n\n\n```\n\n1) Instance method (*show*) need an instance and must use *self* as the first parameter. It can access the instance through *self* and influence the state of an instance.\n\n\n2) Class method (*create\\_with\\_birth\\_year* and *print\\_species*) need no instance and use *cls* to access the class and influence the state of a class. We can use *@classmethod* to make a factory, such as:\n\n\n\n```\nnavy = Person.create_with_birth_year('Navy Cheng', 1989)\nnavy.show()\n\n```\n\nand **this factory can be inherited**:\n\n\n\n```\nzhang = Teacher.create_with_birth_year('zhang', 1980)\nprint(type(zhang))\n\n```\n\nand class method can be used access class variable:\n\n\n\n```\nPerson.print_species()\n\n```\n\n3) Static Method (*get\\_birth\\_year*) need no special parameter(*self* or *cls*) and will change any state of a class or instance. It can privde some helper function about a class.\n\n\n"}
{"questionId":"20ac5e6263fe4e3bb77a054f1eb50c05","question":"Dependency injection for generic interface with two type arguments\nSo I am trying to inject a generic repository that receives as generic types both the entity type and the type of key the entity uses.\n\n\nThe declaration looks something like this:\n\n\n\n```\npublic class GenericRepository<KeyType, T> : BaseRepository<T, NpgsqlConnection>, IGenericRepository<KeyType, T> \n        where T : class\n        where KeyType : struct\n\n```\n\nSo I try to inject them like this:\n\n\n\n```\nservices.AddTransient(typeof(IGenericRepository<>), typeof(GenericRepository<>));\n\n```\n\nThat works for when there is only one generic type, but not for two. I get the following error:\n\n\n\n> \n> Using the generic type 'GenericRepository<KeyType, T>' requires 2 type arguments\n> \n> \n> \n\n\nDoes anyone have a clue how to solve this?\n\n\nI know I could make classes for each one, but I'd like to inject it like this:\n\n\n\n```\npublic class RestaurantTypesService : IRestaurantTypesService\n{\n    private readonly IGenericRepository<long, RestaurantType> _restaurantTypeRepository;\n\n    public RestaurantTypesService(IGenericRepository<long, RestaurantType> repository)\n    {\n        _restaurantTypeRepository = repository;\n    }\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"This is known as an *unbound type*.\n\n\nMultiple *Unbound Generic Type* parameters are denoted by the commas.\n\n\n- 1 - `SomeType<>`\n- 2 - `SomeType<,>`\n- N - `SomeType<,(n-1)>`\n\n\nBasically, it should be as simple as\n\n\n\n```\nservice.AddTransient(typeof(IGenericRepository<,>), typeof(GenericRepository<,>));\n\n```\n\n\n\n---\n\n\n**The worlds most contrived example**\n\n\n\n```\npublic interface IGenericRepository<T, T2> { }\n\npublic class GenericRepository<T, T2> : IGenericRepository<T, T2> { }\n\npublic class Bob\n{\n   private IGenericRepository<int, int> _something;\n\n   public Bob(IGenericRepository<int,int> something)\n   {\n      _something = something;\n      Console.WriteLine(something.GetType().Name);\n   }\n}\n\n...\n\nvar service = new ServiceCollection();\n\nservice.AddTransient(typeof(IGenericRepository<,>), typeof(GenericRepository<,>));\nservice.AddTransient<Bob>();\n\nvar provider = service.BuildServiceProvider();\nvar sdfg=provider.GetService(typeof(Bob));\n\n```\n\n"}
{"questionId":"9b7e868109b442d6b7881a6f40f9ce7f","question":"Ecto order\\_by in preload\nSo I'm having trouble using `order_by` in my query with `preload` function.\n\n\nUsually I use `order_by` like in this `list_member` function\n\n\n**list\\_member**\n\n\n\n```\ndef list_members() do\nquery =\n  from(\n    p in Member,\n    select: p,\n    order_by: [desc: p.inserted_at], # descending order\n    preload: [:avatar]\n  )\n\nRepo.all(query)\nend\n\n```\n\nBut Here in my `get_member_2` function I don't know where to put the `order_by`\n\n\n\n```\ndef get_member_2!(id) do\nquery =\n  from(\n    p in Member,\n    where: p.id == ^id,\n    select: p,\n    preload: [:avatar],\n    preload: [:activities] # How to order_by in here\n  )\n\nRepo.one!(query)\nend\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"elixir"},"answer":"you have to create a subquery and order there\n\n\n\n```\ndef get_member_2!(id) do\n  query =\n    from(\n      p in Member,\n      where: p.id == ^id,\n      select: p,\n      preload: [\n        :avatar,\n        activities:\n          ^from(\n            a in Activity,\n            order_by: [desc: a.inserted_at]\n          )\n      ]\n    )\n\n  Repo.one!(query)\nend\n\n```\n\nThe matching of member and activity is done automatically\n\n\n"}
{"questionId":"73e5eaf1afb641d7a80dabbf42af062a","question":"How to download an excel file in Spring RestController\nI am using Apache POI to generate **.xlsx** file.\n\n\nI would like to return that file from Spring controller. Here's what I've done so far:\n\n\nController:\n\n\n\n```\n@RequestMapping(method = RequestMethod.GET)\n    public HttpEntity<byte[]> createExcelWithTaskConfigurations(HttpServletResponse response) throws IOException {\n        byte[] excelContent = excelService.createExcel();\n\n        HttpHeaders header = new HttpHeaders();\n        header.setContentType(new MediaType(\"application\", \"vnd.openxmlformats-officedocument.spreadsheetml.sheet\"));\n        header.set(HttpHeaders.CONTENT_DISPOSITION, \"attachment; filename=my_file.xls\");\n        header.setContentLength(excelContent.length);\n\n        return new HttpEntity<>(excelContent, header);\n    }\n\n```\n\nIs it possible to return actual excel file from rest controller so user can download it to his computer ? As for now controller returning byte[] but I would like to return it actual file. How can I achieve that ?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"You can use `ByteArrayResource` to download as a file. Below is the modified code snippet of yours\n\n\n\n```\n    @GetMapping(value=\"\/downloadTemplate\")\n    public HttpEntity<ByteArrayResource> createExcelWithTaskConfigurations() throws IOException {\n        byte[] excelContent = excelService.createExcel();\n\n        HttpHeaders header = new HttpHeaders();\n        header.setContentType(new MediaType(\"application\", \"force-download\"));\n        header.set(HttpHeaders.CONTENT_DISPOSITION, \"attachment; filename=my_file.xlsx\");\n\n\n        return new HttpEntity<>(new ByteArrayResource(excelContent), header);\n    }\n\n```\n\nIf you are trying to generate excel using apache poi, please find the code snippet below\n\n\n\n```\n    @GetMapping(value=\"\/downloadTemplate\")\n    public ResponseEntity<ByteArrayResource> downloadTemplate() throws Exception {\n        try {\n            ByteArrayOutputStream stream = new ByteArrayOutputStream();\n            XSSFWorkbook workbook = createWorkBook(); \/\/ creates the workbook\n            HttpHeaders header = new HttpHeaders();\n            header.setContentType(new MediaType(\"application\", \"force-download\"));\n            header.set(HttpHeaders.CONTENT_DISPOSITION, \"attachment; filename=ProductTemplate.xlsx\");\n            workbook.write(stream);\n            workbook.close();\n            return new ResponseEntity<>(new ByteArrayResource(stream.toByteArray()),\n                    header, HttpStatus.CREATED);\n        } catch (Exception e) {\n            log.error(e.getMessage());\n            return new ResponseEntity<>(HttpStatus.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n```\n\n"}
{"questionId":"959cb3f397694d8990567b28431df3c1","question":"How do I read a .env file from a .ps1 script?\nI have a `.env` file like this one:\n\n\n\n```\nTESTCASE_GROUP_SIZE=25\n. . .\n\n```\n\nAnd I want to get its value (read it) into a `.ps1` script.\nHow can I do it?\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"bash\/shell"},"answer":"\n```\nget-content test.env | foreach {\n    $name, $value = $_.split('=')\n    set-content env:\\$name $value\n}\n\n```\n\nassuming you mean \"set one environment variable per line in the file\".\n\n\n"}
{"questionId":"6d898b9d2c904ca693e11f780bc75070","question":"Why do we need to use the builder design pattern when we can do the same thing with setters?\n\n```\npublic class Employee {\n    private String name;\n    private String address;\n    private int id;\n\n    public Employee() {\n        \/\/ TODO Auto-generated constructor stub\n    }\n\n    @Override\n    public String toString() {\n        return \"Employee [name=\" + name + \", address=\" + address + \", id=\" + id + \"]\";\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public String getAddress() {\n        return address;\n    }\n\n    public void setAddress(String address) {\n        this.address = address;\n    }\n\n    public int getId() {\n        return id;\n    }\n\n    public void setId(int id) {\n        this.id = id;\n    }\n\n}\n\npublic class Main { \n    public static void main(String[] args) {\n        Employee e = new Employee();\n        e.setName(\"Priyanka\");\n        Employee e1 = new Employee();\n        e1.setName(\"Rahul\");\n        e1.setAddress(\"Delhi\");\n        System.out.println(\"Value of e :\"+ e);\n        System.out.println(\"Value of e1:\"+ e1);\n    }\n}\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"java"},"answer":"The builder pattern can be useful to:\n\n\n- apply some check on the data used to initialize the object. For example if you need a double check between variables\n- create immutable objects. You can't change an object once initialized, so you can't use setters\n- add readability of code.\n- reduce the code used to initialize the object\n- have the instance in a valid state. Using setters the object instance can be in a not valid state before all the setters are called.\n\n\n\n\n---\n\n\n*Note on using the builder to create immutable objects.*\n\n\nWhen you work in a multithread environment an immutable object can be shared between threads without explicit synchronization. Because the object can't change during the time is not possible to have a race condition accessing and modifying it by two threads at the same time.\n\n\n"}
{"questionId":"60b6197ef83b475c9816bce595b125bd","question":"What does it mean when \"method exists but trait bounds not satisfied\"?\nI'm new to Rust and observed something that I couldn't reason against.\n\n\nWhen I write \n\n\n\n```\nfn main() {\n    ('a'..'z').all(|_| true);\n}\n\n```\n\nThe compiler reports an error:\n\n\n\n```\nerror[E0599]: no method named `all` found for type `std::ops::Range<char>` in the current scope\n --> src\/main.rs:2:16\n  |\n2 |     ('a'..'z').all(|_| true)\n  |                ^^^\n  |\n  = note: the method `all` exists but the following trait bounds were not satisfied:\n          `std::ops::Range<char> : std::iter::Iterator`\n\n```\n\nWhen I change it to \n\n\n\n```\nfn main() {\n    (b'a'..b'z').all(|_| true);\n}\n\n```\n\nit compiles.\n\n\nWhat's happening here? What does Rust mean when it says `the method ... exists but the following trait bounds were not satisfied`?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"rust"},"answer":"The method `all()` is a method of the `Iterator` trait, so you can only call it on types that implement that trait. The type `Range<char>` does not implement the `Iterator` trait, since a range of Unicode characters is not well-defined in the general case. The set of valid Unicode code points has gaps, and building a range of code points is not in general considered useful. The type `Range<u8>` on the other does implement `Iterator`, since iterating over a range of bytes has a well-defined meaning.\n\n\nMore generally, the error message tells you that Rust has found a method with the correct name, but that method does not apply to the type you call it for.\n\n\n"}
{"questionId":"a2b7dd203813453cbaf46317928fd1e1","question":"Adding an array of integers as a data type in a Gorm Model\nI am trying to save an array of numbers in a single postgresql field using Gorm.\n\n\nThe array needs to be a list with between 2 & 13 numbers: [1, 2, 3, 5, 8, 13, 21, 40, 1000]\n\n\nEverything was working when saving a single int64. When I tried changing the model to account for an array of int64's it gives me the following error:\n\n\n\"panic: invalid sql type (slice) for postgres\"\n\n\nmy Gorm model is:\n\n\n\n```\ntype Game struct {\n    gorm.Model\n    GameCode    string\n    GameName    string\n    DeckType    []int64\n    GameEndDate string\n}\n\n```\n\n\n\n---\n\n\nUpdate based on answer from @pacuna. I tried the suggested code and I get a similar error.\n\n\n\"panic: invalid sql type Int64Array (slice) for postgres\"\n\n\nHere is the full code block:\n\n\n\n```\npackage main\n\nimport (\n    \"fmt\"\n\n    \"github.com\/jinzhu\/gorm\"\n    _ \"github.com\/jinzhu\/gorm\/dialects\/postgres\"\n    pq \"github.com\/lib\/pq\"\n)\n\nvar db *gorm.DB\n\n\/\/ Test -- Model for Game table\ntype Test struct {\n    gorm.Model                                           \n    GameCode    string                                      \n    GameName    string                                      \n    DeckType    pq.Int64Array    \n    GameEndDate string   \n}\n\n\nfunc main() {\n    db, err := gorm.Open(\"postgres\", \"host=localhost port=5432 user=fullstack dbname=scratch_game sslmode=disable\")\n    if err != nil {\n        fmt.Println(err.Error())\n        panic(\"Failed to connect to database...\")\n    }\n    defer db.Close()\n\n\n    dt := []int64{1, 2, 3}\n\n\n    db.AutoMigrate(&Test{})\n    fmt.Println(\"Table Created\")\n\n    db.Create(&Test{GameCode: \"xxx\", GameName: \"xxx\", DeckType: pq.Int64Array(dt), GameEndDate: \"xxx\"})\n    fmt.Println(\"Record Added\")\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"go"},"answer":"You need to use custom types from the underlying library:\n\n\n\n```\ntype Game struct {                                           \n        gorm.Model                                           \n        GameCode    string                                      \n        GameName    string                                      \n        DeckType    pq.Int64Array `gorm:\"type:integer[]\"`\n        GameEndDate string    \n}   \n\n\/\/ example insertion\ndt := []int64{1, 2, 3}   \n                                                                                \ndb.Create(&Game{GameCode: \"xxx\", GameName: \"xxx\", DeckType: pq.Int64Array(dt), GameEndDate: \"xxx\"})    \n\n```\n\n"}
{"questionId":"2ee4200e1c974558924870a95e14eb42","question":"Google Query - \"NOT LIKE\" Statement Doesn't work\nThe following line doesn't work:\n\n\n\n```\n=QUERY(AB:AE,\"select AB,AC,AD,AE where AB NOT LIKE '%Info%'\",1)\n\n```\n\nThrows:\n\n\n\n> \n> Unable to parse query string for Function QUERY parameter 2: PARSE\\_ERROR: Encountered \" \"AH \"\" at line 1, column 26. Was expecting one of: \"(\" ... \"(\"\n> \n> \n> \n\n\nHowever, this one does:\n\n\n\n```\n=QUERY(AB:AE,\"select AB,AC,AD,AE where AB LIKE '%Info%'\",1)\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"NOT should be before column identifier: \n\n\n\n```\n=QUERY(AB:AE,\"select AB,AC,AD,AE where NOT AB LIKE '%Info%'\",1)\n\n```\n\n"}
{"questionId":"7a4ec0da35be4abf8b5859a837958a73","question":"What is the difference between \"repeatable read\" and \"snapshot isolation\"\nRepeatable read is defined as \n\n\n\n> \n> a higher isolation level, that in addition to the guarantees of the\n>  read committed level, it also guarantees that any data read cannot\n>  change, if the transaction reads the same data again, it will find the\n>  previously read data in place, unchanged, and available to read.\n> \n> \n> \n\n\nWhich seems very similar to snapshot isolation. \n\n\nHow is repeatable read different from the Snapshot isolation level? \n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"sql"},"answer":"\"Snapshot\" guarantees that all queries within the transaction will see the data as it was at the start of the transaction.\n\n\n\"Repeatable read\" guarantees only that if multiple queries within the transaction read the *same* rows, then they will see the same data each time. (So, different rows might get snapshotted at different times, depending on when the transaction first retrieves them. And if new rows are inserted, a later query might detect them.)\n\n\n"}
{"questionId":"0efdcf09613347d1ad60ac57d32b045e","question":"How to skip task in Airflow operator?\nIs there a way for Airflow to skip current task from the PythonOperator? For example:\n\n\n\n```\ndef execute():\n    if condition:\n        skip_current_task()\n\ntask = PythonOperator(task_id='task', python_callable=execute, dag=some_dag)\n\n```\n\nAnd also marking the task as \"Skipped\" in Airflow UI?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"Figured it out! Skipping task is as easy as:\n\n\n\n```\ndef execute():\n    if condition:\n        raise AirflowSkipException\n\ntask = PythonOperator(task_id='task', python_callable=execute, dag=some_dag)\n\n```\n\n"}
{"questionId":"be1f794c9a674c84a2219c5668ab4235","question":"How to add data to a map of maps in Golang?\nI am learning to use the map of maps. In the following example, there are three nested maps.\n\n\n\n```\npackage main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n\n    var data = map[string]map[string]map[string]string{}\n\n    data[\"Date_1\"] = map[string]map[string]string{}\n    data[\"Date_1\"] = make(map[string]map[string]string, 1)\n    data[\"Date_1\"] = make(map[string]map[string]string, 0)\n\n    data[\"Date_1\"][\"Sistem_A\"] = map[string]string{}\n    data[\"Date_1\"][\"Sistem_A\"] = make(map[string]string, 0)\n    data[\"Date_1\"][\"Sistem_A\"] = make(map[string]string, 0)\n\n    data[\"Date_1\"][\"Sistem_A\"][\"command_1\"] = \"white\"\n    data[\"Date_1\"][\"Sistem_A\"][\"command_2\"] = \"blue\"\n    data[\"Date_1\"][\"Sistem_A\"][\"command_3\"] = \"red\"\n\n    fmt.Println(\"data: \", data)\n}\n\n```\n\nOutput\n\n\n\n```\ndata:  map[Date_1:map[Sistem_A:map[command_1:white command_2:blue command_3:red]]]\n\n```\n\nThe problem is that if I want to add the values \u200b\u200bin one step I get a panic: assignment to entry in nil map.\n\n\n\n```\npackage main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n\n    var data = map[string]map[string]map[string]string{}\n\n    data[\"Date_1\"] = map[string]map[string]string{}\n    data[\"Date_1\"] = make(map[string]map[string]string, 0)\n    data[\"Date_1\"] = make(map[string]map[string]string, 0)\n\n    data[\"Date_1\"][\"Sistem_A\"] = map[string]string{}\n    data[\"Date_1\"][\"Sistem_A\"] = make(map[string]string, 0)\n    data[\"Date_1\"][\"Sistem_A\"] = make(map[string]string, 0)\n\n    data[\"Date_1\"][\"Sistem_A\"][\"command_1\"] = \"white\"\n    data[\"Date_1\"][\"Sistem_A\"][\"command_2\"] = \"blue\"\n    data[\"Date_1\"][\"Sistem_A\"][\"command_3\"] = \"red\"\n\n    data[\"Date_2\"][\"Sistem_A\"][\"command_5\"] = \"violet\"\n\n    fmt.Println(\"data: \", data)\n}\n\n```\n\nOutput\n\n\n\n```\npanic: assignment to entry in nil map\n\n```\n\nThere is very little guidance information at this point. Could you help me?\n\n\nThank you.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"go"},"answer":"It is here:\n\n\n\n```\n    data[\"Date_2\"][\"Sistem_A\"][\"command_5\"] = \"violet\"\n\n```\n\nThe expression `data[\"Date_2\"]` will return a nil-map. It is never initialized, so looking for the index `[\"Sistem_A\"]` panics. Initialize the map first:\n\n\n\n```\n    data[\"Date_2\"] = make(map[string]map[string]string)\n    data[\"Date_2\"][\"Sistem_A\"] = make(map[string]string)\n    data[\"Date_2\"][\"Sistem_A\"][\"command_5\"] = \"violet\"\n\n```\n\n"}
{"questionId":"b4246a70d06445ebb309696ce777bd94","question":"VS Code PHP Formatter Intelephense\nI use Visual Studio Code as my editor and Intelephense as my PHP formatter. It works well for the most part, but Intelephense changes my format in several ways that makes it harder for me to read. For example, I like to format my code as follows:\n\n\n\n```\n    if( $var ) {\n        \/\/ Do something.\n    }\n    elseif( $var ) {\n        \/\/ Do something.\n    }\n\n```\n\nBut! When I save, Intelephense changes the above to:\n\n\n\n```\n    if ($var) {\n        \/\/ Do something.\n    } elseif ($var) {\n        \/\/ Do something.\n    }\n\n```\n\nTo me, that's harder to read, especially when dealing with long expressions. How can I configure Intelephense to format my code as in the first example?\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"php"},"answer":"Extensions > PHP Intelephense > Manage(click the gear) >\nFind Intelephense \u203a Format: Braces and change or Find Intelephense \u203a Format: Enable and remove tick.\n\n\n"}
{"questionId":"81dbca580260456c9798566230b0add0","question":"TypeError: \\_\\_call\\_\\_() missing 1 required positional argument: 'inputs'\nI am trying to predict the close price (1 or 0) based on the features present in the 'input\\_data'. But when I try to run the code I am getting the below error, I am not sure how to fix this. Any help is much appreciated, thanks\n\n\n\n```\nTraceback (most recent call last):\n  File \"F:\/Machine Learning\/SK_Learn\/SVM_Stock.py\", line 71, in <module>\n    estimator.fit(x,y)\n  File \"C:\\Python35\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\", line 210, in fit\n    return super(KerasClassifier, self).fit(x, y, **kwargs)\n  File \"C:\\Python35\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\", line 139, in fit\n    **self.filter_sk_params(self.build_fn.__call__))\nTypeError: __call__() missing 1 required positional argument: 'inputs'\n\n```\n\nHere's the code:\n\n\n\n```\nclass SVM_Stock:\n\n    def __init__(self):\n        pass\n\n    def create_model(self):\n\n        model = Sequential()\n        model.add(Dense(14, input_dim=16, kernel_initializer='normal', activation='relu'))\n        model.add(Dense(7, kernel_initializer='normal', activation='relu'))\n        model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n        model.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n        return model\n\n\nif __name__ == \"__main__\":\n\n    desired_width = 450\n    pd.set_option('display.width', desired_width)\n    pd.set_option('display.max_columns', 17)\n\n    ds = pd.read_csv('F:\\\\Machine Learning\\\\Linear Regression\\\\BIOCON-EQ.csv')\n\n    ds = ds[['Date','Open','High','Low','Close','Volume','Slow VWMA','Fast VWMA']][14:].sort_values('Date')\n\n    ds.loc[ds['Slow VWMA'] > ds['Fast VWMA'], 'Trend UP'] = 1\n    ds.loc[ds['Slow VWMA'] < ds['Fast VWMA'], 'Trend UP'] = 0\n    ds.loc[ds['Slow VWMA'] == ds['Fast VWMA'], 'Trend UP'] = -1\n\n    ds.loc[ds['Slow VWMA'] < ds['Fast VWMA'], 'Trend Down'] = 1\n    ds.loc[ds['Slow VWMA'] > ds['Fast VWMA'], 'Trend Down'] = 0\n    ds.loc[ds['Slow VWMA'] == ds['Fast VWMA'], 'Trend Down'] = -1\n\n    ds.loc[ds['Close'] > ds['Open'], 'Close Price'] = 1\n    ds.loc[ds['Close'] < ds['Open'], 'Close Price'] = 0\n    ds.loc[ds['Close'] == ds['Open'], 'Close Price'] = -1\n\n    input_data = ds[['Date','Open','High','Low','Close','Trend UP', 'Trend \n    Down']]\n    input_data.index = input_data.Date\n    input_data.drop('Date', axis=1, inplace=True)\n    target = ds[['Close Price']]\n\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    x = scaler.fit_transform(input_data)\n    y = target.values.ravel()\n\n    # clf = svm.SVC(gamma=0.1, C=100)\n    # clf.fit(x[:400], y[:400])\n    # print(clf.score(x[:400], y[:400]))\n    #\n    # for i in range(420, len(x)):\n    #     print(\"Prediction :\", clf.predict(x[i].reshape(1, -1)))\n    #     print(i, y[i])\n\n    SS = SVM_Stock()\n    estimator = KerasClassifier(build_fn=SS.create_model(), nb_epoch=10, verbose=0)\n    estimator.fit(x,y)\n\n    '''Cross Validate'''\n    cv_scores = cross_val_score(estimator, x, y, cv=10)\n    print(cv_scores.mean())\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"When creating your estimator you should pass the `create_model` function without calling it (i.e. without brackets):\n\n\n\n```\nestimator = KerasClassifier(build_fn=SS.create_model, nb_epoch=10, verbose=0)\n\n```\n\n"}
{"questionId":"073f5b5b42a64db19b38ca7add9df797","question":"React & TypeScript: Avoid context default value\nIn the effort to better learn React, TypeScript, and Context \/ Hooks, I'm making a simple Todo app. However, the code needed to make the context feels cumbersome.\n\n\nFor example, if I want to change what a Todo has, I have to change it in three places (ITodo interface, default context value, default state value). If I want to pass down something new, I have to do that in three places (TodoContext, TodoContext's default value, and value=). Is there a better way to not have to write so much code?\n\n\n\n```\nimport React from 'react'\n\nexport interface ITodo {\n    title: string,\n    body?: string,\n    id: number,\n    completed: boolean\n}\n\ninterface TodoContext {\n    todos: ITodo[],\n    setTodos: React.Dispatch<React.SetStateAction<ITodo[]>>\n}\n\nexport const TodoContext = React.createContext<TodoContext>({\n    todos: [{title: 'loading', body: 'loading', id: 0, completed: false}],\n    setTodos: () => {}\n})\n\nexport const TodoContextProvider: React.FC<{}> = (props) => {\n    const [todos, setTodos] = React.useState<ITodo[]>([{title: 'loading', body: 'loading', id: 0, completed: false}])\n\n    return (\n        <TodoContext.Provider value={{todos, setTodos}}>\n            {props.children}\n        <\/TodoContext.Provider>\n    )\n}\n\n```\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"typescript"},"answer":"After awhile, I think I've found the best way to go about this.\n\n\n\n```\nimport React from 'react'\n\nexport interface ITodo {\n    title: string,\n    body?: string,\n    id: number,\n    completed: boolean\n}\n\nconst useValue = () => {\n    const [todos, setTodos] = React.useState<ITodo[]>([])\n\n    return {\n        todos,\n        setTodos\n    }\n}\n\nexport const TodoContext = React.createContext({} as ReturnType<typeof useValue>)\n\nexport const TodoContextProvider: React.FC<{}> = (props) => {\n    return (\n        <TodoContext.Provider value={useValue()}>\n            {props.children}\n        <\/TodoContext.Provider>\n    )\n}\n\n```\n\nThis way, there is single point of change when adding something new to your context, rather than triple point of change originally. Enjoy!\n\n\n"}
{"questionId":"63b7c8a5ea8a4154b9ffb0adbf430842","question":"spark [dataframe].write.option(\"mode\",\"overwrite\").saveAsTable(\"foo\") fails with 'already exists' if foo exists\nI think I am seeing a bug in spark where mode 'overwrite' is not respected, rather an exception is thrown on an attempt to do saveAsTable into a table that already exists (using mode 'overwrite').\n\n\nBelow is a little scriptlet that reproduces the issue. The last statement results in a stack trace reading:\n\n\n\n```\n org.apache.spark.sql.AnalysisException: Table `example` already exists.;\n\n```\n\nAny advice much appreciated. \n\n\n\n```\nspark.sql(\"drop table if exists example \").show()\ncase class Person(first: String, last: String, age: Integer)\nval df = List(\n    Person(\"joe\", \"x\", 9),\n    Person(\"fred\", \"z\", 9)).toDF()\ndf.write.option(\"mode\",\"overwrite\").saveAsTable(\"example\")\n\nval recover1 = spark.read.table(\"example\")\nrecover1.show()\n\n\nval df3 = List(\n    Person(\"mouse\", \"x\", 9),\n    Person(\"golf\", \"z\", 9)).toDF()\n\n df3.write.\n    option(\"mode\",\"overwrite\").saveAsTable(\"example\")      \n\nval recover4 = spark.read.table(\"example\")\nrecover4.show()     \n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"`saveAsTable` doesn't check extra options, use `mode` directly\n\n\n\n```\ndf3.write.mode(SaveMode.Overwrite).saveAsTable(\"example\")\n\n```\n\nor\n\n\n\n```\ndf3.write.mode(\"overwrite\").saveAsTable(\"example\")\n\n```\n\n"}
{"questionId":"dde2386b34484473a58b239245ae93bc","question":"How to provide List for a data theory ? \"InlineData\"\nHow to provide List as a data source for a data theory, I can't find anything in InlineData that supports this :\n\n\n\n```\n    [InlineData(null, new[] { 42, 2112 }, null)] \/\/ This doesn't work, I need something that works with List<int>\n    [Trait(\"Category\", \"API\")]\n    [Trait(\"Category\", \"Partner\")]\n    [Trait(\"Category\", \"Smoke\")]\n    public void VerifyGetCarListAsync(int? colorID, List<int> carIDs, int? sellerID){\/\/}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"This cannot be accomplished with `InlineData` you can only do this with `MemberData`, `PropertyData` or `ClassData` see the `MemberData` example below.\n\n\n\n```\n[MemberData(nameof(Data))]\npublic void VerifyGetCarListAsync(int? colorID, List<int> carIDs, int? sellerID){\n    \/\/ test code\n}\n\n\npublic static IEnumerable<object[]> Data(){\n    yield return new object[] { null, new List<int>{ 42, 2112 }, null };\n    yield return new object[] { 1, new List<int>{ 43, 2112 }, null };\n    yield return new object[] { null, new List<int>{ 44, 2112 }, 6 };\n}\n\n```\n\n"}
{"questionId":"3e0b752f2a8f44c791c8e34b65556c6e","question":"simple loop over files in some directory makefile\nI can easily print all the files inside some directory from `bash`:\n\n\n\n```\n$ cat go.sh\nBASEDIR=~\/Downloads\nMYDIR=${BASEDIR}\/ddd\nfor f in $(ls ${MYDIR}); do echo $f; done\n\n$ .\/go.sh\nm.txt\nd.txt\n\n```\n\nWhen I try to do a similar thing from `makefile` it doesn't work well:\n\n\n\n```\n$ cat makefile\nBASEDIR = ${HOME}\/Downloads\nMYDIR = ${BASEDIR}\/ddd\nall:\n    for f in $(ls ${MYDIR}); do echo ${f}; done\n\n$ make\nfor f in ; do echo ; done\n\n```\n\nAnd here is another trial that doesn't work:\n\n\n\n```\n$ cat makefile\nBASEDIR = ${HOME}\/Downloads\nMYDIR = ${BASEDIR}\/ddd\nall:\n    for f in $(shell ls ${MYDIR}); do echo ${f}; done\n\n$ make\nfor f in d.txt m.txt; do echo ; done\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"Maybe you can do it purely Makefile way?\n\n\n\n```\nMYDIR = .\nlist: $(MYDIR)\/*\n        @echo $^\n\n```\n\nYou can still run command from Makefile like this\n\n\n\n```\nMYDIR = .\nlist: $(MYDIR)\/*\n        for file in $^ ; do \\\n                echo \"Hello\" $${file} ; \\\n        done\n\n```\n\nIf I were you, I'd rather not mix Makefile and bash loops based on `$(shell ...)`. I'd rather pass dir name to some script and run loop there - inside script.\n\n\n"}
{"questionId":"7122cfdf367143b19ee7a3033690dfef","question":"If\/else representation with stream and Java 8\nI have the following interface with two implementations:\n\n\n\n```\npublic interface Parser {\n    void parse();\n    boolean canParse(String message);\n}\n\nclass StackParser implements Parser {\n    public void parse(){\n        System.out.println(\"Parsing stackoverflow\");\n    }\n    public boolean canParse(String message){\n        return message.equals(\"stackoverflow\");\n    }\n}\n\nclass YoutubeParser implements Parser {\n    public void parse() {\n        System.out.println(\"Parsing youtube\");\n    }\n    public boolean canParse(String message) {\n        return message.equals(\"youtube\");\n    }\n}\n\n```\n\nI go to check incoming message and parse `\"stackoverflow\"` or `\"youtube\"`:\n\n\n\n```\npublic class Main {\n    private List<Parser> parsers;\n\n\n    public static void main(String[] args) {\n        new Main().doSomething(\"youtube\");\n    }\n\n    void doSomething(String message){\n        parsers.stream()\n                .filter(p -> p.canParse(message))\n                .forEach(p -> p.parse());\n    }\n\n}\n\n```\n\nOkay, pretty good. But what if message is not `\"stackoverflow\"` or `\"youtube\"`? App will be silent, but I want to send another default message if no matches were found, like `\"I can't parse this web!\"`.\n\n\nI know that will not works (even compile), but it's also should print `\"I can't parse this web\"` only one time, not for every `false` condition. \n\n\n\n```\nparsers.stream()\n       .filter(p -> {\n          if (p.canParse(message) == false) {\n               System.out.println(\"I can't parse it!\");\n             }\n      })\n      .forEach(p -> p.parse());\n\n```\n\nHow can I do it? \n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"This is a perfect example of when to use the `Optional#orElse` or the `Optional#orElseThrow` method(s). You want to check if some condition is met so you filter, trying to return a single result. If one does not exist, some other condition is true and should be returned.\n\n\n\n```\ntry {\n    Parser parser = parsers.stream()\n            .filter(p -> p.canParse(message))\n            .findAny()\n            .orElseThrow(NoParserFoundException::new);\n\n    \/\/ parser found, never null\n    parser.parse();\n} catch (NoParserFoundException exception) {\n   \/\/ cannot find parser, tell end-user\n}\n\n```\n\n"}
{"questionId":"42c7ce9a05f54efba79f20250f5222b0","question":"VBA: Single line if statement with multiple actions\nI really should be able to google this, but I can't find what I wanna know about.\n\n\nI want to check if a file exists. If not, a MessageBox should pop up and VBA should exit the sub.\n\n\n\n```\nIf Dir(\"C:\\file.txt\", vbDirectory) = \"\" Then \n    MsgBox \"File doesn't exist\"\n    Exit Sub\nEnd If\n\n```\n\nIt works, I just wanna know if you can you do this in a single line statement? Does VBA allow this when more than one thing is supposed to happen (like it is the case here)? This code doesn't work (syntax error): \n\n\n\n```\nIf Dir(\"C:\\file.txt\", vbDirectory) = \"\" Then  MsgBox \"File doesn't exist\" And Exit Sub\n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"vba"},"answer":"You absolutely can! \n\n\n\n```\nIf Dir(\"C:\\file.txt\", vbDirectory) = \"\" Then  MsgBox \"File doesn't exist\" : Exit Sub\n\n```\n\n"}
{"questionId":"b4ef65a78c5b439d8163a1f8b7548c11","question":"Check what conda environment is currently activated\nI'm wondering if there is an easy way to check which conda environment is currently activated.\n\n\nI know you can do `conda env list` and the active environment will be printed with a \\*. \nHowever, I would like to do this programmatically as an input into an else if statement.\n\n\nparsing the output of conda env list is rather inconvenient so I hope there is an easier way\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"When a `conda` environment is activated, it will `export` following related environment variables:\n\n\n- `$CONDA_DEFAULT_ENV`, name of current activated env\n- `$CONDA_PREFIX`, path to the current activate env\n\n\n"}
{"questionId":"d8a2bea208fe4814b1c3c0cc22a37702","question":"Is a negative integer summed with a greater unsigned integer promoted to unsigned int?\nAfter getting advised to read \"C++ Primer 5 ed by Stanley B. Lipman\" I don't understand this:\n\n\nPage 66. \"Expressions Involving Unsigned Types\"\n\n\n\n```\nunsigned u = 10;\nint i = -42;\nstd::cout << i + i << std::endl; \/\/ prints -84\nstd::cout << u + i << std::endl; \/\/ if 32-bit ints, prints 4294967264\n\n```\n\nHe said: \n\n\n\n> \n> In the second expression, the int value -42 is converted to unsigned before the addition is done. Converting a negative number to unsigned behaves exactly as if we had attempted to assign that negative value to an unsigned object. The value \u201cwraps around\u201d as described above.\n> \n> \n> \n\n\nBut if I do something like this:\n\n\n\n```\nunsigned u = 42;\nint i = -10;\nstd::cout << u + i << std::endl; \/\/ Why the result is 32?\n\n```\n\nAs you can see `-10` is not converted to `unsigned int`. Does this mean a comparison occurs before promoting a `signed integer` to an `unsigned integer`?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c++"},"answer":"`-10` is being converted to a unsigned integer with a very large value, the reason you get a small number is that the addition wraps you back around. With 32 bit unsigned integers `-10` is the same as `4294967286`. When you add 42 to that you get `4294967328`, but the max value is `4294967296`, so we have to take `4294967328` modulo `4294967296` and we get `32`.\n\n\n"}
{"questionId":"5cba9c2ccad04961859266c926d43342","question":"Is there a one-liner for using default values with Read-Host?\nI've written something like this to specify default values for prompts.\n\n\n\n```\n$defaultValue = 'default'\n$prompt = Read-Host \"Press enter to accept the default [$($defaultValue)]\"\nif ($prompt -eq \"\") {} else {\n    $defaultValue = $prompt\n    }\n\n```\n\nCan it be shortened further?\n\n\nHere is my attempt.\n\n\n\n```\n$defaultValue = 'default'\n$prompt = Read-Host \"Press enter to accept the default [$($defaultValue)]\"\nif (!$prompt -eq \"\") {$defaultValue = $prompt}\n\n```\n\nI want a one-liner, so I'm gonna hold out accepting an answer until then.\n\n\nN.b. `$defaultValue` should be stored independently of the one liner. Similar to the example above.\n\n\nI've accepted the answer which lead me to the solution I was looking for.\n\n\n\n```\n$defaultValue = 'default'\nif (($result = Read-Host \"Press enter to accept default value $defaultValue\") -eq '') {$defaultValue} else {$result}\n\n```\n\nAnd for those of you asking why. The reason is because it is easier on the eyes of whoever comes after me. Less is always more, when clarity is not sacrificed. IMHO.\n\n\nEDIT;\n\n\nInstead of a single line, perhaps I should have said a single phrase?\nI've added this edit clarify whilst a few answers I have seen use are using a semi-colon.\n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"bash\/shell"},"answer":"Shortest Version I could came up with:\n\n\n\n```\nif (!($value = Read-Host \"Value [$default]\")) { $value = $default }\n\n```\n\nThis version doesn't have to use else.\n\n\n"}
{"questionId":"39f13b44b76f4a20a7b7cb80d32cc83f","question":"ZSH \"command not found: z\"\n*iterm2*\n*ohmyzsh*\n\n\nIn 'zsh' I can't use 'z' to search folders, appears command not found: z\n\n\nI try to run \n\n\n\n```\nsource \"$(brew --prefix)\/etc\/profile.d\/z.sh\"\n\n```\n\nand\n\n\nsource \/usr\/local\/etc\/profile.d\/z.sh\n\n\nbut **doesn't work**\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"bash\/shell"},"answer":"I got it!, \n\n\nFirst:\n\n\n\n```\nvim ~\/.zshrc\n\n```\n\nand\n\n\n\n```\nplugins=(\n    git\n    z\n)\n\n```\n\nexit with :x!\n\n\nAnd run\n\n\n\n```\nsource ~\/.zshrc\n\n```\n\n"}
{"questionId":"f6100af4ff8947459ecdb6d2869cf227","question":"How to disable and enable scrolling in LazyColumn\/LazyRow in Jetpack Compose?\nI want to dynamically enable and disable scrolling programmatically in a `LazyColumn`.\n\n\nThere don't seem to be any relevant functions on `LazyListState` or relevant parameters on `LazyColumn` itself. How can I achieve this in Compose?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"kotlin"},"answer":"There's not (currently) a built-in way to do this, which is a reasonable feature request.\n\n\nHowever, the `scroll` API is flexible enough that we can add it ourselves. Basically, we create a never-ending fake scroll at `MutatePriority.PreventUserInput` to prevent scrolling, and then use a do-nothing scroll at the same priority to cancel the first \"scroll\" and re-enable scrolling.\n\n\nHere are two utility functions on `LazyListState` to disable\/re-enable scrolling, and a demo of them both in action (some imports will be required, but Android Studio should suggest them for you).\n\n\nNote that because we're taking control of scrolling to do this, calling `reenableScrolling` will also cancel any ongoing scrolls or flings (that is, you should only call it when scrolling is disabled and you want to re-enable it, not just to confirm that it's enabled).\n\n\n\n```\nfun LazyListState.disableScrolling(scope: CoroutineScope) {\n    scope.launch {\n        scroll(scrollPriority = MutatePriority.PreventUserInput) {\n            \/\/ Await indefinitely, blocking scrolls\n            awaitCancellation()\n        }\n    }\n}\n\nfun LazyListState.reenableScrolling(scope: CoroutineScope) {\n    scope.launch {\n        scroll(scrollPriority = MutatePriority.PreventUserInput) {\n            \/\/ Do nothing, just cancel the previous indefinite \"scroll\"\n        }\n    }\n}\n\n@Composable\nfun StopScrollDemo() {\n    val scope = rememberCoroutineScope()\n    val state = rememberLazyListState()\n    Column {\n        Row {\n            Button(onClick = { state.disableScrolling(scope) }) { Text(\"Disable\") }\n            Button(onClick = { state.reenableScrolling(scope) }) { Text(\"Re-enable\") }\n        }\n        LazyColumn(Modifier.fillMaxWidth(), state = state) {\n            items((1..100).toList()) {\n                Text(\"$it\", fontSize = 24.sp)\n            }\n        }\n    }\n}\n\n```\n\n"}
{"questionId":"4a16a529e0af4c0bbc560772bbefbf40","question":"Use the same class as Input and Object type in GraphQL in NestJS\nI am trying to setup my graphql resover to handle an array of objects but cant get the @Args decorator configured.\n\n\nI created my own ArgsType\n\n\n\n```\nimport\u00a0{\u00a0ArgsType,\u00a0Field,\u00a0Int,\u00a0ObjectType\u00a0}\u00a0from '@nestjs\/graphql';\n\n@ArgsType()\u00a0\u00a0\/\/\u00a0to\u00a0be\u00a0used\u00a0as\u00a0type\u00a0in\u00a0the\u00a0resolver\n@ObjectType()\u00a0\u00a0\/\/\u00a0for\u00a0schema\u00a0generation\u00a0\nexport class Club\u00a0{\n\u00a0@Field(type => String)\n president: string;\n\n\u00a0@Field(type => Int)\n members?: number;\n}\n\n```\n\nResolver with adding a single Club works just fine!\n\n\n\n```\n\u00a0@Query(()\u00a0=> Int)\n async addClub(@Args()\u00a0club: Club)\u00a0{\n  \/\/ handle stuff\n\u00a0\u00a0}\n\n```\n\nbut if I want to give an array of Club like this\n\n\n\n```\n\u00a0@Query(()\u00a0=> Int)\n   async addClubs(@Args({name:\u00a0'clubs',\u00a0type:\u00a0()\u00a0=>\u00a0[Club]})\u00a0clubs: Array<Club>)\u00a0{\n   \/\/ handle stuff\n\u00a0\u00a0}\n\n```\n\nthis thows an error when nest is starting up\n\n\n\n```\n UnhandledPromiseRejectionWarning: Error: Cannot determine a GraphQL input type for the \"clubs\". Make sure your class is decorated with an appropriate decorator.\n\n```\n\nalthough I am able to use an array of Strings like this\n\n\n\n```\n @Query(()\u00a0=>\u00a0[String])\n async addStrings(@Args({\u00a0name:\u00a0'clubs',\u00a0type:\u00a0()\u00a0=>\u00a0[String],\u00a0})\u00a0clubs: Array<string>)\u00a0{\n  \/\/ handle stuff\n\u00a0\u00a0}\n\n```\n\nI am pretty sure there should be an easy solution, but cant figure out where to go from here.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"According to the error,\n\n\n\n```\nCannot determine a GraphQL input type for the \"clubs\". Make sure your class is decorated with an appropriate decorator\n\n```\n\nYou're trying to use `Club` class as a GraphQL input type while it is already an object type (According to `@ObjectType` annotation you use).\n\n\n\n\n---\n\n\n**Solution 1:**\n\n\nI would suggest you write a separate GraphQL input type like below (not tested). This is cleaner and less coupled if you need to separate the way you treat input and output of `Club`.\n\n\n\n```\nimport { InputType, Field } from '@nestjs\/graphql';\n\n@InputType()\nexport class ClubInput {\n @Field()\n president: string;\n\n @Field()\n members?: number;\n}\n\n```\n\nThen in your resolver, you can use it like below.\n\n\n\n```\n@Query(() => Int)\nasync addClubs(@Args({name: 'clubs', type: () => [ClubInput]}) clubs: Array<ClubInput>) {\n \/\/ handle stuff\n}\n\n```\n\n\n\n---\n\n\n**Solution 2:**\n\n\nBut if you really need to use the same class for both purposes, you could try to create both input and object types with the same `Club` name. The name of the object type is by default the name of the class. So you need to provide the name explicitly to avoid conflict.\n\n\n\n```\nimport { Field, Int, ObjectType, InputType } from '@nestjs\/graphql';\n\n@InputType(\"ClubInput\")\n@ObjectType(\"ClubType\")\nexport class Club {\n @Field(type => String)\n president: string;\n\n @Field(type => Int)\n members?: number;\n}\n\n```\n\nNow your `Club` has different names for input and object types. Then in your resolver, you can use it like below.\n\n\n\n```\n@Query(() => Int)\nasync addClubs(@Args({name: 'clubs', type: () => [ClubInput]}) clubs: Array<ClubInput>) {\n \/\/ handle stuff\n}\n\n```\n\n**Note:** In this solution, you need to make sure that `Club` will never contain fields that express circular references or references to interfaces and unions. If that is going to happen, you will have to move to Solution 1, else this will make your input throw an error.\n\n\n\n\n---\n\n\nThe take away is that in Typescript GraphQL, `InputType` and `ObjectType` are two different concepts and we need to use it properly to avoid any issues.\n\n\n"}
{"questionId":"1f27927d54614f71871fb18611180a99","question":"React hook useRef not working with styled-components and typescript\nI've some problem using the `useRef` hook with a styled component.\n\n\nLinter alerts me that `Object is possibly 'null'` inside the didMount `useEffect`. Any idea about this?\n\n\n### This is not a duplicate for 2 reason:\n\n\n- The old answer refers to the `ref` used in a class component, that was the only way to use it before React hooks,\n- The `innerRef` props isn't supported anymore in the current version of styled components.\n\n\nHere's a sample snippet of my component:\n\n\n\n```\nimport React, { useRef, useEffect } from 'react';\nimport styled from 'styled-components';\n\nconst StyledInput = styled.input`\n  background: transparent;\n`\n\nconst MyForm = () => {\n  const inputRef = useRef(null);\n  \n  useEffect(() => {\n    if (inputRef && inputRef.current) {\n      inputRef.current.focus(); \/\/Object is possibly 'null'\n    }\n  }, []);\n\n  return (\n    <StyledInput ref={inputRef}\/>\n  );\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"I've finally found a solution:\n\n\n\n```\nconst inputRef = useRef() as React.MutableRefObject<HTMLInputElement>;\n\n```\n\n\n\n---\n\n\nIt works for me:\n\n\n\n```\nimport React, { useRef, useEffect } from 'react';\nimport styled from 'styled-components';\n\nconst StyledInput = styled.input`\n  background: transparent;\n`\n\nconst MyForm = () => {\n  const inputRef = useRef() as React.MutableRefObject<HTMLInputElement>;\n\n  useEffect(() => {\n    if (inputRef && inputRef.current) {\n      inputRef.current.focus();\n    }\n  }, []);\n\n  return (\n    <StyledInput ref={inputRef}\/>\n  );\n}\n\n```\n\n"}
{"questionId":"94569d5cf1904fbf9f5aec4d0113bbd1","question":"How can I set a header field in a response with NestJS?\nI'm trying:\n\n\n\n```\n    @Post('login')\n    async login(@Body() body: AuthDto, @Res() res: Response) {\n        const loginResponse = await this.authService.login(body);\n        console.log('loginResponse', loginResponse)\n        res.headers.set('x-access-token', loginResponse.access_token)\n        return loginResponse\n    }\n\n```\n\nbut no dice. I get an error:\n\n\n\n```\nTypeError: Cannot read property 'set' of undefined\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"Not the most elegant way: `return res.set({ 'x-access-token': loginResponse.access_token }).json(loginResponse);`\n\n\nI'd separate this logic into an interceptor, checking if the response is valid for path \/login, if so return the correct header (using some value from loginResponse)\n\n\n\n```\nimport { Controller, Get, Response } from '@nestjs\/common';\nimport { Response as Res } from 'express';\nimport { AppService } from '.\/app.service';\n\n@Controller()\nexport class AppController {\n  constructor(private readonly appService: AppService) {}\n\n  @Get()\n  getHello(@Response() res: Res): Res {\n    return res.set({ 'x-access-token': 1 }).json({ hello: 'world' });\n  }\n\n  @Get()\n  getHelloAlt(@Response() res: Res): Res {\n    return res.set({ 'x-access-token': 1 }).json({ hello: 'world' });\n  }\n}\n\n```\n\nThis is my working version, notice the Express Response and not Nest.js.\n\n\nEDIT: The type imported from Nest.js\/common is a decorator function, instead either use no type, *or* import Response from Express.js.\n\n\n"}
{"questionId":"46116cb1aae34db7965249784f9750fd","question":"React\/Typescript - expects at least '3' arguments, but the JSX factory 'React.createElement' provides at most '2'. TS622\nI have this component:\n\n\n\n```\nconst TheBarTitle = (\n    theClass: any,\n    columnTitle: string,\n    onClickAction: any,\n) => {\n    return (\n        <div\n            className={theClass}\n            title=\"Click to add this filter\"\n            onClick={onClickAction}\n        >\n            {columnTitle}\n        <\/div>\n    );\n};\n\n```\n\nWhich is used this way:\n\n\n\n```\n render: (rowData): any => {\n                                return (\n                                    <div className={classes.contentxyz}>\n                                        .........                                    <\/div>\n                                );\n                            },\n                        },\n                        {\n                            title: (\n                                <TheBarTitle\n                                    theClass={classes.contentxyz}\n                                    columnTitle=\"THIS IS THE TITLE\"\n                                    onClickAction={(e: any) =>\n                                        this.handleTooltip(\n                                            e,\n                                            'theeetitle:',\n                                        )\n                                    }\n                                \/>\n                            ),\n\n....\n\n```\n\nHowever I get the error: `Tag 'TheBarTitle' expects at least '3' arguments, but the JSX factory 'React.createElement' provides at most '2'. TS622`\n\n\nI am using actually 3 arguments. Any idea what I am doing wrong and it sees only 2?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"You are mixing function call with compoent creation methods.\nChange `TheBarTitle` to a FunctionComponent creation method\n\n\n\n```\ninterface Props {\n  theClass: any\n  columnTitle: string\n  onClickAction: any\n}\nconst TheBarTitle: React.FC<Props> = ({theClass, columnTitle, onClickAction}) => {\n  return (\n    <div\n      className={theClass}\n      title=\"Click to add this filter\"\n      onClick={onClickAction}\n    >\n      {columnTitle}\n    <\/div>\n  )\n}\n\n```\n\nOr your call to the function to this:\n\n\n\n```\ntitle: TheBarTitle(classes.contentxyz, \"THIS IS THE TITLE\", (e: any) =>\n    this.handleTooltip(e, 'theeetitle:')\n    ))\n\n```\n\nFor the latter I would suggest to also change the casing on the naming.\n\n\n"}
{"questionId":"b867f55176784a2c92d613fae6bf3853","question":"Unable to extend Express Request in TypeScript\nThis question has been asked several times, but none of the answers that were given have worked for me. I'm trying to extend the Express `Request` object to include a property to store a `User` object. I created a declaration file, `express.d.ts`, and placed it in the same directory as my `tsconfig.json`:\n\n\n\n```\nimport { User } from \".\/src\/models\/user\";\n\ndeclare namespace Express {\n    export interface Request {\n        user: User;\n    }\n}\n\n```\n\nThen I try to make an assignment to it in `secured-api.ts`:\n\n\n\n```\nimport express from 'express';\nimport { userService } from '..\/services\/user';\n\nrouter.use(async (req, res, next) => {\n    try {\n        const user = await userService.findByUsername(payload.username);\n\n        \/\/ do stuff to user...\n\n        req.user = user;\n        next();\n    } catch(err) {\n        \/\/ handle error\n    }\n});\n\n```\n\nI get the following error:\n\n\n\n```\nsrc\/routes\/secured-api.ts:38:21 - error TS2339: Property 'user' does not exist on type 'Request'.\n\n38                 req.user = user;\n                       ~~~~\n\n```\n\nMy `User` class is:\n\n\n\n```\nimport { Model, RelationMappings } from 'objection';\n\nexport class User extends Model {\n\n    public static tableName = 'User';\n    public static idColumn = 'username';\n\n    public static jsonSchema = {\n        type: 'object',\n        required: ['fname', 'lname', 'username', 'email', 'password'],\n\n        properties: {\n            fname: { type: 'string', minLength: 1, maxLength: 30 },\n            lname: { type: 'string', minLength: 1, maxLength: 30 },\n            username: { type: 'string', minLength: 1, maxLength: 20 },\n            email: { type: 'string', minLength: 1, maxLength: 320 },\n            password: { type: 'string', minLength: 1, maxLength: 128 },\n        }\n    };\n\n    public static modelPaths = [__dirname];\n\n    public static relationMappings: RelationMappings = {\n\n    };\n\n    public fname!: string;\n    public lname!: string;\n    public username!: string;\n    public email!: string;\n    public password!: string;\n}\n\n```\n\nMy `tsconfig.json` is:\n\n\n\n```\n{\n    \"compilerOptions\": {\n      \"target\": \"es6\",                          \/* Specify ECMAScript target version: 'ES3' (default), 'ES5', 'ES2015', 'ES2016', 'ES2017','ES2018' or 'ESNEXT'. *\/\n      \"module\": \"commonjs\",                     \/* Specify module code generation: 'none', 'commonjs', 'amd', 'system', 'umd', 'es2015', or 'ESNext'. *\/\n      \"lib\": [\"es2015\"],                             \/* Specify library files to be included in the compilation. *\/\n      \"outDir\": \".\/build\",                      \/* Redirect output structure to the directory. *\/\n      \"strict\": true,                           \/* Enable all strict type-checking options. *\/\n      \"esModuleInterop\": true                   \/* Enables emit interoperability between CommonJS and ES Modules via creation of namespace objects for all imports. Implies 'allowSyntheticDefaultImports'. *\/\n    }\n}\n\n```\n\nMy directory structure is:\n\n\n\n```\nbackend\/\n    package.json\n    tsconfig.json\n    express.d.ts\n    src\/\n        models\/\n            user.ts\n        routes\/\n            secured-api.ts\n\n```\n\nWhat am I doing wrong here?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"The problem is that you are not augmenting the `Express` global namespace defined by `express` you are creating a new namespace in your module (the file becomes a module once you use an `import`).\n\n\nThe solution is to declare the namespace in `global`\n\n\n\n```\nimport { User } from \".\/src\/models\/user\";\n\ndeclare global {\n    namespace Express {\n        export interface Request {\n            user: User;\n        }\n    }\n}\n\n```\n\nOr not use the module import syntax, just reference the type:\n\n\n\n```\ndeclare namespace Express {\n    export interface Request {\n        user: import(\".\/src\/models\/user\").User;\n    }\n}\n\n```\n\n"}
{"questionId":"97b7d3ddce7847c4a80fab4760669e86","question":"How can I create an enum column in Supabase?\nI want to create an enum column in a Supabase table. Supabase uses Postgres under the hood, so I understand that technically it is possible if I do it manually using SQL. But is there any way I can do it via the frontend in an easier manner?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"sql"},"answer":"Supabase engineer here -- Thanks for the query! You are right that Supabase uses Postgres under the hood and that it can be done via the SQL editor on the Supabase dashboard or directly against the database by using `psql`.\n\n\nTo my knowledge we don't have a way of entering enum types via the frontend as of yet. We will relay this piece of feedback to the frontend team though.\n\n\nLet us know if you have any further questions.\n\n\n"}
{"questionId":"eed6114d134b4dd689563f2ef3a92064","question":"Does casting to or from a double and a float preserve infinity and NaN?\nWhen casting double infinity to float and vice versa, will it still be infinity? Is it the same with NaN?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c++"},"answer":"Converting **any** float to a double is guaranteed to preserve the value. Converting a double to float is guaranteed to preserve the value if the original value is representable as float.\n\n\nIf your system conforms to IEEE-754, then float is able to represent infinity and NaN. Otherwise, you can use `<numeric_limits>` to check whether that is the case. The payload of a double NaN is not necessarily representable by a float NaN.\n\n\n"}
{"questionId":"0ed62a1af13745828944ee544c2d35b1","question":"Using multiple Bundler versions\nI've joined a project which is using bundler version 1.17.1 . Currently my system is using bundler 2.0.2 . Is there any way for me to specify a bundler version just like specifying ruby version with `rbenv`. Something likes:\n\n\n\n```\nbundler --global 1.17.1\n\n```\n\nSo that, when I run the command `bundler -v` I will receive the result `1.71.1` rather than `2.0.2` \n\n\nI tried to uninstall the current bundler with `gem uninstall bundler` and received this message: \n\n\n\n```\n$ gem uninstall bundler\n\nSelect gem to uninstall:\n 1. bundler-1.17.1\n 2. bundler-1.17.3\n 3. bundler-2.0.2\n 4. All versions\n> \n\n```\n\nThat makes me think there is a certain way allows me to work with multiple bundler versions. Any clues just for pointing me to where I should look up would be really appreciated.\n\n\n## EDIT 1\n\n\nI tried `bundle _1.17.1_ --version` and it didn't work as I expected\n\n\n\n```\n$ bundler -v\nBundler version 2.0.2\n$ bundle _1.17.1_ --version\nBundler version 1.17.1\n$ bundler -v\nBundler version 2.0.2\n\n```\n\n**expected:** Bundler version 1.17.1\n\n\n**actual:** Bundler version 2.0.2\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"ruby"},"answer":"**Try this:**\n\n\n**1)** `gem install bundler -v 1.17.1`\n\n\n**2)** `bundle _1.17.1_ install`\n\n\n"}
{"questionId":"2a439861bd9a4ec3ac1100d7778320c3","question":"Difference between npm run dev and npm run production\nI am very new in Laravel and vue.js.Please let me know what is the difference between `npm run dev` and `npm run production`.Is this something related with the environment?\n\n\n","questionMetadata":{"type":"conceptual","level":"beginner","tag":"php"},"answer":"`npm run dev` creates a source map and doesn't minify your js\/css which makes it easier to debug and find errors out\n\n\n`npm run production` on the other hand does not create a source map and minifies all your js\/css files so that they are ready for production and are faster to read by the system.\n\n\nGenerally you would want to use `npm run dev` when you're developing the site, and `npm run production` when it's ready to be deployed.\n\n\n"}
{"questionId":"721898b9e6494dce90247ef63763b79a","question":"Round number down the to the nearest hundredth using php\nI'm probably not looking hard enough but the common question about php rounding is rounding up, not down.\n\n\nFor example I am trying round this..\n\n\n\n```\n<?php $roundDown = 768; ?>\n\n```\n\nDown to..\n\n\n\n```\n<?php var_dump($roundDown) \/* 700 *\/ ?>\n\n```\n\nWhats the simplest method to do this, or is it because the number is closer to 800 that it's not technically rounding?\n\n\nWhats the function that I need to do this if it's not rounding?\n\n\nA little point in the right direction would be much appreciated.\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"php"},"answer":"You can try ceil() and floor() function.\n\n\n\n```\n echo floor(768 \/ 100) * 100;  \/\/ Output:700\n echo ceil(768 \/ 100) * 100;  \/\/ Output:800\n\n```\n\n"}
{"questionId":"edc107fb255a49de828ce8e37431f2cd","question":"print $PATH line by line\nThe usual code to check the $PATH variable `echo $PATH` often print out some long and cumbersome line like:\n`\/anaconda2\/bin:\/anaconda2\/condabin:\/usr\/local\/Cellar\/root\/6.16.00\/bin:~\/bin:\/usr\/local\/bin:\/usr\/bin:\/bin:\/usr\/sbin:\/sbin:\/Library\/TeX\/texbin:\/opt\/X11\/bin`  \n\nwhich has bad readability. Is there any way I can print all the paths line by line instead on a single line? Like this:\n\n\n\n```\n\/anaconda2\/bin\n\/anaconda2\/condabin\n\/usr\/local\/Cellar\/root\/6.16.00\/bin\n~\/bin\n\/usr\/local\/bin\n\/usr\/bin\n\/bin\n\/usr\/sbin\n\/sbin\n\/Library\/TeX\/texbin\n\/opt\/X11\/bin\n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"bash\/shell"},"answer":"Using `tr`:\n\n\n\n```\necho $PATH | tr : '\\n'\n\n```\n\n"}
{"questionId":"e1d6a540955943b688356884fd05ce86","question":"more than one character in rune literal\nI have a string as just `MyString` and I want to append in this data something like this:\n\n\n\n```\nMYString (\"1\", \"a\"), (\"1\", \"b\")  \/\/END result \n\n```\n\nMy code is something like this:\n\n\n\n```\n    query := \"MyString\"; \n    array := []string{\"a\", \"b\"}\n    \n    for i , v :=  range array{\n        id := \"1\" \n        fmt.Println(v,i)\n        query +=  '(\"{}\", \"{}\"), '.format(id, v)\n     }\n\n```\n\nbut I am getting two errors:\n\n\n\n```\n.\/prog.go:15:23: more than one character in rune literal\n.\/prog.go:15:39: '\\u0000'.format undefined (type rune has no field or method format)\n\n```\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"go"},"answer":"You can't use single quotes for Strings in Go. You can only use double-quotes or backticks.\nSingle quotes are used for single characters, called *runes*\n\n\nChange your line to:\n\n\n\n```\nquery +=  \"(\\\"{}\\\", \\\"{}\\\"), \".format(id, v)\n\n```\n\nor\n\n\n\n```\n query +=  `(\"{}\", \"{}\"), `.format(id, v)\n\n```\n\nHowever, Go is not python. Go doesn't have a `format` method like that. But it has `fmt.Sprintf`.\n\n\nSo to really fix it, use:\n\n\n\n```\nquery = fmt.Sprintf(`%s(\"%s\", \"%s\"), `, query, id, v)\n\n```\n\n"}
{"questionId":"aa8a3973a35547778e361d5276974f78","question":"Why does it make a difference if left and right shift are used together in one expression or not?\nI have the following code:\n\n\n\n```\nunsigned char x = 255;\nprintf(\"%x\\n\", x); \/\/ ff\n\nunsigned char tmp = x << 7;\nunsigned char y = tmp >> 7;\nprintf(\"%x\\n\", y); \/\/ 1\n\nunsigned char z = (x << 7) >> 7;\nprintf(\"%x\\n\", z); \/\/ ff\n\n```\n\nI would have expected `y` and `z` to be the same. But they differ depending on whether a intermediary variable is used. It would be interesting to know why this is the case.\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"c"},"answer":"This little test is actually more subtle than it looks as the behavior is implementation defined:\n\n\n- `unsigned char x = 255;` no ambiguity here, `x` is an `unsigned char` with value `255`, type `unsigned char` is guaranteed to have enough range to store `255`.\n- `printf(\"%x\\n\", x);` This produces `ff` on standard output but it would be cleaner to write `printf(\"%hhx\\n\", x);` as `printf` expects an `unsigned int` for conversion `%x`, which `x` is not. Passing `x` might actually pass an `int` or an `unsigned int` argument.\n- `unsigned char tmp = x << 7;` To evaluate the expression `x << 7`, `x` being an `unsigned char` first undergoes the *integer promotions* defined in the C Standard **6.3.3.1**: *If an `int` can represent all values of the original type (as restricted by the width, for a bit-field), the value is converted to an `int`; otherwise, it is converted to an `unsigned int`. These are called the integer promotions.*\n\n\nSo if the number of value bits in `unsigned char` is smaller or equal to that of `int` (the most common case currently being 8 vs 31), `x` is first promoted to an `int` with the same value, which is then shifted left by `7` positions. The result, `0x7f80`, is guaranteed to fit in the `int` type, so the behavior is well defined and converting this value to type `unsigned char` will effectively truncate the high order bits of the value. If type `unsigned char` has 8 bits, the value will be `128` (`0x80`), but if type `unsigned char` has more bits, the value in `tmp` can be `0x180`, `0x380`, `0x780`, `0xf80`, `0x1f80`, `0x3f80` or even `0x7f80`.\n\n\nIf type `unsigned char` is larger than `int`, which can occur on rare systems where `sizeof(int) == 1`, `x` is promoted to `unsigned int` and the left shift is performed on this type. The value is `0x7f80U`, which is guaranteed to fit in type `unsigned int` and storing that to `tmp` does not actually lose any information since type `unsigned char` has the same size as `unsigned int`. So `tmp` would have the value `0x7f80` in this case.\n- `unsigned char y = tmp >> 7;` The evaluation proceeds the same as above, `tmp` is promoted to `int` or `unsigned int` depending on the system, which preserves its value, and this value is shifted right by 7 positions, which is fully defined because `7` is less than the width of the type (`int` or `unsigned int`) and the value is positive. Depending on the number of bits of type `unsigned char`, the value stored in `y` can be `1`, `3`, `7`, `15`, `31`, `63`, `127` or `255`, the most common architecture will have `y == 1`.\n- `printf(\"%x\\n\", y);` again, it would be better t write `printf(\"%hhx\\n\", y);` and the output may be `1` (most common case) or `3`, `7`, `f`, `1f`, `3f`, `7f` or `ff` depending on the number of value bits in type `unsigned char`.\n- `unsigned char z = (x << 7) >> 7;` The integer promotion is performed on `x` as described above, the value (`255`) is then shifted left 7 bits as an `int` or an `unsigned int`, always producing `0x7f80` and then right shifted by 7 positions, with a final value of `0xff`. This behavior is fully defined.\n- `printf(\"%x\\n\", z);` Once more, the format string should be `printf(\"%hhx\\n\", z);` and the output would always be `ff`.\n\n\nSystems where bytes have more than 8 bits are becoming rare these days, but some embedded processors, such as specialized DSPs still do that. It would take a perverse system to fail when passed an `unsigned char` for a `%x` conversion specifier, but it is cleaner to either use `%hhx` or more portably write `printf(\"%x\\n\", (unsigned)z);`\n\n\nShifting by `8` instead of `7` in this example would be even more contrived. It would have undefined behavior on systems with 16-bit `int` and 8-bit `char`.\n\n\n"}
{"questionId":"ff188a5c383a4821b2abd3124331e37b","question":"Is there a way to create a CTE with values in postgres?\nI often just want to test out a function, or see what a variable might look like. This could be done by creating a temporary table, but I suspect that there is an easier way.\n\n\nBasically,\n\n\n\n```\nWITH cte1 AS (\n   SELECT \n      VALUES(1,2,3) AS temp_var1\n    , VALUES(4,5,6) AS temp_var2\n)\nSELECT \n    temp_var1\n  , temp_var2\n  , (temp_var1 + temp_var2) AS temp_var3\nFROM cte1\n\n```\n\nThis would return\n\n\n\n\n\n| temp\\_var1 | temp\\_var2 | temp\\_var3 |\n| --- | --- | --- |\n| 1 | 4 | 5 |\n| 2 | 5 | 7 |\n| 3 | 6 | 9 |\n\n\n\nDarn she's fine.\n\n\nNote I'm using PostgreSQL 9.2.15.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"sql"},"answer":"I think you are looking for\n\n\n\n```\nWITH cte1 (temp_var1, temp_var2) AS (\n  VALUES\n   (1,4),\n   (2,5),\n   (3,6)\n)\nSELECT \n    temp_var1\n  , temp_var2\n  , (temp_var1 + temp_var2) AS temp_var3\nFROM cte1\n\n```\n\n"}
{"questionId":"4c5f6e54d178446cb6a12c46771fcc15","question":"Trying to create room database with android, but keep getting dependency error\nAndroid.com says to add these dependencies to your gradle file. I keep getting an error on the lines that start with kapt and ksp saying could not find those methods. Any ideas? Sorry I am not smart.\n\n\n\/\/below is what android says to put in\n\n\n\n```\ndef room_version = \"2.3.0\"\n\nimplementation(\"androidx.room:room-runtime:$room_version\")\nannotationProcessor \"androidx.room:room-compiler:$room_version\"\n\n\/\/ To use Kotlin annotation processing tool (kapt)\nkapt(\"androidx.room:room-compiler:$room_version\")\n\/\/ To use Kotlin Symbolic Processing (KSP)\nksp(\"androidx.room:room-compiler:$room_version\")\n\n\/\/ optional - Kotlin Extensions and Coroutines support for Room\nimplementation(\"androidx.room:room-ktx:$room_version\")\n\n\/\/ optional - RxJava2 support for Room\nimplementation \"androidx.room:room-rxjava2:$room_version\"\n\n\/\/ optional - RxJava3 support for Room\nimplementation \"androidx.room:room-rxjava3:$room_version\"\n\n\/\/ optional - Guava support for Room, including Optional and \nListenableFuture\nimplementation \"androidx.room:room-guava:$room_version\"\n\n\/\/ optional - Test helpers\ntestImplementation(\"androidx.room:room-testing:$room_version\")\n\n\/\/ optional - Paging 3 Integration\nimplementation(\"androidx.room:room-paging:2.4.0-alpha04\")\n\n```\n\nBuild file 'C:\\Users\\tanne\\AndroidStudioProjects\\LazyAlarm\\app\\build.gradle' line: 51\n\n\nA problem occurred evaluating project ':app'.\n\n\n\n> \n> Could not find method kapt() for arguments [androidx.room:room-compiler:2.3.0] on object of type org.gradle.api.internal.artifacts.dsl.dependencies.DefaultDependencyHandler.\n> \n> \n> \n\n\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\/\/gradle file below\n\n\n\n```\nplugins {\nid 'com.android.application'\nid 'kotlin-android'\nid 'kotlin-android-extensions'\n}\n\nandroid {\ncompileSdk 31\n\ndefaultConfig {\n    applicationId \"com.example.lazyalarm\"\n    minSdk 22\n    targetSdk 31\n    versionCode 1\n    versionName \"1.0\"\n\n\n    testInstrumentationRunner \"androidx.test.runner.AndroidJUnitRunner\"\n}\n\nbuildTypes {\n    release {\n        minifyEnabled false\n        proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'\n    }\n}\ncompileOptions {\n    sourceCompatibility JavaVersion.VERSION_1_8\n    targetCompatibility JavaVersion.VERSION_1_8\n}\nkotlinOptions {\n    jvmTarget = '1.8'\n}\n}\n\n dependencies {\n\nimplementation 'androidx.core:core-ktx:1.6.0'\nimplementation 'androidx.appcompat:appcompat:1.3.1'\nimplementation 'com.google.android.material:material:1.4.0'\nimplementation 'androidx.room:room-ktx:2.3.0'\ntestImplementation 'junit:junit:4.13.2'\nandroidTestImplementation 'androidx.test.ext:junit:1.1.3'\nandroidTestImplementation 'androidx.test.espresso:espresso-core:3.4.0'\ndef room_version = \"2.3.0\"\n\nimplementation(\"androidx.room:room-runtime:$room_version\")\nannotationProcessor \"androidx.room:room-compiler:$room_version\"\n\n\/\/ To use Kotlin annotation processing tool (kapt)\nkapt(\"androidx.room:room-compiler:$room_version\")\n\/\/ To use Kotlin Symbolic Processing (KSP)\nksp(\"androidx.room:room-compiler:$room_version\")\n\n\/\/ optional - Kotlin Extensions and Coroutines support for Room\nimplementation(\"androidx.room:room-ktx:$room_version\")\n\n\/\/ optional - RxJava2 support for Room\nimplementation \"androidx.room:room-rxjava2:$room_version\"\n\n\/\/ optional - RxJava3 support for Room\nimplementation \"androidx.room:room-rxjava3:$room_version\"\n\n\/\/ optional - Guava support for Room, including Optional and ListenableFuture\nimplementation \"androidx.room:room-guava:$room_version\"\n\n\/\/ optional - Test helpers\ntestImplementation(\"androidx.room:room-testing:$room_version\")\n\n\/\/ optional - Paging 3 Integration\nimplementation(\"androidx.room:room-paging:2.4.0-alpha04\")\n\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"kotlin"},"answer":"You appear to have copied a bunch of lines into your `build.gradle` file verbatim from Google's documentation. Unfortunately, their documentation has issues. In particular, many of those lines represent *choices*, and you need to choose which ones you need. In particular, pick exactly one of these two lines:\n\n\n\n```\nkapt(\"androidx.room:room-compiler:$room_version\")\nksp(\"androidx.room:room-compiler:$room_version\")\n\n```\n\n...and pick one of these four lines:\n\n\n\n```\nimplementation(\"androidx.room:room-ktx:$room_version\")\nimplementation \"androidx.room:room-rxjava2:$room_version\"\nimplementation \"androidx.room:room-rxjava3:$room_version\"\nimplementation \"androidx.room:room-guava:$room_version\"\n\n```\n\nThen, depending on your choice from that first pair, you may need to add another plugin to your `plugins` block at the top of the file. If you choose the `kapt` line, you need to add `id 'kotlin-kapt'`. I have not used the `ksp` option and do not know what plugin you need for it, if any.\n\n\n"}
{"questionId":"73cdac38dace41f7a00e846f004cb0e9","question":"In Dart, what different about List.unmodifiable() and UnmodifiableListView?\nIn Dart, if want to create a unmodifiable list, it can use List.unmodifiable() or UnmodifiableListView\n\n\n\n```\nList<int> list = [1, 2, 3];\nList<int> unmodifiableList = List.unmodifiable(list);\nUnmodifiableListView unmodifiableListView = UnmodifiableListView(list);\n\n```\n\nwhat different about this?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"dart"},"answer":"`List.unmodifiable` is a `List` constructor; it creates a new `List` object. It creates a *copy* of the original `List`, and that copy cannot be mutated. Mutating the original `List` will not affect the copy.\n\n\n`UnmodifiableListView` is a wrapper (a \"view\") around the original `List`, and the original cannot be mutated through the `UnmodifiableListView`. Mutations to the original `List` can still be observed in the `UnmodifiableListView`.\n\n\nFor example:\n\n\n\n```\nimport 'dart:collection';\n\nvoid main() {\n  var originalList = [1, 2, 3];\n  var unmodifiableCopy = List<int>.unmodifiable(originalList);\n  var unmodifiableView = UnmodifiableListView(originalList);\n  \n  originalList[0] = -1;\n  print('$unmodifiableCopy'); \/\/ Prints: [1, 2, 3]\n  print('$unmodifiableView'); \/\/ Prints: [-1, 2, 3]\n}\n\n```\n\n"}
{"questionId":"a3204fa03d9a423ca6c890294f8c3bfe","question":"Is it possible to clone a polymorphic object without manually adding overridden clone method into each derived class in C++?\nThe typical pattern when you want to copy a polymorphic class is adding a virtual clone method and implement it in each derived class like this:\n\n\n\n```\nBase* Derived::clone()\n{\n    return new Derived(*this);\n}\n\n```\n\nThen in a calling code you can:\n\n\n\n```\nBase *x = new Derived();\nBase *y = x->clone();\n\n```\n\nHowever if you have 50+ derived classes and realize you need polymorphic copy, it's tedious to copy-paste the cloning method into each of them. And it's essentially a boilerplate that works around a language limitation that you have to spell out the actual name to call the constructor.\n\n\nI haven't keep track with the new features in recent C++ standards... Is there a way to avoid this in modern C++?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c++"},"answer":"You can use this generic CRTP code \n\n\n\n```\ntemplate <class Derived, class Base>\nstruct Clonable : Base {\n    virtual Base* do_clone() {\n        return new Derived(*static_cast<Derived*>(this));\n    }\n    Derived* clone() { \/\/ not virtual\n        return static_cast<Derived*>(do_clone());\n    }\n\n    using Base::Base;\n};\n\nstruct empty {};\nstruct A : Clonable<A, empty> {};\nstruct B : Clonable<B, A> {};\n\n```\n\nIt can be generalised to smart pointers and multiple bases if desired.\n\n\n"}
{"questionId":"3c0a1d3d98fd4276abbe87471656cefb","question":"Error with Redux DevTools Extension using TS: \"Property '\\_\\_REDUX\\_DEVTOOLS\\_EXTENSION\\_COMPOSE\\_\\_' does not exist on type 'Window'.\"?\nI'm getting this error on my index.tsx. \n\n\nProperty '**REDUX\\_DEVTOOLS\\_EXTENSION\\_COMPOSE**' does not exist on type 'Window'.\n\n\nHere is my index.tsx code: \n\n\n\n```\nimport * as React from 'react';\nimport * as ReactDOM from 'react-dom';\nimport App from '.\/App';\nimport '.\/index.css';\nimport registerServiceWorker from '.\/registerServiceWorker';\n\nimport { Provider } from 'react-redux';\n\nimport { createStore, compose, applyMiddleware } from 'redux';\nimport rootReducer from '.\/store\/reducers';\n\nimport thunk from 'redux-thunk';\n\nconst composeEnhancers = window.__REDUX_DEVTOOLS_EXTENSION_COMPOSE__ || compose;\n\n const store = createStore(rootReducer, composeEnhancers(\n     applyMiddleware(thunk)\n ));\n\nReactDOM.render(  <Provider store={store}><App \/><\/Provider>, document.getElementById('root'));\n\nregisterServiceWorker();\n\n```\n\nI've installed @types\/npm install --save-dev redux-devtools-extension and I'm using create-react-app-typescript. Thanks alot for any tips for what's going on in advance.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"This is how you can use `redux-dev-tools` in a typescript react application.\n\n\n### Create global Interface for the `Window` object:\n\n\n\n```\ndeclare global {\n  interface Window {\n    __REDUX_DEVTOOLS_EXTENSION_COMPOSE__?: typeof compose;\n  }\n}\n\n```\n\n### Then, create `composeEnhancers` as follows:\n\n\n\n```\nconst composeEnhancers = window.__REDUX_DEVTOOLS_EXTENSION_COMPOSE__ || compose;\n\n```\n\n### Then create a `store`.\n\n\n\n```\nconst store = createStore(rootReducers, composeEnhancers());\n\n```\n\n**`rootReducers`** - in my case refers to `combinedReducers` created in a surparate file.\n\n\nNow you can use `Provider` as usual in `React.js` as:\n\n\n\n```\nReactDOM.render(\n  <React.StrictMode>\n    <Provider store={store}>\n      <App \/>\n    <\/Provider>\n  <\/React.StrictMode>,\n  document.getElementById(\"root\")\n);\n\n```\n\n### All the code in the `index.tsx`\n\n\n\n```\nimport React from \"react\";\nimport ReactDOM from \"react-dom\";\nimport \".\/index.css\";\nimport App from \".\/App\";\nimport reportWebVitals from \".\/reportWebVitals\";\nimport rootReducers from \".\/reducers\";\n\nimport { Provider } from \"react-redux\";\nimport { createStore, compose, applyMiddleware } from \"redux\";\n\ndeclare global {\n  interface Window {\n    __REDUX_DEVTOOLS_EXTENSION_COMPOSE__?: typeof compose;\n  }\n}\n\nconst composeEnhancers = window.__REDUX_DEVTOOLS_EXTENSION_COMPOSE__ || compose;\nconst store = createStore(rootReducers, composeEnhancers());\n\nReactDOM.render(\n  <React.StrictMode>\n    <Provider store={store}>\n      <App \/>\n    <\/Provider>\n  <\/React.StrictMode>,\n  document.getElementById(\"root\")\n);\nreportWebVitals();\n\n\n```\n\n"}
{"questionId":"781ddb64d87e4eeea45335a5f5cb2a7a","question":"Is there a way to dynamically generate enums on TypeScript based on Object Keys?\nI'm defining an object and I want to dynamically generate enum based on its keys, so I get IDE suggestions and do not call wrong keys.\n\n\n\n```\nconst appRoutes = {\n   Login,\n   Auth,\n   NotFound\n} \n\nenum AppRoutes = {[Key in keyof appRoutes]: [keyof appRoutes]}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"You can't build an actual enum from the object keys. \n\n\nYou can get a union of all keys with just `keyof typeof appRoutes` and that will have the type safe effect you want:\n\n\n\n```\ntype AppRoutes = keyof typeof appRoutes\n\nlet ok: AppRoutes = \"Auth\";\nlet err: AppRoutes = \"Authh\";\n\n```\n\nAn enum is not just a type though, it's also a runtime object that contains the keys and values of the enum. Typescript does not offer a way to automatically create such an object from a string union. We can however create a type that will ensure that the keys of an object and the members of the union stay in sync and we get a compiler error if t hey are not in sync:\n\n\n\n```\ntype AppRoutes = keyof typeof appRoutes\nconst AppRoutes: { [P in AppRoutes]: P } = {\n    Auth : \"Auth\",\n    Login: \"Login\",\n    NotFound: \"NotFound\" \/\/ error if we forgot one \n    \/\/ NotFound2: \"NotFound2\" \/\/ err\n}\nlet ok: AppRoutes = AppRoutes.Auth;\n\n```\n\n"}
{"questionId":"05415e67d365494487c438c39d323f4b","question":"PreFlight Request 404 not found .net web api ; response to preflight request doesn't pass access control check: it does not have http ok status\nSo I want to enable CORS on my .net web API but the client application keeps getting 404 for the options request and a second error: \n\n\n\n```\nHas been blocked by CORS policy: Response to preflight request doesn\u2019t pass access control check: it does not have http ok status\n\n```\n\nI have followed step by step what Microsoft website outline and I am still unsuccessful.\n\n\n\n\n\n```\npublic static class WebApiConfig\r\n    {\r\n        public static void Register(HttpConfiguration config)\r\n        {\r\n            \/\/ Added this line to my WebApiConfig.cs file\r\n            var cors = new EnableCorsAttribute(\"*\", \"*\", \"*\");\r\n            config.EnableCors(cors);\r\n\r\n            config.Routes.MapHttpRoute(\r\n                name: \"DefaultApi\",\r\n                routeTemplate: \"api\/{controller}\/{id}\",\r\n                defaults: new { id = RouteParameter.Optional }\r\n            );\r\n        }\r\n    }\r\n\r\n\r\nI also tried it by adding the headers in the Web.config in the httpProtocol tag file like so: \r\n\r\n<httpProtocol>\r\n  <customHeaders>\r\n    <add name=\"Access-Control-Allow-Origin\" value=\"*\" \/>\r\n    <add name=\"Access-Control-Allow-Headers\" value=\"*\" \/>\r\n    <add name=\"Access-Control-Allow-Methods\" value=\"*\" \/>\r\n  <\/customHeaders>\r\n<\/httpProtocol>\r\n    \n```\n\n\n\n\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"\n\n\n```\nTry to enable from the web.config file under <system.webserver><\/system.webserver> like so:\r\n\r\n<httpProtocol>\r\n    <customHeaders>\r\n        <add name=\"Access-Control-Allow-Origin\" value=\"*\" \/>\r\n        <add name=\"Access-Control-Allow-Credentials\" value=\"true\"\/>\r\n        <add name=\"Access-Control-Allow-Headers\" value=\"Content-Type\" \/>\r\n        <add name=\"Access-Control-Allow-Methods\" value=\"GET, POST, PUT, OPTIONS\" \/>\r\n    <\/customHeaders>\r\n<\/httpProtocol>\r\n\r\nTo handle the preflight request error add the line below in the <handlers><\/handlers> tags:\r\n\r\n<add name=\"OPTIONSVerbHandler\" path=\"*\" verb=\"OPTIONS\" modules=\"ProtocolSupportModule\" requireAccess=\"None\" responseBufferLimit=\"4194304\" \/>\r\n\r\n\r\nThen in the  Global.asax file, add the following method:\r\n\r\nprotected void Application_BeginRequest(object sender, EventArgs e)\r\n{\r\n  if (HttpContext.Current.Request.HttpMethod == \"OPTIONS\")\r\n  {\r\n     HttpContext.Current.Response.Flush();\r\n  }\r\n}\n```\n\n\n\n\n\n\n"}
{"questionId":"f810219f28664c289bf964d73078ec21","question":"getline() vs. fgets(): Control memory allocation\nTo read lines from a file there are the `getline()` and `fgets()` POSIX functions (ignoring the dreaded `gets()`). It is common sense that `getline()` is preferred over `fgets()` because it allocates the line buffer as needed.\n\n\nMy question is: Isn\u2019t that dangerous? What if by accident or malicious intent someone creates a 100GB file with no `'\\n'` byte in it \u2013 won\u2019t that make my `getline()` call allocate an insane amount of memory?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"\n> \n> My question is: Isn\u2019t that dangerous? What if by accident or malicious\n>  intent someone creates a 100GB file with no '\\n' byte in it \u2013 won\u2019t\n>  that make my getline() call allocate an insane amount of memory?\n> \n> \n> \n\n\nYes, what you describe is a plausible risk. However,\n\n\n- if the program requires loading an entire line into memory at once, then allowing `getline()` to attempt to do that is not inherently more risky than writing your own code to do it with `fgets()`; and\n- if you have a program that has such a vulnerability, then you can mitigate the risk by using `setrlimit()` to limit the total amount of (virtual) memory it can reserve. This can be used to cause it to fail instead of successfully allocating enough memory to interfere with the rest of the system.\n\n\nBest overall, I'd argue, is to write code that does not require input in units of full lines (all at once) in the first place, but such an approach has its own complexities.\n\n\n"}
{"questionId":"42d64dfab4244871b4abea6d18c72787","question":"Jenkins parameters with powershell\nhow to pass the jenkins string parameters to power shell script param value?\n\n\nI have to pass the source path url to the powershell script param value.\nGiven the Source path as string parameter in jenkins. I want to know how to pass the jenkins parameter value into powershell script param value.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"Every parameter in a job is available to PowerShell as an environment variable, so if your parameter is named i.e. `SourcePath` you can write it in PowerShell as `${env:SourcePath}`\n\n\n"}
{"questionId":"5c1fdb46004c4983b9e68d4a51a57370","question":"How do you use Go 1.16 embed features in subfolders\/packages?\nGo 1.16 is out and I want to use the new embed features. I can get it to work if everything is in the main package. But it's not clear how to handle accessing resources from subfolders\/packages. Trying to do it with embed.FS support.\n\n\ne.g. I have a main.go and also an HTTP handler in a handlers package\/folder\n\n\nIf I put the handler in the main, it works. If I put it in the handlers package, it can't find the templates. I get:\n\n\n`handlers\/indexHandler.go:11:12: pattern templates: no matching files found exit status 1`\n\n\nSimilarly, I can get it to serve an image from the static folder if I serve it from \/. But I can't serve both a handler from \/ and the static\/images from \/. If I put images on \/static\/ it can't find the images.\n\n\nI think it has to do with relative paths. But I can't find the right combination through trial and error... Could it be too early to rely on these features?\n\n\nPreviously I was using go-rice and I did not have these problems. But I would like to use the std library as much as possible.\n\n\nmain.go:\n\n\n\n```\npackage main\n\nimport (...)\n\n\/\/go:embed static\nvar static embed.FS\n\nfunc main() {\n\n    fsRoot, _ := fs.Sub(static, \"static\")\n    fsStatic := http.FileServer(http.FS(fsRoot))\n    http.Handle(\"\/\", fsStatic)\n    http.HandleFunc(\"\/index\", Index)\n    http.ListenAndServe(\":8080\", nil)\n}\n\n```\n\nhandlers\/indexHandler.go:\n\n\n\n```\npackage handlers\n\nimport (...)\n\n\/\/go:embed templates\nvar templates embed.FS\n\n\/\/ Index handler\nfunc Index(w http.ResponseWriter, r *http.Request) {\n\n    tmpl := template.New(\"\")\n    var err error\n    if tmpl, err = tmpl.ParseFS(templates, \"simple.gohtml\"); err != nil {\n        fmt.Println(err)\n    }\n    if err = tmpl.ExecuteTemplate(w, \"simple\", nil); err != nil {\n        log.Print(err)\n    }    \n}\n\n```\n\nStructure is as follows...\n\n\n\n```\n.\n\u251c\u2500\u2500 go.mod\n\u251c\u2500\u2500 go.sum\n\u251c\u2500\u2500 handlers\n\u2502   \u2514\u2500\u2500 indexHandler.go\n\u251c\u2500\u2500 main.go\n\u251c\u2500\u2500 static\n\u2502   \u251c\u2500\u2500 css\n\u2502   \u2502   \u2514\u2500\u2500 layout.css\n\u2502   \u2514\u2500\u2500 images\n\u2502       \u2514\u2500\u2500 logo.png\n\u2514\u2500\u2500 templates\n    \u2514\u2500\u2500 simple.gohtml\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"go"},"answer":"I finally figured it out...\n\n\nYou can keep the templates folder in the main folder and embed them from there. Then you need to inject the FS variable into the other handler package. It's always easy after you figure it out.\n\n\ne.g.\n\n\n\n```\npackage main\n\n\/\/go:embed templates\/*\nvar templateFs embed.FS\n\nfunc main() {\n    handlers.TemplateFs = templateFs\n...\n\n```\n\n\n```\npackage handlers\n\nvar TemplateFs embed.FS\n\nfunc handlerIndex() {\n    ...\n    tmpl, err = tmpl.ParseFS(TemplateFs, \"templates\/layout.gohtml\",...\n...\n\n```\n\n"}
{"questionId":"c9a45b78968647858bc54329403c2c85","question":"object is not subscriptable using django and python\nI am having this error `TypeError: 'StudentSubjectGrade' object is not subscriptable` of course the data filtered is exist in the database, and i am sure that the filter is correct. what should i do to correct this ?\n\n\n*note: this is recycle question, please dont mind the comment below,*\n\n\n\n```\ndef SummaryPeriod(request):\n    period = request.GET.get('period')\n\n    subject = request.GET.get('subject')\n    teacher = request.GET.get('teacher')\n    print(period, \"period\", \"subject\", subject)\n    cate = gradingCategories.objects.all()\n\n    students = StudentSubjectGrade.objects.filter(\n        grading_Period=period).filter(\n        Subjects=subject).filter(\n        Teacher = teacher\n    )\n\n    print(students)\n\n    Categories = list(cate.values_list('id', flat=True).order_by('id'))\n\n    table = []\n    student_name = None\n    table_row = None\n    columns = len(Categories) + 1\n\n    table_header = ['Student Names']\n\n    table_header.extend(list(cate.values('CategoryName', 'PercentageWeight')))\n\n    table.append(table_header)\n\n    for student in students:\n        if not student['Students_Enrollment_Records__Students_Enrollment_Records__Student_Users__Lastname'] + ' ' + \\\n               student[\n                   'Students_Enrollment_Records__Students_Enrollment_Records__Student_Users__Firstname'] == student_name:\n\n            if not table_row is None:\n                table.append(table_row)\n\n            table_row = [None for d in range(columns)]\n\n            student_name = student[\n                               'Students_Enrollment_Records__Students_Enrollment_Records__Student_Users__Lastname'] + ' ' + \\\n                           student['Students_Enrollment_Records__Students_Enrollment_Records__Student_Users__Firstname']\n            table_row[0] = student_name\n\n            id = student['id']\n            table_row.append(id)\n        table_row[Categories.index(student['Grading_Categories']) + 1] = student['Average'] * student[\n            'Grading_Categories__PercentageWeight'] \/ 100\n\n    table.append(table_row)\n\n    return render(request, 'Homepage\/summaryPeriod.html',\n                  {'table': table, \"teacher\": teacher, \"subject\": subject, \"period\": period})\n\n```\n\nthis is my traceback\n\n\n\n```\nInternal Server Error: \/SummaryPeriod\/\nTraceback (most recent call last):\n  File \"C:\\Users\\USER\\Desktop\\venv\\lib\\site-packages\\django\\core\\handlers\\exception.py\", line 47, in inner\n    response = get_response(request)\n  File \"C:\\Users\\USER\\Desktop\\venv\\lib\\site-packages\\django\\core\\handlers\\base.py\", line 179, in _get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"C:\\Users\\Desktop\\Homepage\\views.py\", line 2693, in SummaryPeriod\n    if not student['Students_Enrollment_Records__Students_Enrollment_Records__Student_Users__Lastname'] + ' ' + \\\nTypeError: 'StudentSubjectGrade' object is not subscriptable\n[01\/Dec\/2020 21:21:01] \"GET \/SummaryPeriod\/?period=3&subject=18&teacher=5 HTTP\/1.1\" 500 70398\n\n```\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"python"},"answer":"\n```\nTypeError: 'StudentSubjectGrade' object is not subscriptable\n\n```\n\nthis means that `student` is not a dictionary, you cannot use `student['key']` to get what you want.\n\n\nyou should use `student.sth` instead.\n\n\n"}
{"questionId":"c580cdad3e504806b8394ec040bce318","question":"Disambiguation of sizeof\nWhen using the `sizeof` I always enclose it in parentheses, as it's a bit easier for me to read, even if I can sometimes omit it, in the first case below\n\n\n\n```\nsizeof unary-expression\nsizeof ( type-name )\n\n```\n\nMy question is how does the parentheses disambiguate things to the compiler? What would be an example where something like:\n\n\n\n```\nsizeof char\n\n```\n\nWould be ambiguous to a compiler?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"If `sizeof` *type-name* were allowed, then `sizeof char * + 3` could be either:\n\n\n- `(sizeof (char *)) + 3`, which is the size of a `char *` added to `3` or\n- `(sizeof (char)) * (+ 3)`, which is the size of a `char` multiplied by `+ 3`.\n\n\nBoth of those would be valid parsings and fully defined by the standard (aside from the implementation-defined size of the pointer). So accepting `sizeof` *type-name* creates an ambiguity not resolved by the grammar or semantics.\n\n\n## Earlier Example\n\n\nIf `sizeof` *type-name* were allowed, then `sizeof char [x]` could be either `(sizeof (char)) [x]` (which is a valid expression if `x` is a pointer or array; the subscript operator accepts `index[array]`) or `sizeof (char [x])` (which is a valid expression if `x` is an integer; it is the size of an array of `x` elements of `char`). Further, the grammar would provide no way to distinguish these; both would be valid parsings. Semantic rules could distinguish them based on the type of `x`, but then you have to parse before you can evaluate the semantic rules and would need some way for the compiler to undo the parsing.\n\n\n"}
{"questionId":"419e3395f68c441ea8d539b2caa52f90","question":"Visual Studio Code (Mac OS) rename symbol doesn't work\nWhen I right click and try to rename a variable name in Microsoft Visual Studio Code on Mac OS Mojave, it prompts for the new name, I hit enter and nothing happens. \n\n\nI have Python extension and Latex extension installed.\n\n\nUsually there are no errors, no nothing. \n\n\nSometimes, there's a little box that pops up saying \"No Result. No Result\".\n\n\nThe python interpreter I selected was a Conda install.\n\n\nI ensured rope, and pylint were installed.\n\n\nExpected behaviour:\nright click > rename symbol > type new name > enter > all instances of variable renamed.\n\n\nObserved behaviour\nright click > rename symbol > type new name > enter > variable has same name everywhere, including spot of renaming.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"It turns out the solution was simple, but frustrating. To do refactoring in VS Code (at least for python) you need to be in a workspace. \n\n\nI solved the problem by first closing the open folder I was in:\n\n\nFile > Close Folder.\n\n\nthen navigating to a sub folder of .py file I was editing, and opening it. In my case it was\n\n\n1. Click the little document icon in the upper left of screen\n2. In the welcome screen, choose \"open folder\"\n3. navigate to my desktop folder (where my .py file is)\n\n\nNow refactoring works as expected\n\n\nClick on file icon in upper left of screen > in the welcome screen \n\n\n"}
{"questionId":"c6bfdacd9f80455486d40737fb27a0d2","question":"Function interface in Kotlin 1.4\nThis feature will be coming `Kotlin 1.4`. Here is an excerpt from `KotlinConf'19`.\n\n\n\n```\nfun interface Action {\n    fun run()\n}\n\nfun runAction(a: Action) = a.run()\n\nrunAction{\n    println(\"Hello\")\n}\n\n```\n\nIt looks nice, but I still don't know what it does.\n\n\nWhat is the function interface? What is its practical value? What specific scenarios can it be used for?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"kotlin"},"answer":"This is about *functional interfaces* \u2014 interfaces with a Single Abstract Method (also called SAM interfaces).\n\n\nTo understand the point, I'll need to cover a little history\u2026\u00a0 In Java, lambdas were added relatively recently.\u00a0 Before that, you implemented callbacks and similar by implementing a suitable interface.\u00a0 For example, if you wanted to be informed when an AWT component was actioned, you'd create an object which implemented the `ActionListener` interface.\u00a0 That has a single method (called `actionPerformed()`); you'd put your code inside that method:\n\n\n\n```\nmyButton.addActionListener(new ActionListener() {\n    public void actionPerformed(ActionEvent e) {\n        \/\/ Do something\n    }\n});\n\n```\n\nWhen they added lambdas, they wanted to blend in with all the existing code, and change as little as possible, so they did it exactly the same way: the compiler infers which interface you're implementing, and creates an object implementing that interface.\u00a0 You could write:\n\n\n\n```\nmyButton.addActionListener(e -> {\n    \/\/ Do something\n});\n\n```\n\nwhich is shorter \u2014 but it compiles down to pretty much the same as the first example.\n\n\nSo in Java, functions are not first-class objects; lambdas are simply a more concise way to implement functional interfaces.\n\n\nIn Kotlin, however, functions *are* first-class objects: you can write a lambda (or an anonymous function) on its own, assign it, pass it to functions, return it from functions, and so on \u2014 so there's no need for SAM interfaces at all!\n\n\nFor easier interoperability with Java, Kotlin lets you easily implement Java SAM interfaces, in the same way you do from Java:\n\n\n\n```\nmyButton.addActionListener {\n    \/\/ Do something\n}\n\n```\n\nBut Kotlin <= 1.3 doesn't let you implement Kotlin interfaces that way; you need to implement those explicitly.\u00a0 (I suspect this was partly to encourage developers to use proper functions, with all their other advantages, and not rely on the Java-style workaround.)\n\n\nYour example illustrates this.\u00a0 It has an interface (`Action`) with one abstract method (`run()`).\u00a0 It has a function (`runAction()`) which takes an instance of that interface.\u00a0 And it has some code which wants to call that function, passing just the code for the `run()` method.\n\n\nIn Kotlin <= 1.3, you'd have to do the latter explicitly, e.g.:\n\n\n\n```\nrunAction(object : Action {\n    override fun run() {\n        println(\"Hello\")\n    }\n})\n\n```\n\nBut from Kotlin 1.4, you can mark the interface as `fun interface`, and use the Java-style shortcut, as in your example.\n\n\n(You may or may not think this is a good thing\u2026)\n\n\n"}
{"questionId":"02d4665857a94f7fa7e7deb5a66e1b2b","question":"How do you serve static files with chi router\nI just started using chi for smaller projects, and I was curious how static file serving is handled here. What's the shortest way to accomplish it?\n\n\nHere's what I tried for file serving\n\n\n\n```\nfs := http.FileServer(http.Dir(\"static\"))\nrouter.Handle(\"\/static\", http.StripPrefix(\"\/static\/\", fs))\n\n```\n\nHowever, this did not work, so I tried this example straight from their repo:\n\n\n\n```\nfunc fileServer(r chi.Router, serverRoute string, pathToStaticFolder http.FileSystem) {\n    if strings.ContainsAny(serverRoute, \"{}*\") {\n        panic(\"FileServer does not permit any URL parameters.\")\n    }\n\n    if serverRoute != \"\/\" && serverRoute[len(serverRoute)-1] != '\/' {\n        r.Get(serverRoute, http.RedirectHandler(serverRoute+\"\/\", 301).ServeHTTP)\n        serverRoute += \"\/\"\n    }\n    serverRoute += \"*\"\n\n    r.Get(serverRoute, func(w http.ResponseWriter, r *http.Request) {\n        rctx := chi.RouteContext(r.Context())\n        serverRoutePrefix := strings.TrimSuffix(rctx.RoutePattern(), \"\/*\")\n        fs := http.StripPrefix(serverRoutePrefix, http.FileServer(pathToStaticFolder))\n        fs.ServeHTTP(w, r)\n    })\n}\n\ndir, _ := os.Getwd()\nfilesDir := http.Dir(filepath.Join(dir, \"static\"))\nfileServer(router, \"\/\", filesDir)\n\n```\n\nUnfortunately, neither way works, and I'm currently at a loss. Anyone who's ever used Chi as their router, how do you serve static files?\n\n\nHere's my folder structure\n\n\n\n```\nproject\n  |_ main.go\n  |_ static\n      |_css\n         |_index.css\n      |_img\n      |_js\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"go"},"answer":"You should modify the path registered in the router to\n\n\n\n```\nrouter.Handle(\"\/static\/*\", http.StripPrefix(\"\/static\/\", fs))\n\n```\n\nThe `*` represent different file names.\n\n\n"}
{"questionId":"d8371074e8e4458b81b2f159d6430d3a","question":"Testing a React component with Enzyme. Typescript can't find instance methods\nI want to test a React class component.\n\n\nLet's say I have a method in my class that computes something based on the current state and props.\n\n\n\n```\nimport Component from '.\/Component'\n\nconst wrapper = enzyme.shallow(<Component {...props} \/>);\n\nit('does something', () => {\n  expect(wrapper.instance().someInstanceMethod(input)).toBe(true);\n});\n\n```\n\nTypescript says `Property 'someInstanceMethod' is not defined on type Component<any, any>`. How can I tell Typscript how my class is looking and what methods it has?\n\n\nIs there a good example for this?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"You can set the component type in the call to `shallow`. This is a little bit of boilerplate, but it makes things typesafe. The good thing is that the wrapper is typesafe, not just the instance you pull out.\n\n\n\n```\nimport Component from '.\/Component'\n\n\/\/ Wrapper will know the type of the component.\nconst wrapper = enzyme.shallow<Component>(<Component {...props} \/>);\n\nit('does something', () => {\n  expect(wrapper.instance().someInstanceMethod(input)).toBe(true);\n  \/\/ You can also get the state from the wrapper.\n  expect(wrapper.state().someComponentState).toBeTruthy();\n});\n\n```\n\n"}
{"questionId":"01d554d6a0334034a38725bc2a23cc34","question":"Angular - How to fix \"ERROR TS2322: Type 'ModuleWithProviders' is not assignable to type 'any[] | Type'\"\nI'm trying to create a webapp using Angular, but at the time of compilation I get this error which I can't solve:\n\n\n\n> \n> ERROR in src\/app\/app.module.ts(22,5): error TS2322: Type\n>  'ModuleWithProviders' is not assignable to type 'any[] |\n>  Type'. Type 'ModuleWithProviders' is missing the following\n>  properties from type 'Type': apply, call, bind, prototype, and 5\n>  more.\n> \n> \n> \n\n\nThat's my app.module.ts:\n\n\n\n```\nimport { BrowserAnimationsModule } from '@angular\/platform-browser\/animations';\nimport { NgModule } from '@angular\/core';\nimport { FormsModule } from '@angular\/forms';\nimport { HttpClientModule } from '@angular\/common\/http';\nimport { RouterModule } from '@angular\/router';\nimport { NgbModule } from '@ng-bootstrap\/ng-bootstrap';\nimport { ToastrModule } from 'ngx-toastr';\nimport { AppRoutingModule } from '.\/app-routing.module';\nimport { ComponentsModule } from '.\/components\/components.module';\nimport { AppComponent } from '.\/app.component';\nimport { AdminLayoutComponent } from '.\/layouts\/admin-layout\/admin-layout.component';\n\n@NgModule({\n  declarations: [\n    BrowserAnimationsModule,\n    FormsModule,\n    HttpClientModule,\n    ComponentsModule,\n    RouterModule,\n    AppRoutingModule,\n    NgbModule,\n    ToastrModule.forRoot()\n  ],\n  imports: [\n    AppComponent,\n    AdminLayoutComponent\n  ],\n  providers: [],\n  bootstrap: [AppComponent]\n})\nexport class AppModule { }\n\n```\n\nThe error is reported on the line: **ToastrModule.forRoot()**\n\n\nAnd that's my package.json:\n\n\n\n```\n{\n  \"name\": \"pum-pum-pum\",\n  \"version\": \"0.0.0\",\n  \"scripts\": {\n    \"ng\": \"ng\",\n    \"start\": \"ng serve\",\n    \"build\": \"ng build\",\n    \"test\": \"ng test\",\n    \"lint\": \"ng lint\",\n    \"e2e\": \"ng e2e\"\n  },\n  \"private\": true,\n  \"dependencies\": {\n    \"@angular\/animations\": \"~7.2.0\",\n    \"@angular\/common\": \"~7.2.0\",\n    \"@angular\/compiler\": \"~7.2.0\",\n    \"@angular\/core\": \"~7.2.0\",\n    \"@angular\/forms\": \"~7.2.0\",\n    \"@angular\/platform-browser\": \"~7.2.0\",\n    \"@angular\/platform-browser-dynamic\": \"~7.2.0\",\n    \"@angular\/router\": \"~7.2.0\",\n    \"core-js\": \"^2.5.4\",\n    \"rxjs\": \"~6.3.3\",\n    \"tslib\": \"^1.9.0\",\n    \"zone.js\": \"~0.8.26\"\n  },\n  \"devDependencies\": {\n    \"@angular-devkit\/build-angular\": \"~0.13.0\",\n    \"@angular\/cli\": \"~7.3.7\",\n    \"@angular\/compiler-cli\": \"~7.2.0\",\n    \"@angular\/language-service\": \"~7.2.0\",\n    \"@types\/node\": \"~8.9.4\",\n    \"@types\/jasmine\": \"~2.8.8\",\n    \"@types\/jasminewd2\": \"~2.0.3\",\n    \"codelyzer\": \"~4.5.0\",\n    \"jasmine-core\": \"~2.99.1\",\n    \"jasmine-spec-reporter\": \"~4.2.1\",\n    \"karma\": \"~4.0.0\",\n    \"karma-chrome-launcher\": \"~2.2.0\",\n    \"karma-coverage-istanbul-reporter\": \"~2.0.1\",\n    \"karma-jasmine\": \"~1.1.2\",\n    \"karma-jasmine-html-reporter\": \"^0.2.2\",\n    \"protractor\": \"~5.4.0\",\n    \"ts-node\": \"~7.0.0\",\n    \"tslint\": \"~5.11.0\",\n    \"typescript\": \"~3.2.2\"\n  }\n}\n\n```\n\nPlease someone help me.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"As i see, you have added Modules under declarations and components under imports, you should add Modules under imports and components under declarations.\n\n\n"}
{"questionId":"49c2afb15c524ac2bf57b6dc78247368","question":"Deep Omit with typescript\nIs it possible to maintain type coverage on a function that deeply removes all instances of a key in an object?\n\n\nMy function looks like this.\n\n\n\n```\nfunction omitDeep<T extends object>(obj: T, key: string): TWithoutProvidedKey {\n  return JSON.parse(\n    JSON.stringify(obj),\n    (key: string, value: any) => key === \"__typename\" ? undefined : value\n  );\n}\n\n```\n\nIs there any way to make `TWithoutProvidedKey` a reality?\n\n\n","questionMetadata":{"type":"implementation","level":"advanced","tag":"typescript"},"answer":"This can easily be done, you just need to use mapped types to recurse down the properties:\n\n\n\n```\ntype Primitive = string | Function | number | boolean | Symbol | undefined | null \ntype DeepOmitHelper<T, K extends keyof T> = {\n    [P in K]: \/\/extra level of indirection needed to trigger homomorhic behavior \n        T[P] extends infer TP ? \/\/ distribute over unions\n        TP extends Primitive ? TP : \/\/ leave primitives and functions alone\n        TP extends any[] ? DeepOmitArray<TP, K> : \/\/ Array special handling\n        DeepOmit<TP, K> \n        : never\n}\ntype DeepOmit<T, K> = T extends Primitive ? T : DeepOmitHelper<T,Exclude<keyof T, K>> \n\ntype DeepOmitArray<T extends any[], K> = {\n    [P in keyof T]: DeepOmit<T[P], K>\n}\ntype Input =  {\n    __typename: string,\n    a: string,\n    nested: {\n        __typename: string,\n        b: string\n    }\n    nestedArray: Array<{\n        __typename: string,\n        b: string\n    }>\n    nestedTuple: [{\n        __typename: string,\n        b: string\n    }]\n}\n\ntype InputWithoutKey = DeepOmit<Input, '__typename'>\n\nlet s: InputWithoutKey = {\n    a: \"\",\n    nested: {\n        b:\"\"\n    },\n    nestedArray: [\n        {b: \"\"}\n    ],\n    nestedTuple: [\n        { b: \"\"},\n    ]\n}\n\n```\n\nJust a caveat, this works on 3.4, the handling of mapped types on arrays and tuples has changed recently, so depending on version you might need to handle arrays as a special case.\n\n\n"}
{"questionId":"a1cca434d8df4210886b3d842f86b7b8","question":"TypeScript: Multiple constructor implementations are not allowed\nI have an Object where I use it in multiple services, and each one should take some parameters, so I create two constructors, but TypeScript did not allow me to do this. My example is:\n\n\n\n```\nclass User {\n    id: number;\n    username: string;\n    password: string;\n    email: string;\n    firstName: string;\n    lastName: string;\n    roles: string[];\n\n    constructor(username: string, password: string){\n        this.username = username;\n        this.password = password;\n    }\n\n    constructor(id: number, username: string, firstname: string, lastname: string, roles: string[]){\n        this.id = id;\n        this.username= username;\n        this.firstname= firstname;\n        this.lastname= lastname;\n        this.roles = roles;\n    }\n    \/\/.. and maybe another constructor also\n}\n\n```\n\nIs there a trick to solve this issue, please?\n\n\n\n\n---\n\n\nWhen I use the *optional* `?` in constructors for example:\n\n\n\n```\nconstructor(\n    public id?: number,\n    public username?: string,\n    public email?: string,\n    public password?: string,\n    public firstName?: string,\n    public lastName?: string,\n    public roles?: string[]) {\n}\n\n```\n\nand when I get my data from backend:\n\n\n\n```\nthis.service.usersList().subscribe(users => {\n  console.log(users);\n  this.dataSource.data = users;\n});\n\n```\n\nThe `roles` is set in the password and not in the roles failed:\n\n\n\n```\n{\n  \"id\": 1,\n  \"username\": \"user1\",\n  \"email\": \"user1@email.com\",\n  \"password\": [\n    \"USER\",\n    \"MODR\"\n  ]\n}\n\n```\n\nFor that I'm not sure about this trick.\n\n\n\n\n---\n\n\nMaybe I was not precise, but I use this method to parse my data:\n\n\n\n```\nstatic fromJson(item: Object): any {\n    return new User(\n        item['id'],\n        item['username'],\n        item['email'],\n        item['roles']\n    );\n}\n\n```\n\nFor that, when I create a constructor with optional, it will set the attributes in order of my call.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"You can't use multiple constructors, but you can add a few optional parameters and verify if it exists, like the following:\n\n\n\n```\nclass User {\n    id: number;\n    username: string;\n    password: string;\n    email: string;\n    firstname: string;\n    lastname: string;\n    roles: string[];\n    \/\/ The \"?\" says that its optional parameter\n    constructor(id?: number, username?: string, firstname?: string,\n        lastname?: string, roles?: string[], password?: string) {\n        if (id) { \/\/ if id exists , you can implement the first constructor\n            this.id = id;\n            this.username = username;\n            this.firstname = firstname;\n            this.lastname = lastname;\n            this.roles = roles;\n        }\n        if (password) { \/\/ if password exists : you can implement the second one\n            this.username = username;\n            this.password = password;\n        }\n    }\n}\n\n```\n\nYour response should be like this before this works fine:\n\n\n\n```\nstatic fromJson(item: Object): any {\n    return new User({\n        id : item['id'],\n        username : item['username'],\n        email : item['email'],\n        roles : item['roles']\n    });\n}\n\n```\n\nSo your constructor should be like this:\n\n\n\n```\nconstructor(user: any){\n    if (user.id) { \/\/ if id exists , you can implement the first constructor\n        this.id = user.id;\n        this.username = user.username;\n        this.firstname = user.firstname;\n        this.lastname = user.lastname;\n        this.roles = user.roles;\n    }\n    if (user.password) { \/\/ if password exists : you can implement the second one\n        this.username = user.username;\n        this.password = user.password;\n    }\n}\n\n```\n\nOr if you don't want to do that, you can set the response regarding the order, like this:\n\n\n\n```\nstatic fromJson(item: Object): any {\n    return new User(\n        item['id'],\n        item['username'],\n        undefined,\n        item['email'],\n        undefined,\n        undefined,\n        item['roles']\n    );\n}\n\n```\n\n"}
{"questionId":"83615f19a9824de5b14a458f7b031525","question":"How to specify a struct with a multi-column unique index for Gorm?\nHow do I define my `struct`s to specify a multi-column unique index to Gorm in Go?\n\n\nSuch as:\n\n\n\n```\ntype Something struct {\n    gorm.Model\n    First  string `sql:\"unique_index:unique_index_with_second\"`\n    Second string `sql:\"unique_index:unique_index_with_first\"`\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"go"},"answer":"this is how you do it: You need to use the gorm struct tag and specify that the index is unique\n\n\n\n```\ntype Something struct {\n    gorm.Model\n    First  string `gorm:\"index:idx_name,unique\"`\n    Second string `gorm:\"index:idx_name,unique\"`\n}\n\n```\n\n"}
{"questionId":"4a63a2bdf406446fb8eb598ebe9c0f3e","question":"How to post form-data IFormFile with HttpClient?\nI have backend endpoint `Task<ActionResult> Post(IFormFile csvFile)` and I need to call this endpoint from HttpClient. Currently I am getting `Unsupported media type error`.\nHere is my code:\n\n\n\n```\nvar filePath = Path.Combine(\"IntegrationTests\", \"file.csv\");\nvar gg = File.ReadAllBytes(filePath);\nvar byteArrayContent = new ByteArrayContent(gg);\nvar postResponse = await _client.PostAsync(\"offers\", new MultipartFormDataContent\n{\n    {byteArrayContent }\n});\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"You need to specify parameter name in `MultipartFormDataContent` collection matching action parameter name (`csvFile`) and a random file name\n\n\n\n```\nvar multipartContent = new MultipartFormDataContent();\nmultipartContent.Add(byteArrayContent, \"csvFile\", \"filename\");\nvar postResponse = await _client.PostAsync(\"offers\", multipartContent);\n\n```\n\nor equivalent\n\n\n\n```\nvar postResponse = await _client.PostAsync(\"offers\", new MultipartFormDataContent {\n    { byteArrayContent, \"csvFile\", \"filename\" }\n});\n\n```\n\n"}
{"questionId":"47dcb375b5b640ecb02d9f73c57e488b","question":"How do you source neovim config file without restarting nvim?\nIs there a way to source the `~\/.config\/nvim\/init.vim` file from within nvim?\n\n\nWith vanilla vim you can source .vimrc with `:so %` : Is there an equivalent method do with similarly in neovim?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"`$MYVIMRC` is always available from inside vim or neovim, so you can just use\n\n\n`:source $MYVIMRC`\n\n\nand bind it to a convenient mapping:\n\n\n`nnoremap <Leader>sv :source $MYVIMRC<CR>`\n\n\n2021 update: If you are using neovim with a lua config, you can use `:luafile $MYVIMRC`\n\n\n"}
{"questionId":"aef1e1fd1b17450a92c935f9b543a21c","question":"implicit declaration of function \u2018atoi\u2019?\nWhy am I getting an error when using the `atoi()` function?\n\n\n\n```\n#include <stdio.h>\n#include <string.h>\nint main()\n{\n    char s1[10], s2[10];\n    int x=5, y=6, z;\n    sprintf(s1, \"%d\", x);\n    sprintf(s2, \"%d\", y);\n    strcat(s1, s2);\n    z = atoi(s1);\n    printf(\"%d, %s\", z, s1);\n    return 0;\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"c"},"answer":"\n```\n#include <stdlib.h>\n\n```\n\nWill fix it.\n\n\n"}
{"questionId":"75a20ad78a7a4696a651973d3744463a","question":"How to find the largest number(s) in a list of elements, possibly non-unique?\nHere is my program,\n\n\n\n```\nitem_no = []\nmax_no = 0\nfor i in range(5):\n    input_no = int(input(\"Enter an item number: \"))\n    item_no.append(input_no)\nfor no in item_no:\n    if no > max_no:\n       max_no = no\nhigh = item_no.index(max_no)\nprint (item_no[high])\n\n```\n\nExample input: `[5, 6, 7, 8, 8]`\n\n\nExample output: `8`\n\n\nHow can I change my program to output the same highest numbers in an array?\n\n\nExpected output: `[8, 8]`\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"Just get the maximum using `max` and then its `count` and combine the two in a list-comprehension.\n\n\n\n```\nitem_no = [5, 6, 7, 8, 8]\n\nmax_no = max(item_no)\nhighest = [max_no for _ in range(item_no.count(max_no))]\nprint(highest)  # -> [8, 8]\n\n```\n\nNote that this will return a list of a single item in case your maximum value appears only once.\n\n\n\n\n---\n\n\nA solution closer to your current programming style would be the following:\n\n\n\n```\nitem_no = [5, 6, 7, 8, 8]\nmax_no = 0  # Note 1 \nfor i in item_no:\n    if i > max_no:\n        max_no = i\n        high = [i]\n    elif i == max_no:\n        high.append(i)\n\n```\n\nwith the same results as above of course. \n\n\nNotes\n\n\n1. I am assuming that you are dealing with N\\* (*1, 2, ...*) numbers only. If that is not the case, initializing with `-math.inf` should be used instead.\n\n\n\n\n---\n\n\n**Note that the second code snippet is less efficient than the first by quite a margin. Python allows you to be more efficient than these explicit, fortran-like loops and it is more efficient itself when you use it properly.**\n\n\n"}
{"questionId":"25f258c11d8f47d3bd4a52fae1f8e72c","question":"Is Kotlin's runCatching..also equivalent to try..finally?\nI want to run cleanup code after a certain block of code completes, regardless of exceptions. This is not a closeable resource and I cannot use try-with-resources (or Kotlin's `use`).\nIn Java, I could do the following:\n\n\n\n```\ntry {\n  \/\/ ... Run some code\n} catch(Exception ex) {\n  \/\/ ... Handle exception \n} finally {\n  \/\/ ... Cleanup code\n}\n\n```\n\nIs the following Kotlin code equivalent?\n\n\n\n```\nrunCatching {\n  \/\/ ... Run some code\n}.also {\n  \/\/ ... Cleanup code\n}.onFailure {\n  \/\/ ... Handle exception\n}\n\n```\n\nEdit: added boilerplate exception handling - my concern is with ensuring the cleanup code runs, and maintainability.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"kotlin"},"answer":"There is one important difference, where the code inside `runCatching` contains an early return. A `finally` block will be executed *even after a return*, whereas `also` has no such magic.\n\n\nThis code, when run, will print nothing:\n\n\n\n```\nfun test1()\n    runCatching {\n        return\n    }.also {\n        println(\"test1\")\n    }\n}\n\n```\n\nThis code, when run, will print \"test2\":\n\n\n\n```\nfun test2() {\n    try {\n        return\n    } finally {\n        println(\"test2\")\n    }\n}\n\n```\n\n"}
{"questionId":"6667ecbfc1914d1a9911448e40b8d336","question":"Prevent model hydration on Eloquent queries\nIs it possible to have an *eloquent query builder* return `StdClass` rather then `Model`?\n\n\nFor example `User::where('age', '>', 34)->get()` returns a `Collection` of `User` models.\n\n\nWhereas `DB::table('users')->where('age', '>', 34)->get()` returns a `Collection` of `StdClass` objects. Much faster.\n\n\nTherefore:\n\n\nIs it possible to prevent hydrating eloquent models and return `StdClass` objects as a *database query builder* would, but still leverage the usefulness of an *eloquent query builder* syntax?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"php"},"answer":"Yes, is possible using the 'getQuery' or 'toBase' method. For example:\n\n\n\n```\nUser::where('age', '>', 34)->getQuery()->get();\n\n```\n\nor\n\n\n\n```\nUser::where('age', '>', 34)->toBase()->get();\n\n```\n\n"}
{"questionId":"977c30abb8a74d27961c8779105fbfd0","question":"How to pass arguments from Flutter to Kotlin?\ni've got started studying Flutter. I'm trying to using MethodChannel and MethodCall to communicate with Android platform. I don't know how to pass arguments to the Android code. \n\n\nBelow is my code.\n\n\n\n```\n\/\/ dart\nvoid _onClick() async {\n    var parameters = {'image':'starry night'};\n    await platform.invokeMethod('showToast', new Map.from(parameters));\n}\n\n\n\/\/ kotlin\nMethodChannel(flutterView, CHANNEL).setMethodCallHandler { call, result ->\n    Log.d(\"MainActivity\", \">> ${call.method}, ${call.arguments}\")\n    when (call.method) {\n        \"showToast\" -> {\n        showToast(\"toast\")\n    }\n    else -> {\n        Log.d(\"MainActivity\", \"fail\");\n    }\n}\n\n```\n\nI can check an arguement value what I passed by log message what I printed.\n`{image=starry night}`\nBut I don't know how to parse to a map object.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"kotlin"},"answer":"### Flutter\n\n\nOn the Flutter side, you can pass arguments by including them as a map in the `invokeMethod` call.\n\n\n\n```\n_channel.invokeMethod('showToast', {'text': 'hello world'});\n\n```\n\n### Kotlin\n\n\nOn the Kotlin side you can get the parameters by casting `call.arguments` as a Map or getting a particular argument from `call.argument()`.\n\n\n\n```\noverride fun onMethodCall(call: MethodCall, result: Result) {\n  when (call.method) {\n    \"showToast\" -> {\n      val text = call.argument<String>(\"text\") \/\/ hello world\n      showToast(text)\n    }   \n  }\n}\n\n```\n\n"}
{"questionId":"fa19f703ef0445078b4217a28866138c","question":"SetActive() can only be called from the main thread\nI am stuck with this problem for 3 days, I did a lot of research, but couldn't find any answer, Here is a brief explanation of what is happening, trying to work with Firebase Database and Authentication with Unity3D, Here are the steps:\n\n\nFirst user signs in and if it's successful, it will fetch user's data from Database and then Authentication panel should be deactivated and User's panel activated. \n\n\nIt Gives me this error when trying to SetActive panel. \n\n\n**SetActive can only be called from the main thread.\nConstructors and field initializers will be executed from the loading thread when loading a scene.\nDon't use this function in the constructor or field initializers, instead move initialization code to the Awake or Start function.\nUnityEngine.GameObject:SetActive(Boolean)**\n\n\n\n```\npublic void SignInWithEmail()\n{\n    auth.SignInWithEmailAndPasswordAsync(email, password).ContinueWith(task => {\n      DatabaseReference.GetValueAsync().ContinueWith(task => {\n\n        \/\/here after successful signing in, it gets the data from the Database\n        \/\/and after that it should activate the user panel\n        \/\/and deactivate the authentication panel\n\n        \/\/HERE IS THE PROBLEM \n        userPanel.SetActive(true);\n        authPanel.SetActive(false);\n    }\n  }\n}\n\n```\n\nI'm not trying to load another scene or anything else.\n\n\n## If needed I can provide more information\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"So my answer is very similar to the accepted answer from ***Milod's***, but a little different, as it took me a while to wrap my head around his, even though his still works.\n\n\n1. **The Issue:**\nNormally, all your code runs on a single thread in Unity, since Unity is single-threaded, \nhowever when working with APIs like Firebase, which require callbacks, the callback functions will be handled by a new thread.\nThis can lead to race-conditions, especially on a single-threaded engine like Unity.\n2. **The solution (from Unity):**\nStarting from Unity 2017.X, unity now requires changes to UI components to be run on the Main thread (i.e. the first thread that was started with Unity).\n3. **What is impacted ?:**\nMainly calls that modify the UI like...\n\n\n\n> \n> \n> ```\n> gameObject.SetActive(true);  \/\/ (or false)\n> textObject.Text = \"some string\" \/\/ (from UnityEngine.UI)\n> \n> ```\n> \n>\n4. **How this relates to your code:**\n\n\n\n```\npublic void SignInWithEmail() {\n    \/\/ auth.SignInWithEmailAndPasswordAsyn() is run on the local thread, \n    \/\/ ...so no issues here\n    auth.SignInWithEmailAndPasswordAsync(email, password).ContinueWith(task => {\n\n     \/\/ .ContinueWith() is an asynchronous call \n     \/\/ ...to the lambda function defined within the  task=> { }\n     \/\/ and most importantly, it will be run on a different thread, hence the issue\n      DatabaseReference.GetValueAsync().ContinueWith(task => {\n\n        \/\/HERE IS THE PROBLEM \n        userPanel.SetActive(true);\n        authPanel.SetActive(false);\n    }\n  }\n}\n\n\n```\n\n5. **Suggested Solution:**\nFor those calls which require callback functions, like...\n\n\n\n> \n> \n> ```\n> DatabaseReference.GetValueAsync()\n> \n> ```\n> \n> \n\n\n...you can...\n\n\n- send them to a function which is set up to run on that initial thread.\n- ...and which uses a queue to ensure that they will be run in the order that they were added.\n- ...and using the singleton pattern, in the way advised by the Unity team.\n\n\n## Actual solution\n\n\n1. Place the code below into your scene on a gameObject that will always be enabled, so that you have a worker that...\n\t- always runs on the local thread\n\t- can be sent those callback functions to be run on the local thread.\n\n\n\n```\nusing System;\nusing System.Collections.Generic;\nusing UnityEngine;\n\ninternal class UnityMainThread : MonoBehaviour\n{\n    internal static UnityMainThread wkr;\n    Queue<Action> jobs = new Queue<Action>();\n\n    void Awake() {\n        wkr = this;\n    }\n\n    void Update() {\n        while (jobs.Count > 0) \n            jobs.Dequeue().Invoke();\n    }\n\n    internal void AddJob(Action newJob) {\n        jobs.Enqueue(newJob);\n    }\n}\n\n\n```\n\n2. Now from your code, you can simply call...\n\n\n\n> \n> \n> ```\n>  UnityMainThread.wkr.AddJob();\n> \n> ```\n> \n>\n\n\n...so that your code remains easy to read (and manage), as shown below...\n\n\n\n```\npublic void SignInWithEmail() {\n    auth.SignInWithEmailAndPasswordAsync(email, password).ContinueWith(task => {\n\n      DatabaseReference.GetValueAsync().ContinueWith(task => {\n        UnityMainThread.wkr.AddJob(() => {\n            \/\/ Will run on main thread, hence issue is solved\n            userPanel.SetActive(true);\n            authPanel.SetActive(false);            \n        })\n\n    }\n  }\n}\n\n```\n\n"}
{"questionId":"e4d8ae5930b949d4bd0071aec776f7a9","question":"Using jwt-decode with typescript front-end project\nDescription\nI am trying to use jwt-decode in a typescript project i.e. Stencil Project & it is throwing following error:\n\n\n\n> \n> This expression is not callable.Type '{ default: (token: string,\n>  options?: Options) => TTokenDto; }' has no call signatures.\n> \n> \n> \n\n\n\n```\nimport * as jwt_decode from 'jwt-decode';\n.\n.\n.\nlet token = \"........\";\nlet decoded = jwt_decode(token);\n\n```\n\n**Reproduction**\n\n\n- install jwt-decode in any typescript project npm install --save\n@types\/jwt-decode npm install --save jwt-decode import it in your\ncode & use import \\* as jwt\\_decode from 'jwt-decode'; . . . let token\n= \"........\"; let decoded = jwt\\_decode(token);\n- build project\n- Version of this library used: ^2.2.0 Version of the platform or\n- framework used, if applicable: stencil - ^1.3.3 , typescript - 3.7.2\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"Following correction to the import statement works fine:\n\n\n\n```\nimport jwt_decode from 'jwt-decode';\n\n```\n\n"}
{"questionId":"5cbd63bffec84d8eaab64442933edea0","question":"Generating migration file for a project using NestJS and TypeORM\nI'm trying to generate migration files for my entities, but whenever I run the command to create the entity, it creates an \"empty\" file, just the up and down methods are created.\n\n\nI have added this script in my package.json file: `\"typeorm\": \"node --require ts-node\/register .\/node_modules\/typeorm\/cli.js\"`.\n\n\nIn my app.module.ts, the connection is configured like this:\n\n\n\n```\n TypeOrmModule.forRoot({\n      type: 'mysql',\n      host: database().host,\n      port: parseInt(database().port),\n      username: database().username,\n      password: database().password,\n      database: database().schema,\n      entities: [Question, QuestionOption],\n      migrations: ['src\/migration\/*{.ts,.js}'],\n      cli: {\n        migrationsDir: 'src\/migration'\n      },\n      synchronize: true,\n    })\n\n```\n\nWhere `database()` it's a nestjs config file and get the values from an .env file.\n\n\nThe script I'm using to create the migration is: `npm run typeorm migration:create -- -n QuestionTables -d src\/migrations` where need to specify the **-d**, otherwise the migration file is not created (even if it's specified in the cli of the forRoot method.\n\n\nDo I need to write manually the SQL to create the tables?\n\n\nWhat if I need to add a new column to an existing table, should I create a new migration file and write manually the SQL code to add that?\n\n\nAnother command that I tried to run was this one: `npm run typeorm migration:generate -- -n QuestionTables -d src\/migrations` and here it gives me an error: \" Error: No connection options were found in any orm configuration files.\"\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"The command `npm run typeorm migration:create` will generate empty migration file.\n\n\nThe command for migrations auto generation is: `npm run typeorm migration:generate`\n\n\nAs written in the error you received you need to specify the configuration file for the cli. Than means should extract the configuration passed to `forRoot` to a ts\/json file. You'll need 2 files for that, 1 for the server's connection and another for migrations configuration.\n\n\nFor example:\n\n\n\n```\n\/\/ ormconfig.ts\nexport const config: TypeOrmModuleOptions = {\n      type: 'mysql',\n      host: database().host,\n      port: parseInt(database().port),\n      username: database().username,\n      password: database().password,\n      database: database().schema,\n      entities: [Question, QuestionOption], \/\/ maybe you should also consider chage it to something like:  [__dirname + '\/**\/*.entity.ts', __dirname + '\/src\/**\/*.entity.js']\n      migrations: ['src\/migration\/*{.ts,.js}'],\n      cli: {\n        migrationsDir: 'src\/migration'\n      },\n      synchronize: true,\n    }\n\n\n```\n\n\n```\n\/\/ ormconfig-migrations.ts\n\nimport {config} from '.\/ormconfig';\n\nexport = config;\n\n\n```\n\n\n```\nimport {config} from '.\/ormconfig';\n\nTypeOrmModule.forRoot(config);\n\n```\n\n\n```\n\/\/ package.json\n\n\"scripts\": {\n     ...\n     \"typeorm:cli\": \"ts-node -r tsconfig-paths\/register .\/node_modules\/typeorm\/cli -f .\/ormconfig-migrations.ts\",\n     \"migration-generate\": \"npm run typeorm:cli -- migration:generate -n\"\n}\n\n```\n\n"}
{"questionId":"4d60928a3e49402ea7301b4af690ce86","question":"How to initialize Kotlin IntArray from IntRange?\nI'm trying to initialize an `IntArray` in Kotlin like so:\n\n\n\n```\nintArrayOf(1..9)\n\n```\n\nBut I get a `TypeError` that `Int` is required, but I'm providing an `IntRange`. Is there a way to initialize the array with a range, or do I have to explicitly write out each value?\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"kotlin"},"answer":"Using built in functions, this is how you could get to an `IntArray` from an `IntRange`:\n\n\n\n```\nval array: IntArray = (1..9).toList().toIntArray()\n\n```\n\nThis is a bit wasteful, because it first constructs a list where it puts all the elements, and then it constructs an array as well. To do this directly, you could use your own extension, something like...\n\n\n\n```\nfun IntRange.toIntArray(): IntArray {\n    if (last < first)\n        return IntArray(0)\n\n    val result = IntArray(last - first + 1)\n    var index = 0\n    for (element in this)\n        result[index++] = element\n    return result\n}\n\n```\n\nWhich would give you this syntax:\n\n\n\n```\nval array: IntArray = (1..9).toIntArray()\n\n```\n\n"}
{"questionId":"c8f23cf1a32b4974903c6eda3bb9dfdb","question":"How to sort a string alphabetically in Kotlin\nI want to reorder the string \"hearty\" to be in alphabetical order: \"aehrty\"\n\n\nI've tried:\n\n\n\n```\nval str = \"hearty\"\nval arr = str.toCharArray()\nprintln(arr.sort())\n\n```\n\nThis throws an error. I've also tried the `.split(\"\")` method with the `.sort()`. That also throws an error.\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"kotlin"},"answer":"You need to use `sorted()` and after that `joinToString`, to turn the array back into a String:\n\n\n\n```\nval str = \"hearty\"\nval arr = str.toCharArray()\nprintln(arr.sorted().joinToString(\"\")) \/\/ aehrty\n\n```\n\n**Note:** `sort()` will mutate the array it is invoked on, `sorted()` will return a new sorted array leaving the original untouched.\n\n\n"}
{"questionId":"771050daafe949699522719ffb976aa2","question":"Laravel Valet php-fpm already listening on valet sock\nI've upgraded valet on my macbook (running catalina) and followed the laravel docs including re-running the `valet install` command and am seeing unexpected `502 Bad Gateway` errors. I was checking the logs and found\n\n\n\n```\n[27-Aug-2019 20:39:06] ERROR: Another FPM instance seems to already listen on \/Users\/myuser\/.config\/valet\/valet.sock\n[27-Aug-2019 20:39:06] ERROR: Another FPM instance seems to already listen on \/Users\/myuser\/.config\/valet\/valet.sock\n[27-Aug-2019 20:39:06] ERROR: FPM initialization failed\n[27-Aug-2019 20:39:06] ERROR: FPM initialization failed\n[27-Aug-2019 20:39:17] ERROR: Another FPM instance seems to already listen on \/Users\/myuser\/.config\/valet\/valet.sock\n[27-Aug-2019 20:39:17] ERROR: Another FPM instance seems to already listen on \/Users\/myuser\/.config\/valet\/valet.sock\n[27-Aug-2019 20:39:17] ERROR: FPM initialization failed\n[27-Aug-2019 20:39:17] ERROR: FPM initialization failed\n\n```\n\nIt seems there's 3 `php-fpm` processes running though they all are the same php version (7.3). \n\n\nCan anyone offer ideas of how to find where the other `php-fpm` process is being triggered from, and how to fix this issue? \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"After days of screwing around I found an answer on serverfault that suggested deleting the listening sock. So I ran `rm ~\/.config\/valet\/valet.sock` and immediately the tailed php log showed\n\n\n\n```\n[08-Sep-2019 16:55:48] NOTICE: fpm is running, pid 10316\n[08-Sep-2019 16:55:48] NOTICE: ready to handle connections\n\n```\n\nSo I guess that's all there was to it!\n\n\n"}
{"questionId":"a61862a4868e4196963c37722091189f","question":"Appending row to dataframe with concat()\nI have defined an empty data frame with\n\n\n\n```\ndf = pd.DataFrame(columns=['Name', 'Weight', 'Sample'])\n\n```\n\nand want to append rows in a for loop like this:\n\n\n\n```\nfor key in my_dict:\n   ...\n   row = {'Name':key, 'Weight':wg, 'Sample':sm}\n   df = pd.concat(row, axis=1, ignore_index=True) \n\n```\n\nBut I get this error\n\n\n\n```\ncannot concatenate object of type '<class 'str'>'; only Series and DataFrame objs are valid\n\n```\n\nIf I use `df = df.append(row, ignore_index=True)`, it works but it seems that `append` is deprecated. So, I want to use `concat()`. How can I fix that?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"You can transform your dict in pandas DataFrame\n\n\n\n```\nimport pandas as pd\ndf = pd.DataFrame(columns=['Name', 'Weight', 'Sample'])\nfor key in my_dict:\n  ...\n  #transform your dic in DataFrame\n  new_df = pd.DataFrame([row])\n  df = pd.concat([df, new_df], axis=0, ignore_index=True)\n\n```\n\n"}
{"questionId":"d1400d0ee3c04a8995858b1d2ee01e67","question":"Query Clickhouse for currently installed version\nI know there's a bunch of system tables. If one has access to those where do I find the currently installed version?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"sql"},"answer":"\n```\nSELECT version()\n\n\u250c\u2500version()\u2500\u2500\u2500\u2510\n\u2502 20.9.1.4571 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\nSELECT *\nFROM system.build_options\n\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 VERSION_FULL              \u2502 ClickHouse 20.9.1.4571                                                                                                                       \u2502\n\u2502 VERSION_DESCRIBE          \u2502 v20.9.1.4571-testing\n\n```\n\n"}
{"questionId":"e3adcded62c849a299440ce929f7c6b8","question":"PHP Fatal error: Uncaught Error: Call to undefined function Whoops\\Exception\\xdebug\\_is\\_enabled() with Laravel 5.8 and PHP 7.4\nhope you're doing great, I'm working on a Laravel project I didn't work on since a couple of months, and found that anytime an exception is raised, I get the following error:\n\n\n[Fri Jan 15 15:51:11 2021] PHP Fatal error: Uncaught Error: Call to undefined function Whoops\\Exception\\xdebug\\_is\\_enabled() in \/var\/www\/html\/project\/vendor\/filp\/whoops\/src\/Whoops\/Exception\/Inspector.php:254\n\n\nStack trace:\n\n\n\n```\n#0 \/var\/www\/html\/project\/vendor\/filp\/whoops\/src\/Whoops\/Exception\/Inspector.php(175): Whoops\\Exception\\Inspector->getTrace()\n#1 \/var\/www\/html\/project\/vendor\/filp\/whoops\/src\/Whoops\/Handler\/PrettyPageHandler.php(280): Whoops\\Exception\\Inspector->getFrames()\n#2 \/var\/www\/html\/project\/vendor\/filp\/whoops\/src\/Whoops\/Handler\/PrettyPageHandler.php(197): Whoops\\Handler\\PrettyPageHandler->getExceptionFrames()\n#3 \/var\/www\/html\/project\/vendor\/filp\/whoops\/src\/Whoops\/Run.php(296): Whoops\\Handler\\PrettyPageHandler->handle()\n#4 \/var\/www\/html\/project\/vendor\/laravel\/framework\/src\/Illuminate\/Foundation\/Exceptions\/Handler.php(345): Whoops\\Run->handleException()\n#5 \/var\/www\/html\/project\/vendor\/laravel\/framework\/src\/Illuminate\/Foundation\/Exceptions\/Handler.php(324): Illuminate\\Foundation\\Exceptions\\Handler->renderExceptionWithWhoops()\n#6 \/var\/www\/html\/project\/ in \/var\/www\/html\/project\/vendor\/filp\/whoops\/src\/Whoops\/Exception\/Inspector.php on line 254\n\n```\n\nHas anyone seen this error before and know a way to fix it?\nThanks beforehand.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"turns out I was facing this issue and found out that just by updating \"filp\/whoops\": \"^2.0\" to \"filp\/whoops\": \"^2.9\" the problem was solved.\n\n\n"}
{"questionId":"de501e535e9d43498e8295497d6df2d5","question":"TypeORM jsonb array column\nI'm working on a node micro-service, orm and db are respectively `typeorm` and `postgresql`\n\n\nI'm trying to create `jsonb` array column but I'm probably not doing it the correct way.\n\n\n**Notes**\n\n\n1. I would normally have accomplished this by simply adding a simple extra entity and relation. In this case I have been asked to use the `jsonb` type in oder to be able to amend the interface without catering for schema changes.\n2. Storing a simple list of indexed ids would be equally good enough for now.\n3. I am not sure I should be using the `array: true` column option. I made a few attempts to use a plain `jsonb` object without succeeding ( `\"{}\"::jsonb` ).\n\n\n**My aim:**\n\n\nTo store an array of objects with an **indexed** `id` column, and to be able to add and remove ids. In case this is not possible a flat indexed string array would do.\n\n\ne.g:\n\n\n`[ {id: 'some-uuid-000'}, {id: 'some-uuid-001'}, ... ]`\n\n\nor:\n\n\n`['some-uuid-000', 'some-uuid-001', 'some-uuid-002']`\n\n\n**Code:**\n\n\nMy column definition :\n\n\n\n```\n@Column({\n        type: 'jsonb',\n        array: true,\n        default: () => 'ARRAY[]::jsonb[]',\n        nullable: false,\n    })\n    public users: Array<{ id: string }> = [];\n\n```\n\nI manage to fetch the empty array with\n\n\n\n```\nconst group = await repo.findOneOrFail({ id: groupId });\nconsole.log('>>>>>', group.users);\n\n```\n\nwhich outputs:\n\n\n\n```\n>>>>> []\n\n```\n\nwhen trying to add an item to the array and persist as below\n\n\n\n```\nreturn repo.update(groupId, { users: [...group.users, { id: userId }] });\n\n```\n\nI get the following output:\n\n\n\n```\n2019-12-21 14:40:44.088 UTC [556] ERROR:  malformed array literal: \"[{\"id\":\"cc135b8a-b6ed-4cd7-99fc-396228e74509\"}]\"\n2019-12-21 14:40:44.088 UTC [556] DETAIL:  \"[\" must introduce explicitly-specified array dimensions.\n2019-12-21 14:40:44.088 UTC [556] STATEMENT:  UPDATE \"group\" SET \"users\" = $2, \"created_at\" = CURRENT_TIMESTAMP WHERE \"id\" IN ($1)\n(node:5050) UnhandledPromiseRejectionWarning: QueryFailedError: malformed array literal: \"[{\"id\":\"cc135b8a-b6ed-4cd7-99fc-396228e74509\"}]\"\n\n```\n\nThe output error tells me that the configuration must be wrong as postgres seems to be provided with a plain objects array while expecting a different format\/notation. I haven't found much details about this sort of scenarios in the docs.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"Eventually I found the following solution: the culprit was the `array` column option which needs to be explicitly set to false as in the example below:\n\n\n\n```\n@Column({\n        type: 'jsonb',\n        array: false,\n        default: () => \"'[]'\",\n        nullable: false,\n    })\n    public users!: Array<{ id: string }>;\n\n```\n\nWhen not set typeorm automatically inferred the postgres column type to be `jsonb[]` (rather than plain `jsonb`) which doesn't allow performing `jsonb_set` operations.\n\n\n"}
{"questionId":"6f1dc8cae7bb48c0a393cdd5a52cc415","question":"How to define typescript for React.Children.map\nI have a function that looks at the provided children and if a particular element type is found, it adds some properties to it automatically.\n\n\nThe function is called like this:\n\n\n\n```\nrender () {\n\n    const { children, name, className } = this.props;\n\n    return (\n        <div className={className}>\n            {this.enrichRadioElements(children, name)}\n        <\/div>\n    )\n}\n\n```\n\nand it is implemented like this:\n\n\n\n```\nenrichRadioElements = (children: Array<any>, name: string) => (\n    React.Children.map(children, child => {\n        if (!React.isValidElement(child)) {\n            return child;\n        }\n\n        \/\/@ts-ignore\n        if (child.props.children) {\n            child = React.cloneElement(child, {\n                \/\/@ts-ignore\n                children: this.enrichRadioElements(child.props.children, name)\n            });\n        }\n\n        if (child.type === Radio) {\n            return React.cloneElement(child, { \n                onChange: this.handleFieldChange,\n                selectedValue: this.state.selectedValue,\n                name: name\n            })\n        }\n        else {\n            return child;\n        }\n    })\n)\n\n```\n\nThe two `\/\/@ts-ignore` comments are what I'm trying to get rid of by writing code that will satisfy typescript. If I remove the first one, the error message I see is this:\n\n\n\n> \n> Property 'children' does not exist on type '{}'.(ts-2339)\n> \n> \n> \n\n\nHow can I properly modify my code so I can remove the `\/\/@ts-ignore` comments? I did go to the definition of child.props and I found this:\n\n\n\n```\ninterface ReactElement<P = any, T extends string | JSXElementConstructor<any> = string | JSXElementConstructor<any>> {\n    type: T;\n    props: P;\n    key: Key | null;\n}\n\n```\n\nwhich looks to have a 'props' of type any (if I'm reading it correctly), but typescript doesn't recognize the children property.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"The problem is a couple of things. I started by changing `children: Array<any>` to `children: React.ReactNode`. You already have a check in there to narrow the type from ReactNode to ReactElement. The trick was 1. using the generic type arguments in `isValidElement` and 2. using a new variable with a type assignment on it `elementChild` rather than dealing with and mutating the `child` argument. `EnrichedChildren` may need to be updated to match your use case.\n\n\n\n```\ninterface EnrichedChildren {\n  onChange(): void\n  selectedValue: string\n  name: string\n  children?: React.ReactNode\n}\n\nenrichRadioElements = (children: React.ReactNode, name: string): any =>\n  React.Children.map(children, child => {\n    if (!React.isValidElement<EnrichedChildren>(child)) {\n      return child\n    }\n\n    let elementChild: React.ReactElement<EnrichedChildren> = child\n    if (child.props.children) {\n      elementChild = React.cloneElement<EnrichedChildren>(elementChild, {\n        children: this.enrichRadioElements(elementChild.props.children, name),\n      })\n    }\n\n    if (elementChild.type === 'Radio') {\n      return React.cloneElement(elementChild, {\n        onChange: () => {},\n        selectedValue: 'value',\n        name: name,\n      })\n    } else {\n      return elementChild\n    }\n  })\n\n```\n\n"}
{"questionId":"c8edf79a11ad45b4b9c8f3c530a9e761","question":"Check JSON data type using jq\nI'm trying to check the actual data type of a JSON value belonging to a specific key.\n\n\n`test.json`\n\n\n\n```\n{\n    \"id\": 50,\n    \"name\": \"Joe\"\n}\n\n```\n\nI want something analogous to this:\n\n\n\n```\n$ `jq 'typeof(\"id\")' < test.json`\n(output)>> string\n\n```\n\nIs this possible using `jq`?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"Use `type`:\n\n\n\n```\njq -r '[1.23,\"abc\",true,[],{},null][]| type' <<< '\"\"'\nnumber\nstring\nboolean\narray\nobject\nnull\n\n```\n\nIn your example you could check:\n\n\n\n```\njq '.id|type==\"number\"' file.json\n\n```\n\nOr use it in a `select` filter to display those ids which are **not** numbers for example:\n\n\n\n```\njq '.[]|select(id|type==\"number\"|not)' file.json\n\n```\n\n"}
{"questionId":"99e1e87e55414be3b31aa7d9543d47b0","question":"How to Autowire conditionally in spring boot?\nI have created one scheduler class\n\n\n\n```\npublic class TestSchedulderNew {\n\n@Scheduled(fixedDelay = 3000)\npublic void fixedRateJob1() {\nSystem.out.println(\"Job 1 running\");\n}\n\n@Scheduled(fixedDelay = 3000)\npublic void fixedRateJob2() {\nSystem.out.println(\"Job 2 running\");\n}\n}\n\n```\n\nIn the configuration, I have put @ConditionalOnProperty annotation to enable this on conditional purpose.\n\n\n\n```\n @Bean\n@ConditionalOnProperty(value = \"jobs.enabled\")\npublic TestSchedulderNew testSchedulderNew() {\nreturn new TestSchedulderNew();\n}\n\n```\n\nNow in the controller, I have created \"stopScheduler\" method to stop this scheduler, in this controller I have autowired\nTestSchedulderNew class\n\n\n\n```\n @RestController\n @RequestMapping(\"\/api\")\n public class TestCont {\n\nprivate static final String SCHEDULED_TASKS = \"testSchedulderNew\";\n\n @Autowired\n private ScheduledAnnotationBeanPostProcessor postProcessor;    \/]\n\n @Autowired\n private TestSchedulderNew testSchedulderNew;\n\n\n @GetMapping(value = \"\/stopScheduler\")\n public String stopSchedule(){\n  postProcessor.postProcessBeforeDestruction(testSchedulderNew, \n   SCHEDULED_TASKS);\n  return \"OK\";\n  }\n }     \n\n```\n\nNow the problem is if the conditional property is false then I get the below exception\n\n\n\n```\n   Field testSchedulderNew in com.sbill.app.web.rest.TestCont required a bean of type 'com.sbill.app.schedulerJob.TestSchedulderNew\n\n```\n\nIn case of true everything works fine,\n\n\nDo we have any option to solve this?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"You can use `@Autowired(required=false)` and null check in `stopScheduler` method.\n\n\n\n```\n @Autowired(required=false)\n private TestSchedulderNew testSchedulderNew;\n\n @GetMapping(value = \"\/stopScheduler\")\n public String stopSchedule() {\n     if (testSchedulderNew != null) {\n         postProcessor.postProcessBeforeDestruction(testSchedulderNew, \n          SCHEDULED_TASKS);\n         return \"OK\";\n     }\n     return \"NOT_OK\";\n }\n\n```\n\n"}
{"questionId":"4df2691e59ba443f8e575b320c16afb7","question":"How can I add a new value to an ENUM in Postgres without locking the table?\nI've tried two approaches.\n\n\nApproach 1: Create a new ENUM with the new value added and switch the data type in place:\n\n\n\n```\n-- Rename existing enum\nALTER TYPE animal_species RENAME TO animal_species_old;\n\n-- Create new enum with new value\nCREATE TYPE animal_species AS ENUM (\n  'dog',\n  'cat',\n  'elephant'\n);\n\n-- Update the column of Animals to use the new enum\nALTER TABLE \"Animals\" ALTER COLUMN species SET DATA TYPE animal_species USING species::text::animal_species;\n\nDROP TYPE animal_species_old;\n\n```\n\nApproach 2: Use a temporary column\n\n\n\n```\n-- Create new enum type with a new name (this will be the name of the enum from now on)\nCREATE TYPE animal_type_enum AS ENUM (\n  'dog',\n  'cat',\n  'elephant'\n);\n\n-- Create a temporary column\nALTER TABLE \"Animals\" ADD COLUMN species_new animal_species_enum;\n\n-- Copy existing species into new column\nUPDATE \"Animals\" SET species_new = species::text::animal_species_enum;\n\n-- Drop old species column\nALTER TABLE \"Animals\" DROP COLUMN species;\n\n-- Rename new column\nALTER TABLE \"Animals\" RENAME COLUMN species_new TO species;\n\n-- Drop old enum\nDROP TYPE animal_species;\n\n```\n\nIn both cases, lock(s) were created and brought my application down. I believe the second way performed better than the first, but the downtime was still unacceptable. The table is in the millions of rows.\n\n\nNote that I am very much open to using something other than an ENUM--I was thinking of creating a \"Species\" table with a foreign key \"species\\_id\" in \"Animals\", but as far as I can tell this would create the same locking problem (and might be even worse given the introduction of a new foreign key constraint).\n\n\nThanks for any help!\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"sql"},"answer":"Approach 3, just add a new value to the enum:\n\n\n\n```\nALTER TYPE animal_type_enum ADD VALUE 'snake';\n\n```\n\n\n\n---\n\n\nIf you frequently add or remove new lookup values, a separate lookup table is a **much** better choice. \n\n\nAdding a new value is a simple `INSERT` operation that doesn't lock anything (*especially* not the table referencing the lookup table).\n\n\nWhile the foreign key checks do add some overhead, they shouldn't matter that much (assuming the FK column is properly indexed) unless you do bulk INSERTs or DELETEs very frequently. \n\n\nFor single row INSERTs or DELETEs (or only \"hundreds\" of rows) you probably won't even notice the overhead of the FK lookup - especially if the lookup table is small and only contains a few rows. \n\n\n"}
{"questionId":"7660b10839c64819a4778dfa65797739","question":"tuples as function arguments\na `tuple` in python (in a code block) is defined by the commas; the parentheses are not mandatory (in the cases below). so these three are all equivalent:\n\n\n\n```\na, b = 1, 2\na, b = (1, 2)\n(a, b) = 1, 2\n\n```\n\nif i define a function \n\n\n\n```\ndef f(a, b):\n    print(a, b)\n\n```\n\ncalling it this way will work:\n\n\n\n```\nf(2, 3)\n\n```\n\nthis will not:\n\n\n\n```\nf((2, 3))\n# TypeError: f() missing 1 required positional argument: 'b'\n\n```\n\nhow does python treat tuples differently when they are function arguments? here the parentheses are necessary (i understand *why* this is the case and i am happy python works this way!).\n\n\nmy question is: how does python treat tuples differently when they are function arguments.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"python"},"answer":"For convenience, Python constructs a temporary tuple as needed for an assignment statement. Thus, all three of your assignment statements are exactly the same once they reach data movement.\n\n\nA function call is not an assignment statement; it's a reference mapping. Therefore, the semantics are different.\n\n\nIf you want Python to unpack your tuple into two separate arguments, use the `*` operator:\n\n\n\n```\nf(*(2, 3))\n\n```\n\n"}
{"questionId":"014ff2cee23940248c6448f805940983","question":"How to choose multiple service when installing laravel\/sail on existing an Laravel project?\nHow can I choose multiple services when running `php artisan sail:install`?\n\n\nWhen I run it, I get this prompt asking me to choose services but I can't select more than one option. When I chose `redis`, it scaffolded the app with `redis` only.\n\n\n\n```\n$ php artisan sail:install\nWhich services would you like to install? [mysql]:\n  [0] mysql\n  [1] pgsql\n  [2] mariadb\n  [3] redis\n  [4] memcached\n  [5] meilisearch\n  [6] minio\n  [7] mailhog\n  [8] selenium\n> 3\n\nSail scaffolding installed successfully.\n\n```\n\nI tried using `[0,3,5,7,8]` but it never worked. How can I choose multiple options? I am using Ubuntu 20.04.\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"php"},"answer":"You don't need to specify the service options as an array, just a comma separated string.\n\n\nSo in your example, you only need to do:\n\n\n\n```\n0,3,5,7\n\n```\n\n"}
{"questionId":"bb4181d11e5849b5af1bbe33dda1bc5f","question":"Disable Xdebug 3 \"Could not connect\" message in CLI\nWhen working with Xdebug 3 in CLI, it constantly reports the message when there are no breakpoints set:\n\n\n\n```\n\"Xdebug: [Step Debug] Could not connect to debugging client. Tried: localhost:9003 (through xdebug.client_host\/xdebug.client_port) :-(\"\n\n```\n\nIs there a way to disable that message form showing in CLI?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"Unfortunately, the only way to disable this error is to disable generally ALL errors & warnings in xdebug.ini:\n\n\n\n```\nxdebug.log_level = 0\n\n```\n\nHopefully there are other ways in future xdebug versions (imho this should only be a weak warning).\n\n\nEDIT: As LazyOne mentioned, it's also possible to set an value for `error_log` in `php.ini`, for example `\/var\/log\/php_error.log`. With that change, the log entries are written to this file and not sent to stderr.\n\n\n"}
{"questionId":"288201f32d4a4a0abf8a1a5d45469328","question":"Is enum { a } e = 1; valid?\nA simple question: is `enum { a } e = 1;` valid?\n\n\nIn other words: does assigning a value, which *isn't present* in the set of values of enumeration constants, lead to well-defined behavior?\n\n\nDemo:\n\n\n\n```\n$ gcc t0.c -std=c11 -pedantic -Wall -Wextra -c\n<nothing>\n\n$ clang t0.c -std=c11 -pedantic -Wall -Wextra -c\n<nothing>\n\n$ icc t0.c -std=c11 -pedantic -Wall -Wextra -c\nt0.c(1): warning #188: enumerated type mixed with another type\n# note: the same warning for enum { a } e = 0;\n\n$ cl t0.c \/std:c11 \/Za \/c\n<nothing>\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c"},"answer":"From the C18 standard in 6.7.2.2:\n\n\n\n> \n> Each enumerated type shall be compatible with char, a signed integer type, or an unsigned integer type. The choice of type is implementation-defined, but shall be capable of representing the values of all the members of the enumeration.\n> \n> \n> \n\n\nSo yes `enum { a } e = 1;` is valid. `e` is a 'integer' type so it can take the value `1`. The fact that `1` is not present as an enumeration value is no issue.\nThe enumeration members only give handy identifiers for some of possible values.\n\n\n"}
{"questionId":"d5283e479d174536b3318be7852eea9c","question":"How to implement Comparable so it is consistent with identity-equality\nI have a class for which equality (as per `equals()`) *must* be defined by the object identity, i.e. `this == other`.\n\n\nI want to implement `Comparable` to order such objects (say by some `getName()` property). To be consistent with `equals()`, `compareTo()` must not return `0`, even if two objects have the same name.\n\n\nIs there a way to compare object identities in the sense of `compareTo`? I could compare `System.identityHashCode(o)`, but that would still return `0` in case of hash collisions.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"I think the real answer here is: don't implement Comparable then. Implementing this interface *implies* that your objects have a **natural** order. Things that are \"equal\" should be in the same place when you follow up that thought.\n\n\nIf at all, you should use a custom comparator ... but even that doesn't make much sense. If the thing that defines a < b ... is not allowed to give you a == b (when a and b are \"equal\" according to your < relation), then the whole approach of *comparing* is broken for your use case. \n\n\nIn other words: just because you can put code into a class that \"somehow\" results in what you want ... doesn't make it a **good** idea to do so. \n\n\n"}
{"questionId":"8a1c2f492c5b425b9ab4044c94ff3714","question":"Unexpected result with right shift after bitwise negation\nI expected that below code will output `10` because `(~port)` equal to `10100101`\nSo, when we right shift it by `4` we get `00001010` which is `10`.\nBut the output is `250`! Why?\n\n\n\n```\nint main()\n{\n    uint8_t port = 0x5a;\n    uint8_t result_8 =  (~port) >> 4;\n    \/\/result_8 = result_8 >> 4;\n\n    printf(\"%i\", result_8);\n\n    return 0;\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c"},"answer":"C promotes `uint8_t` to `int` before doing operations on it. So:\n\n\n1. `port` is promoted to signed integer `0x0000005a`.\n2. `~` inverts it giving `0xffffffa5`.\n3. An arithmetic shift returns `0xfffffffa`.\n4. It's truncated back into a `uint8_t` giving `0xfa == 250`.\n\n\nTo fix that, either truncate the temporary result:\n\n\n\n```\nuint8_t result_8 = (uint8_t)(~port) >> 4;\n\n```\n\nmask it:\n\n\n\n```\nuint8_t result_8 = (~port & 0xff) >> 4;\n\n```\n\nor xor it (thanks @Nayuki!):\n\n\n\n```\nuint8_t result_8 = (port ^ 0xff) >> 4;\n\n```\n\n"}
{"questionId":"ad879914b2ce4646abb72e4daa8152e0","question":"Visual Studio Code extensions stopped working suddenly\nI'm having problem with my Visual Studio Code. Yesterday I had my pc shut down with VS Code open, and when I turned pc on again, all VS Code extensions stopped working. I'm using React and Typescript in my project and I really need those extensions. My eslint and prettier configurations also stopped working.\nIf somebody had the same problem please answer.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"I had this problem too. You can fix with the following:\n\n\n1. Open the command palette (`Ctrl` + `Shift` + `P`)\n2. Run `Disable All Installed Extensions`\n3. Then run `Enable All Extensions`.\n4. Restart Visual Studio Code\n\n\nYou can make sure again that the extensions are enabled.\n\n\n"}
{"questionId":"e68b6e0dc160462090b371f83aef8f2d","question":"Result values in '? :' expression have mismatching types 'some View' and '...'\nI get this error when I\u2019m trying to ternary the `navigationBarItems` to have various views:\n\n\n\n> \n> Result values in '? :' expression have mismatching types 'some View' and 'ProfileImageBarButton'\n> \n> \n> \n\n\n\n```\n@State var searchTapped: Bool = false\n\nvar body: some View {\n    NavigationView {\n        Text(\"lol\")\n            \n   --> here i get the error .navigationBarItems(leading: searchTapped ? backButton : ProfileImageBarButton(showMenu: $showMenu))\n        .navigationBarTitle(Text(\"\"), displayMode: .inline)\n    }.overlay(searchTextField)\n}\n\nprivate var backButton: some View {\n    Image(systemName: \"arrow.left\")\n        .foregroundColor(Color.blue)\n        .onTapGesture {\n            self.searchTapped = false\n        }\n}\n\n```\n\nThis is `ProfileImageBarButton`:\n\n\n\n```\nstruct ProfileImageBarButton: View {\n    @Binding var showMenu: Bool\n    \n    var body: some View {\n        Image(uiImage: UserDefaults.standard.getProfileImage()!)\n            .resizable()\n            .renderingMode(.original)\n            .frame(width: 30, height: 30)\n            .clipShape(Circle())\n            .onTapGesture {\n                self.showMenu.toggle()\n            }\n    }\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"swift"},"answer":"The error is telling you that, in an expression:\n\n\n\n```\ncondition ? true_result : false_result\n\n```\n\nboth `true_result` and `false_result` need to have the same type.\n\n\nThere are multiple ways to overcome this, here are two:\n\n\n\n```\n.navigationBarItems(leading: searchTapped ? AnyView(backButton) : AnyView(ProfileImageBarButton(showMenu: $showMenu)))\n\n```\n\nor\n\n\n\n```\n.navigationBarItems(leading: barItems())\n\n...\n\nfunc barItems() -> some View {\n    return Group {\n        if searchTapped {\n            backButton\n        } else {\n            ProfileImageBarButton(showMenu: $showMenu)\n        }\n    }\n}\n\n```\n\n"}
{"questionId":"bb4f03e96a894833910d6a2d9e7256de","question":"Using case\\_when with dplyr across\nI'm trying to translate a mutate\\_at() to a mutate() using dplyr's new \"across\" function and a bit stumped.\n\n\nIn a nutshell, I need to compare the values in a series of columns to a \"baseline\" column. When the values in the columns are higher than the baseline, I need to use the baseline value. When the values in the columns are lower than or equal to the baseline, I need to keep the value. Here's an example dataset (my actual dataset is much larger):\n\n\n\n```\ntest <- structure(list(baseline = c(5, 7, 8, 4, 9, 1, 0, 46, 47), bob = c(7, \n11, 34, 9, 6, 8, 3, 49, 12), sally = c(3, 5, 2, 2, 6, 1, 3, 4, \n56), rita = c(6, 4, 6, 7, 6, 0, 3, 11, 3)), class = c(\"spec_tbl_df\", \n\"tbl_df\", \"tbl\", \"data.frame\"), row.names = c(NA, -9L), spec = structure(list(\n    cols = list(baseline = structure(list(), class = c(\"collector_double\", \n    \"collector\")), bob = structure(list(), class = c(\"collector_double\", \n    \"collector\")), sally = structure(list(), class = c(\"collector_double\", \n    \"collector\")), rita = structure(list(), class = c(\"collector_double\", \n    \"collector\"))), default = structure(list(), class = c(\"collector_guess\", \n    \"collector\")), skip = 1), class = \"col_spec\"))\n\n\n```\n\nMy current code uses mutate\\_at() and works fine:\n\n\n\n```\ntrial1 <- test %>% \n  mutate_at(\n    vars('bob','sally', 'rita'),\n    funs(case_when(\n      . > baseline ~ baseline, \n      . <= baseline ~ .)))\n\n\n```\n\nBut when I try to update it to reflect across() from dplyr 1.0, I keep getting an error. Here is my attempt:\n\n\n\n```\ntrial2 <- test %>% \n  mutate(across(c(bob, sally, rita), \n                case_when(. > baseline ~ baseline, \n                          . <= baseline ~ .)))\n\n```\n\nAnd here is the error:\n\n\n\n> \n> error: Problem with `mutate()` input `..1`.\n> x `. > baseline ~ baseline`, `. <= baseline ~ .` must be length 36 or one, not 9, 4.\n> \u2139 Input `..1` is `across(...)`\n> \n> \n> \n\n\nAny ideas what I might be doing wrong? Does case\\_when() work with across?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"r"},"answer":"We can use the `~` to specify the anonymous function\/lambda function call\n\n\n\n```\nlibrary(dplyr)\ntest %>% \n   mutate(across(c(bob, sally, rita), \n             ~ case_when(. > baseline ~ baseline, \n                       . <= baseline ~ .)))\n\n```\n\n-output\n\n\n\n```\n# A tibble: 9 x 4\n#  baseline   bob sally  rita\n#     <dbl> <dbl> <dbl> <dbl>\n#1        5     5     3     5\n#2        7     7     5     4\n#3        8     8     2     6\n#4        4     4     2     4\n#5        9     6     6     6\n#6        1     1     1     0\n#7        0     0     0     0\n#8       46    46     4    11\n#9       47    12    47     3\n\n```\n\n\n\n---\n\n\nAccording to `?across` the arguments to `fns` can be either\n\n\n\n> \n> Functions to apply to each of the selected columns. Possible values are:\n> \n> \n> \n\n\n\n> \n> NULL, to returns the columns untransformed.\n> \n> \n> \n\n\n\n> \n> A function, e.g. mean.\n> \n> \n> \n\n\n\n> \n> A purrr-style lambda, e.g. ~ mean(.x, na.rm = TRUE)\n> \n> \n> \n\n\n\n> \n> A list of functions\/lambdas, e.g. list(mean = mean, n\\_miss = ~ sum(is.na(.x))\n> \n> \n> \n\n\n\n\n---\n\n\nAlso, instead of `case_when`, we can make use of the `pmin`\n\n\n\n```\ntest %>% \n    mutate(across(c(bob, sally, rita), ~ pmin(baseline, .)))\n\n```\n\n-output\n\n\n\n```\n# A tibble: 9 x 4\n#  baseline   bob sally  rita\n#     <dbl> <dbl> <dbl> <dbl>\n#1        5     5     3     5\n#2        7     7     5     4\n#3        8     8     2     6\n#4        4     4     2     4\n#5        9     6     6     6\n#6        1     1     1     0\n#7        0     0     0     0\n#8       46    46     4    11\n#9       47    12    47     3\n\n```\n\n"}
{"questionId":"7fc6fb1b8c5f4cf98eae67eee7b75b61","question":"RVM install ruby 2.6.4 fails with makefile error: implicit declaration of function 'ffi\\_prep\\_closure' is invalid in C99\n**RVM rvm 1.29.12-next**\n\n\n**Mac OS Big Sur on M1 chip**\n\n\n`rvm install 2.6.4`\nfails with:\n\n\n\n```\nError running '__rvm_make -j8',\nplease read \/Users\/jason\/.rvm\/log\/1626110300_ruby-2.6.4\/make.log\n\nThere has been an error while running make. Halting the installation.\n\n```\n\nthe makefile error is:\n\n\n\n```\nerror: implicit declaration of function 'ffi_prep_closure' is invalid in C99 [-Werror,-Wimplicit-function-declaration]\n    result = ffi_prep_closure(pcl, cif, callback, (void *)self);\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"ruby"},"answer":"this is caused by the environment flags not being set in your shell.\n\n\nyou need to run `brew install libffi`, but to do that, the env vars must be set in your shell\n\n\n`brew info libffi`\n\n\nwill tell you the variables you need:\n\n\nFor compilers to find libffi you may need to set:\n\n\n\n```\nexport LDFLAGS=\"-L\/opt\/homebrew\/opt\/libffi\/lib\"\nexport CPPFLAGS=\"-I\/opt\/homebrew\/opt\/libffi\/include\"\n\n```\n\nFor pkg-config to find libffi you may need to set:\n\n\n\n```\nexport PKG_CONFIG_PATH=\"\/opt\/homebrew\/opt\/libffi\/lib\/pkgconfig\"\n\n```\n\nfor ZSH, edit `~\/.zshenv` and add:\n\n\n\n```\nexport LDFLAGS=\"-L\/opt\/homebrew\/opt\/libffi\/lib\"\nexport CPPFLAGS=\"-I\/opt\/homebrew\/opt\/libffi\/include\"\nexport PKG_CONFIG_PATH=\"\/opt\/homebrew\/opt\/libffi\/lib\/pkgconfig\"\n\n```\n\nClose & reopen your terminal window or `source ~\/.zshrc` to load your changes\n\n\nCheck your work with\n\n\n\n```\n% echo $LDFLAGS\n-L\/opt\/homebrew\/opt\/libffi\/lib\n% echo $CPPFLAGS\n-I\/opt\/homebrew\/opt\/libffi\/include\n% echo $PKG_CONFIG_PATH\n\/opt\/homebrew\/opt\/libffi\/lib\/pkgconfig\n\n```\n\n(confirm that the environment variables are correct)\n\n\nre-try to install the rvm version you want to install.\n\n\n"}
{"questionId":"50fe73e0ae554bc7a456e9917597afce","question":"AWS Lambda keeps returning \"\\\"Hello from Lambda!\\\"\nI'm having some issues with AWS Lambda for Python 3.8. No matter what code I try running, AWS Lambda keeps returning the same response. I am trying to retrieve a information from a DynamoDB instance with the code below:\n\n\n\n```\nimport json\nimport boto3\n\ndynamodb = boto3.resource('dynamodb')\ntable = dynamodb.Table('planets')\n\ndef lambda_handler(event, context):\n    response = table.get_item(\n        Key = {\n            'id':'mercury'\n        }\n    )\n    print(response)\n    # TODO implement\n    return {\n        'statusCode': 200,\n        'body': response)\n    }\n\n```\n\nI am expecting an output like `'body':{'Item': {'id':'mercury', 'temp':'sizzling hot'}}`, or an error even, but I keep getting the response below:\n\n\n\n```\nResponse:\n{\n  \"statusCode\": 200,\n  \"body\": \"\\\"Hello from Lambda!\\\"\"\n}\n\n```\n\nI even change up the code, expecting an error, but I still get the same output.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"Usually this is due to one of the following reasons:\n\n\n1. You are **not deploying** your code changes. In the new UI, you have to explicitly `Deploy` your function using **Orange** button.\n2. You are invoking **old lambda version**, rather then your latest version, if you are versioning your functions. You must explicitly choose the correct version to invoke.\n\n\n"}
{"questionId":"6064a6cf1a874c3aba6699ac6cbfb0bb","question":"Clojure: destructure and rename with {:keys [...]}\nIs it possible to both destructure and rename keys in one go?\n\n\nConsider this:\n\n\n\n```\n(let [{:keys [response]} {:response 1}]\n  (println response))\n\n```\n\nHowever, if I want to instead refer to `1` as `my-response`, I have to do something like:\n\n\n\n```\n(let [{:keys [my-response]} (clojure.set\/rename-keys {:response 1} {:response :my-response})]\n  (println my-response))\n\n```\n\nObviously this does not work with `defn` destructuring.\n\n\nIs there any way in Clojure to both destructure and rename keys?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"clojure"},"answer":"Use destructuring without `:keys`:\n\n\n\n```\n(let [{my-response :response} {:response 1}]\n  (println my-response))\n\n```\n\n`{:keys [response]}` is syntactic sugar for `{response :response}`.\n\n\n"}
{"questionId":"1621168ec5a64a498eb647555bbd45ea","question":"How can I validate the string length using java spring validation?\nI have this field declared in a model class:\n\n\n\n```\n@Size(min = 2, max = 200, message = \"{validation.name.size}\")\nprivate String name;\n\n```\n\nwhere `validation.name.size` is the path to a localized message. The problem is that I do not want to output a message like 'The name is too long or too short.'.\n\n\nIs there any way to use two different messages and check minimum and maximum string length? `@Min` and `@Max` are only working for numeric types, so they can not be used. What is the alternative for strings?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"You can just use the \"@Size\"-annotation twice:\n\n\n\n```\n@Size(min = 2, message = \"{validation.name.size.too_short}\")\n@Size(max = 200, message = \"{validation.name.size.too_long}\")\nprivate String name;\n\n```\n\n"}
{"questionId":"335de58a24224091b666ed6c2dd81d7e","question":"C++ copy constructor called at return\n\n```\nerror: use of deleted function 'A::A(const A&)'\n return tmp;\n        ^~~\n\n```\n\nWhy is the copy constructor called only when there is a virtual destructor in `A`? How to avoid this?\n\n\n\n```\nstruct B {};\n\nstruct A{\n    std::unique_ptr<B> x;\n    virtual ~A() = default;\n};\n\nA f() {\n    A tmp;\n    return tmp;\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c++"},"answer":"`virtual ~A() = default;` is a user declared destructor. Because of that, `A` no longer has a move constructor. That means `return tmp;` can't move `tmp` and since `tmp` is not copyable, you get a compiler error.\n\n\nThere are two ways you can fix this. You can add a move constructor like\n\n\n\n```\nstruct A{\n    std::unique_ptr<B> x;\n\n    A() = default; \/\/ you have to add this since the move constructor was added\n    A(A&&) = default; \/\/ defaulted move\n    virtual ~A() = default;\n};\n\n```\n\nor you can create a base class that has the virtual destructor and inherit from that like\n\n\n\n```\nstruct C {\n    virtual ~C() = default;\n};\n\nstruct A : C {\n    std::unique_ptr<B> x;\n};\n\n```\n\nThis works because `A` no longer has a user declared destructor (Yes, `C` does but we only care about `A`) so it will still generate a move constructor in `A`. The important part of this is that `C` doesn't have a deleted move constructor, it just doesn't have one period, so trying to move it will cause a copy. That means\n`C`'s copy constructor is called in `A`'s implicitly generated move constructor since `C(std::move(A_obj_to_move_from))` will copy as long as it doesn't have a deleted move constructor.\n\n\n"}
{"questionId":"dcf731b2429941f09d69fad63e2c771b","question":"How to delete entries from fish shell's command history?\nIn an attempt to unlock the screen, I have accidentally entered my password into a terminal running the fish shell.\n\n\nHow can I remove it from the command history of fish again?\n\n\nThe solution does not need to be forensically secure. Traces left in backup files or on the drive are fine with me. I just want to avoid e.g. my password accidentally popping up in a screen sharing session when I use fish's history while sharing. And I want to keep all the other commands in the history, because I often recall complicated commands from the history, sometimes editing them before reusing.\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"bash\/shell"},"answer":"You want `history delete`. That should ask you for a search term, show you matching entries and ask you for which to delete.\n\n\n"}
{"questionId":"699780e681c34019933965cdd1ce22bb","question":"Write output of `npm run start` to a file\nI'd like to capture the output of `npm run start` in a file (I'm getting a ton of errors and I'd like to have more control over how I sift through the output). \n\n\nWhen I try \n\n\n\n```\nnpm run start > log.txt\n\n```\n\nI get a very abbreviated file (8 lines) that ends with `[34m\u2139[39m [90m\uff62wdm\uff63[39m: Failed to compile.`\n\n\nWhen I try \n\n\n\n```\nnpm run start &> log.txt \/\/ redirect stderr and stdout to a file\n\n```\n\nI get a similarly abbreviated file (11 lines) that ends with similarly garbled output.\n\n\nWhat am I missing?\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"bash\/shell"},"answer":"**This will work**\n\n\n\n```\nnpm run start 2>&1| tee npm.txt\n\n```\n\n**Explanation:**\n\n\n`2>&1` will redirect error `stderr` to `stdout` and tee command will write terminal output to file.\n\n\n"}
{"questionId":"c649b90ff142438bbfc16b4afc720549","question":"How to use DTOs in the Controller, Service and Repository pattern\nI'm following the Controller, Service and Repository pattern and I'm just wondering where DTOs come into this.\n\n\nShould the controller only receive DTOs? My understanding is you wouldn't want the outside world to know about the underlying domain model? \n\n\nShould the conversion from domain model to DTO happen in the controller or service layer? \n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"java"},"answer":"In today programming with Spring MVC and interactive UIs, there are really 4 layers to a web application:\n\n\n- UI Layer (Web Browser, JavaScript)\n- MVC Controller, i.e. Spring components annotated with `@Controller`\n- Service Layer, i.e. Spring components annotated with `@Service`\n- Data Access Layer, i.e. Spring components annotated with `@Repository`\n\n\nEvery time one of these layers interact with the underlying layer, they need to send\/receive data, which generally are POJOs, to transfer the data between layers. These POJOs are DTOs, aka Data Transfer Objects.\n\n\nOnly DTOs should be used between layers, and they are not necessarily the same, e.g. the Service Layer may apply business logic to DTOs received from the Data Access Layer, so the DTOs of the Service Layer API is different from the Data Access Layer API. Similarly, the Controller may rearrange the data to prepare it for presentation (grouping, summaries, ...), so the data sent to the web browser is different from the data received from the Service Layer.\n\n\nWith full abstraction, the Data Access Layer's API should not reflect the technology of the Data Access, i.e. whether it is using JDBC, JPA, NoSQL, a web service, or some other means of storing\/retrieving the data. This means that Entity classes shouldn't make it outside the Data Access Layer.\n\n\nMost projects don't need that level of abstraction, so it is common for a DTO to be an Entity class, and to flow all the way from the Data Access Layer to the Controller, where it is used either by a View, or is send to the web browser, encoded as JSON.\n\n\nIt depends on the size and complexity of the project. The bigger the project, the more important it becomes to make each layer as abstract\/standalone as possible.\n\n\n"}
{"questionId":"fb52bcb8e4e04d86966e16b2f88a8aca","question":"Create database if db does not exist\nI want to make `SQL Server` script for creating database if it does not exist.\n\n\n\n```\nIF NOT EXISTS(SELECT * FROM sys.databases WHERE name = 'DataBase')\n  BEGIN\n    CREATE DATABASE DataBase\n    \n    USE DataBase\n    \n    CREATE TABLE TableName (\n        Id INT PRIMARY KEY IDENTITY (1, 1),\n        Name VARCHAR(100)\n    )\n\n    --more tables here\n    --some procedures here too\n\n  END\n\n```\n\nFrom code above I'm getting this error:\n\n\n\n> \n> Msg 911, Level 16, State 1, Line 5\n> Database 'DataBase' does not exist. Make sure that the name is entered correctly.\n> \n> \n> \n\n\nHow to make database with tables and procedures when database does not exist? I want to make it in one query.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"Could you check the following script :\n\n\n\n```\n    IF NOT EXISTS(SELECT * FROM sys.databases WHERE name = 'DataBase')\n  BEGIN\n    CREATE DATABASE [DataBase]\n\n\n    END\n    GO\n       USE [DataBase]\n    GO\n--You need to check if the table exists\nIF NOT EXISTS (SELECT * FROM sysobjects WHERE name='TableName' and xtype='U')\nBEGIN\n    CREATE TABLE TableName (\n        Id INT PRIMARY KEY IDENTITY (1, 1),\n        Name VARCHAR(100)\n    )\nEND\n\n```\n\n"}
{"questionId":"f7ae5cba6aa542a881214d977f8148b1","question":"Trait `Borrow is not implemented for `&str`\nI am trying out some examples in the `The Rust Programming Language` book, and have the following code snippet:\n\n\n\n```\nfn main() {\n    let mut map: HashMap<&str, i32, RandomState> = HashMap::new();\n    let hello: String = String::from(\"hello\");\n    map.insert(&hello, 100);\n    println!(\"{:?}\", map); \/\/{\"hello\": 100}\n    let first_hello_score: Option<&i32> = map.get(\"hello\"); \/\/ This compiles\n    let hello_score: Option<&i32> = map.get(&hello); \/\/ This does not compile\n}\n\n```\n\nOn running `cargo check`, I see:\n\n\n\n```\nerror[E0277]: the trait bound `&str: Borrow<String>` is not satisfied\n  --> src\/main.rs:26:27\n   |\n26 |     let hello_score = map.get(&hello);\n   |                           ^^^ the trait `Borrow<String>` is not implemented for `&str`\n\nerror: aborting due to previous error\n\nFor more information about this error, try `rustc --explain E0277`.\n\n```\n\nCould someone explain why this happens?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"rust"},"answer":"`.get` looks for `&Q` as the parameter, where the key type `K` is `Borrow<Q>`. Since there's a blanket implementation that borrows `&T` into `&T`, `&str` (the key type) can be borrowed into `&str` (the argument type)\n\n\nHowever, when doing `&hello`, you actually have an `&String`, which means Rust infers `String` to be `Q`, so it tries to borrow `&str` into `&String`, which is obviously not possible. So, be **explicit** about the deref coercion so that Rust knows it should deref the `&String` into `&str`:\n\n\n\n```\nlet hello_score: Option<&i32> = map.get(&hello as &str);\n\n```\n\nOr,\n\n\n\n```\nlet hello_score: Option<&i32> = map.get(&*hello);\n\n```\n\n"}
{"questionId":"176d3290a1344f49aa7c830bcf28cd40","question":"Angular How to catch mat-tab changed event\nHere I need to send tab.staffMemberId to a service and get values and fill matInput values. I need to send tab.staffMemberId to service when tab is changing\n\n\n\n```\n<mat-tab-group>\n<mat-tab *ngFor=\"let tab of StaffMemberList; let index = index\" [label]=\"tab.staffMemberId\">\n{{tab.id}}\n<mat-grid-list cols=\"3\" rowHeight=\"8:1\">\n    <mat-grid-tile>\n      <mat-form-field class=\"full-width\">\n        <input matInput placeholder=\"Position\" >\n      <\/mat-form-field>\n    <\/mat-grid-tile>\n<\/mat-grid-list>\n\n```\n\n\n\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"You can do it in this way \n\n\n\n```\n <mat-tab-group  [(selectedIndex)]=\"selectedTabIndex\" (selectedTabChange)=\"onTabChanged($event);\">\n      <mat-tab formArrayName=\"staffMembers\" #pft  *ngFor=\"let staffMember of formData.get('staffMembers').controls; let i = index\">\n        <div [formGroupName]=\"i\">\n          <ng-template mat-tab-label>\n            {{staffMember.controls.staffMemberId.value}} <a>\n          <\/a>\n          <\/ng-template>\n            <div class=\"form-group\">\n              <label for=\"name\"> Name<\/label>\n              <input type=\"text\" class=\"form-control\" id=\"name\" formControlName=\"name\">\n           <\/div> \n        <\/div>\n    <\/mat-tab>\n <\/mat-tab-group>\n\n```\n\nand Create FormArray For staffMember Like this in Component\n\n\n\n```\nformData: FormGroup = this.formBuilder.group({\n    staffMembers: this.formBuilder.array([this.createItem()])\n\n }\n\n```\n\nOn Tab Change you can assign value To form Array \n\n\n\n```\n createItem(): FormGroup {\n    return this.formBuilder.group({\n      staffMemberId: ['',],\n      name: ['',],\n\n    });\n  }\n   addItem(): void {\n    this.projectFundingItems = this.formProjectGeneralData.get('staffMembers') as FormArray;\n    this.projectFundingItems.push(this.createItem());\n  }\n\n\n  onTabChanged($event) {\n    let clickedIndex = $event.index;\n    let length = (<FormArray>this.formData.controls['staffMembers']).length;\n    if (clickedIndex === length) {\n      if ((<FormArray>this.formData.controls['staffMembers']).at(length - 1).dirty) {\n        this.addItem();\n        this.selectedTabIndex = clickedIndex - 1;\n      } else {\n        if (this.formData.staffMembers.length === clickedIndex) {\n          this.addItem();\n        }\n        this.selectedTabIndex = clickedIndex - 1;\n      }\n    }\n  }\n\n```\n\n"}
{"questionId":"02fe128cef5042f7bc6984b9ac14e1b9","question":"Typescript interface type values to union type\nIs it possible to get a union type with all type values from an interface in typescript?\n\n\nFor example, when an interface is given as\n\n\n\n```\ninterface A {\n  a: string;\n  b: () => void;\n  c: number;\n  d: string;\n  e: 'something';\n}\n\n```\n\nthe result should be\n\n\n\n```\ntype B = string | () => void | number | 'something';\n\n```\n\nI have no clue, how I would approach this problem, if it is even possible.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"You can use `keyof` e.g.\n\n\n\n```\n type B = A[keyof A] \/\/ will be string | number | (() => void)\n\n```\n\nThe type `something` will not appear since the compiler does not differentiate between the type `string` and `something` - since both are strings it will omit it in the `type B`.\n\n\n"}
{"questionId":"0ecbcd01aa2c4ec683c0d5e9c0305139","question":"Forcing excess-property checking on variable passed to TypeScript function\nIs there a way to *force* excess-property checking to happen, not just for an inlined object literal but one that derives from a variable?\n\n\nFor example, suppose I have an interface and a function\n\n\n\n```\ninterface Animal {\n\u00a0 \u00a0 speciesName: string\n\u00a0 \u00a0 legCount: number,\n}\n\nfunction serializeBasicAnimalData(a: Animal) {\n\u00a0 \u00a0 \/\/ something\n}\n\n```\n\n  \n\n\n\nIf I call\u00a0\n\n\n\n```\nserializeBasicAnimalData({\n\u00a0 \u00a0 legCount: 65,\n\u00a0 \u00a0 speciesName: \"weird 65-legged animal\",\n\u00a0 \u00a0 specialPowers: \"Devours plastic\"\n})\n\n```\n\nI would get an error -- which for my situation, is what I want. \u00a0I only want the function to take in a generic animal description without extra specifics.\n\n\n  \n\nOn the other hand, if I first create a variable for it, I don't get the error:\n\n\n\n```\nvar weirdAnimal = {\n\u00a0 \u00a0 legCount: 65,\n\u00a0 \u00a0 speciesName: \"weird 65-legged animal\",\n\u00a0 \u00a0 specialPowers: \"Devours plastic\"\n};\nserializeBasicAnimalData(weirdAnimal);\n\n```\n\n  \n\n\n\nSo my question is: Is there a way to somehow force TypeScript to apply \"excess property checking\" to the function parameter irrespective of whether it's an inlined object or an object that has previously been assigned to a variable?\n\n\n\u00a0\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"Hope this helps, this will cause it to fail. The underlying cause here is Typescripts reliance on structural typing which is alot better than the alternative which is Nominal typing but still has its problems. \n\n\n\n```\ntype StrictPropertyCheck<T, TExpected, TError> = Exclude<keyof T, keyof TExpected> extends never ? {} : TError;\n\ninterface Animal {\n    speciesName: string\n    legCount: number,\n}\n\nfunction serializeBasicAnimalData<T extends Animal>(a: T & StrictPropertyCheck<T, Animal, \"Only allowed properties of Animal\">) {\n    \/\/ something\n}\n\nvar weirdAnimal = {\n    legCount: 65,\n    speciesName: \"weird 65-legged animal\",\n    specialPowers: \"Devours plastic\"\n};\nserializeBasicAnimalData(weirdAnimal); \/\/ now correctly fails\n\n```\n\n"}
{"questionId":"5c23753b43e743fcaf1417342e4cb8dc","question":"Are threads copied when calling fork?\nIf I have a program running with threads and call `fork()` on a unix-based system, are the threads copied? I know that the virtual memory for the current process is copied 1:1 to the new process spawned. I know that threads have their own stack in the virtual memory of a process. Thus, at least the stack of threads should be copied too. However, I do not know if there is anything more to threads that does not reside in virtual memory and is thus NOT copied over. If there is not, do the two processes share the threads or are they independent copies?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"### No.\n\n\nThreads are not copied on `fork()`. POSIX specification says (emphasize is mine):\n\n\n\n> \n> fork - create a new process\n> \n> \n> **A process shall be created with a single thread**. If a multi-threaded process calls fork(), the new process shall contain a replica of the calling thread and its entire address space, possibly including the states of mutexes and other resources. Consequently, to avoid errors, the child process may only execute async-signal-safe operations until such time as one of the exec functions is called.\n> \n> \n> \n\n\nTo circumvent this problem, there exists a `pthread_atfork()` function to help.\n\n\n"}
{"questionId":"b4c1b7dbdc9a48a0908afbf40034c69a","question":"zsh not updating vcs\\_info\nI'm just trying to show the current branch of the git repository I'm inside (if available) by using `vcs_info`. The relevant portion of my `.zshrc` file is as follows:\n\n\n\n```\nautoload -Uz vcs_info\nzstyle ':vcs_info:*' enable git\nzstyle ':vcs_info:*' formats \"%F{010}(%b)%f \"\n\nprecmd() { vcs_info }\nsetopt prompt_subst\n\nPROMPT=\"%F{226}%m:%n @ %F{214}%1d %F{226}\\$%f ${vcs_info_msg_0_}\"\n\n```\n\n### I expect:\n\n\n1. I load the terminal and start at `~` (the home directory). zsh prompt should read\n\n\n`hostname:username @ user $`\n2. `cd dev\/repo` takes me into a git repo, zsh prompt should read\n\n\n`hostname:username @ repo $ (master)`\n3. `cd ..` takes me back to `dev`, which isn't a git repo, prompt should read\n\n\n`hostname:username @ dev $`\n\n\n### I experience:\n\n\nThe prompt never changes \/ updates automatically; I have to run `source ~\/.zshrc` to make the prompt update as I change directories.\n\n\n### What I have tried:\n\n\nI've tried updating the `precmd()` block to be as follows:\n\n\n\n```\nprecmd() {\n    vcs_info\n    echo \"This has been executed\"\n}\n\n```\n\nAnd I see `This has been executed` right before every prompt, so I know that the precmd block is being entered correctly. It seems that the `vcs_info` just isn't working.\n\n\nPerhaps I'm missing something; can someone point out what the issue could be? Thanks!\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"Figured it out by happenstance a few months later after not really caring... the prompt has to use single quotes instead of double-quotes.\n\n\n`PROMPT='%F{226}%m:%n @ %F{214}%1d %F{226}\\$%f ${vcs_info_msg_0_}'`\n\n\n"}
{"questionId":"1079d418cf9f48bf806307a71e68bf03","question":"Why are there two ways of expressing NULL in C?\nAccording to \u00a76.3.2.3 \u00b63 of the C11 standard, a null pointer constant in C can be defined by an implementation as either the integer constant expression `0` or such an expression cast to `void *`. In C the null pointer constant is defined by the `NULL` macro.\n\n\nMy implementation (GCC 9.4.0) defines `NULL` in `stddef.h` in the following ways:\n\n\n\n```\n#define NULL ((void *)0)\n#define NULL 0\n\n```\n\nWhy are both of the above expressions considered semantically equivalent in the context of `NULL`? More specifically, why do there exist two ways of expressing the same concept rather than one?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"`((void *)0)` has stronger typing and could lead to better compiler or static analyser diagnostics. For example since implicit conversions between pointers and plain integers aren't allowed in standard C.\n\n\n`0` is likely allowed for historical reasons, from a pre-standard time when everything in C was pretty much just integers and wild implicit conversions between pointers and integers were allowed, though possibly resulting in undefined behavior.\n\n\nAncient K&R 1st edition provides some insight (7.14 the assignment operator):\n\n\n\n> \n> The compilers currently allow a pointer to be assigned to an integer, an integer to a pointer, and a pointer to a pointer of another type. The assignment is a pure copy operation, with no conversion. This usage is nonportable, and may produce pointers which cause addressing exceptions when used. However, it is guaranteed that assignment of the constant 0 to a pointer will produce a null pointer distinguishable from a pointer to any object.\n> \n> \n> \n\n\n"}
{"questionId":"290fa7b6897945adb90f51ac0d0e4e13","question":"How do I convert an float to an integer in Elixir\nGiven a float, say (2.0), I want to convert it to an Integer type. It looks like Integer.parse only works for strings as far as I can tell.\n\n\n\n```\nInteger.parse(2.0)\n(FunctionClauseError) no function clause matching in Integer.count_digits\/2\n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"elixir"},"answer":"To summarize the different answers listed on this question, there are good four options as of writing this: `trunc\/1`, `round\/1`, `floor\/1`, and `ceil\/1`. All accept both floats and integers.\n\n\n### trunc\/1\n\n\nRemoves the decimal part of a float.\n\n\n\n```\niex> trunc(2.3)\n2\niex> trunc(-2.3)\n-2\n\n```\n\n### round\/1\n\n\nRounds to the nearest integer.\n\n\n\n```\niex> round(2.3)\n2\niex> round(2.7)\n3\niex> round(-2.3)\n-2\niex> round(-2.7)\n-3\n\n```\n\n### floor\/1\n\n\nAlways rounds down. Available as of Elixir 1.8.0.\n\n\n\n```\niex> floor(2.3)\n2\niex> floor(-2.3)\n-3\n\n```\n\n### ceil\/1\n\n\nAlways rounds up. Available as of Elixir 1.8.0.\n\n\n\n```\niex> ceil(2.3)\n3\niex> ceil(-2.3)\n-2\n\n```\n\n"}
{"questionId":"0b616509ff3d4fc2a706a1d53783523f","question":"Cannot find name 'describe'. Do you need to install type definitions for a test runner?\nWhen using TypeScript in conjunction with Jest, my specs would fail with error messages like:\n\n\n\n```\ntest\/unit\/some.spec.ts:1:1 - error TS2582: Cannot find name 'describe'. Do you need to install type definitions for a test runner? Try `npm i @types\/jest` or `npm i @types\/mocha`.\ntest\/unit\/some.spec.ts:2:3 - error TS2582: Cannot find name 'it'. Do you need to install type definitions for a test runner? Try `npm i @types\/jest` or `npm i @types\/mocha`.\ntest\/unit\/some.spec.ts:3:7 - error TS2304: Cannot find name 'expect'.\ntest\/unit\/some.spec.ts:7:1 - error TS2582: Cannot find name 'test'. Do you need to install type definitions for a test runner? Try `npm i @types\/jest` or `npm i @types\/mocha`.\n\n```\n\nThe types are already installed.\n\n\nI use:\n\n\n\n```\n    \"@types\/jest\": \"^23.3.12\",\n    \"jest\": \"^23.6.0\",\n    \"ts-jest\": \"^23.10.5\",\n    \"typescript\": \"^3.1.6\"\n\n```\n\nI run tests using `jest --forceExit --coverage --verbose`\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"After fiddling with the `tsconfig.json` for a while I finally figured that commenting the `\"types\": [],` will work.\n\n\n### Failing configuration (before)\n\n\n\n```\n\/\/ tsconfig.json\n{\n  \"compilerOptions\": {\n    \"types\": []\n  }\n}\n\n```\n\n### Working configuration\n\n\n\n```\n\/\/ tsconfig.json\n{\n  \"compilerOptions\": {\n    \/\/ \"types\": []\n  }\n}\n\n```\n\n"}
{"questionId":"68697b927b6f43a7af8ea1bb887cbf28","question":"STRING\\_AGG aggregation result exceeded the limit of 8000 bytes error\nI need to combine texts by group. I found a function called `STRING_AGG`.\n\n\n\n```\nselect c.id\n, c.bereichsname\n, STRING_AGG(j.oberbereich,',') oberBereiches \nfrom stellenangebote_archiv as j\njoin bereiche as c on j.bereich_id = c.id\ngroup by c.id, c.bereichsname\n\n```\n\nBut I am getting the following error:\n\n\n\n> \n> STRING\\_AGG aggregation result exceeded the limit of 8000 bytes. Use LOB types to avoid result truncation.\n> \n> \n> \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"Try as below\n\n\n\n```\nselect c.id\n, c.bereichsname\n, STRING_AGG( CAST(j.oberbereich as nvarchar(MAX)),',') oberBereiches \nfrom stellenangebote_archiv j\njoin bereiche c on j.bereich_id = c.id\ngroup by c.id, c.bereichsname\n\n```\n\nSo the problem is the length of the concatenated string is exceeding the character limit of the result column.\n\n\nSo we are setting the limit to max by converting all values to \"nvarchar(max)\" to solve the problem.\n\n\nAnd \"STRING\\_AGG()\" function returns what it gets.\n\n\n"}
{"questionId":"eac340809ed74cffb193797565b975b5","question":"Laravel 8 factory state afterCreating\nLaravel 7 factories had method **afterCreatingState()** where you could define what should happen after Model with specific state was saved into database.\n\n\n\n```\n$factory->afterCreatingState(App\\User::class, 'active', function ($user, $faker) {\n    \/\/ ...\n});\n\n```\n\nLaravel 8 factories don't have this method, instead there is only general **afterCreating()**.\n\n\n\n```\npublic function configure()\n{\n    return $this->afterCreating(function (User $user) {\n        \/\/\n    });\n}\n\n```\n\nHow to achieve this behavior?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"php"},"answer":"It is possible to define this behavior right in the state definition method.\n\n\n\n```\npublic function active()\n{\n    return $this->state(function (array $attributes) {\n        return [\n            'active' => true,\n        ];\n    })->afterCreating(function (User $user) {\n        \/\/ ...\n    });\n}\n\n```\n\n"}
{"questionId":"bdf7ba973ec44ff4969aa9716e05e994","question":"EditorConfig control File-scoped namespace declaration\nI'm using C# 10 new feature `File-scoped namespace declaration`.\n\n\nI have old code like this\n\n\n\n```\nnamespace SampleCode\n{\n    public class MyClass\n    {\n    }\n}\n\n```\n\nI'm moving this code to\n\n\n\n```\nnamespace SampleCode;\n\npublic class MyClass\n{\n}\n\n```\n\nBut I have a bunch of warnings : `IDE0160: Convert to block scoped namespace`\n\n\nHow do I make sure people will have warnings only with old syntax ?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"To control the code style in editorconfig use this line :\n\n\nTo enforce this style\n\n\n\n```\nnamespace SampleCode\n{\n    public class MyClass\n    {\n    }\n}\n\n```\n\nAdd this line in `.editorconfig`\n\n\n\n```\n# IDE0160: Convert to block-scoped namespace\ncsharp_style_namespace_declarations = block_scoped:warning\n\n```\n\nTo enforce this style\n\n\n\n```\nnamespace SampleCode;\n\npublic class MyClass\n{\n}\n\n```\n\nAdd this line in `.editorconfig`\n\n\n\n```\n# IDE0160: Convert to file-scoped namespace\ncsharp_style_namespace_declarations = file_scoped:warning\n\n```\n\n"}
{"questionId":"68e2ae59118d4f23a57a074f4cafa292","question":"I synced the settings and extensions on vscode and I want to revert the changes\nSo the issue is rather something I caused myself by accident. I was trying to sync two of my computers, a Mac and a Linux. I wanted to sync the Mac to the Linux so I connected my GitHub account to my Mac vs-code when a pop-up displayed that gave two options like \"Replace\" and \"Merge\". I wanted to merge the sync but I accidentally hit Replace when all the Settings on my Mac vs-code was replaced.\n\n\n***What errors do you see?***\n\n\nAnd one thing I noticed is that every time I open the terminal it takes me to a path that was defined in my Linux instead of starting at default or $PATH.\n\n\n***What's the environment and are there recent changes?***\n\n\nLocal, I haven't noticed changes except for the starting path in my VS-code terminal.\n\n\n***What have you tried to troubleshoot this?***\n\n\nI did try and I stumbled upon the fact that I can change the terminal's default path in the settings or even in settings.json but I didn't want to cause more problems by overriding any default settings. I also noticed that my Mac vs-code basically just took every setting in my Linux vs-code.\nAlso, only the terminal in my vs-code was impacted the default terminal on my Mac displayed ~ on login.\n\n\nAnd because this was annoying me I basically went on and just added cd ~ to my ~\/.zsh\\_profile so that it just brings me back to ~ but I believe that the Linux settings still have an influence over my Mac vs-code settings and I just want to reverse this. Is there any way I can get back my old configuration? If so how can I do this? Can Github possibly have my old configurations? or is there a reset that I can do that will take me to the very first settings?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"Yes, you can restore all of your tweaks. You just need to follow these steps:\n\n\n\n\n---\n\n\n**EDIT**: It seems like VS Code has updated since I made this post and certain things have changed. Try these first:\n\n\n1. Press `ctrl` + `shift` + `p` (open command palette, shortcuts might be different on your machine).\n2. ~~Type `Settings Sync: Focus on Sync Activity (Local) View`.~~\n\n\n\n> \n> **Edit (Aug 2023)**: Type `Settings Sync: Show Synced Data`.\n> \n> \n> \n\n\n3. You'll notice that the `Primary sidebar` showing three portions of sync related data. The one you need to focus on is called `SYNC ACTIVITY (LOCAL)`.\n4. All the changes that you have done to your preferences will show up there, simply click on back arrow icon next to its title to revert the change.\n\n\n\n\n---\n\n\nGo to **File** > **Preferences** > **Setting Sync is on** (its the last option in the menu).\n\n\nThen click on **Settings sync: Show Synced Data**. You'll notice *Side Bar* open with 2 sections: **SYNC ACTIVITY** and **SYNCED MACHINES**. Right click on **SYNC ACTIVITY** then tick the **SYNC ACTIVITY(LOCAL)**.\n\n\nNow you can finally revert the changes that you made before *Accidental Sync* by clicking on the restore icons of each of them.\n\n\n\n\n---\n\n\nBTW sorry for my broken English, I'm not a native.\n\n\n"}
{"questionId":"fddd364b81a14b18a0900ddbc2512701","question":"How to check if a string is all upper or lower case in Go?\nWhat is an easy way in Golang to check if all characters in a string are upper case or lower case? \n\n\nAlso, how to handle a case where the string has punctuation?\n\n\nSee these examples:\n\n\n\n```\npackage main\n\nimport (\n    \"fmt\"\n    \"unicode\"\n)\n\nfunc main() {\n    s := \"UPPERCASE\"\n    fmt.Println(s.IsUpper())  \/\/ Should print true\n\n    s = \"lowercase\"\n    fmt.Println(s.IsUpper())  \/\/ Should print false\n\n    s = \"lowercase\"\n    fmt.Println(s.IsLower())  \/\/ Should print true\n\n    s = \"I'M YELLING AT YOU!\"\n    fmt.Println(s.IsUpper())  \/\/ Should print true\n}\n\n```\n\nNote: s.IsUpper() and s.IsLower() doesn't really exist, but would be nice to find an equivalent. \n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"go"},"answer":"You can of course compare the upper and lower cased strings in their entirety, or you can short-circuit the comparisons on the first failure, which would be more efficient when comparing long strings. \n\n\n\n```\nfunc IsUpper(s string) bool {\n    for _, r := range s {\n        if !unicode.IsUpper(r) && unicode.IsLetter(r) {\n            return false\n        }\n    }\n    return true\n}\n\nfunc IsLower(s string) bool {\n    for _, r := range s {\n        if !unicode.IsLower(r) && unicode.IsLetter(r) {\n            return false\n        }\n    }\n    return true\n}\n\n```\n\n"}
{"questionId":"9a94536124304533aa6fb54c881f944a","question":"Most efficient way to check if $string starts with $needle in perl\nGiven two string variables `$string` and `$needle` in `perl`, what's the most efficient way to check whether `$string` starts with `$needle`.\n\n\n- `$string =~ \/^\\Q$needle\\E\/` is the closest match I could think of that does what is required but is the least efficient (by far) of the solutions I tried.\n- `index($string, $needle) == 0` works and is relatively efficient for some values of `$string` and `$needle` but needlessly searches for the needle in other positions (if not found at the start).\n- `substr($string, 0, length($needle)) eq $needle` should be quite simple and efficient, but in most of my few tests is not more efficient than the previous one.\n\n\nIs there a canonical way to do that in `perl` which I wouldn't be aware of or any way to optimise any of the above solutions?\n\n\n(in my particular use case, `$string` and `$needle` are going to be different in each run, so precompiling a regexp is not an option).\n\n\n\n\n---\n\n\nExample of how to measure the performance of a given solution (here from a POSIX `sh`):\n\n\n\n```\nstring='somewhat not so longish string' needle='somew'\ntime perl -e '\n  ($n,$string,$needle) = @ARGV;\n  for ($i=0;$i<$n;$i++) {\n\n    index($string, $needle) == 0\n\n  }' 10000000 \"$string\" \"$needle\"\n\n```\n\nWith those values, `index()` performs better than `substr()+eq` with this system with perl 5.14.2, but with:\n\n\n\n```\nstring=\"aaaaabaaaaabaaaaabaaaaabaaaaabaaaaab\" needle=\"aaaaaa\"\n\n```\n\nThat's reversed.\n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"perl"},"answer":"\n```\nrindex $string, $substring, 0\n\n```\n\nsearches for `$substring` in `$string` *at position <=0* which is only possible if `$substring` is a prefix of `$string`. Example:\n\n\n\n```\n> rindex \"abc\", \"a\", 0\n0\n> rindex \"abc\", \"b\", 0\n-1\n\n```\n\n"}
{"questionId":"123379d55403424992fde86c0cbb1f7c","question":"How can I generate three random integers that satisfy some condition?\nI'm a beginner in programming and I'm looking for a nice idea how to generate three integers that satisfy a condition.\n\n\nExample:\n\n\nWe are given `n = 30`, and we've been asked to generate three integers a, b and c, so that `7*a + 5*b + 3*c = n`.\nI tried to use `for` loops, but it takes too much time and I have a maximum testing time of 1000 ms.\n\n\nI'm using Python 3.\n\n\nMy attempt:\n\n\n\n```\nx = int(input())\nc = []\nk = []\nw = []\nfor i in range(x):\n    for j in range(x):\n        for h in range(x):\n           if 7*i + 5*j + 3*h = x:\n              c.append(i)\n              k.append(j)\n              w.append(h)\nif len(c) == len(k) == len(w) \n    print(-1)\nelse: \n    print(str(k[0]) + ' ' + str(c[0]) + ' ' + str(w[0]))\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"\n```\nimport numpy as np\n\n\ndef generate_answer(n: int, low_limit:int, high_limit: int):\n    while True:\n        a = np.random.randint(low_limit, high_limit + 1, 1)[0]\n        b = np.random.randint(low_limit, high_limit + 1, 1)[0]\n        c = (n - 7 * a - 5 * b) \/ 3.0\n        if int(c) == c and low_limit <= c <= high_limit:\n            break\n\n    return a, b, int(c)\n\n\nif __name__ == \"__main__\":\n    n = 30\n    ans = generate_answer(low_limit=-5, high_limit=50, n=n)\n    assert ans[0] * 7 + ans[1] * 5 + ans[2] * 3 == n\n    print(ans)\n\n```\n\nIf you select two of the numbers a, b, c, you know the third. In this case, I randomize ints for a, b, and I find c by `c = (n - 7 * a - 5 * b) \/ 3.0`.\n\n\nMake sure c is an integer, and in the allowed limits, and we are done.\n\n\nIf it is not, randomize again.\n\n\n\n\n---\n\n\nIf you want to generate all possibilities,\n\n\n\n```\ndef generate_all_answers(n: int, low_limit:int, high_limit: int):\n    results = []\n    for a in range(low_limit, high_limit + 1):\n        for b in range(low_limit, high_limit + 1):\n            c = (n - 7 * a - 5 * b) \/ 3.0\n            if int(c) == c and low_limit <= c <= high_limit:\n                results.append((a, b, int(c)))\n\n    return results\n\n```\n\n"}
{"questionId":"ae3b6c0893b44604a4217bca8d58fab0","question":"How to find the average of the differences between all the numbers of a Python List\nI have a python list like this,\n\n\n\n```\narr = [110, 60, 30, 10, 5] \n\n```\n\nWhat I need to do is actually find the difference of every number with all the other numbers and then find the average of all those differences.\n\n\nSo, for this case, it would first find the difference between `110` and then all the remaining elements, i.e. `60, 30, 10, 5`, and then it will find the difference of `60` with the remaining elements, i.e. `30, 10, 5` and etc.\n\n\nAfter which, it will compute the Average of all these differences.\n\n\nNow, this can easily be done with two For Loops but in `O(n^2)` time complexity and also a little bit of \"messy\" code. I was wondering if there was a faster and more efficient way of doing this same thing?\n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"python"},"answer":"I'll just give the formula first:\n\n\n\n```\nn = len(arr)\nout = np.sum(arr * np.arange(n-1, -n, -2) ) \/ (n*(n-1) \/ 2)\n# 52\n\n```\n\nExplanation: You want to find the mean of\n\n\n\n```\na[0] - a[1], a[0] - a[2],..., a[0] - a[n-1]\n             a[1] - a[2],..., a[1] - a[n-1]\n                         ...\n\n```\n\nthere, your\n\n\n\n```\n`a[0]` occurs `n-1` times with `+` sign, `0` with `-` -> `n-1` times\n`a[1]` occurs `n-2` times with `+` sign, `1` with `-` -> `n-3` times\n... and so on \n\n```\n\n"}
{"questionId":"4cb52d323f9e4a6487f073a41522bba4","question":"Retrofit call in Kotlin Coroutines viewModelScope\nRecently I've updated my `ViewModel` to use new `viewModelScope`. From its implementation, I see that `Dispatchers.Main.immediate` is set as the default `CoroutineDispatcher` for `viewModelScope`.\n\n\nSo when printing the current `Thread` in `viewModelScope.launch` it gives **Thread[main,5,main]**\n\n\nBut even though this runs in Main Thread, the following code works for me which performs a network call.\n\n\n\n```\nviewModelScope.launch {\n    userRepo.login(email, password)\n}\n\n```\n\nHere `userRepo.login(email, password)` is `suspend` function, which calls `Retrofit` `suspend` function.\n\n\nSo how this works, if my Current Thread is Main Thread?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"kotlin"},"answer":"It works because Retrofit's `suspend` implementation delegates to `Call<T>.enqueue`. This means it already executes on its own background executor by default instead of using the caller's `Dispatcher`.\n\n\n"}
{"questionId":"9bb97c22962c439191e5d3449c170e47","question":"OpenCV: \"[ WARN:0] terminating async callback\" when attempting to take a picture\nI am trying to take a picture from the defualt carmera with python, to do this I am using openCV (`import cv2 as cv` from python shell). However, when I attempt to disable the camera it closes but with the error `[ WARN:0] terminating async callback`.\n\n\nThis is code I am trying to run:\n\n\n\n```\nimport cv2 as cv\n\ncamera_port = 0\ncamera = cv.VideoCapture(camera_port)\nreturn_value, image = camera.read()\ncv.imwrite(\"image.png\", image)\n\ncamera.release() # Error is here\n\n```\n\nThe code outputs the desired result, it takes and saves an image, but I do not understand *why* the error message occurs or how to *remove* it\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"I had the same warning.\n\n\nJust modify the line\n\n\n\n```\ncamera = cv.VideoCapture(camera_port)\n\n```\n\nto\n\n\n\n```\ncamera = cv.VideoCapture(camera_port, cv.CAP_DSHOW)\n\n```\n\n"}
{"questionId":"d68859f6a5a34eeaa62c6273474b4b82","question":"TS2339: Property 'getBoundingClientRect' does not exist on type 'never'\nI understand optional chaining should be enough but I have gone a bit overboard here to try satisfy TypeScript:\n\n\n\n```\nconst ref = useRef()\n\n    if (ref !== undefined) {\n        if(ref.hasOwnProperty('current')) {\n            if (ref.current !== undefined &&  ref.current !== null)\n             console.log(ref?.current?.getBoundingClientRect())\n        }\n    }\n\n```\n\nError:\n\n\n\n```\nTS2339: Property 'getBoundingClientRect' does not exist on type 'never'.\n\n```\n\nI miss my non-typing days... Any solution other than `@ts-ignore`\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"You just have to provide an element type to useRef\n\n\n\n```\nconst Test = () => {\n    const ref = useRef<HTMLInputElement>(null);\n    const rect = ref?.current?.getBoundingClientRect();\n}\n\n```\n\n"}
{"questionId":"08ed87c006f048a6aa28a9dc115f0246","question":"Define an empty object type in TypeScript\nI'm looking for ways to define an empty object type that can't hold any values.\n\n\n\n```\ntype EmptyObject = {}\n\nconst MyObject: EmptyObject = {\n  thisShouldNotWork: {},\n};\n\n```\n\nObjects with the type are free to add any properties. How can I force `MyObject` to always be an empty object instead?\n\n\nMy actual use case is using the EmptyObject type inside an interface.\n\n\n\n```\ninterface SchemaWithEmptyObject {\n  emptyObj: EmptyObject; \n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"\n```\ntype EmptyObject = {\n    [K in any] : never\n}\n\nconst one: EmptyObject = {}; \/\/ yes ok\nconst two: EmptyObject = {a: 1}; \/\/ error\n\n```\n\nWhat we are saying here is that all eventual properties of our `EmptyObject` can be only `never`, and as `never` has no representing value, creating such property is not possible, therefor the object will remain empty, as this is the only way we can create it without compilation error.\n\n\n"}
{"questionId":"2fd4106443724f5f8bb58698d54efe20","question":"Django 'model' object is not iterable\nI have a table which shows me the employees registered. I want to generate a simple HTML page according to their DB which includes their name, id, designation, etc.\n\n\nTo do so, I pass an id to the view so it can get the respective user's details and show me. Everything works fine until the error occurs **object is not iterable**. Here is my code below.\n\n\n`report.html`\n\n\n\n```\n{% if emp_item %}\n {% for some in emp_item %}\n    <title> {{ some.employee_name }} Report<\/title>\n\n    <h3>{{ some.employee_name }}<\/h3>\n\n    <table style=\"width:30%\" border=\"4\">\n\n\n        <td>{{some.id}}<\/td>\n        <td>{{some.Annual_leave}} <\/td>\n        <td>{{some.Sick_leave}} <\/td>\n        <td>{{some.allowed}} <\/td>\n\n        <\/table>\n\n{% endfor %}\n\n<h2>No User<\/h2>\n{% else %}\n{% endif %}\n\n\n```\n\n`views.py`\n\n\n\n```\n@staff_member_required  # for admin login required\ndef report(request, id):\n    emp_item = Employee.objects.get(id=id)\n    context = {'emp_item': emp_item}\n    return render(request, 'projectfiles\/report.html', context)\n\n```\n\n`urls.py`\n\n\n\n```\n    url(r'^(?i)Rejectleaves\/$', views.rejected_leave_show,\n        name='Reject_show'),  # user leaves\n\n    url(r'^(?i)report\/(?P<id>\\d+)$', views.report,\n        name='Report'),  # user Report\n\n```\n\n`models.py`\n\n\n\n```\nclass Employee(models.Model):\n\n    allowed = models.BooleanField(default=True)\n    employee_name = models.OneToOneField(User, on_delete = models.CASCADE)\n    employee_designation = models.CharField(max_length = 5)\n    employee_department = models.CharField(max_length = 5)\n    Annual_leave = models.PositiveSmallIntegerField(default=5)\n    Sick_leave = models.PositiveSmallIntegerField(default=5)\n\n```\n\nI want to see each individual user's data according to the process they have made.\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"python"},"answer":"Change `Employee.objects.get(id=id)` to `Employee.objects.filter(id=id)`\n\n\n\"`filter()` will always give you a QuerySet\" - it's iterable\n\n\n`get()` - return single object and it's not iterable\n\n\n"}
{"questionId":"67ce0da0186841f0a91f772c157c602e","question":"Build new Rails app Error loading the 'sqlite3' without evidently write verion\nWhen generate new rails app, and start the server \"rails s\", first massage I got:\n\n\n\n> \n> Puma caught this error: Error loading the 'sqlite3' Active Record adapter. Missing a gem it depends on? can't activate sqlite3 (~> 1.3.6), already activated sqlite3-1.4.0. Make sure all dependencies are added to Gemfile. (LoadError)\n> \n> \n> \n\n\nafter reload a page:\n\n\n\n> \n> ActiveRecord::ConnectionNotEstablished\n>  No connection pool with 'primary' found.\n> \n> \n> \n\n\n\n```\n  def retrieve_connection(spec_name) #:nodoc:\n    pool = retrieve_connection_pool(spec_name)\n    raise ConnectionNotEstablished, \"No connection pool with '#{spec_name}' found.\" unless pool\n    pool.connection\n  end\n\n```\n\nI reinstall ruby, rails, bundler, all except rvm\nand I don't know what to do\n\n\nP.S.\nthis error disappears when I evidently write sqlite3 verion, but it should work fine from a without it!!!\nHelp!What to do? or maybe reinstall all of it?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"ruby"},"answer":"Try to add this on your Gemfile\n\n\n\n```\ngem 'sqlite3', '~> 1.3.6'\n\n```\n\nInstead of having only the SQLite3 without version. \n\n\nIt worked for me\n\n\n"}
{"questionId":"ad2651b10a134aa98e1d33f4b99e59d5","question":"Starting container process caused \"exec: \\\"\/bin\/sh\\\": stat \/bin\/sh: no such file or directory\": unknown\nI want to understand how CMD and ENTRYPOINT works. So, I just created a very simple `Dockerfile`\n\n\n\n```\nFROM scratch\n\nCMD echo \"Hello First\"\n\nENTRYPOINT echo \"Hello second\" \n\n```\n\nThen I build image of this :\n\n\n\n```\ndocker build -t my_image .\n\n```\n\nThe logs are as below: \n\n\n\n> \n> Step 1\/3 : FROM scratch ---> Step 2\/3 : CMD echo \"Hello First\" --->\n>  Using cache ---> 9f2b6a00982f Step 3\/3 : ENTRYPOINT echo \"Hello\n>  second\" ---> Using cache ---> 1bbe520f9526 Successfully built\n>  1bbe520f9526 Successfully tagged my\\_image:latest SECURITY WARNING: You\n>  are building a Docker image from Windows against a non-Windows Docker\n>  host. All files and directories added to build context w ill have\n>  '-rwxr-xr-x' permissions. It is recommended to double check and reset\n>  permissions for sensitive files and directories.\n> \n> \n> \n\n\nWhen I create a container of this image it returns: \n\n\n\n```\ndocker run my_image\n\n```\n\nError is:\n\n\n\n> \n> docker: Error response from daemon: OCI runtime create failed:\n>  container\\_linux.go:344: starting container process caused \"exec:\n>  \\\"\/bin\/sh\\\": stat \/b in\/sh: no such file or directory\": unknown.\n> \n> \n> \n\n\nCan someone please help me about error?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"There are two things happening here.\n\n\nA Dockerfile that starts `FROM scratch` starts from a base image that has absolutely nothing at all in it. It is totally empty. There is not a set of base tools or libraries or anything else, beyond a couple of device files Docker pushes in for you.\n\n\nThe `ENTRYPOINT echo ...` command gets rewritten by Docker into `ENTRYPOINT [\"\/bin\/sh\", \"-c\", \"echo ...\"]`, and causes the `CMD` to be totally ignored. Unless overridden with `docker run --entrypoint`, this becomes the main process the container runs.\n\n\nSince it is a `FROM scratch` image and contains absolutely nothing at all, it doesn't contain a shell, hence the \"\/bin\/sh: no such file or directory\" error.\n\n\n"}
{"questionId":"8a7f47533e74418896503663e0531eb7","question":"Typescript async function return type void vs Promise\nIs there a difference between typescript void and Promise< void > types?\n\n\nAsking because I got confused why this is a valid typescript?\n\n\n\n```\nconst asyncFunc: () => void = async () => {\n    await new Promise(resolve => resolve());\n};\n\n```\n\nShouldn't this be the only valid case? \n\n\n\n```\nconst asyncFunc: () => Promise<void> = async () => {\n    await new Promise(resolve => resolve());\n};\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"typescript"},"answer":"Yes there's a difference\n\n\n\n> \n> void is a little like the opposite of any: the absence of having any type at all. You may commonly see this as the return type of functions that do not return a value\n> \n> \n> \n\n\nI guess `Promise<void>` doesn't require explanation. \n\n\n\n\n---\n\n\nNow why the assignment in the question is allowed? Because target function (`() => void`) can be called in all (\\*almost) the same situations as the source function. Have a look at simplified example:\n\n\n\n```\ndeclare let voidFunc: () => void;\ndeclare let promiseFunc: () => Promise<void>;\n\nvoidFunc = promiseFunc; \/\/ OK\npromiseFunc = voidFunc; \/\/ Error: Type 'void' is not assignable to type 'Promise<void>'\n\n```\n\n"}
{"questionId":"f1cb167a255d4ae58ecbcfc4b71cfbc0","question":"React useState empty array type\n\n```\ninterface Crumb {\n  title: string;\n  url: string;\n}\ninterface Crumbies {\n  crumbsArray: Crumb[];\n}\n\n\n\/\/ component\nconst [breadcrumbs, setBreadcrumbs] = useState<Crumbies>([]);\n\n```\n\nI'm getting an error:\n\n\n\n```\nTS2345: Argument of type 'never[]' is not assignable to parameter of type 'Crumbies | (() => Crumbies)'. \u00a0\u00a0Type 'never[]' is not assignable to type '() => Crumbies'. \u00a0\u00a0\u00a0\u00a0Type 'never[]' provides no match for the signature '(): Crumbies'.\n\n```\n\nHow to provide a correct typings for an empty array in `useState` hook?\n\n\n**UPDATE 1**\n\n\n\n```\nconst Breadcrumbs: React.FC<Crumbies> = ({ crumbsArray }) => {}\n\n```\n\nThat's why i've created another interface `Crumbies` to wrap `Crumb`. Is there a better approach to this?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"The interface called Crumbies requires you to pass an object with a field crumbsArray:\n\n\n\n```\nconst [breadcrumbs, setBreadcrumbs] = useState<Crumbies>({crumbsArray: []});\n\n```\n\nIf you want to simply have an array of Crumb you don't need to create a new interface, you can simply do it in useState:\n\n\n\n```\nconst [breadcrumbs, setBreadcrumbs] = useState<Crumb[]>([]);\n\n```\n\nThis will initialise it with an empty array.\n\n\n"}
{"questionId":"e07b598a38e14d27a9a20c75b9f24eb7","question":"How to show generated SQL \/ raw SQL in TypeORM queryBuilder\nI developed `typeorm` `querybuilder`. For the purpose of debugging, I'd like to show the generated SQL query.\n\n\nI tested `printSql()` method, but it didn't show any SQL query.\n\n\n\n```\nconst Result = await this.attendanceRepository\n  .createQueryBuilder(\"attendance\")\n  .innerJoin(\"attendance.child\", \"child\")\n  .select([\"attendance.childId\",\"child.class\",\"CONCAT(child.firstName, child.lastName)\"])\n  .where(\"attendance.id= :id\", { id: id })\n  .printSql()\n  .getOne()\n\nconsole.log(Result);\n\n```\n\nIt returned the following:\n\n\n\n```\nAttendance { childId: 4, child: Child { class: 'S' } }\n\n```\n\nMy desired result is to get the generated SQL query.\n\n\nIs there any wrong point? Is there any good way to get the SQL query?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"`.getQuery()` or `.getSql()`\n\n\n\n```\nconst sql1 = await this.attendanceRepository\n    .createQueryBuilder(\"attendance\")\n    .innerJoin(\"attendance.child\", \"child\")\n    .select([\"attendance.childId\",\"child.class\",\"CONCAT(child.firstName, child.lastName)\"])\n    .where(\"attendance.id= :id\", { id: id })\n    .getQuery();\nconsole.log(sql1);\n\n```\n\n\n```\nconst sql2 = await this.attendanceRepository\n    .createQueryBuilder(\"attendance\")\n    .innerJoin(\"attendance.child\", \"child\")\n    .select([\"attendance.childId\",\"child.class\",\"CONCAT(child.firstName, child.lastName)\"])\n    .where(\"attendance.id= :id\", { id: id })\n    .getSql();\nconsole.log(sql2);    \n\n```\n\n"}
{"questionId":"688a371c91b14b45848b1c5bf20fb06d","question":"BigQuery - Using INSERT INTO to copy data from one nested table into another nested table\nHelping a customer out. I'm trying to copy one nested BigQuery table into another nested table and am running into the following error: \"Syntax error: Expected \")\" or \",\" but got \".\"\"\n\n\nQuery: \n\n\n\n```\n  INSERT INTO `<GCP_PROJECT_NAME>.Test_Tables.Nested_Person_Table2` (id,\n    first_name,\n    last_name,\n    dob,\n    address.status,\n    address.address,\n    address.city,\n    address.state,\n    address.zip,\n    address.numberOfYears)\nSELECT\n  id,\n  first_name,\n  last_name,\n  dob,\n  address.status,\n  address.address,\n  address.city,\n  address.state,\n  address.zip,\n  address.numberOfYears\nFROM\n  `<GCP_PROJECT_NAME>.Test_Tables.Nested_Person_Table`\n\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"Answer below. Hope this helps someone else out too!\n\n\n\n```\nINSERT INTO\n  `<GCP_PROJECT_NAME>.Test_Tables.Nested_Person_Table2` \n    (id,\n    first_name,\n    last_name,\n    dob,\n    addresses)\nSELECT\n  id,\n  first_name,\n  last_name,\n  dob,\n  ARRAY_AGG(STRUCT(a1.status,\n      a1.address,\n      a1.city,\n      a1.state,\n      a1.zip,\n      a1.numberOfYears)) AS addresses\nFROM\n  `<GCP_PROJECT_NAME>.Test_Tables.Nested_Person_Table`,\n  UNNEST(addresses) AS a1\nGROUP BY\n  id,\n  first_name,\n  last_name,\n  dob\n\n```\n\n"}
{"questionId":"06b3942e11074e6d8f36e90a693bb6ec","question":"Error \"no supported WSMan client library was found.\" with macOS pwsh\nFrom macOS Terminal, when I execute:\n\n\n\n```\npwsh -command \"Enter-PSSession myhost\"\n\n```\n\nI get error from PowerShell:\n\n\n\n```\nEnter-PSSession: This parameter set requires WSMan, and no supported WSMan client\nlibrary was found. WSMan is either not installed or unavailable for this system.\n\n```\n\nI've spent 2 hours to find a solution. I am going to answer my own question.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"Here's what I did:\n\n\n\n```\npwsh -Command 'Install-Module -Name PSWSMan'\nsudo pwsh -Command 'Install-WSMan'\n\n```\n\n"}
{"questionId":"f23ca6782c7b477487654fb1690bd15d","question":"Linux\/Ubuntu set: Illegal option -o pipefail\nThe below mentioned line of code used to work for me all the time on a Ubuntu 16.04 distribution, but suddenly option-name `pipefail` is an illegal option: \n\n\n\n```\nset -eu -o pipefail\n\n```\n\nreturns:\n\n\n\n> \n> set: Illegal option -o pipefail\n> \n> \n> \n\n\nWhy does this happen? I run the command on a completely new installed system and as part of a shell script. The code is placed right at the beginning:\n\n\nmyscript.sh:\n\n\n\n```\n1 #!\/bin\/bash\n2 set -eu -o pipefail\n3 ...\n\n```\n\nThe script is run as sudo:\n\n\n\n```\nsudo sh .\/myscript.sh\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"You are running `bin\/sh`, on Ubuntu it is a symbolic link pointing to `\/bin\/dash`, but `pipefail` is a bashism.\n\n\nMake the script executable:\n\n\n\n```\nchmod +x myscript.sh\n\n```\n\nand then run the script as follows:\n\n\n\n```\nsudo .\/myscript.sh\n\n```\n\n"}
{"questionId":"d606d947ea6d433088fed61878a072da","question":"how to use a type for the response from axios.get\nI have a simple code to fetch data via Axios:\n\n\n\n```\nconst response= await axios.get(\"blabla\");\n\n```\n\nand now I'm trying to use typescript. \n\n\nWhen I add the type to the get method it works:\n\n\n\n```\nconst response= await axios.get<Todo[]>(\"blabla\");\n\n```\n\nbut what i need is something like:\n\n\n\n```\nconst response:Todo[] = await axios.get(\"blabla\");\n\n```\n\nbut if i do that i get an error on `response.data` saying: *Property 'data' does not exist on type 'Todo[]'*\n\n\nso 2 questions:\n1) why didn't it happen for the first approach?\n2) how to do the second way?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"`axios.get()` returns an `AxiosResponse<any>` object, where `response.data` is `any`.\n\n\n`axios.get<Todo[]>()` returns an `AxiosResponse<Todo[]>` object, where `response.data` is `Todo[]`.\n\n\nSo you can type `response` as:\n\n\n\n```\nconst response: AxiosResponse<Todo[]> = await axios.get(\"blabla\");\n\n```\n\n"}
{"questionId":"01b0b49b2e464d6183379d96d158d581","question":"Null pointer check via \"myPtr > 0\"\nIn some legacy code I came across the following **null pointer check**.\n\n\n\n```\nif( myPtr > 0 ) {\n...\n}\n\n```\n\nAre there any technical risks of checking for a null pointer via this if-check?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c++"},"answer":"Ordered comparison between a pointer and an integer is ill-formed in C++ (even when the integer is a null pointer constant such as it is in this case). The risk is that compilers are allowed to, and do, refuse to compile such code.\n\n\n\n\n---\n\n\nYou can rewrite it as either of these:\n\n\n\n```\nif(myPtr != nullptr)\nif(myPtr)\n\n```\n\n"}
{"questionId":"f3a6533aa80f40ae82f1d624679931dd","question":"List Gradle dependencies for all subprojects\nI can query the dependency tree for a Gradle project with `.\/gradlew -q dependencies`.\n\n\nI can also run the query for the `service` subproject with `.\/gradlew service:dependencies`.\n\n\nHow can I list the dependencies automatically **for all subprojects** from the command line **without modifying the build.gradle file**?\n\n\nThank you in advance.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"I believe there\u2019s no built-in way in Gradle to achieve this (without adapting the build configuration) \u2013 unless you manually list the `dependencies` task for all subprojects as in:\n\n\n\n```\n.\/gradlew sub1:dependencies sub2:dependencies sub1:subsub:dependencies\n\n```\n\nHowever, if you need this feature often enough, then you could create a shell alias for it. Example in bash (e.g., put this in your `~\/.bashrc` file):\n\n\n\n```\nalias gradle-all-deps='.\/gradlew dependencies $(.\/gradlew -q projects \\\n    | grep -Fe ---\\ Project \\\n    | sed -Ee \"s\/^.+--- Project '\"'([^']+)'\/\\1:dependencies\/\"'\")'\n\n```\n\nThen simply call `gradle-all-deps` from the root project directory.\n\n\n"}
{"questionId":"17e73093d2e84f63aa0384b47afc9027","question":"print environment variables sorted by name including variables with newlines\nI couldn't find an existing answer to this specific case: I would like to simply display all exported environment variables sorted by their name. Normally I can do this like simply like:\n\n\n\n```\n$ env | sort\n\n```\n\nHowever, if some environment variables contain newlines in their values (as is the case on the CI system I'm working with), this does not work because the multi-line values will get mixed up with other variables.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"Answering my own question since I couldn't find this elsewhere:\n\n\n\n```\n$ env -0 | sort -z | tr '\\0' '\\n'\n\n```\n\n`env -0` separates each variable by a null character (which is more-or-less how they are already stored internally). `sort -z` uses null characters instead of newlines as the delimiter for fields to be sorted, and finally `tr '\\0' '\\n'` replaces the nulls with newlines again.\n\n\nNote: `env -0` and `sort -z` are non-standard extensions provided by the GNU coreutils versions of these utilities. Open to other ideas for how to do this with POSIX `sort`--I'm sure it is possible but it might require a for loop or something; not as easy as a one-liner.\n\n\n"}
{"questionId":"5fffd3de92674bf082429f4f4341e96c","question":"Unable to locate System.Data.SqlClient reference\nI have a fresh Visual Studio 2017 Professional install. I'm building a quick POC Console application using .NET 4.7.1, and I'm unable to find the reference for System.Data.SqlClient.\n\n\nI have scoured my system, and located 4 versions of System.Data.SqlClient.dll, but none are correct and won't compile.\nI have also attempted to use System.Data, but no reference to SqlClient is located within. I have manually added the dll\/reference for System.Data, but also did not resolve the reference issue.\n\n\nMy application is really simple at the moment, and it will NOT compile due to this missing reference.\n\n\nWhat steps do I need to do to get this resolved?\n\n\n\n```\nusing System;\nusing System.Data;\nusing System.Data.SqlClient;\n\nnamespace ConsoleApp1\n{\n    class Database\n    {\n        public void Start()\n        {\n\n            string connString = @\"server=(local);initial     catalog=MyDatabase;Integrated Security=SSPI;\";\n            using (SqlConnection conn = new SqlConnection(connString))\n            {\n                conn.Open();\n                using (SqlCommand cmd = new SqlCommand(\"SELECT TOP 10 ID, Name FROM TableA\", conn))\n                {\n                    using (SqlDataReader reader = cmd.ExecuteReader())\n                    {\n                        while(reader.Read())\n                        {\n                            Console.WriteLine(\"ID: [{0}], Name: [{1}]\", reader.GetValue(0), reader.GetValue(1));\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"`dotnet add package System.Data.SqlClient`\n\n\n"}
{"questionId":"4ad21aef573248548f538f68f4613ed5","question":"Installing PHP-zip on a php:7.4-fpm image\nI want to install php-zip on my docker image (the end goal is to use the PhpWord Library). I use the php:7.4-fpm, which runs on Debian.\n\n\nIn my dockerfile, i use the command :\n\n\n\n```\nRUN apt-get update docker-php-ext-install zip\n\n```\n\nWhen executed, the script crashes because it can't find \/usr\/src\/php\/ext\/libzip.\n\n\nSo i add a good old \"apt-get install libzip\", but it can't locate the package. Same thing if it try to install \"libzip4\" :\n\n\n\n```\nchecking for libzip >= 0.11 libzip != 1.3.1 libzip != 1.7.0... no\nconfigure: error: Package requirements (libzip >= 0.11 libzip != 1.3.1 libzip != 1.7.0) were not met:\n\nNo package 'libzip' found\nNo package 'libzip' found\nNo package 'libzip' found\n\nConsider adjusting the PKG_CONFIG_PATH environment variable if you installed software in a non-standard prefix.\n\nAlternatively, you may set the environment variables LIBZIP_CFLAGS and LIBZIP_LIBS to avoid the need to call pkg-config.\nSee the pkg-config man page for more details.\nERROR: Service 'webapi' failed to build : The command '\/bin\/sh -c apt-get update          &&       apt-get install libzip4          && docker-php-ext-install zip' returned a non-zero code: 1\n\n```\n\nI assume either libzip4 isn't a valid version or it can't make the link between libzip and libzip4.\n\n\nThe main issue is that, without that library, i'm stuck and can't install the extension at all. Also i'm quiet a rookie when it comes to docker so i call out your help !\n\n\nThanks for your time !\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"Ok, so the issue is that you have to guess that the package you need to install is called \"libzip-dev\", not just \"libzip\".\n\n\nSo the solution was :\n\n\n\n```\nRUN apt-get update \\\n     && apt-get install -y libzip-dev \\\n     && docker-php-ext-install zip\n\n```\n\n"}
{"questionId":"b1d251e74ea448fea4838d56dc14ed14","question":"What is the meaning of 'for \\_ in range()\nI'm looking at some tensorflow stuff and I understand for loops or atleast I think I do, however I came across `for _ in range(20)` and was wondering what is the meaning of the \\_ in this case. I am used to for x in range or for i in range stuff and understand those but haven't been able to understand what i've read on the underscore\n\n\n","questionMetadata":{"type":"conceptual","level":"beginner","tag":"python"},"answer":"When you are not interested in some values returned by a function we use underscore in place of variable name . Basically it means you are not interested in how many times the loop is run till now just that it should run some specific number of times overall.\n\n\n"}
{"questionId":"08f64ac7006245fbb1f054c924e854e2","question":"Lifecycle OnLifecycleEvent is deprecated\nAfter updating `lifecycle` library to 2.4.0 Android studio marked all Lifecycle events as deprecated.\n\n\n\n```\n@OnLifecycleEvent(Lifecycle.Event.ON_CREATE)\nfun create() {\n    tts = TextToSpeech(context, this)\n}\n\n@OnLifecycleEvent(Lifecycle.Event.ON_PAUSE)\nfun stopTTS() {\n    tts?.stop()\n}\n\n```\n\nIs there any equivalent replacement such as `DefaultLifecycleObserver` ?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"It's deprecated because they now expect you to use Java 8 and implement the interface DefaultLifecycleObserver. Since Java 8 allows interfaces to have default implementations, they defined DefaultLifecycleObserver with empty implementations of all the methods so you only need to override the ones you use.\n\n\nThe old way of marking functions with `@OnLifecycleEvent` was a crutch for pre-Java 8 projects. This was the only way to allow a class to selectively choose which lifecycle events it cared about. The alternative would have been to force those classes to override all the lifecycle interface methods, even if leaving them empty.\\*\n\n\nIn your case, change your class to implement DefaultLifecycleObserver and change your functions to override the applicable functions of DefaultLifecycleObserver. If your project isn't using Java 8 yet, you need to update your Gradle build files. Put these in the `android` block in your module's `build.gradle`:\n\n\n\n```\n    compileOptions {\n        sourceCompatibility JavaVersion.VERSION_1_8\n        targetCompatibility JavaVersion.VERSION_1_8\n    }\n    kotlinOptions {\n        jvmTarget = '1.8'\n    }\n\n```\n\n\n\n---\n\n\n\\*Note: Well, they could have used the old Java pattern of providing an interface Adapter class that has open, empty implementations of each interface function. The downside with this approach, though, is that the listener must be solely a listener. But I think that should usually be the case anyway if you care about encapsulation.\n\n\n"}
{"questionId":"ec421f2e235f4f33a00a828175bac9a5","question":"Error occurred while access Microsoft.Extensions.HostingServices. Could not parse JSON file\nI was trying to create a migration for an MVC ASP.NET Core 3.10 project using Visual Studio 2019. I got two errors:\n\n\n1. An error occurred while accessing the Microsoft.Extensions.Hosting services. Continuing without the application service provider. Error: Could not parse the JSON file.\n2. No database provider has been configured for this DbContext. A provider can be configured by overriding the DbContext.OnConfiguring method or by using AddDbContext on the application service provider. If AddDbContext is used, then also ensure that your DbContext type accepts a DbContextOptions object in its constructor and passes it to the base constructor for DbContext.\n\n\nI assume the first one is the problem, since I already have my DbContext set up as the second one asked.\n\n\nWhat I've tried so far:\n\n\n1. Rolling back to .NET Core 3.0\n2. Using a name other than \"InitialCreate\" for the migration.\n3. Installing EntityFrameworkCore.Design, EntityFrameworkCore.SQlite, EntityFrameworkCore.SQlServer, EntityFrameworkCore.Logging.Debug, EntityFrameworkCore.Tools through the NuGet Package Manager\n4. Restarting the computer and trying again\n5. Quadruple-checking that Startup.cs had my DbContext added, and that my DbContext has the constructor set up to take a DbContextOptions in its constructor, passing the input to the base constructor\n\n\nWhat should I do about the first error?\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"c#"},"answer":"It turns out that my appsettings.json was missing a closing curly brace. Adding that in fixed the problem.\n\n\n"}
{"questionId":"ba5665aacfea40e9b604486e650a8f12","question":"Parallel request with Retrofit, Coroutines and Suspend functions\nI'm using Retrofit in order to make some network requests. I'm also using the Coroutines in combination with 'suspend' functions.\n\n\nMy question is: Is there a way to improve the following code. The idea is to launch multiple requests in parallels and wait for them all to finish before continuing the function.\n\n\n\n```\nlifecycleScope.launch {\n    try {\n        itemIds.forEach { itemId ->\n            withContext(Dispatchers.IO) { itemById[itemId] = MyService.getItem(itemId) }\n        }\n    } catch (exception: Exception) {\n        exception.printStackTrace()\n    }\n\n    Log.i(TAG, \"All requests have been executed\")\n}\n\n```\n\n(Note that \"MyService.getItem()\" is a 'suspend' function.)\n\n\nI guess that there is something nicer than a *foreach* in this case.\n\n\nAnyone with an idea?\n\n\n","questionMetadata":{"type":"optimization","level":"advanced","tag":"kotlin"},"answer":"I've prepared three approaches to solving this, from the simplest to the most correct one. To simplify the presentation of the approaches, I have extracted this common code:\n\n\n\n```\nlifecycleScope.launch {\n    val itemById = try {\n        fetchItems(itemIds)\n    } catch (exception: Exception) {\n        exception.printStackTrace()\n    }\n    Log.i(TAG, \"Fetched these items: $itemById\")\n}\n\n```\n\nBefore I go on, a general note: your `getItem()` function is suspendable, you have no need to submit it to the `IO` dispatcher. All your coroutines can run on the main thread.\n\n\nNow let's see how we can implement `fetchItems(itemIds)`.\n\n\n### 1. Simple forEach\n\n\nHere we take advantage of the fact that all the coroutine code can run on the main thread:\n\n\n\n```\nsuspend fun fetchItems(itemIds: Iterable<Long>): Map<Long, Item> {\n    val itemById = mutableMapOf<Long, Item>()\n    coroutineScope {\n        itemIds.forEach { itemId ->\n            launch { itemById[itemId] = MyService.getItem(itemId) }\n        }\n    }\n    return itemById\n}\n\n```\n\n`coroutineScope` will wait for all the coroutines you `launch` inside it. Even though they all run concurrently to each other, the launched coroutines still dispatch to the single (main) thread, so there is no concurrency issue with updating the map from each of them.\n\n\n### 2. Thread-Safe Variant\n\n\nThe fact that it leverages the properties of a single-threaded context can be seen as a limitation of the first approach: it doesn't generalize to threadpool-based contexts. We can avoid this limitation by relying on the `async-await` mechanism:\n\n\n\n```\nsuspend fun fetchItems(itemIds: Iterable<Long>): Map<Long, Item> = coroutineScope {\n    itemIds.map { itemId -> async { itemId to MyService.getItem(itemId) } }\n            .map { it.await() }\n            .toMap()\n}\n\n```\n\nHere we rely on two non-obvious properties of `Collection.map()`: \n\n\n1. It performs all the transformation eagerly, so the first transformation to a collection of `Deferred<Pair<Long, Item>>` is completely done before entering the second stage, where we await on all of them.\n2. It is an inline function, which allows us to write suspendable code in it even though the function itself is not a `suspend fun` and gets a non-suspendable lambda `(Deferred<T>) -> T`.\n\n\nThis means that all the fetching is done concurrently, but the map gets assembled in a single coroutine.\n\n\n### 3. Flow-Based Approach with Improved Concurrency Control\n\n\nThe above solved the concurrency for us, but it lacks any backpressure. If your input list is very large, you'll want to put a limit on how many simultaneous network requests you're making. \n\n\nYou can do this with a `Flow`-based idiom:\n\n\n\n```\nsuspend fun fetchItems(itemIds: Iterable<Long>): Map<Long, Item> = itemIds\n        .asFlow()\n        .flatMapMerge(concurrency = MAX_CONCURRENT_REQUESTS) { itemId ->\n            flow { emit(itemId to MyService.getItem(itemId)) }\n        }\n        .toMap()\n\n```\n\nHere the magic is in the `.flatMapMerge` operation. You give it a function `(T) -> Flow<R>` and it will execute it sequentially on all the input, but then it will concurrently collect all the flows it got. Note that I couldn't simplify `flow { emit(getItem()) } }` to just `flowOf(getItem())` because `getItem()` must be called lazily, while collecting the flow.\n\n\n`Flow.toMap()` is not currently provided in the standard library, so here it is:\n\n\n\n```\nsuspend fun <K, V> Flow<Pair<K, V>>.toMap(): Map<K, V> {\n    val result = mutableMapOf<K, V>()\n    collect { (k, v) -> result[k] = v }\n    return result\n}\n\n```\n\n"}
{"questionId":"adf109898ea14d63a2c7ddcb81472640","question":"Weird behaviour with class fields when adding to a std::vector\nI have found some very weird behaviour (on clang and GCC) in the following situation. I have a vector, `nodes`, with one element, an instance of class `Node`.\nI then call a function on `nodes[0]` that adds a new `Node` to the vector.\nWhen the new Node is added, the calling object's fields are reset! However, they seem to return to normal again once the function has finished.\n\n\nI believe this is a minimal reproducible example:\n\n\n\n```\n#include <iostream>\n#include <vector>\n\nusing namespace std;\n\nstruct Node;\nvector<Node> nodes;\n\nstruct Node{\n    int X;\n    void set(){\n        X = 3;\n        cout << \"Before, X = \" << X << endl;\n        nodes.push_back(Node());\n        cout << \"After, X = \" << X << endl;\n    }\n};\n\nint main() {\n    nodes = vector<Node>();\n    nodes.push_back(Node());\n\n    nodes[0].set();\n    cout << \"Finally, X = \" << nodes[0].X << endl;\n}\n\n```\n\nWhich outputs\n\n\n\n```\nBefore, X = 3\nAfter, X = 0\nFinally, X = 3\n\n```\n\nThough you would expect X to remain unchanged by the process.\n\n\nOther things I have tried:\n\n\n- If I remove the line that adds a `Node` inside `set()`, then it outputs X = 3 every time.\n- If I create a new `Node` and call it on that (`Node p = nodes[0]`) then the output is 3, 3, 3\n- If I create a reference `Node` and call it on that (`Node &p = nodes[0]`) then the output is 3, 0, 0 (perhaps this one is because the reference is lost when the vector resizes?)\n\n\nIs this undefined behaviour for some reason? Why?\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"c++"},"answer":"Your code has undefined behavior. In\n\n\n\n```\nvoid set(){\n    X = 3;\n    cout << \"Before, X = \" << X << endl;\n    nodes.push_back(Node());\n    cout << \"After, X = \" << X << endl;\n}\n\n```\n\nThe access to `X` is really `this->X` and `this` is a pointer to the member of the vector. When you do `nodes.push_back(Node());` you add a new element to the vector and that process reallocates, which **invalidates all iterators, pointers and references** to elements in the vector. That means \n\n\n\n```\ncout << \"After, X = \" << X << endl;\n\n```\n\nis using a `this` that is no longer valid.\n\n\n"}
{"questionId":"a8ce14328d2c40f6a8d97f7ffc911af7","question":"Is it possible for a missing #include to break the program at runtime?\nIs there any case, where missing a `#include` would break the software at runtime, while the build still goes through?\n\n\nIn other words, is it possible that\n\n\n\n```\n#include \"some\/code.h\"\ncomplexLogic();\ncleverAlgorithms();\n\n```\n\nand\n\n\n\n```\ncomplexLogic();\ncleverAlgorithms();\n\n```\n\nwould both build successfully, but behave differently?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c++"},"answer":"Yes, it's perfectly possible. I'm sure there are lots of ways, but suppose the include file contained a global variable definition which called a constructor. In the first case the constructor would execute, and in the second it wouldn't.\n\n\nPutting a global variable definition in a header file is poor style, but it's possible.\n\n\n"}
{"questionId":"70e33ce92f314c41a3a848ec7ffb65ac","question":"Mapstruct : Ambiguous mapping methods found for mapping collection element\nI have two methods for mapping entity to domain.\n\n\n\n```\nRDomain entityToDomain(REntity rEntity)\n\n\/*\nthis method ignores some of the fields in the domain.\n*\/\nRDomain entityToDomainLight(REntity rEntity)\n\n```\n\nI'm getting **Ambiguous mapping methods found for mapping collection** element when I try to define mapping method for List of entities to domains.\n\n\n\n```\nList<RDomain> entitiesToDomains(List<REntity> rEntities)\n\n```\n\nIs there a way to define which method to use for mapping collection of objects\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"As @Filip suggested, it is better to do something like that :\n\n\n\n```\nRDomain entityToDomain(REntity rEntity)\n\n@Named(value = \"useMe\")\nRDomain entityToDomainLight(REntity rEntity)\n\n@IterableMapping(qualifiedByName = \"useMe\")\nList<RDomain> entitiesToDomains(List<REntity> rEntities)\n\n```\n\n"}
{"questionId":"4fc325628fd34f1abe0184cc41b50435","question":"How to reduce code duplication when dealing with recursive sum types\nI am currently working on a simple interpreter for a programming language and I have a data type like this:\n\n\n\n```\ndata Expr\n  = Variable String\n  | Number Int\n  | Add [Expr]\n  | Sub Expr Expr\n\n```\n\nAnd I have many functions that do simple things like:\n\n\n\n```\n-- Substitute a value for a variable\nsubstituteName :: String -> Int -> Expr -> Expr\nsubstituteName name newValue = go\n  where\n    go (Variable x)\n      | x == name = Number newValue\n    go (Add xs) =\n      Add $ map go xs\n    go (Sub x y) =\n      Sub (go x) (go y)\n    go other = other\n\n-- Replace subtraction with a constant with addition by a negative number\nreplaceSubWithAdd :: Expr -> Expr\nreplaceSubWithAdd = go\n  where\n    go (Sub x (Number y)) =\n      Add [go x, Number (-y)]\n    go (Add xs) =\n      Add $ map go xs\n    go (Sub x y) =\n      Sub (go x) (go y)\n    go other = other\n\n```\n\nBut in each of these functions, I have to repeat the part that calls the code recursively with just a small change to one part of the function. Is there any existing way to do this more generically? I would rather not have to copy and paste this part:\n\n\n\n```\n    go (Add xs) =\n      Add $ map go xs\n    go (Sub x y) =\n      Sub (go x) (go y)\n    go other = other\n\n```\n\nAnd just change a single case each time because it seems inefficient to duplicate code like this.\n\n\nThe only solution I could come up with is to have a function that calls a function first on the whole data structure and then recursively on the result like this:\n\n\n\n```\nrecurseAfter :: (Expr -> Expr) -> Expr -> Expr\nrecurseAfter f x =\n  case f x of\n    Add xs ->\n      Add $ map (recurseAfter f) xs\n    Sub x y ->\n      Sub (recurseAfter f x) (recurseAfter f y)\n    other -> other\n\nsubstituteName :: String -> Int -> Expr -> Expr\nsubstituteName name newValue =\n  recurseAfter $ \\case\n    Variable x\n      | x == name -> Number newValue\n    other -> other\n\nreplaceSubWithAdd :: Expr -> Expr\nreplaceSubWithAdd =\n  recurseAfter $ \\case\n    Sub x (Number y) ->\n      Add [x, Number (-y)]\n    other -> other\n\n```\n\nBut I feel like there should probably be a simpler way to do this already. Am I missing something?\n\n\n","questionMetadata":{"type":"optimization","level":"advanced","tag":"haskell"},"answer":"Congratulations, you just rediscovered anamorphisms!\n\n\nHere's your code, rephrased so that it works with the `recursion-schemes` package. Alas, it's not shorter, since we need some boilerplate to make the machinery work. (There might be some automagic way to avoid the boilerplate, e.g. using generics. I simply do not know.)\n\n\nBelow, your `recurseAfter` is replaced with the standard `ana`.\n\n\nWe first define your recursive type, as well as the functor it is the fixed point of.\n\n\n\n```\n{-# LANGUAGE DeriveFunctor, TypeFamilies, LambdaCase #-}\n{-# OPTIONS -Wall #-}\nmodule AnaExpr where\n\nimport Data.Functor.Foldable\n\ndata Expr\n  = Variable String\n  | Number Int\n  | Add [Expr]\n  | Sub Expr Expr\n  deriving (Show)\n\ndata ExprF a\n  = VariableF String\n  | NumberF Int\n  | AddF [a]\n  | SubF a a\n  deriving (Functor)\n\n```\n\nThen we connect the two with a few instances so that we can unfold `Expr` into the isomorphic `ExprF Expr`, and fold it back.\n\n\n\n```\ntype instance Base Expr = ExprF\ninstance Recursive Expr where\n   project (Variable s) = VariableF s\n   project (Number i) = NumberF i\n   project (Add es) = AddF es\n   project (Sub e1 e2) = SubF e1 e2\ninstance Corecursive Expr where\n   embed (VariableF s) = Variable s\n   embed (NumberF i) = Number i\n   embed (AddF es) = Add es\n   embed (SubF e1 e2) = Sub e1 e2\n\n```\n\nFinally, we adapt your original code, and add a couple of tests.\n\n\n\n```\nsubstituteName :: String -> Int -> Expr -> Expr\nsubstituteName name newValue = ana $ \\case\n    Variable x | x == name -> NumberF newValue\n    other                  -> project other\n\ntestSub :: Expr\ntestSub = substituteName \"x\" 42 (Add [Add [Variable \"x\"], Number 0])\n\nreplaceSubWithAdd :: Expr -> Expr\nreplaceSubWithAdd = ana $ \\case\n    Sub x (Number y) -> AddF [x, Number (-y)]\n    other            -> project other\n\ntestReplace :: Expr\ntestReplace = replaceSubWithAdd \n   (Add [Sub (Add [Variable \"x\", Sub (Variable \"y\") (Number 34)]) (Number 10), Number 4])\n\n```\n\nAn alternative could be to define `ExprF a` only, and then derive `type Expr = Fix ExprF`. This saves some of the boilerplate above (e.g. the two instances), at the cost of having to use `Fix (VariableF ...)` instead of `Variable ...`, as well as the analogous for the other constructors.\n\n\nOne could further alleviate that using pattern synonyms (at the cost of a little more boilerplate, though).\n\n\n\n\n---\n\n\nUpdate: I finally found the automagic tool, using template Haskell. This makes the whole code reasonably short. Note that the `ExprF` functor and the two instances above still exist under the hood, and we still have to use them. We only save the hassle of having to define them manually, but that alone saves a lot of effort.\n\n\n\n```\n{-# LANGUAGE DeriveFunctor, DeriveTraversable, TypeFamilies, LambdaCase, TemplateHaskell #-}\n{-# OPTIONS -Wall #-}\nmodule AnaExpr where\n\nimport Data.Functor.Foldable\nimport Data.Functor.Foldable.TH\n\ndata Expr\n  = Variable String\n  | Number Int\n  | Add [Expr]\n  | Sub Expr Expr\n  deriving (Show)\n\nmakeBaseFunctor ''Expr\n\nsubstituteName :: String -> Int -> Expr -> Expr\nsubstituteName name newValue = ana $ \\case\n    Variable x | x == name -> NumberF newValue\n    other                  -> project other\n\ntestSub :: Expr\ntestSub = substituteName \"x\" 42 (Add [Add [Variable \"x\"], Number 0])\n\nreplaceSubWithAdd :: Expr -> Expr\nreplaceSubWithAdd = ana $ \\case\n    Sub x (Number y) -> AddF [x, Number (-y)]\n    other            -> project other\n\ntestReplace :: Expr\ntestReplace = replaceSubWithAdd \n   (Add [Sub (Add [Variable \"x\", Sub (Variable \"y\") (Number 34)]) (Number 10), Number 4])\n\n```\n\n"}
{"questionId":"59a94803daae454eb74817eb778899b7","question":"Set value to one of the property in Java 15 record\nI am using Java 15 preview feature record in my code, and defined the record as follow\n\n\n\n```\npublic record ProductViewModel\n        (\n                String id,\n                String name,\n                String description,\n                float price\n        ) {\n}\n\n```\n\nIn the controller level I have the below code\n\n\n\n```\n@Put(uri = \"\/{id}\")\npublic Maybe<HttpResponse> Update(ProductViewModel model, String id) {\n        LOG.info(String.format(\"Controller --> Updating the specified product\"));\n        return iProductManager.Update(id, model).flatMap(item -> {\n            if(item == null)\n                return Maybe.just(HttpResponse.notFound());\n            else\n                return Maybe.just(HttpResponse.accepted());\n        });\n    }\n\n```\n\nFrom the UI in the model the value of **id** is not passed, however, it is passed as a route parameter. Now I want to set the value in the controller level, something like\n\n\n\n```\nmodel.setid(id) \/\/ Old style\n\n```\n\nHow can I set the value to the record particular property\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"You can't. Record properties are immutable. What you can do however is add a wither to create a new record with same properties but a new id:\n\n\n\n```\npublic record ProductViewModel(String id,\n                               String name, \n                               String description,\n                               float price) {\n\n    public ProductViewModel withId(String id) {\n        return new ProductViewModel(id, name(), description(), price());\n    }\n} \n\n```\n\n"}
{"questionId":"7f3df0a71a654139bcc80db8ffd6ebbf","question":"Receiving \"failed to query\" code 13: Access is denied when using virtualenv -p on my windows system\nI have two versions of Python installed on my Windows system. 3.7 is installed in C:\\Python37 and 3.8 installed in Python 3.8.\n\n\nMy PATH variables include the Python 3.7 executable.\n\n\nWhen I try to run 'virtualenv -p C:\\Python38 ProjectFolder' I get the following error:\n\n\n\n```\nRuntimeError: failed to query C:\\Python38 with code 13 err: 'Access is denied'\n\n```\n\nThis is true if I specify C:\\Python37 as well. Isn't this supposed to create a virutalenv using the specified Python binaries? What am I doing wrong?\nThanks in advance!\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"\n```\nvirtualenv -p C:\\Python38\\python.exe ProjectFolder\n\n```\n\nI.e. point `-p` to python executable, not to a directory.\n\n\n"}
{"questionId":"2d0708f1c92f49b3841845bb4ddb9efd","question":"How to get RGB components from Color in SwiftUI\nIf I have a SwiftUI `Color`:\n\n\n\n```\nlet col: Color = Color(red: 0.5, green: 0.5, blue: 0.5)\n\n```\n\nHow do I get the RGB components from `col`?\n  \nLike this maybe:\n\n\n\n```\nprint(col.components.red)\n\n```\n\nIn UIKit, I could use `UIColor.getRed` but there doesn't seem to be an equivalent in SwiftUI.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"swift"},"answer":"# iOS 17\/ macOS 14 (advanced but native)\n\n\nYou can ask for resolving `Color` components **in the given environment**, because colors are different in different environments (for example in dark and light environments). In the following sample, I resolved it using the current environment of the used color.\n\n\n\n```\nstruct ContentView: View {\n    @Environment(\\.self) var environment\n    @State private var color = Color.red\n    @State private var components: Color.Resolved?\n\n    var body: some View {\n        VStack {\n            ColorPicker(\"Select your favorite color\", selection: $color)\n\n            if let components {\n                Text(\"R: \\(components.red)\")\n                Text(\"G: \\(components.green)\")\n                Text(\"B: \\(components.blue)\")\n                Text(\"A: \\(components.opacity)\")\n                Text(\"HEX: \\(components.description)\")\n            }\n        }\n        .padding()\n        .onChange(of: color, initial: true) { components = color.resolve(in: environment) }\n    }\n}\n\n```\n\nThe code above has been written for iOS 17 beta 1 using Xcode 15 beta 1\n\n\n\n\n---\n\n\n# iOS 14 \/ macOS 10.16\n\n\nThere is a new initializer that takes a `Color` and returns a `UIColor` for **iOS** or `NSColor` for **macOS** now. With the help of those you can implement the following extensions:\n\n\n\n```\nimport SwiftUI\n\n#if canImport(UIKit)\nimport UIKit\n#elseif canImport(AppKit)\nimport AppKit\n#endif\n\nextension Color {\n    var components: (red: CGFloat, green: CGFloat, blue: CGFloat, opacity: CGFloat) {\n\n        #if canImport(UIKit)\n        typealias NativeColor = UIColor\n        #elseif canImport(AppKit)\n        typealias NativeColor = NSColor\n        #endif\n\n        var r: CGFloat = 0\n        var g: CGFloat = 0\n        var b: CGFloat = 0\n        var o: CGFloat = 0\n\n        guard NativeColor(self).getRed(&r, green: &g, blue: &b, alpha: &o) else {\n            \/\/ You can handle the failure here as you want\n            return (0, 0, 0, 0)\n        }\n\n        return (r, g, b, o)\n    }\n\n    var hex: String {\n        String(\n            format: \"#%02x%02x%02x%02x\",\n            Int(components.red * 255),\n            Int(components.green * 255),\n            Int(components.blue * 255),\n            Int(components.opacity * 255)\n        )\n    }\n}\n\n```\n\n\n\n---\n\n\n# Usage\n\n\n\n```\nColor.red.components.red \/\/ 0.9999999403953552 \/\/ <- SwiftUI Colors are not pure!\n\n```\n\n"}
{"questionId":"687139d6ff6143ac87e10f2b00596c42","question":"Unknown database type enum requested, Doctrine\\DBAL\\Platforms\\MySQL57Platform may not support it. Symfony\nI created a fresh symfony4 project. Made user Entity using `php bin\/console make:user`, then tried to migrate using `php bin\/console make:migration`. But then the error pops up \n\n\n\n> \n> In AbstractPlatform.php line 434:\n> \n> \n> Unknown database type enum requested,\n>  Doctrine\\DBAL\\Platforms\\MySQL57Platform may not support it.\n> \n> \n> \n\n\nThe strange thing is the User entity doesn't have any `enum` type rather it has a json column of roles, I suppose this is the reason. \n\n\n\n```\n \/**\n * @ORM\\Column(type=\"json\")\n *\/\n private $roles = [];\n\n```\n\nI have seen some answers for the similar question for laravel, But don't know how to fix it in symfony4. \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"Couldn't reproduce your issue. But anyway you can set up enum type in doctrine.yaml like\n\n\n\n```\ndoctrine:\n    dbal:\n       .....\n        mapping_types:\n            enum: string\n\n```\n\n"}
{"questionId":"0bce75e0d171449991909356be60cee2","question":"MySQL Error 1064 when adding foreign key with MySQL Workbench\nSo I have an error, when I want to add a foreign key. I added the foreign key in MySQL Workbench with an EER Diagram Model. The lines workbench tries to add are:`\n\n\n\n```\nCREATE SCHEMA IF NOT EXISTS `barberDB` DEFAULT CHARACTER SET latin1 ;\nUSE `barberDB` ;\n\n-- -----------------------------------------------------\n-- Table `barberDB`.`BARBER`\n-- -----------------------------------------------------\nCREATE TABLE IF NOT EXISTS `barberDB`.`BARBER` (\n  `ID` INT NOT NULL AUTO_INCREMENT,\n  `name` VARCHAR(45) NULL,\n  PRIMARY KEY (`ID`))\nENGINE = InnoDB;\n\n\n-- -----------------------------------------------------\n-- Table `barberDB`.`CUSTOMER`\n-- -----------------------------------------------------\nCREATE TABLE IF NOT EXISTS `barberDB`.`CUSTOMER` (\n  `ID` INT NOT NULL AUTO_INCREMENT,\n  `name` VARCHAR(45) NULL,\n  `isHaircut` INT NULL,\n  `isBeard` INT NULL,\n  `isEyebrows` INT NULL,\n  `BARBER_ID` INT NOT NULL,\n  PRIMARY KEY (`ID`),\n  INDEX `fk_CUSTOMER_BARBER_idx` (`BARBER_ID` ASC) VISIBLE,\n  CONSTRAINT `fk_CUSTOMER_BARBER`\n    FOREIGN KEY (`BARBER_ID`)\n    REFERENCES `barberDB`.`BARBER` (`ID`)\n    ON DELETE CASCADE\n    ON UPDATE CASCADE)\nENGINE = InnoDB;\n\n\nSET SQL_MODE=@OLD_SQL_MODE;\nSET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS;\nSET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS;\n\n```\n\nThe error I get is:\n\n\n\n```\nERROR: Error 1064: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'VISIBLE,\n  CONSTRAINT `fk_CUSTOMER_BARBER`\n    FOREIGN KEY (`BARBER_ID`)\n    REF' at line 12\nSQL Code:\n        -- -----------------------------------------------------\n        -- Table `barberDB`.`CUSTOMER`\n        -- -----------------------------------------------------\n        CREATE TABLE IF NOT EXISTS `barberDB`.`CUSTOMER` (\n          `ID` INT NOT NULL AUTO_INCREMENT,\n          `name` VARCHAR(45) NULL,\n          `isHaircut` INT NULL,\n          `isBeard` INT NULL,\n          `isEyebrows` INT NULL,\n          `BARBER_ID` INT NOT NULL,\n          PRIMARY KEY (`ID`),\n          INDEX `fk_CUSTOMER_BARBER_idx` (`BARBER_ID` ASC) VISIBLE,\n          CONSTRAINT `fk_CUSTOMER_BARBER`\n            FOREIGN KEY (`BARBER_ID`)\n            REFERENCES `barberDB`.`BARBER` (`ID`)\n            ON DELETE CASCADE\n            ON UPDATE CASCADE)\n        ENGINE = InnoDB\n\nSQL script execution finished: statements: 6 succeeded, 1 failed\n\nFetching back view definitions in final form.\nNothing to fetch\n\n```\n\nI searched for solutions and find that parenthesis are important or backticks, but since the code is created by the tool, it seems correct to me. \nWhat could cause the error?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"For anyone looking this, in the EER Diagram Model window you can set wich MySQL version want use to generate SQL script. Go to Edit, Preferences, then Modeling section, MySQL and set your custom MySQL version.\n\n\n"}
{"questionId":"5e9d93e70b6948e6b23c760b572cb251","question":"VBA: Why would the Not operator stop working?\nThis has me utterly baffled.\n\n\n\n```\nSub testChangeBoolean()\n  Dim X As Boolean       ' default value is False\n  X = True               ' X is now True\n  X = Not X              ' X is back to False\nEnd Sub\n\n```\n\nBut I'm trying to toggle the .FitText property in a table cell (in Word). .FitText starts as True. \n\n\nIf I assign it to X:\n\n\n\n```\nSub testChangeBoolean()\n  Dim X As Boolean                  ' again, default is False\n  X = Selection.Cells(1).FitText    ' does set X to True\n  X = Not X                         ' X is still True!\nEnd Sub\n\n```\n\nI just don't understand what I'm doing wrong.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"vba"},"answer":"I believe the explanation has to do with how older programming languages (WordBasic and early VBA) stored the integer values of True and False. In those days, True = -1 and False = 0.\n\n\nNewer programming languages still use 0 for False, but 1 for True.\n\n\nThe majority of Word's Boolean type properties continue to use -1 for True (Font.Bold, for example), which has been cause for confusion and frustration for programmers working with the Interop in newer languages. So, at some point, some developers at Microsoft decided to use the new way and assigned the integer value of 1 to True for some new functionality. Such as `FitText`.\n\n\nConsidering the following code sample, where `X` is of type `Boolean` and `y` of type `Integer`:\n\n\n- If `FitText` is True, the integer value is 1\n- If reversing the values, using `Not` shows that the Boolean remains \"True\" because its integer value is not 0, it's -2\n- Setting the integer value directly to True gives -1\n\n\nThis is confusing, indeed, but does explain why `Not` is not giving the expected result.\n\n\n\n```\nSub testChangeBoolean()\n  Dim X As Boolean                  ' again, default is False\n  Dim Y As Integer\n  X = Selection.Cells(1).FitText    ' does set X to True\n  Y = Selection.Cells(1).FitText\n  Debug.Print X, Y                  ' result: True    1\n  X = Not X                         ' X is still True!\n  Y = Not Y\n  Debug.Print X, Y                  ' result: True   -2\n  X = False\n  Y = True\n  Debug.Print X, Y                  ' result: False  -1\nEnd Sub\n\n```\n\n"}
{"questionId":"9a5a53ce0cb1432d937e0bab9ccacc02","question":"Flutter\/Dart - FlutterFirebaseMessagingPlugin.java - Build fails with Exception \"error: cannot find symbol\"\nAfter upgrading to Null Safety `sdk: \">=2.12.0 <3.0.0\"` I'm getting errors when I try to build my Flutter app in Android Studio.\n\n\n\n```\nC:\\flutter\\.pub-cache\\hosted\\pub.dartlang.org\\firebase_messaging-9.1.4\\android\\src\\main\\java\\io\\flutter\\plugins\\firebase\\messaging\\FlutterFirebaseMessagingPlugin.java:20: error: cannot find symbol\n    import com.google.firebase.iid.FirebaseInstanceId;\n                                  ^\n      symbol:   class FirebaseInstanceId\n      location: package com.google.firebase.iid\n    C:\\flutter\\.pub-cache\\hosted\\pub.dartlang.org\\firebase_messaging-9.1.4\\android\\src\\main\\java\\io\\flutter\\plugins\\firebase\\messaging\\FlutterFirebaseMessagingPlugin.java:21: error: cannot find symbol\n    import com.google.firebase.iid.Metadata;\n                                  ^\n      symbol:   class Metadata\n      location: package com.google.firebase.iid\n    C:\\flutter\\.pub-cache\\hosted\\pub.dartlang.org\\firebase_messaging-9.1.4\\android\\src\\main\\java\\io\\flutter\\plugins\\firebase\\messaging\\FlutterFirebaseMessagingPlugin.java:152: error: cannot find symbol\n                      : Metadata.getDefaultSenderId(FirebaseApp.getInstance());\n                        ^\n      symbol:   variable Metadata\n      location: class FlutterFirebaseMessagingPlugin\n    C:\\flutter\\.pub-cache\\hosted\\pub.dartlang.org\\firebase_messaging-9.1.4\\android\\src\\main\\java\\io\\flutter\\plugins\\firebase\\messaging\\FlutterFirebaseMessagingPlugin.java:153: error: cannot find symbol\n              FirebaseInstanceId.getInstance().deleteToken(senderId, \"*\");\n              ^\n      symbol:   variable FirebaseInstanceId\n      location: class FlutterFirebaseMessagingPlugin\n    C:\\flutter\\.pub-cache\\hosted\\pub.dartlang.org\\firebase_messaging-9.1.4\\android\\src\\main\\java\\io\\flutter\\plugins\\firebase\\messaging\\FlutterFirebaseMessagingPlugin.java:165: error: cannot find symbol\n                      : Metadata.getDefaultSenderId(FirebaseApp.getInstance());\n                        ^\n      symbol:   variable Metadata\n      location: class FlutterFirebaseMessagingPlugin\n    C:\\flutter\\.pub-cache\\hosted\\pub.dartlang.org\\firebase_messaging-9.1.4\\android\\src\\main\\java\\io\\flutter\\plugins\\firebase\\messaging\\FlutterFirebaseMessagingPlugin.java:166: error: cannot find symbol\n              String token = FirebaseInstanceId.getInstance().getToken(senderId, \"*\");\n                             ^\n      symbol:   variable FirebaseInstanceId\n      location: class FlutterFirebaseMessagingPlugin\n\n```\n\nAndroid Studio tells me FlutterFirebaseMessagingPlugin.java \"cannot find symbol\". So what does this mean and how do I fix it\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"Update your `firebase_messaging` in pusbpec.yaml to version that supports null-safety (for example: `firebase_messaging: ^10.0.0`, and call `flutter pub get from` terminal\n\n\n"}
{"questionId":"186f25c5592c4fcfb815aea9a18ab1de","question":"UIButton font size isn't changing\n\n```\nprivate func updateViewFromModel() {\n    for index in cardButtons.indices {\n        let button = cardButtons[index]\n        let card = game.cards[index]\n        if card.isFaceUp {\n            button.setTitle(emoji(for: card), for: .normal)\n            button.titleLabel?.font = UIFont.systemFont(ofSize: 50)\n            button.backgroundColor = .lightGray\n        } else {\n            button.setTitle(\"\", for: .normal)\n            button.backgroundColor = card.isMatched ? .clear : .systemIndigo\n        }\n        \n    }\n}\n\n```\n\nCan anyone tell me what's wrong with this code? title in IB is empty. I successfully set the title. but font size isn't changing.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"swift"},"answer":"In Xcode 13 ,UIButton has four type are `Plain,Grain,Tinted,Filled` .When you create a button in storyboard , button type automatically set with Plain that means new UIButton configurations is on. If you want to old behaviour , you must set style `plain` to `default`.\n\n\nOr , If you want one of the style above . You need to set font like\n\n\n\n```\nbutton.titleTextAttributesTransformer = UIConfigurationTextAttributesTransformer { incoming in\n  var outgoing = incoming\n  outgoing.font = UIFont.systemFont(ofSize: 50)\n  return outgoing\n }\n\n```\n\n"}
{"questionId":"ce2ac754e56d41ea9db20bafb927b757","question":"Android Jetpack Compose mutableStateListOf not doing Recomposition\nSo, I have a `mutableStateListOf` in `viewModel`:\n\n\n\n```\nvar childTravellersList = mutableStateListOf<TravellersDetails>()\n\n```\n\n`TravellersDetails` is a data class having a field called `error`.\n\n\nthis `childTravellersList` is used in the UI as:\n\n\n\n```\nval list = remember{viewModel.childTravellersList}\n\nLazyColumn(state = lazyColumnState) {\n    itemsIndexed(list) { index, item ->\n        SomeBox(show = if(item.error) true else false)\n    }\n  }\n\n```\n\nI have wrote a function in `viewModel` that updates `error` of TravellersDetails at given `index` of `childTravellersList` as:\n\n\n\n```\nfun update(index){\n    childTravellersList[index].error = true\n}\n\n```\n\nSo, whenever I call this function, the list should get updated.\n\n\nThis updates the list, but UI recomposition is not triggered .\nWhere am I doing wrong?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"kotlin"},"answer":"`mutableStateListOf` can only notify about adding\/removing\/replacing some element in the list. When you change any class inside the list, the mutable state cannot know about it.\n\n\nData classes are very good for storing immutable state in unidirectional data flow, because you can always \"change\" it with `copy`, while you see the need to pass the new data to view or mutable state. So avoid using `var` variables with data classes, always declare them as `val` to prevent such errors.\n\n\n\n```\nvar childTravellersList = mutableStateListOf<TravellersDetails>()\n\nfun update(index){\n    childTravellersList[index] = childTravellersList[index].copy(error = true)\n}\n\n```\n\nAn other problem is that you're using `val list = remember{viewModel.childTravellersList}`: it saves the first list value and prevents updates in future. With ViewModel you can use it directly `itemsIndexed(viewModel.childTravellersList)`\n\n\n"}
{"questionId":"444cc65dfa1849e39457a4823f1e9fc8","question":"Kotlin getter \/ setter when using a primary constructor\nThe example is from a Kotlin-course I'm doing:\n\n\n\n```\nclass Car {\n    var speed: Int = 0\n        get() = field \n        set(value) {\n            field = value\n        }\n}\n\n```\n\nIf I like to use a primary constructor like this:\n\n\n\n```\nclass Car(var speed: Int)\n\n```\n\n**How would I have to write the getter \/ setter in that case?**\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"kotlin"},"answer":"You cannot write getters\/setters inside of the constructor, you can do the following:\n\n\n1. Create variable inside class whose value taken from constructor.\n\n\n\n```\nclass Car(speed: Int) {\n    var speed = speed\n        get() = field \n        set(value) {\n            field = value\n        }\n}\n\n```\n\n2. Use `@JvmField` annotation to restrict compiler not to auto-generate getters\/setters and implement one yourself\n\n\n\n```\nclass Car(@JvmField private var speed: Int) {\n    fun getSpeed() = speed\n    fun setSpeed(value: Int) { speed = value }\n}\n\n```\n\n"}
{"questionId":"7cad90f40f4a4e72a15df3830f1193d0","question":"React typescript error Parsing error: '>' expected\nI am migrating js to ts and have an error with my modified code:\n\n\nLine 26:8: Parsing error: '>' expected\n\n\n\n```\nimport React from \"react\";\nimport { Route, Redirect, RouteProps } from \"react-router-dom\";\nimport {render} from \"react-dom\"\nimport {AppProps} from \"..\/App\"\n\nfunction querystring(name: string, url = window.location.href) {\n  name = name.replace(\/[[]]\/g, \"\\\\$&\");\n\n  const regex = new RegExp(\"[?&]\" + name + \"(=([^&#]*)|&|#|$)\", \"i\");\n  const results = regex.exec(url);\n\n  if (!results) {\n    return null;\n  }\n  if (!results[2]) {\n    return \"\";\n  }\n\n  return decodeURIComponent(results[2].replace(\/\\+\/g, \" \"));\n}\nexport default function UnauthenticatedRoute({ component: C, appProps, ...rest }:\n                                                 RouteProps & {appProps: AppProps}) {\n  const redirect = querystring(\"redirect\");\n  return (\n    <Route\n        {...rest} \/\/ the issue is here\n        render={ props => !appProps.isAuthenticated ?\n            <C {...props} {...appProps} \/>\n          :\n            <Redirect to={redirect === \"\" || redirect === null ? \"\/\" : redirect}\/>\n    }\n    \/>\n  );\n}\n\n```\n\nIf someone sees the problem, that would be kind :). Thanks!\n\n\n\n\n---\n\n\n# Solution\n\n\n1\/ File needs to have tsx extension\n\n\n2\/ syntax was also wrong in the tsx syntax. I changed to this and now it's ok:\n\n\n\n```\nexport default function UnauthenticatedRoute({ component: C, appProps, ...rest }:\n                                                 RouteProps & {appProps: AppProps}) {\n  const redirect = querystring(\"redirect\");\n  return (\n    <Route {...rest}\n        render={ props => !appProps.isAuthenticated ?\n            <C {...props} {...appProps} \/>\n          :\n            <Redirect to={redirect === \"\" || redirect === null ? \"\/\" : redirect}\/>\n        }\n    \/>\n  );\n}\n\n```\n\nNow I have another issue with binding element 'C', but it's another story.\n\n\nThanks everyone!\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"Change the file extension from `.ts` to `.tsx`.\n\n\n"}
{"questionId":"2f7aaede6b904d2f923dacd46c41207c","question":"How do I copy over all secrets from one Azure Keyvault to another using Powershell\nWe recently found ourselves needing to copy over every single secret (name and value) from one Azure KeyVault to a newly created one. I found ways to restore the secrets from a backup, but we didn't have a backup. Is there a Powershell script that can just loop through every name\/value combo in a source vault and copy it to a destination vault?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"this is just too triggering (no offense), here's a more \"powershelly\" version:\n\n\n\n```\nParam(\n    [Parameter(Mandatory)]\n    [string]$sourceVaultName,\n    [Parameter(Mandatory)]\n    [string]$destVaultName\n)\n\nConnect-AzAccount\n\n$secretNames = (Get-AzKeyVaultSecret -VaultName $sourceVaultName).Name\n$secretNames.foreach{\n    Set-AzKeyVaultSecret -VaultName $destVaultName -Name $_ `\n        -SecretValue (Get-AzKeyVaultSecret -VaultName $sourceVaultName -Name $_).SecretValue\n}\n\n```\n\nJust to sum it up:\n\n\nParameters are mandatory with this change and you can tab complete them, so you dont have to remember which one is first.  \n\nUsing `foreach` is a bit cleaner than using `do\\while` (certainly less cognitive effort).  \n\nYou dont have to cast values to text and encrypt it back, you can just use encrypted value to assign it to new secret\n\n\n"}
{"questionId":"244022db606b402fa1e0b4440246ae66","question":"Check that a List parameter is null in a Spring data JPA query\nI have a **Spring Boot** application and use **Spring Data JPA** to query a **MySQL** database.\n\n\nI need to get a list of courses filtered with some parameters.\n\n\nI usually use the syntax `param IS NULL or (\/*do something with param*\/)` so that it ignores the parameter if it is null.\n\n\nWith simple datatypes I have no problems but when it comes to a List of objects I don't know how to check for `NULL` value. How can I check if the `?3` parameter is `NULL` in the following query ?\n\n\n\n```\n@Query(\"SELECT DISTINCT c FROM Course c\\n\" +\n       \"WHERE c.courseDate < CURRENT_TIME\\n\" +\n       \"AND (?1 IS NULL OR (c.id NOT IN (3, 4, 5)))\\n\" +\n       \"AND (?2 IS NULL OR (c.title LIKE ?2 OR c.description LIKE ?2))\\n\" +\n       \"AND ((?3) IS NULL OR (c.category IN ?3)) \")\nList<Course> getCoursesFiltered(Long courseId, String filter, List<Category> categories);\n\n```\n\nError is :\n\n\n\n> \n> could not extract ResultSet; SQL [n\/a]; nested exception is org.hibernate.exception.DataException: could not extract ResultSet[SQL: 1241, 21000]\n> \n> \n> \n\n\nAnd in the stack trace I can see :\n\n\n\n> \n> Caused by: java.sql.SQLException: Operand should contain 1 column(s)\n> \n> \n> \n\n\nIndeed generated query would be `... AND ((?3, ?4, ?5) IS NULL OR (c.category IN (?3, ?4, ?5)))` if my list contains 3 elements. But `IS NULL` cannot be applied to multiple elements (query works fine if my list contain only one element).\n\n\nI have tried `size(?3) < 1`, `length(?3) < 1`, `(?3) IS EMPTY`, etc. but still no luck.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"sql"},"answer":"OK I though of a thing after waking up at the middle of the night :\n\n\n\n```\n@Query(\"SELECT DISTINCT c FROM Course c\\n\" +\n       \"WHERE c.courseDate < CURRENT_TIME\\n\" +\n       \"AND (?1 IS NULL OR (c.id NOT IN (3, 4, 5)))\\n\" +\n       \"AND (?2 IS NULL OR (c.title LIKE ?2 OR c.description LIKE ?2))\\n\" +\n       \"AND (COALESCE(?3) IS NULL OR (c.category IN ?3)) \")\nList<Course> getCoursesFiltered(Long courseId, String filter, List<Category> categories);\n\n```\n\nThe solution was simply to use `COALESCE` in addition to `IS NULL` so it can work with multiple values. That way if the list contain at least one non-null value, the second expression (`(c.category IN ?3)`) will do the job of filtering.\n\n\nI will wait at least a whole night next time before asking a question :)\n\n\n"}
{"questionId":"711975bee26e45c48ab5cb4fbfa714d0","question":"asyncio.get\\_event\\_loop(): DeprecationWarning: There is no current event loop\nI'm building an SMTP server with `aiosmtpd` and used the examples as a base to build from. Below is the code snippet for the entry point to the program.\n\n\n\n```\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    loop.create_task(amain(loop=loop))\n    try:\n        loop.run_forever()\n    except KeyboardInterrupt:\n        pass\n\n```\n\nWhen I run the program, I get the following warning:\n\n\n\n```\nserver.py:61: DeprecationWarning: There is no current event loop\n  loop = asyncio.get_event_loop()\n\n```\n\nWhat's the correct way to implement this?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"Your code will run on Python3.10 but as of 3.11 it will be an error to call `asyncio.get_event_loop` when there is no running loop in the current thread. Since you need loop as an argument to `amain`, apparently, you must explicitly create and set it.\n\n\nIt is better to launch your main task with asyncio.run than loop.run\\_forever, unless you have a specific reason for doing it that way. [But see below]\n\n\nTry this:\n\n\n\n```\nif __name__ == '__main__':\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    try:\n        asyncio.run(amain(loop=loop))\n    except KeyboardInterrupt:\n        pass\n\n```\n\nAdded April 15, 2023:\n\n\nThere is a difference between calling `asyncio.run()`, which I have done here, and calling `loop.run_forever()` (as in the original question) or `loop.run_until_complete()`. When I wrote this answer I did not realize that asyncio.run() *always* creates a new event loop. Therefore in my code above, the variable `loop` that is passed to `amain` will not become the \"running loop.\" So my code avoids the DeprecationWarning\/RuntimeException, but it doesn't pass a useful loop into `amain.`\n\n\nTo correct that, replace the line\n\n\n\n```\nasyncio.run(amain(loop=loop))\n\n```\n\nwith\n\n\n\n```\nloop.run_until_complete(amain(loop=loop))\n\n```\n\nIt would be best to modify `amain` to obtain the running event loop inside the function instead of passing it in. Then you could launch the program with `asyncio.run`. But if amain cannot be changed that won't be possible.\n\n\nNote that `run_until_complete`, unlike `asyncio.run`, does not clean up async generators. This is documented in the standard docs.\n\n\n"}
{"questionId":"08850233fe62471fac84beb0ce0c3cd8","question":"Ref object is possibly undefined TypeScript React\nI have a component that uses a hook to set the input value into component state. I want to add a class when the input value is more than 0 characters in length, however I'm coming across an issue where TypeScript says my ref may be undefined.\n\n\nI can't get rid of this error even if I check if the ref exists in the conditional that wraps the code to add the class. I'm not sure on the solution to this.\n\n\n**Error:** `Object is possibly 'undefined'` on `inputValue.current.classList.add(inputHasContentClassName);`\n\n\n\n```\nimport React, { useState, useEffect, useRef } from 'react';\n\nconst Component: React.FC = () => {\n  const [inputValue, setInputValue] = useState('');\n  const myRef = useRef();\n\n  useEffect(() => {\n    const inputHasContentClassName = 'input--has-value';\n\n    if (inputValue.length > 0 && inputValue) {\n      inputValue.current.classList.add(inputHasContentClassName);\n    } else {\n      inputValue.current.classList.remove(inputHasContentClassName);\n    }\n  }, [inputValue]);\n\n  function handleInputChange(e: React.FormEvent<HTMLInputElement>) {\n    setInputValue(e.currentTarget.value);\n  }\n\n  function handleSubmit(e: React.FormEvent) {\n    e.preventDefault();\n\n    console.log('Submit form');\n  }\n\n  return (\n    <>\n      <section>\n        <form onSubmit={handleSubmit}>\n          <div>\n            <input\n              ref={myRef}\n              onChange={handleInputChange}\n              type=\"number\"\n            \/>\n            <label>Input Label<\/label>\n          <\/div>\n          <button\n            type=\"submit\"\n            disabled={inputValue.length === 0}\n          >\n            Submit\n          <\/button>\n        <\/form>\n      <\/section>\n    <\/>\n  );\n};\n\nexport default Component;\n\n```\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"typescript"},"answer":"`useRef()` returns an object with a `current` property, which contains the object you actually care about. And before the first render is complete, that `current` property will be null. This means the type of that ref is:\n\n\n\n```\n{ current: WhateverTypeYouCareAbout | null }\n\n```\n\nAnd that means you have to handle `null` as a possible value of the `current` property. But the ref object itself will always exist, it's just that its `current` property may be `null`.\n\n\nI would simply store the `current` value of your ref in a variable, test that existence, and then use it.\n\n\n\n```\n  useEffect(() => {\n    const inputHasContentClassName = 'input--has-value';\n    const inputElement = inputValue.current;        \n\n    if (inputElement && inputElement.value.length > 0) {\n      inputElement.classList.add(inputHasContentClassName);\n    } else {\n      inputElement.classList.remove(inputHasContentClassName);\n    }\n  }, [inputValue]);\n\n```\n\nYou can also tell the TypeScript compiler the type of your ref (in this case `HTMLInputElement`) by doing the following:\n\n\n\n```\nconst myRef = useRef<HTMLInputElement>();\n\n```\n\n"}
{"questionId":"ed9046e260754b47aa63df08f1f2591f","question":"How to configure VSCode to run Yarn 2 (with PnP) powered TypeScript\nHow to configure VSCode to run Yarn 2 (with PnP) powered TypeScript\n\n\nI like to use Yarn 2 (with PnP) and a few months ago I setup a project for which it worked fine. Now I tried to setup a fresh project, but whatever I try, I cannot get VSCode to resolve the modules properly. The old project still works and my test case works properly in it, so it must be the new project and not VSCode wherein the problem lies.\n\n\nMy new project is setup as follows:\n\n\n\n```\nmkdir my-project\ncd my-project\nnpm install -g npm\nnpm install -g yarn\nyarn set version berry\nyarn init\nyarn add --dev @types\/node typescript ts-node prettier\nyarn dlx @yarnpkg\/pnpify --sdk vscode\ncat <<'EOF' > tsconfig.json\n{\n  \"compilerOptions\": {\n    \"types\": [\n      \"node\"\n    ]\n  }\n}\nEOF\nmkdir src\ncat <<'EOF' > src\/test.ts\nprocess.once(\"SIGINT\", () => process.exit(0));\nEOF\n\n```\n\nI did check similar questions on StackExchange and elsewhere, but they come down to running `pnpify` and selecting the TypeScript version within VSCode to be its workbench `-pnpify` version, which I both did. I also made sure to preform a `Reload Window`, but I still get the following errors:\n\n\nIn `tsconfig.json`: Cannot find type definition file for 'node'.\n\n\nAnd in `test.ts`: Cannot find name 'process'. Do you need to install type definitions for node? Try `npm i --save-dev @types\/node` and then add `node` to the types field in your tsconfig.\n\n\nIt is important to note that I can run `test.ts` without any problems like so: `yarn ts-node src\/test.ts`. Thus the problem seems limited to the workbench configuration of VSCode (VSCode can still resolve modules for my old project).\n\n\n**What steps am I missing in my setup to make Yarn 2 (with PnP) powered TypeScript properly work within VSCode?**\n\n\nVSCode about information:\n\n\n\n```\nVersion: 1.51.1\nCommit: e5a624b788d92b8d34d1392e4c4d9789406efe8f\nDate: 2020-11-10T23:31:29.624Z\nElectron: 9.3.3\nChrome: 83.0.4103.122\nNode.js: 12.14.1\nV8: 8.3.110.13-electron.0\nOS: Linux x64 5.7.19\n\n```\n\nReported TypeScript version in VSCode: `4.1.3-pnpify`.\n\n\n\n```\n> cd my-project\n> yarn --version\n2.4.0\n\n```\n\nUpdate: I tried adding `nodeLinker: node-modules` to `.yarnrc.yml` and when I `Reload Window` VSCode no longer reports errors and it correctly returns `NodeJS.Process` when I hover `process` in my `test.ts`. This at least shows that most of the setup should be correct, and its only PnP that is making trouble for VSCode.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"I had this problem last night while migrating to Yarn v2 and using PnP.\n\n\n1. Make sure that after running `yarn dlx @yarnpkg\/sdks vscode` the following folder structure was created inside your `.yarn` directory: `.yarn\/sdks\/typescript\/lib`.\n2. Change your VSCode workspace configuration to add the following:   \n`\"typescript.tsdk\": \".yarn\/sdks\/typescript\/lib\"`\n\n\nI also had another problem with step 1 while using Yarn workspaces in a monorepo, what I had to do was to install `typescript`, `prettier` and `eslint` as devDependencies to the root package (where it usually doesn't have any dependencies). And on step 2 I changed the path to `frontend\/.yarn\/sdks\/typescript\/lib`\n\n\n"}
{"questionId":"97449b842d9a45ebb1bdd0e3e70fdd01","question":"count() parameter must be an array or an object that implements countable in laravel\nThis is code here:\n\n\n\n```\nprotected function credentials(Request $request)\n{\n    $admin=admin::where('email',$request->email)->first();\n    if(count($admin))\n    {\n       if($admin->status==0){\n           return ['email'=>'inactive','password'=>'You are not an active person, Please contact to admin'];\n           }\n           else{\n               return ['email'=>$request->email,'password'=>$request->password,'status'=>1];\n           }\n       }\n       return $request->only($this->username(), 'password');\n    }\n\n```\n\nWhen i run the code this error become: \n\n\n\n> \n> \"count(): Parameter must be an array or an object that implements Countable\"\n> \n> \n> \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"You may cast a variable to array. But be careful: using `array($variable)` will give you **incorrect** result, returning an array with a single value of $variable in it, always returning 1 as a count.\n\n\n`(array)` type casting operator must be used instead:\n\n\n\n```\ncount((array)$variable));\n\n```\n\n"}
{"questionId":"3494394f9b4e424f874c7901d9451b3e","question":"kotlin const val Const 'val' are only allowed on top level or in objects\nIn my Kotlin project I want to declare constant on compile time:\n\n\nSo I use this:\n\n\n\n```\n@RunWith(AndroidJUnit4::class)\nclass TradersActivityTest {\n\n    private lateinit var mockServer: MockWebServer\n    private const val ONE_TR = \"no_wallets.json\" \/\/ error here\n\n```\n\nBut I has compile time error:\n\n\n\n```\nConst 'val' are only allowed on top level or in objects\n\n```\n\nHow declare compile time constant?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"kotlin"},"answer":"`const val`s cannot be in a class. For you, this means you need to declare it top-level, in an object, or in a companion object (which also is exactly what the error message says).\n\n\nSince your value is private, a `companion object` is one of the two options you can use:\n\n\n\n```\nclass TradersActivityTest {\n    ...\n    companion object {\n        private const val ONE_TR = \"no_wallets.json\"\n    }\n}\n\n```\n\nDoing that makes it accessible to the class only.\n\n\n\n\n---\n\n\nThe second option is top-level. However, note that this exposes it to the rest of the file, not just the one class:\n\n\n\n```\nprivate const val ONE_TR = \"no_wallets.json\"\n\n...\n\nclass TradersActivityTest {\n    ...\n}\n\n```\n\n\n\n---\n\n\nFor the sake of completeness, the third option was using an object:\n\n\n\n```\nobject Constants {\n    const val ONE_TR = \"no_wallets.json\"\n}\n\n```\n\nHowever, it needs to be public to be accessed. It can alternatively be internal, but it again depends on what you want to have access.\n\n\n"}
{"questionId":"2b8dc30f7181492c89c13b8a6f4e7e10","question":"VS Code Error: (this.configurationService.getValue(...) || []).filter is not a function\nI just started getting this error in VS Code that prevents me from create a new file or even opening a file. The pop error that VS Code shows is `(this.configurationService.getValue(...) || []).filter is not a function`\n\n\nThis error\/bug even stops me from opening the extensions tab or launching basic hotkeys. Anybody else have this issue?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"I had the same problem.\n\n\nFound few errors in settings.json. Try to check it and let me know how it will go.\n\n\nThis error pops up every time I try to create a new file saying\n\n\n\n> \n> \"(this.configurationService.getValue(...) || []).filter is not a\n> function\"\n> \n> \n> \n\n\nIn my case, I had to fix \"workbench.editorAssociations\" flag to:\n\n\n\n```\n\"workbench.editorAssociations\": [\n    {\n        \"viewType\": \"jupyter-notebook\",\n        \"filenamePattern\": \"*.ipynb\"\n    }\n],\n\n```\n\nPreviously it was:\n\n\n\n```\n\"workbench.editorAssociations\": {\n    \"*.ipynb\": \"jupyter-notebook\"\n}\n\n```\n\n"}
{"questionId":"f76c15097b0f4ee7995579913580c997","question":"What is going on: C++ std::move on std::shared\\_ptr increases use\\_count?\nI was always assuming that `std::move()` on a `std::shared_ptr` steals the pointer and sets the pointer of the original to `nullptr`-thus not increasing the reference count. That does not seem to be true in my world.\n\n\n**SETUP**: \n\n\nMacOS, g++ -version => \"Apple LLVM version 10.0.1 (clang-1001.0.46.3)\"\n\n\n**CODE**:\n\n\n\n```\n#include <cstdio>                                                                                                                                                                                  \n#include <memory>\nclass Thing { public: Thing(int N) : value(N) {} int value; };\n\nvoid print(const char* name, std::shared_ptr<Thing>& sp)\n{ printf(\"%s: { use_count=%i; }\\n\", name, (int)sp.use_count()); }\n\nint main(int argc, char** argv) {\n    std::shared_ptr<Thing>  x(new Thing(4711));\n    print(\"BEFORE x\", x);\n    std::shared_ptr<Thing>  y = std::move(x);\n    y->value = 4712;\n    print(\" AFTER x\", x);\n    print(\" AFTER y\", y);\n    return 0;\n}\n\n```\n\n**OUTPUT**:\n\n\nCompiling (`g++ tmp.cpp -o test`), and running (`.\/test`), delivers\n\n\n\n```\nBEFORE x: { use_count=1; }\n AFTER x: { use_count=2; }\n AFTER y: { use_count=2; }\n\n```\n\nSo, the reference count is increased while using `std::move()`. \n\n\n**QUESTION**:\n\n\nWhat is going on, here?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c++"},"answer":"\n> \n> What is going on, here?\n> \n> \n> \n\n\nOn MacOS, it seems that you must explicitly enable move-sematics with `-std=c++11` (or later standards)\u00b9. Otherwise, the example happens to compile (i.e., `std::shared_ptr` from the related library implementation is usable) but doesn't work correctly as the required language features aren't enabled. This results in actual copies being made instead of move constructions. It would have been better if the AppleClang package didn't even allow an instantiation of `std::shared_ptr` when the required language features isn't enabled.\n\n\n\u00b9) Thanks to @t.niese for testing the given compiler\/platform.\n\n\n"}
{"questionId":"f97734939c214cbca49a64ad6755a6d6","question":"Basic if else statement in Makefile\nI'm trying to execute a simple if else statement in a Makefile:\n\n\n\n```\ncheck:\n  if [ -z \"$(APP_NAME)\" ]; then \\\n    echo \"Empty\" \\\n  else \\\n    echo \"Not empty\" \\\n  fi\n\n```\n\nWhen I execute `make check` I get the following error:\n\n\n\n```\nif [ -z \"\" ]; then\n\/bin\/bash: -c: line 1: syntax error: unexpected end of file\nmake: *** [check] Error 2\n\n```\n\nAny idea what I'm doing wrong?\n\n\nI know I can use the following, but I have a lot of logic after the echos so I need to spread it out across multiple lines:\n\n\n\n```\ncheck:\n  [ -z \"$(PATH)\" ] && echo \"Empty\" || echo \"Not empty\"\n\n```\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"bash\/shell"},"answer":"Change your **make** target to this (adding semicolons):\n\n\n\n```\ncheck:\n    if [ -z \"$(APP_NAME)\" ]; then \\\n        echo \"Empty\"; \\\n    else \\\n        echo \"Not empty\"; \\\n    fi\n\n```\n\nFor evaluating a statement in a shell without newlines (newlines get eaten by the backslash `\\`) you need to properly end it with a semicolon. You cannot use real newlines in a Makefile for conditional shell-script code (see **Make-specific background**)\n\n\n`[ -z \"$(APP_NAME)\" ]`,\n`echo \"Empty\"`,\n`echo \"Not empty\"` are all statements that need to be evaluated (similar to pressing enter in terminal after you typed in a command).\n\n\n## Make-specific background\n\n\n**make** spawns a new shell for each command on a line, so you cannot use true multi line shell code as you would e.g. in a script-file.\n\n\nTaking it to an extreme, the following would be possible in a shell script file, because the *newline* acts as command-evaluation (like in a terminal hitting enter is a newline-feed that evaluates the entered command):\n\n\n\n```\nif\n[ 0 ]\nthen\necho \"Foo\"\nfi\n\n```\n\n*Listing 1*\n\n\nIf you would write this in a Makefile though, `if` would be evaluated in its own shell (changing the shell-state to **if**) after which technically the condition `[ 0 ]` would be evaluated in its own shell again, without any connection to the previous `if`.\nHowever, **make** will not even get past the first `if`, because it expects an exit code to go on with the next statement, which it will not get from just changing the shell's state to `if`.\n\n\nIn other words, if two commands in a make-target are completely independent of each other (no conditions what so ever), you could just perfectly fine separate them solely by a normal newline and let them execute each in its own shell.\n\n\nSo, in order to make **make** evaluate multi line conditional shell scripts correctly, you need to evaluate the whole shell script code in one line (so it all is evaluated in the same shell).\n\n\nHence, for evaluating the code in *Listing 1* inside a Makefile, it needs to be translated to:\n\n\n\n```\nif \\\n[ 0 ]; \\\nthen \\\necho \"Foo\"; \\\nfi\n\n```\n\nThe last command `fi` does not need the backslash because that's where we don't need to keep the spawned shell open anymore.\n\n\n"}
{"questionId":"6779578ba455484888f6c217981bc55c","question":"Android 3.3.0 update, ERROR: Cause: invalid type code: 68\nAfter the new update of Android Studio (3.3.0) I'm getting error from Gradle sync saying \"ERROR: Cause: invalid type code: 68\". Even in project, that have been created before the update and hasn't changed at all.\nI've tried to reinstall Android Studio, which hasn't helped either, so there has to be something incompatible with my project, but it doesn't say what and why.\n\n\nApp gradle:\n\n\n\n```\napply plugin: 'com.android.application'\n\napply plugin: 'kotlin-android'\n\napply plugin: 'kotlin-android-extensions'\n\nandroid {\n    compileSdkVersion 28\n    defaultConfig {\n        applicationId \"cz.cubeit.cubeit\"\n        minSdkVersion 21\n        targetSdkVersion 28\n        versionCode 1\n        versionName \"1.0\"\n        testInstrumentationRunner \"android.support.test.runner.AndroidJUnitRunner\"\n    }\n    buildTypes {\n        release {\n            minifyEnabled false\n            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'\n        }\n    }\n}\n\ndependencies {\n    implementation fileTree(dir: 'libs', include: ['*.jar'])\n    implementation\"org.jetbrains.kotlin:kotlin-stdlib-jdk7:$kotlin_version\"\n    implementation 'com.android.support:appcompat-v7:28.0.0'\n    implementation 'com.android.support:cardview-v7:28.0.0'\n    implementation 'com.android.support.constraint:constraint-layout:1.1.3'\n    testImplementation 'junit:junit:4.12'\n    androidTestImplementation 'com.android.support.test:runner:1.0.2'\n    androidTestImplementation 'com.android.support.test.espresso:espresso-core:3.0.2'\n    implementation 'com.android.support:support-v4:28.0.0'\n    implementation 'com.android.support:design:28.0.0'\n}\n\n```\n\nProject gradle:\n\n\n\n```\n\/\/ Top-level build file where you can add configuration options common to all sub-projects\/modules.\n\nbuildscript {\n    ext.kotlin_version = '1.3.11'\n    ext.anko_version='0.10.7'\n    repositories {\n        google()\n        jcenter()\n    }\n    dependencies {\n        classpath 'com.android.tools.build:gradle:3.3.0'\n        classpath \"org.jetbrains.kotlin:kotlin-gradle-plugin:$kotlin_version\"\n\n        \/\/ NOTE: Do not place your application dependencies here; they belong\n        \/\/ in the individual module build.gradle files\n    }\n}\n\nallprojects {\n    repositories {\n        google()\n        jcenter()\n    }\n}\n\ntask clean(type: Delete) {\n    delete rootProject.buildDir\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"I had the same problem. I was working on a project on two PCs: one at my office and one at home. In my office, after the AS 3.3 update, everything was fine but at home, after the same steps that I did at the office, the Error code 68 came out.\n\n\nAfter a couple of hours, I figured out how to solve it.\nAndroid Studio 3.3 has the **\" Only sync the active variant\"** option enabled by default in **Settings>Experimental**. **Disabling this feature resolved the problem.** But reactivating the feature the same error shows up, even after a successful Gradle sync. So I think this isn't a complete solution, but at least now I can work.\n\n\n"}
{"questionId":"1c8709c3e0f44ab4922b11cfdfc634c5","question":"Athena: Query exhausted resources at scale factor\nI am running a query like: \n\n\n\n```\nSELECT f.*, p.countryName, p.airportName, a.name AS agentName\nFROM (\n    SELECT \n        f.outboundlegid, \n        f.inboundlegid,\n        f.querydatetime,\n        cast(f.agent as bigint) as agent,\n        cast(f.querydestinationplace as bigint) as querydestinationplace,\n        f.queryoutbounddate,\n        f.queryinbounddate,\n        f.quoteageinminutes,\n        f.price\n    FROM flights f\n    WHERE querydatetime >= '2018-01-02'\n    AND querydatetime <= '2019-01-10'\n) f\nINNER JOIN (\n  SELECT airportId, airportName, countryName\n  FROM airports\n  WHERE countryName IN ('Philippines', 'Indonesia', 'Malaysia', 'Hong Kong', 'Thailand', 'Vietnam')\n) p\nON f.querydestinationplace = p.airportId\nINNER JOIN agents a\nON f.agent = a.id\nORDER BY f.outboundlegid, f.inboundlegid, f.agent, querydatetime DESC\n\n```\n\nWhat's wrong with it? Or how can I optimize it? It gives me \n\n\n\n> \n> Query exhausted resources at this scale factor\n> \n> \n> \n\n\nI have a flights table and I want to query for flights inside a specific country\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"sql"},"answer":"I have been facing this problem since the begining of Athena, the problem is the `ORDER BY` clause. Athena is just an EMR cluster with hive and prestodb installed. The problem you are facing is: Even if your query is distributed across X numbers of nodes, the ordering phase must be done by just a single node, the master node in this case. So at the end, you can order as much data as memory have the master node.\n\n\nYou can test it by reducing the amount of data the query returns maybe reducing the time range.\n\n\n"}
{"questionId":"68c55b783b70495da5191d95fde90c60","question":"How to generate controller using dotnetcore command line\nIn Ruby on Rails, you can generate controllers using something like the following in command line:\n\n\n`rails generate controller ControllerName action1 action2` `...etc`\n\n\nIs there something similar in the **dotnetcore cli** for generating controllers? \n\n\nFrom what I can find the dotnetcore cli seems quite limited in the commands that you can do. I did find something from Microsoft's docs about extending the cli but I am not confident about how to do that for a command such as this. \n\n\n**UPDATE: Jan 29th 2019**\n\n\n**@Jspy**'s answer is the new way of generating controllers using dotnetcore cmd since mid 2018. \n\n\n**UPDATE: Dec 21st 2016**\n\n\nUsing **@Sanket**'s answer I was able to generate controllers for my dotnetcore application. However I encountered an error \n\n\n\n```\nPackage Microsoft.Composition 1.0.27 is not compatible with netcoreapp1.1 (.NETCoreApp,Version=v1.1). Package Microsoft.Composition 1.0.27 supports: portable-net45+win8+wp8+wpa81 (.NETPortable,Version=v0.0,Profile=Profile259)\nOne or more packages are incompatible with .NETCoreApp,Version=v1.1.\n\n```\n\nTo solve this issue I added `\"net451\"` to the framework import statement for the `netcoreapp1.1` dependency.\n\n\nMy simple `project.json` file for my empty project (using @Sanket's `project.json` template) looked like this:\n\n\n\n```\n{\n  \"version\": \"1.0.0-*\",\n  \"buildOptions\": {\n    \"debugType\": \"portable\",\n    \"emitEntryPoint\": true\n  },\n  \"dependencies\": {\n    \"Microsoft.VisualStudio.Web.CodeGeneration.Tools\": {\n      \"version\": \"1.1.0-preview4-final\",\n      \"type\": \"build\"\n    },\n    \"Microsoft.VisualStudio.Web.CodeGenerators.Mvc\": {\n      \"version\": \"1.1.0-preview4-final\",\n      \"type\": \"build\"\n    },\n    \"Microsoft.AspNetCore.Mvc\": \"1.0.0-*\",\n    \"Microsoft.AspNetCore.StaticFiles\": \"1.0.0-*\"\n  },\n  \"tools\": {\n    \"Microsoft.AspNetCore.Server.IISIntegration.Tools\": \"1.1.0-preview4-final\",\n    \"Microsoft.EntityFrameworkCore.Tools\": \"1.1.0-preview4-final\",\n    \"Microsoft.VisualStudio.Web.CodeGeneration.Tools\": {\n      \"version\": \"1.1.0-preview4-final\",\n      \"imports\": [\n        \"portable-net45+win8\"\n      ]\n    }\n  },\n  \"frameworks\": {\n    \"netcoreapp1.1\": {\n      \"dependencies\": {\n        \"Microsoft.NETCore.App\": {\n          \"type\": \"platform\",\n          \"version\": \"1.1.0\"\n        }\n      },\n      \"imports\": [\n        \"netcoreapp1.1\",\n        \"net451\"\n      ]\n    }\n  }\n}\n\n```\n\nAfter running (in terminal) `$` `dotnet restore` I could run the following command to generate a basic controller.\n\n\n`dotnet aspnet-codegenerator --project . controller -name SimpleController`\n\n\nThis generated an *empty* controller **SimpleController.cs** with the following code:\n(Note that my dotnet project was called `ToolsAppDotNetCore`)\n\n\n\n```\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Microsoft.AspNetCore.Mvc;\n\nnamespace ToolsAppDotNetCore\n{\n    public class SimpleController : Controller\n    {\n        public IActionResult Index()\n        {\n            return View();\n        }\n    }\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"### This is the new way since mid 2018\n\n\nYou have to install dotnet-aspnet-codegenerator.  \n\nThis is now done **globally** and **not** through a Nuget package:\n\n\nPowerShell:\n\n\n\n```\ndotnet tool install --global dotnet-aspnet-codegenerator\n\n```\n\nThen this is how you create a REST-Controller from an existing EF Model in PowerShell:\n\n\n\n```\ndotnet-aspnet-codegenerator -p \"C:\\MyProject\\MyProject.csproj\" controller -name MyDemoModelController -api -m My.Namespace.Models.MyDemoModel -dc MyDemoDbContext -outDir Controllers -namespace My.Namespace.Controllers\n\n```\n\n### Some helpful calls\n\n\nShow available generators (`-p... -h`):\n\n\n\n```\ndotnet-aspnet-codegenerator -p \"C:\\MyProject\\MyProject.csproj\" -h\n\n```\n\nShow available options of the \"controller\" generator (`-p... controller -h`):\n\n\n\n```\ndotnet-aspnet-codegenerator -p \"C:\\MyProject\\MyProject.csproj\" controller -h\n\n```\n\n### Generate controllers for many models in a loop\n\n\nThis is how you would generate REST controllers for all models in a given path from a PowerShell:\n\n\n\n```\nGet-ChildItem \"C:\\MyProject\\Models\" -Filter *.cs | \nForeach-Object {\n    $scaffoldCmd = \n    'dotnet-aspnet-codegenerator ' + \n    '-p \"C:\\MyProject\\MyProject.csproj\" ' +\n    'controller ' + \n    '-name ' + $_.BaseName + 'Controller ' +\n    '-api ' + \n    '-m My.Namespace.Models.' + $_.BaseName + ' ' +\n    '-dc MyDemoDbContext ' +\n    '-outDir Controllers ' +\n    '-namespace My.Namespace.Controllers'\n\n    # List commands for testing:\n    $scaffoldCmd\n\n    # Excute commands (uncomment this line):\n    #iex $scaffoldCmd\n}\n\n```\n\n"}
{"questionId":"4a7299a2594143dfb7e8764340c8e9f6","question":"How can I check type T is among parameter pack Ts...?\nI want to write a function to return true if `T` is one of `Ts...`\n\n\n\n```\ntemplate<class T, class... Ts>\nbool is_one_of<T, Ts...>();\n\n```\n\nFor example, `is_one_of<int, double, int, float>` returns `true`, and `is_one_of<int, double, std::string, bool, bool>` returns `false`.\n\n\nMy own implementation is\n\n\n\n```\ntemplate<class T1, class T2>\nbool is_one_of<T1, T2>() {\n    return std::is_same<T1, T2>;\n}\n\ntemplate<class T1, class T2, class... Ts>\nbool is_one_of<T1, T2, Ts...>() {\n    if (std::is_same<T1, T2>) {\n        return true;\n    }\n    else {\n        return is_one_of<T1, Ts...>();\n    }\n}\n\n```\n\nThis check seems common to me so I wonder if there's already such a function in the standard library. \n\n\n","questionMetadata":{"type":"implementation","level":"advanced","tag":"c++"},"answer":"In your own implementation, one issue is that C++ doesn't allow partial specialization on function templates.\n\n\nYou can use the fold expression (which is introduced in C++17) instead of recursive function call.\n\n\n\n```\ntemplate<class T1, class... Ts>\nconstexpr bool is_one_of() noexcept {\n    return (std::is_same_v<T1, Ts> || ...);\n}\n\n```\n\nIf you are using C++11 where fold expression and `std::disjunction` are not available, you can implement `is_one_of` like this:\n\n\n\n```\ntemplate<class...> struct is_one_of: std::false_type {};\ntemplate<class T1, class T2> struct is_one_of<T1, T2>: std::is_same<T1, T2> {};\ntemplate<class T1, class T2, class... Ts> struct is_one_of<T1, T2, Ts...>: std::conditional<std::is_same<T1, T2>::value, std::is_same<T1, T2>, is_one_of<T1, Ts...>>::type {};\n\n```\n\n"}
{"questionId":"e32501a6279b4149979bbd23873e5dd8","question":"Why is lint giving a warning ineffectual assignment to (ineffassign)\nGetting a lint warning `ineffectual assignment to \"cfg\"` at line `cfg := &utils.Config{}`. Why is that ?\n\n\n\n```\n    cfg := &utils.Config{}\n    env := os.Getenv(\"TEST\")\n    if strings.EqualFold(env, \"INT\") {\n        cfg = utils.GetIntConfig()\n    } else {\n        cfg = utils.GetConfig()\n    }\n\n    cgw.Cgw(cfg)\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"go"},"answer":"After the following `if` statement, `cfg` is written, thus the value assigned to `cfg` using `cfg := &utils.Config{}` is never used. You are using an assignment where a declaration would do.\n\n\n\n```\nvar cfg *utils.Config\n...\n\n```\n\n"}
{"questionId":"eae3b87c005341e6879fd7981e15a423","question":"PHP Laravel PDOException General error referencing column and referenced column in a foreign key constraint are incompatible\nI am currently doing migrations in Laravel via the Terminal, and have these two errors when attempting to use php artisan migrate; I will print errors below:\n\n\n\n\n\n```\nException trace:\r\n\r\n  1   PDOException::(\"SQLSTATE[HY000]: General error: 3780 Referencing column 'room_id' and referenced column 'id' in foreign key constraint 'contacts_room_id_foreign' are incompatible.\")\r\n      \/Users\/shaquilenoor\/Desktop\/chatapi\/vendor\/laravel\/framework\/src\/Illuminate\/Database\/Connection.php:458\r\n\r\n  2   PDOStatement::execute()\r\n      \/Users\/shaquilenoor\/Desktop\/chatapi\/vendor\/laravel\/framework\/src\/Illuminate\/Database\/Connection.php:458\n```\n\n\n\n\n\n\nBased on the code contained within the Exception Trace, the issue seems to be within the code below:\n\n\n\n\n\n```\nuse Illuminate\\Support\\Facades\\Schema;\r\nuse Illuminate\\Database\\Schema\\Blueprint;\r\nuse Illuminate\\Database\\Migrations\\Migration;\r\n\r\nclass CreateContactsTable extends Migration\r\n{\r\n    public function up()\r\n    {\r\n        Schema::create('contacts', function (Blueprint $table) {\r\n            $table->increments('id');\r\n            $table->unsignedInteger('user1_id');\r\n            $table->unsignedInteger('user2_id');\r\n            $table->integer('room_id')->unique();\r\n            $table->timestamps();\r\n            $table->foreign('room_id')->references('id')->on('rooms');\r\n            $table->foreign('user1_id')->references('id')->on('users');\r\n            $table->foreign('user2_id')->references('id')->on('users');\r\n        });\r\n    }\r\n\r\n    public function down()\r\n    {\r\n        Schema::dropIfExists('contacts');\r\n    }\r\n}\n```\n\n\n\n\n\n\nany ideas on how I can resolve?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"If you're on **Laravel 5.8** new migration changed to big increments, so for fixining refrencing error just change **integer** to **bigInteger**, for example:\n\n\n\n```\n$table->integer('user_id')->unsigned()->index();\n$table->foreign('user_id')->references('id')->on('users')->onDelete('cascade');\n\n```\n\nWill changed to:\n\n\n\n```\n$table->bigInteger('user_id')->unsigned()->index();\n$table->foreign('user_id')->references('id')->on('users')->onDelete('cascade');\n\n```\n\n"}
{"questionId":"97c176edbc4e42dca4c8e8cb378ba2da","question":"@composable invocations can only happen from the context of an @composable function\nI'm trying to show a toast message when clicking on a toolbar action, but I got this error\n\n\n\n> \n> @composable invocations can only happen from the context of an\n> @composable function\n> \n> \n> \n\n\nCode:\n\n\n\n```\n@Composable\nfun Toolbar() {\n    TopAppBar(title = { Text(text = \"Jetpack Compose\") }, navigationIcon = {\n        IconButton(onClick = {}) {\n            Icon(Icons.Filled.Menu)\n        }\n    }, actions = {\n        IconButton(onClick = {\n            showMessage(message = \"test\")\n        }) {\n            Icon(vectorResource(id = R.drawable.ic_baseline_save_24))\n        }\n    })\n}\n\n@Preview\n@Composable\nfun ToolbarPreview(){\n    Toolbar()\n}\n\n@Composable\nfun showMessage(message:String){\n    Toast.makeText(ContextAmbient.current, message, Toast.LENGTH_SHORT).show()\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"kotlin"},"answer":"The `onClick` parameter doesn't accept a composable function.\nRemove the `@Composable` annotation in the `showMessage`.  \n\nUse something like:\n\n\n\n```\n@Composable\nfun Toolbar() {\n\n    val context = LocalContext.current\n\n    TopAppBar(title = {},\n            actions = {\n        IconButton(onClick = {\n            showMessage(context, message = \"test\")\n        }){}\n    })\n}\n\nfun showMessage(context: Context, message:String){\n    Toast.makeText(context, message, Toast.LENGTH_SHORT).show()\n}\n\n```\n\n"}
{"questionId":"036f5f73b9494598bea8e353e3e1cbab","question":"What does \"is { }\" mean?\nI see the following code sometimes, and have no idea what the expression is actually testing.\n\n\n\n```\npublic static void Something(string[] value)\n{\n   if (value is { })\n   {\n      DoSomethingElse();\n   }\n}\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c#"},"answer":"That's just the empty property pattern in C# 8, meaning the value not `null`. It matches any value type or reference type. As Panagiotis Kanavos notes in the comments, this is equivalent to the good old `value is object` check which has been in C# for a long time.\n\n\nGenerally if you were to specify a property, then it would match or not. This esoteric example illustrates that:\n\n\n\n```\nif (value is { Length: 2 })\n{\n   \/\/ matches any object that isn't `null` and has a property set to a length of 2\n}\n\n```\n\nThe property patterns work best and are most clear when comparing with other patterns in cases such as `switch` expressions.\n\n\n"}
{"questionId":"b729ce0c60114817a2ecb0b4a3e1de7a","question":"\"Expected response code 250 but got code \"554\", with message \"554 5.2.0 STOR EDRV \"\nI would like to send an email with an attachment. Using smtp.office365.com\n\n\n## **IN Production: ubuntu**\n\n\nsmtp.office365.com - Laravel 5.\n\n\nExpected response code 250 but got code \"554\", with message \"554 5.2.0 \nSTOR EDRV.Submission.Exception:SendAsDeniedException.MapiExceptionSendAsDenied; Failed to process message due to a permanent exception with message Cannot submit message.\n\n\n## **IN Localhost :**\n\n\nExpected response code 250 but got code \"530\", with message \"530 5.7.57 SMTP; Client was not authenticated to send anonymous mail during MAIL FROM [xxxxxx.xxxx.PROD.OUTLOOK.COM]\"\n\n\n## .env\n\n\n\n```\nMAIL_DRIVER=smtp\nMAIL_HOST=smtp.office365.com\nMAIL_PORT=587\nMAIL_USERNAME=xxx@org.io\nMAIL_PASSWORD='xxxxx'\nMAIL_ENCRYPTION=tls\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"For Office 365, `From_Email` must be the same as the logon user. You are changing the From address, and that is not allowed.\n\n\n"}
{"questionId":"24599da037114c7c8b11ad185e136b49","question":"How can I improve readability and length of a method with many if statements?\nI have a method with 195 ifs. Here is a shorter version:\n\n\n\n```\nprivate BigDecimal calculateTax(String country, BigDecimal amount) throws Exception {\n    if(country.equals(\"POLAND\")){\n        return new BigDecimal(0.23).multiply(amount);\n    }\n    else if(country.equals(\"AUSTRIA\")) {\n        return new BigDecimal(0.20).multiply(amount);\n    }\n    else if(country.equals(\"CYPRUS\")) {\n        return new BigDecimal(0.19).multiply(amount);\n    }\n    else {\n        throw new Exception(\"Country not supported\");\n    }\n}\n\n```\n\nI can change ifs to switches:\n\n\n\n```\nprivate BigDecimal calculateTax(String country, BigDecimal amount) throws Exception {\n    switch (country) {\n        case \"POLAND\":\n            return new BigDecimal(0.23).multiply(amount);\n        case \"AUSTRIA\":\n            return new BigDecimal(0.20).multiply(amount);\n        case \"CYPRUS\":\n            return new BigDecimal(0.19).multiply(amount);\n        default:\n            throw new Exception(\"Country not supported\");\n    }\n}\n\n```\n\nbut 195 cases is still so long. How could I improve readability and length of that method? What pattern would be the best in this case?\n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"java"},"answer":"Create a `Map<String,Double>` that maps country names to their corresponding tax rates:\n\n\n\n```\nMap<String,Double> taxRates = new HashMap<> ();\ntaxRates.put(\"POLAND\",0.23);\n...\n\n```\n\nUse that `Map` as follows:\n\n\n\n```\nprivate BigDecimal calculateTax(String country, BigDecimal amount) throws Exception {\n    if (taxRates.containsKey(country)) {\n        return new BigDecimal(taxRates.get(country)).multiply(amount);\n    } else {\n        throw new Exception(\"Country not supported\");\n    }\n}\n\n```\n\n"}
{"questionId":"1d397c1f60014b64948a619c25e4b2b3","question":"Error: Each parameter in constructor must bind to an object property or field on deserialization\ni'm trying to send data to save it in my db, basic, so i send a model to my controller, and i get this error:\n\n\n\"Each parameter in constructor 'Void .ctor(SmgApi.Models.Entity.EquipmentEntity)' on type 'SmgApi.Models.Dto.EquipementDto' must bind to an object property or field on deserialization. Each parameter name must match with a property or field on the object. The match can be case-insensitive.\"\n\n\nbut i don't deserialize anything, i don't understand.\nmy controller :\n\n\n\n```\n[HttpPost]\npublic async Task<IActionResult> UpdateEquipment(EquipementDto equipment)\n{\n    return Ok(await _equipmentRepository.UpdateEquipment(new EquipmentEntity(equipment)));\n}\n\n```\n\nEquipmentDto:\n\n\n\n```\npublic class EquipementDto\n{\n    [Required]\n    public string Id { get; set; }\n    public List<PropertyDto> Properties { get; set; }\n\n    [Required]\n    public string Type { get; set; }\n    public bool isInPalette { get; set; }\n    public EquipementDto(EquipmentEntity equipment)\n    {\n        if (equipment == null)\n            return;\n\n        this.Id = equipment.Id;\n        this.Type = equipment.Type;\n        this.isInPalette = equipment.IsInPalette;\n        this.Properties = equipment.Properties.Select(x => new PropertyDto(x)).ToList();\n    }\n}\n\n```\n\nmy Equipment interface in front:\n\n\n\n```\nexport interface Equipment {\n  id: string;\n  type: Type;\n  isInPalette: boolean;\n  properties: Array<Property>;\n}\n\n```\n\nmy request:\n\n\n\n```\nprivate equipmentUrl: string = environment.apiUrl + '\/equipment';\n\n  constructor(private http: HttpClient) {}\n\n  public saveEquipment(equipment: Equipment): Observable<Equipment> {\n    return this.http.post<Equipment>(this.equipmentUrl, equipment);\n  }\n\n```\n\nAny ideas on what I'm doing wrong?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"\n> \n> but i don't deserialize anything, i don't understand.\n> \n> \n> \n\n\nYou might not do it directly, but you certainly ask the framework to deserialize something for you:\n\n\n\n```\n[HttpPost]\npublic async Task<IActionResult> UpdateEquipment(EquipementDto equipment)\n\n```\n\nHere you tell the framework you expect the call to contain a serialized string of json representing an object matching your `EquipementDto` and to please deserialize that string for you into an actual instance of the `EquipementDto` and pass that to the method.\n\n\nSo now that we've determined where the deserialization takes place we can take a look at what's going on. During deserialization the framework that does the deserialization needs to be able to construct the object you want deserialized.\n\n\nTo be able to construct that object it needs a valid constructor. It seems like it'll accept 2 types of constructors:\n\n\n1. An empty constructor\n2. A constructor with parameters that match the property names of the object (so instead of setting the properties directly the framework can populate them through the constructor.\n\n\nYour `EquipementDto` has only 1 constructor and that constructor takes some unknown `EquipmentEntity` that the deserialization framework knows nothing about.\n\n\nNow I'm not completely sure but I think this will be solved by just adding a parameterless constructor to your `EquipementDto` class:\n\n\n\n```\n[JsonConstructor]\npublic EquipementDto(){}\n\n```\n\n"}
{"questionId":"960ed1e0ee534e50acb1d9d4b58fedd5","question":"System.NotSupportedException: Serialization and deserialization of 'System.Action' instances are not supported. Path: $.MoveNextAction\nI am following a simple tutorial on .NET 6 and it should work really simple, but apparently I get the exception. The sample code is the following:\n\n\n\n```\npublic async Task<ServiceResponse<List<GetCharacterDto>>> GetAllCharacters()\n{\n    var response = new ServiceResponse<List<GetCharacterDto>>();\n    var dbCharacters = await _context.Characters.ToListAsync();\n    response.Data = dbCharacters.Select(c => _mapper.Map<GetCharacterDto>(c)).ToList();\n\n    return response;\n}\n\n```\n\nThe code in GetCharacterDto is:\n\n\n\n```\npublic class GetCharacterDto\n{ \n    public int Id { get; set; }\n    \n    public string Name { get; set; } = \"Frodo\";\n\n    public int HitPoints { get; set; } = 100;\n\n    public int Strength { get; set; } = 10;\n\n    public int Defense { get; set; } = 10;\n\n    public int Intelligence { get; set; } = 10;\n\n    public RpgClass Class { get; set; } = RpgClass.Knight;\n}\n\n\n```\n\nRpgClass:\n\n\n\n```\n[JsonConverter(typeof(JsonStringEnumConverter))]\npublic enum RpgClass\n{\n    Knight = 1,\n    Mage = 2,\n    Cleric = 3\n}\n\n```\n\nThe exception\n\n\n\n> \n> System.NotSupportedException: Serialization and deserialization of 'System.Action' instances are not supported. Path: $.MoveNextAction.\n> Is thrown right at\n> \n> \n> \n\n\n\n```\nvar dbCharacters = await _context.Characters.ToListAsync(); \n\n```\n\nIf I call it synchronously\n\n\n\n```\n_context.Characters.ToList();\n\n```\n\nit works alright, but can't get it to work asynchronously.\n\n\nI have both .NET 5 SDK and .NET 6 SDK installed, if that could be a potential issue.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"I was missing an await in the controller where I was calling the `GetAllCharacters()` method.\n\n\n"}
{"questionId":"186c5f1c0e0c4105920d25f939ab8fe8","question":"Remove ViewPager2 Overscroll animation\nCan't find a way to remove the ViewPager2 overscroll shadow animation.\nI know on ViewPager, you can directly just set the overscrollMode attribute to never, however, it does not work on ViewPager2\n\n\nAlready tried the following\n\n\n\n```\n<androidx.viewpager2.widget.ViewPager2\n        android:id=\"@+id\/viewPager\"\n        android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n        android:overScrollMode=\"never\"\/>\n\n```\n\n\n```\nbinding.viewPager.apply {\n        adapter = adapter\n        orientation = ViewPager2.ORIENTATION_VERTICAL\n        overScrollMode = ViewPager2.OVER_SCROLL_NEVER\n        offscreenPageLimit = if (containsVideo) 2 else 5\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"kotlin"},"answer":"Solution\n\n\n\n```\nbinding.viewPager2.apply {\n    adapter = vpAdapter\n    orientation = ViewPager2.ORIENTATION_VERTICAL\n    registerOnPageChangeCallback(pageChangeCallback)\n    (getChildAt(0) as RecyclerView).overScrollMode = RecyclerView.OVER_SCROLL_NEVER\n}\n\n```\n\n"}
{"questionId":"283482fcbf064dce9508f913331a061c","question":"How to declare a variable and its type is Boolean in PowerShell?\nWhen I study PowerShell scripting language, I knew the data type is auto-assignment. If I want to declare a Boolean type variable, how can I do it?\n\n\nExample: $myvariable = \"string\" or $myvalue = 3.14159\n$myboolean ?\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"bash\/shell"},"answer":"Boolean values (which can be either 1 or 0) are defined in PowerShell using the .Net System.Boolean type (the short from of which is [bool]). For example, the following command assigns true to a variable of boolean type:\n\n\n\n```\nPS C:\\Users\\Administrator> [bool] $myval = 1\nPS C:\\Users\\Administrator> $myval.gettype().fullname\nSystem.Boolean\n\n```\n\nWhen working with boolean variables, the pre-defined \n\n\n$true\n\n\nand \n\n\n\n> \n> $false\n> \n> \n> \n\n\nvariables may also be used:\n\n\n\n```\nPS C:\\Users\\Administrator> [bool] $myval = $false\nPS C:\\Users\\Administrator> $myval\nFalse\n\n```\n\n"}
{"questionId":"f3649f5a5ae34df09bf703c854bfadb9","question":"JWT token authentication fails with message \"PII is hidden\"\nin my .net core 2.2 microservice, I try to extract claims from a JWT token to do some authorization. authentication is done on another part of the system so I don't need to do it at this point. \n\n\nI am using this code in the Startup.cs:\n\n\n\n```\n  services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)\n            .AddJwtBearer(options =>\n            {\n                var signingKey = Encoding.UTF8.GetBytes(\"SECRET_KEY\");\n                options.TokenValidationParameters = new TokenValidationParameters\n                {\n                    ValidateIssuer = false,\n                    ValidateAudience = false,\n                    IssuerSigningKey = new SymmetricSecurityKey(signingKey)\n                };\n            });\n\n```\n\nOn the controller I have this code:\n\n\n\n```\n    [Authorize]\n    [HttpPost]\n    public async Task<ActionResult<CreateResponse>> Create()\n    {\n        var userIdClaim = HttpContext.User.Claims.Where(x => x.Type == \"empId\").SingleOrDefault();\n        return Ok($\"Your User ID is {userIdClaim.Value} and you can create invoices!\");\n    }\n\n```\n\nI always get this error message and \"Unauthorized\" response:\n\n\n\n> \n> Microsoft.IdentityModel.Tokens.SecurityTokenInvalidSignatureException: IDX10503: Signature validation failed. Keys tried: '[PII is hidden]'.\n>  Exceptions caught:\n>  '[PII is hidden]'.\n>  token: '[PII is hidden]'.\n> \n> \n> \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"You can see the hidden details in development by adding the following to Configure() in the Startup class:\n\n\n\n```\nif (env.IsDevelopment())\n{\n     IdentityModelEventSource.ShowPII = true; \n}\n\n```\n\nOnce you have the full message check the key being used is correct for the token.\n\n\n"}
{"questionId":"bb9cac00eeb745e095cf5cb4a777576f","question":"Jetpack Compose pass parameter to viewModel\nHow can we pass parameter to `viewModel` in `Jetpack Compose`?\n\n\nThis is my composable\n\n\n\n```\n    @Composable\n    fun UsersList() {\n      val myViewModel: MyViewModel = viewModel(\"db2name\") \/\/ pass param like this\n    }\n\n\n```\n\nThis is `viewModel`\n\n\n\n```\n    class MyViewModel(private val dbname) : ViewModel() {\n        private val users: MutableLiveData<List<User>> by lazy {\n            MutableLiveData<List<User>>().also {\n                loadUsers()\n            }\n        }\n    \n        fun getUsers(): LiveData<List<User>> {\n            return users\n        }\n    \n        private fun loadUsers() {\n            \/\/ Do an asynchronous operation to fetch users.\n        }\n    }\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"kotlin"},"answer":"you need to create a factory to pass dynamic parameter to ViewModel like this:\n\n\n\n```\nclass MyViewModelFactory(private val dbname: String) :\n    ViewModelProvider.NewInstanceFactory() {\n    override fun <T : ViewModel?> create(modelClass: Class<T>): T = MyViewModel(dbname) as T\n}\n\n```\n\nthen use your factory like this in composable functions:\n\n\n\n```\n@Composable\nfun UsersList() {\n    val myViewModel: MyViewModel =\n        viewModel(factory = MyViewModelFactory(\"db2name\")) \/\/ pass param like this\n}\n\n```\n\nand now you have access to dbname parameter in your ViewModel:\n\n\n\n```\nclass MyViewModel(private val dbname:String) : ViewModel() {\n    \/\/ ...rest of the viewModel logics here\n}\n\n```\n\n"}
{"questionId":"fba7c09ae85146bead7100faafb362ed","question":"\"package XXX is not in GOROOT\" when building a Go project\nI have a weird issue that arose when I took a break from this project. Upon starting up Golang, I'm riddled with errors when trying to run my project.\n\n\nThe specific error, when building one of my packages, is:\n`start.go: package project\/game is not in GOROOT (C:\\Go\\src\\project\\game)`\n\n\nI have a folder structure as such under `C:\\Users\\username`\n\n\n\n```\ngo\n|-src\n   |-project\n        |-game\n            |-entity\n                 |-whatever.go\n            |-game_stuff.go\n        |-server\n\n```\n\nand my env vars are as such:\n\n\n\n```\nGOROOT=C:\\Go \nGOPATH=C:\\Users\\ketchup\\go \n\n```\n\nfor each of the modules (project\/game\/entity, project\/game, project\/server), I did a `git mod init`.\n\n\nWhen building, Goland will try to run this:\n\n\n\n```\nC:\\Go\\bin\\go.exe build -o C:\\Users\\ketchup\\AppData\\Local\\Temp\\___go_build_project_server.exe project\/server\n\n```\n\nand return the error.\n\n\nCan anyone help me with this issue? Kind of lost since Goland was working fine the last time I opened it. Also not even sure what direction to look at - I'm pretty new to Go and I'm not really sure what documentation to look at :\\ Thank you everyone!\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"go"},"answer":"A pretty dumb conclusion (mostly on my part) but my issue came from having done `go mod init` in each of the folders. after removing `go.mod` and `go.dep` from each of the folders I did `go mod init` in, I could build without issue (through terminal)\n\n\nAlso, my packages in GoLand were not being detected because I had Go Modules enabled in the Settings. I disabled it and GoLand was able to index the external packages and my own packages.\n\n\n"}
{"questionId":"919eea3ca58846c3866a2fb0b76eea0c","question":"Angular N - what is the best practice of declaring a constant file?\nTo declare a constant file, I first create it in the same tree level as the component where the constant is used `email.constants.ts` and like this:\n\n\n\n```\nexport class EmailConstants {\n\n  public static MAXIMUM_NUMBER = 10;\n}\n\n```\n\nAnd I import it that way from the controller:\n\n\n\n```\nimport { EmailConstants } from '.\/emails.constants';\n\n```\n\nIs this practice good? I ask the question here because I can not find the answer in the official guide style\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"I will do it like this:\n\n\n\n```\nexport const MAXIMUM_NUMBER = 10;\n\n```\n\nand\n\n\n\n```\nimport { MAXIMUM_NUMBER } from '.\/emails.constants';\n\n```\n\nSo only import what you use, not everything.\n\n\nbut if you still want to use everything, you can do it similarly as you have done, just change it a bit:\n\n\n\n```\nimport * as EmailConstants from '.\/emails.constants';\n\n\n```\n\nThen you can still use\n\n\n\n```\nEmailConstants.MAXIMUM_NUMBER\n\n```\n\n"}
{"questionId":"4a4412995f194c038fb360b1117f3c97","question":"dplyr summarise: Equivalent of \".drop=FALSE\" to keep groups with zero length in output\nWhen using `summarise` with `plyr`'s `ddply` function, empty categories are dropped by default. You can change this behavior by adding `.drop = FALSE`. However, this doesn't work when using `summarise` with `dplyr`. Is there another way to keep empty categories in the result?\n\n\nHere's an example with fake data.\n\n\n\n```\nlibrary(dplyr)\n\ndf = data.frame(a=rep(1:3,4), b=rep(1:2,6))\n\n# Now add an extra level to df$b that has no corresponding value in df$a\ndf$b = factor(df$b, levels=1:3)\n\n# Summarise with plyr, keeping categories with a count of zero\nplyr::ddply(df, \"b\", summarise, count_a=length(a), .drop=FALSE)\n\n  b    count_a\n1 1    6\n2 2    6\n3 3    0\n\n# Now try it with dplyr\ndf %.%\n  group_by(b) %.%\n  summarise(count_a=length(a), .drop=FALSE)\n\n  b     count_a .drop\n1 1     6       FALSE\n2 2     6       FALSE\n\n```\n\nNot exactly what I was hoping for. Is there a `dplyr` method for achieving the same result as `.drop=FALSE` in `plyr`?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"r"},"answer":"Since *dplyr 0.8* `group_by` gained the `.drop` argument that does just what you asked for:\n\n\n\n```\ndf = data.frame(a=rep(1:3,4), b=rep(1:2,6))\ndf$b = factor(df$b, levels=1:3)\n\ndf %>%\n  group_by(b, .drop=FALSE) %>%\n  summarise(count_a=length(a))\n\n#> # A tibble: 3 x 2\n#>   b     count_a\n#>   <fct>   <int>\n#> 1 1           6\n#> 2 2           6\n#> 3 3           0\n\n```\n\nOne additional note to go with @Moody\\_Mudskipper's answer: Using `.drop=FALSE` can give potentially unexpected results when one or more grouping variables are not coded as factors. See examples below:\n\n\n\n```\nlibrary(dplyr)\ndata(iris)\n\n# Add an additional level to Species\niris$Species = factor(iris$Species, levels=c(levels(iris$Species), \"empty_level\"))\n\n# Species is a factor and empty groups are included in the output\niris %>% group_by(Species, .drop=FALSE) %>% tally\n\n#>   Species         n\n#> 1 setosa         50\n#> 2 versicolor     50\n#> 3 virginica      50\n#> 4 empty_level     0\n\n# Add character column\niris$group2 = c(rep(c(\"A\",\"B\"), 50), rep(c(\"B\",\"C\"), each=25))\n\n# Empty groups involving combinations of Species and group2 are not included in output\niris %>% group_by(Species, group2, .drop=FALSE) %>% tally\n\n#>   Species     group2     n\n#> 1 setosa      A         25\n#> 2 setosa      B         25\n#> 3 versicolor  A         25\n#> 4 versicolor  B         25\n#> 5 virginica   B         25\n#> 6 virginica   C         25\n#> 7 empty_level <NA>       0\n\n# Turn group2 into a factor\niris$group2 = factor(iris$group2)\n\n# Now all possible combinations of Species and group2 are included in the output, \n#  whether present in the data or not\niris %>% group_by(Species, group2, .drop=FALSE) %>% tally\n\n#>    Species     group2     n\n#>  1 setosa      A         25\n#>  2 setosa      B         25\n#>  3 setosa      C          0\n#>  4 versicolor  A         25\n#>  5 versicolor  B         25\n#>  6 versicolor  C          0\n#>  7 virginica   A          0\n#>  8 virginica   B         25\n#>  9 virginica   C         25\n#> 10 empty_level A          0\n#> 11 empty_level B          0\n#> 12 empty_level C          0\n\nCreated on 2019-03-13 by the reprex package (v0.2.1)\n\n```\n\n"}
{"questionId":"e12305f5a2ca4f90a46484df0d62a64b","question":"What is reason to use is operator in TypeScript?\nWhat is reason to use is operator in TypeScript in this case?\n\n\n\n```\ntype Species = \"cat\" | \"dog\";\n\n\ninterface Pet {\n   species: Species\n}\n\ninterface Cat extends Pet {\n\n}\n\nfunction petIsCat(pet: Pet): pet is Cat {\n   return pet.species === \"cat\";\n}\n\n```\n\nWhy to use `pet is Cat` instead `boolean` if body returns bool type? \n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"typescript"},"answer":"boolean is just a data type while 'is' operator is used for type-testing. Let me change your example a little bit:\n\n\n\n```\ntype Species = 'cat' | 'dog';\n\ninterface Pet {\n    species: Species;\n}\n\nclass Cat implements Pet {\n    public species: Species = 'cat';\n    public meow(): void {\n        console.log('Meow');\n    }\n}\n\nfunction petIsCat(pet: Pet): pet is Cat {\n    return pet.species === 'cat';\n}\n\nfunction petIsCatBoolean(pet: Pet): boolean {\n    return pet.species === 'cat';\n}\n\nconst p: Pet = new Cat();\n\np.meow(); \/\/ ERROR: Property 'meow' does not exist on type 'Pet'.\n\nif (petIsCat(p)) {\n    p.meow(); \/\/ now compiler knows for sure that the variable is of type Cat and it has meow method\n}\n\nif (petIsCatBoolean(p)) {\n    p.meow(); \/\/ ERROR: Property 'meow' does not exist on type 'Pet'.\n}\n\n```\n\nOf course you can use type casting:\n\n\n\n```\n(<Cat>p).meow(); \/\/ no error\n(p as Cat).meow(); \/\/ no error\n\n```\n\nBut it depends on the scenario.\n\n\n"}
{"questionId":"19d319af9722494c8be824550af45d0f","question":"Host name may not be empty\nAfter upgrading my Android Studio, I got this error whenever I wanted to generate a signed Apk (without any detail of where the problem is). There was no problem when I just built Apk.\n\n\n\n```\nThe Host name may not be empty\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"Check your Gradle scripts root. If there is this file: **gradle.properties (Global Properties)**, check that values are correct.\n\n\nIn my case, **I removed all these properties** and my problem solved\n\n\n\n```\nsystemProp.http.proxyHost=\nsystemProp.http.proxyPort=80\nsystemProp.https.proxyHost=\nsystemProp.https.proxyPort=80\n\n```\n\n"}
{"questionId":"ad350eb50d224e43b15bce2ecee9ccbe","question":"How do I resolve the issue the request matched multiple endpoints in .Net Core Web Api\nI notice that there are a bunch of similar questions out there about this topic. \n\n\nI'm getting this error when calling any of the methods below.\n\n\n\n> \n> *Microsoft.AspNetCore.Routing.Matching.AmbiguousMatchException: The request matched multiple endpoints.* \n> \n> \n> \n\n\nI can't however sort out what is best practice in resolving the issue.\nSo far I haven't set up any specific routing middleware.\n\n\n\n```\n\/\/ api\/menus\/{menuId}\/menuitems\n[HttpGet(\"{menuId}\/menuitems\")]\npublic IActionResult GetAllMenuItemsByMenuId(int menuId)\n{            \n    ....\n}\n\n\/\/ api\/menus\/{menuId}\/menuitems?userId={userId}\n[HttpGet(\"{menuId}\/menuitems\")]\npublic IActionResult GetMenuItemsByMenuAndUser(int menuId, int userId)\n{\n    ...\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"c#"},"answer":"What you're trying to do is impossible because the actions are dynamically activated. The request data (such as a query string) cannot be bound until the framework knows the action signature. It can't know the action signature until it follows the route. Therefore, you can't make routing dependent on things the framework doesn't even know yet.\n\n\nLong and short, you need to differentiate the routes in some way: either some other static path or making the `userId` a route param. However, you don't actually need separate actions here. All action params are optional by default. Therefore, you can just have:\n\n\n\n```\n[HttpGet(\"{menuId}\/menuitems\")]\npublic IActionResult GetMenuItemsByMenu(int menuId, int userId)\n\n```\n\nAnd then you can branch on whether `userId == 0` (the default). That should be fine here, because there will never be a user with an id of `0`, but you may also consider making the param nullable and then branching on `userId.HasValue` instead, which is a bit more explicit.\n\n\nYou can also continue to keep the logic separate, if you prefer, by utilizing private methods. For example:\n\n\n\n```\n[HttpGet(\"{menuId}\/menuitems\")]\npublic IActionResult GetMenuItems(int menuId, int userId) =>\n    userId == 0 ? GetMenuItemsByMenuId(menuId) : GetMenuItemsByUserId(menuId, userId);\n\nprivate IActionResult GetMenuItemsByMenuId(int menuId)\n{\n    ...\n}\n\nprivate IActionResult GetMenuItemsByUserId(int menuId, int userId)\n{\n    ...\n}\n\n```\n\n"}
{"questionId":"82814a4b6b62432a8fba7fabccc7c7db","question":"CSV file with Arabic characters is displayed as symbols in Excel\nI am using python to extract Arabic tweets from twitter and save it as a CSV file, but when I open the saved file in excel the Arabic language displays as symbols. However, inside python, notepad, or word, it looks good.\n\n\nMay I know where is the problem?\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"python"},"answer":"This is a problem I face frequently with Microsoft Excel when opening CSV files that contain Arabic characters. Try the following workaround that I tested on latest versions of Microsoft Excel on both Windows and MacOS:\n\n\n1. **Open Excel** on a blank workbook\n2. Within the **Data** tab, click on **From Text** button (if not\n activated, make sure an empty cell is selected)\n3. Browse and **select the CSV file**\n4. In the Text Import Wizard, change the **File\\_origin** to \"**Unicode (UTF-8)**\"\n5. Go next and from the **Delimiters**, select the delimiter used in your file e.g. **comma**\n6. **Finish** and select where to import the data\n\n\nThe Arabic characters should show correctly.\n\n\n"}
{"questionId":"eb8729253fbd40dda265980ea51d4b1d","question":"flake8 disable linter only for a block of code\nI have a file in python like:\n\n\n\n```\ndef test_constructor_for_legacy_json():\n    \"\"\"Test if constructor works for a legacy JSON in an old database\"\"\"\n\n    a = A(**{\n        'field1': 'BIG TEXT WITH MORE THAN 500 CHARACTERS....(...)',\n        'field2': 'BIG TEXT WITH MORE THAN 500 CHARACTERS....(...)',\n        'field3': 'BIG TEXT WITH MORE THAN 500 CHARACTERS....(...)',\n        # (...)\n        'field1000': 'BIG TEXT WITH MORE THAN 500 CHARACTERS....(...)',\n    })\n\n    assert type(a) == A\n\n```\n\nWhen I run `flake8` + `hacking` I receive an error because the lines are too big.\n\n\nIf I put this command at the beginning of the file `# flake8: noqa` all file will be ignored from linter. But I only want to exclude from linter the block where `a` is declared.\n\n\nI want to lint the rest of the file, and I cannot put at the end of each `fieldx` an `# noqa: E501`.\n\n\nSome one know how can I solve this?\nThanks\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"There isn't a way in flake8 to ignore a block of code\n\n\nYour options are:\n\n\n1. ignore each line that produces an error by putting `# noqa: E501` on it\n2. ignore the entire file (but this turns off all other errors as well) with a `# flake8: noqa` on a line by itself\n3. ignore `E501` in the entire file by using `per-file-ignores`:\n\n\n\n```\n[flake8]\nper-file-ignores =\n     path\/to\/file.py: E501\n\n```\n\n\ngenerally I'd prefer the third one, maybe even sequestering your long-strings into their own file to be ignored\n\n\n\n\n---\n\n\ndisclaimer: I'm the current flake8 maintainer\n\n\n"}
{"questionId":"93e81a29092543c098c2101e2e5e643d","question":"How to pipe \/ map an Observable in Angular\nA nested object is showing up as [object Object] so I'm trying to cast it via pipe and map but I'm not getting any where. I've tried the models as classes and interfaces but no help. Can someone tell me what I'm doing wrong? Thanks.\n\n\nThe function:\n\n\n\n```\n  getClients(customerId: number): Observable<Client[]> {\n    let clientUrl = 'SOME_URL';\n    return this.http.get<Client[]>(clientUrl)\n      .pipe(map(client: Client) => client.address as Address);\n  }\n\n```\n\nThe models:\n\n\n\n```\nimport { Address } from '.\/address.model';\n\nexport class Client{\n  id: number;\n  name: string;\n  accountNumber: string;\n  addressId: number;\n  phoneNumber: string;\n  address: Address;\n}\n\n\nexport class Address{\n  id: number;\n  name: string;\n  addressLine1: string;\n  addressLine2: string;\n  city: string;\n  postalCode: string;\n}\n\n```\n\nI'm getting the error:\nError TS2345 (TS) Argument of type 'Address' is not assignable to parameter of type 'OperatorFunction<{}, Client[]>'.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"1) remove the piping part from your getClients() method\n\n\n2) do the pipe-map before subscribing to getClients() or create another method, that will do only the piping part with the observable returned from getClients()\n\n\n\n```\nmapToAddress(): Observable<Address[]> {\n  this.getClients.pipe(\n    map((clients: Client[]) => clients.map(client => client.address))\n  )\n}\n\n```\n\nThis is important to understand: when you call .map() method inside .pipe(), you're not getting a single client in this case, you get the whole clients array, pushed to Observable. Because you map the values, that are stored in the Observable - the values of type: < Client[] >. \n\n\nYour pipe-map would work on some Observable, that emits a single client of type < Client >, not an array.\n\n\n"}
{"questionId":"dc932bb81ecc491580c122f7d8c0450b","question":"Error in phpmyadmin Warning in .\/libraries\/plugin\\_interface.lib.php#551\n**Error:**\n\n\n\n> \n> Warning in .\/libraries\/plugin\\_interface.lib.php#551 count(): Parameter\n>  must be an array or an object that implements Countable\n> \n> \n> \n\n\n**Backtrace:**\n\n\n\n```\n.\/libraries\/display_export.lib.php#381: PMA_pluginGetOptions(\nstring 'Export',\narray,\n)\n.\/libraries\/display_export.lib.php#883: PMA_getHtmlForExportOptionsFormat(array)\n.\/libraries\/display_export.lib.php#1099: PMA_getHtmlForExportOptions(\nstring 'table',\nstring 'bpapluswpdb',\nstring 'wp_commentmeta',\nstring '',\ninteger 0,\narray,\ninteger 0,\n)\n.\/tbl_export.php#143: PMA_getExportDisplay(\nstring 'table',\nstring 'bpapluswpdb',\nstring 'wp_commentmeta',\nstring '',\ninteger 0,\ninteger 0,\nstring '',\n)\n\n```\n\nHow can I fix it?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"Just edit the plugin **\\_interface.lib.php**\n\n\n\n```\nsudo gedit \/usr\/share\/phpmyadmin\/libraries\/plugin_interface.lib.php\n\n```\n\nFind this line\n\n\n\n```\nif ($options != null && count($options) > 0) {\n\n```\n\nAdd (array) or replace with this\n\n\n\n```\nif ($options != null && count((array)$options) > 0) {\n\n```\n\n"}
{"questionId":"01d0370326ab4b0a89ca975418c5ee8f","question":"Kotlin -  versus \nThere are some cases in Kotlin where the compiler will complain about a generic type parameter defined as `<T>` and expects `<T : Any>`. What is the difference?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"kotlin"},"answer":"The difference is that a plain `<T>` means that it can be `null`able. (which is represented by `Any?`). Using `<T: Any>` will restrict `T` to non-nullable types.\n\n\nSo the difference is that `<T>` is an implicit `<T: Any?>`.\n\n\n"}
{"questionId":"236e867589a349ae942c17033099bcb2","question":"Laravel 5.8 showing \"419 Page Expired\" after clicking logout from an already cleared session\nI run the **php artisan make:auth** command and I will explain step by step what I do after that to understand the scenario,\n\n\n- Login to a new session (example.com\/home)\n- opened a new tab and pasted the URL, ie example.com\/home.\n- Now 2 tabs are open with the same session.\n- I clicked logout from one of the tabs and it works perfectly fine\n- Then when I tried to logout from the other tab, it gave me an error saying \"419 Page Expired\" and it is going nowhere even after reloading.\n\n\nThe thing is, these kinds of scenarios may arise, and I don't want to see this error message, just logout after clicking logout, even if the session is expired.\n\n\n**Note: This issue is not because of not adding @csrf**\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"php"},"answer":"Well that's an obvious message you can maybe try to make a better layout for that page, but still it is good to show it so the user knows what happened. If you want to handle it differently you can try to redirect to the login page. \n\n\nSo in your `app\\Exceptions\\Handler.php` file within the render method add this:\n\n\n\n```\nif ($exception instanceof \\Illuminate\\Session\\TokenMismatchException) {\n    return redirect()->route('login');\n}\n\n```\n\n"}
{"questionId":"6f74e8e22dc943df8f5e7859dfe2cdfa","question":"Why does my NavController cannot find an ID that I already have?\nEvery time when I launch my app it crashes and Logcat says this:\n\n\n\n> \n> Caused by: java.lang.IllegalArgumentException: ID does not reference a View inside this Activity\n> \n> \n> \n\n\nSo I think this is saying ID is not found, but here's my Java file and XML file corresponding to each other.\nJava:\n\n\n\n```\nNavController navController = Navigation.findNavController(this,R.id.nav_host_fragment);\n\n```\n\nXML:\n\n\n\n```\n<com.google.android.material.bottomnavigation.BottomNavigationView\n        android:id=\"@+id\/nav_view\"\n        android:layout_width=\"0dp\"\n        android:layout_height=\"wrap_content\"\n        android:layout_marginStart=\"0dp\"\n        android:layout_marginEnd=\"0dp\"\n        android:background=\"?android:attr\/windowBackground\"\n        app:layout_constraintBottom_toBottomOf=\"parent\"\n        app:layout_constraintLeft_toLeftOf=\"parent\"\n        app:layout_constraintRight_toRightOf=\"parent\"\n        app:menu=\"@menu\/bottom_nav_menu\" \/>\n\n    <fragment\n        android:id=\"@+id\/nav_host_fragment\"\n        android:name=\"androidx.navigation.fragment.NavHostFragment\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"\n        app:defaultNavHost=\"true\"\n        app:layout_constraintBottom_toTopOf=\"@id\/nav_view\"\n        app:layout_constraintHorizontal_bias=\"1.0\"\n        app:layout_constraintLeft_toLeftOf=\"parent\"\n        app:layout_constraintRight_toRightOf=\"parent\"\n        app:layout_constraintTop_toTopOf=\"parent\"\n        app:layout_constraintVertical_bias=\"0.285\"\n        app:navGraph=\"@navigation\/mobile_navigation\" \/>\n\n```\n\nThe Logcat says failed because the java file, ID does not reference a View inside this Activity. But I clearly typed the ID: nav\\_host\\_fragment.\nI don't know what's wrong and when I'm inside Android Studio and it DID NOT report ANY error. Please help.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"`Navigation.findNavController` relies on `findViewById()`, which only works after you've called `setContentView()` - i.e., actually added your Views to your activity.\n\n\n"}
{"questionId":"ad6d70efa7f44d8b95f585581160650e","question":"Flutter local notification body not showing all notification text (flutter\\_local\\_notifications package)\nI used `flutter_local_notifications: ^0.7.1+3` in my Flutter app to push schedule notifications. All is well in this but the problem in my notification body is that it shows just one line of text, and I can't expand or stretch notification to show all the notification body text.\n\n\nThis is my try:\n\n\n\n```\nclass NotificationUtil {\n  final notifications = FlutterLocalNotificationsPlugin();\n  final int checkOutNotifyId = 0;\n\n  NotificationUtil(BuildContext context) {\n    final settingsAndroid = AndroidInitializationSettings('ic_notify_icon');\n    final settingsIOS = IOSInitializationSettings(\n        onDidReceiveLocalNotification: (id, title, body, payload) =>\n            onSelectNotification(context));\n    notifications.initialize(\n        InitializationSettings(settingsAndroid, settingsIOS),\n        onSelectNotification: (context) async => onSelectNotification);\n  }\n\n  Future<void> showCheckOutNotify([int maximumCheckoutHours]) async {\n    await notifications.periodicallyShow(\n        checkOutNotifyId,\n        AttendanceConstants.SCHEDULE_NOTIFICATION_TITLE,\n        AttendanceConstants.SCHEDULE_NOTIFICATION_BODY +\n            '$maximumCheckoutHours Hour\/s of your attendance',\n        RepeatInterval.Hourly,\n        _ongoing);\n  }\n\n  NotificationDetails get _ongoing {\n    final androidChannelSpecifics = AndroidNotificationDetails(\n      'your channel id',\n      'your channel name',\n      'your channel description',\n      importance: Importance.Max,\n      priority: Priority.High,\n      ongoing: true,\n    );\n    final iOSChannelSpecifics = IOSNotificationDetails();\n    return NotificationDetails(androidChannelSpecifics, iOSChannelSpecifics);\n  }\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"dart"},"answer":"add [ BigTextStyleInformation('') ] in [ AndroidNotificationDetails() ]\n\n\n\n```\nNotificationDetails get _ongoing {\n    final androidChannelSpecifics = AndroidNotificationDetails(\n      'your channel id',\n      'your channel name',\n      'your channel description',\n      importance: Importance.Max,\n      priority: Priority.High,\n      ongoing: true,\n\n      styleInformation: BigTextStyleInformation(''),\n    );\n\n```\n\n"}
{"questionId":"68ef4a6e7b5e4bdf889b33f43e5a4217","question":"How to calculate percentage in Kotlin\nI need to calculate a percentage in Kotlin. I tried but failed to get the correct answer:\n\n\n\n```\nvar percentage = (count\/totalCount) * 100\nit.toast(\"Percentage: $percentage\")\n\n```\n\nWhat is the proper syntax in Kotlin?\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"kotlin"},"answer":"Most likely, you're struggling with the fact that applying the division operator on two integers will result in an integer division being performed, yielding an integer result.\n\n\nThe trick is to promote one of the operands to a floating point type:\n\n\n\n```\nvar percentage = (count.toDouble() \/ totalCount) * 100\n\n```\n\n"}
{"questionId":"8128aadbec5e46eeab8adb19db8d7042","question":"HIstory command only showing last 15 commands\nI enter the command `history` \nIt shows the last 15 commands, where are the previous 988 commands.\n\n\n\n```\n  989  yarn android\n  990  \/Dir\/firebase_dummy_app\/chatroom\/node_modules\/react-native\/scripts\/launchPackager.command ; exit;\n  991  yarn android\n  992  source ~\/.bashrc\n  993  cd Documents\n  994  ls\n  995  rm -rf firebase_dummy_app\n  996  expo init\n  997  cd firesbaseDummy\n  998  yarn android\n  999  cd ..\n 1000  rm -rf firesbaseDummy\n 1001  \/Dir\/firesbaseDummy\/node_modules\/react-native\/scripts\/launchPackager.command ; exit;\n 1002  \/Dir\/firebaseChat\/node_modules\/react-native\/scripts\/launchPackager.command ; exit;\n 1003  history\n 1004  history\n\n```\n\nEven if I grep them `history | grep 'rm'` for something I know I did, none of the previous commands show up\n\n\nIf I continue to enter terminal commands the numbers go up but I'm still limited to only 15. (command 995-1010 will be shown for example\n\n\nIt continues happening when I close the end the terminal app and reopen it\n\n\nThe terminal is zsh on MacOS Catalina\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"The `history n` command, where `n` is a number shows all history since line n. So in your case, `history 904` will show the last 100 commands.\n\n\n"}
{"questionId":"fcfdef6a2151460f8f4414c77fbff2ac","question":"After Upgrading Angular from 8 to 10 I got ERROR in ts.updateIdentifier is not a function\nAfter Upgrading Angular from 8 to 10\nI got\n\n\n\n```\n ERROR in ts.updateIdentifier is not a function\n\n```\n\nthis is my\n\n\nng --version\n\n\n\n```\nAngular CLI: 10.0.6\nNode: 10.15.2\nOS: win32 x64\n\nAngular: 10.0.11\n... animations, common, compiler, compiler-cli, core, forms\n... language-service, platform-browser, platform-browser-dynamic\n... router\nIvy Workspace: <error>\n\nPackage                           Version\n-----------------------------------------------------------\n@angular-devkit\/architect         0.803.29\n@angular-devkit\/build-angular     0.803.29\n@angular-devkit\/build-optimizer   0.803.29\n@angular-devkit\/build-webpack     0.803.29\n@angular-devkit\/core              10.0.6\n@angular-devkit\/schematics        10.0.6\n@angular\/cdk                      10.1.3\n@angular\/cli                      10.0.6\n@angular\/fire                     5.4.2\n@angular\/http                     7.2.16\n@ngtools\/webpack                  8.3.29\n@schematics\/angular               10.0.6\n@schematics\/update                0.1000.6\nrxjs                              6.6.2\ntypescript                        4.0.2\nwebpack                           4.39.2\n\n```\n\nI am using yarn , could anyone help me , I am using code-sharing native script\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"I ran into the same error today. You should downgrade your typescript dependency to 3.9.7. That fixed it for me.\n\n\n"}
{"questionId":"a2a612a028734db6b61fb44eeb8e8ad8","question":"How to split type definitions and resolvers into separate files in Apollo Server\nindex.ts:\n\n\n\n```\n  const server = new ApolloServer({\n    typeDefs,\n    resolvers,\n    context: ({ req, res }: any) => ({ req, res })\n  });\n\n```\n\nUserSchema.ts\n\n\n\n```\nexport const typeDefs = gql`\n  scalar TimeStamp\n  type Query {\n    getUser(id: Int!): User\n  }\n  type Mutation {\n    addUser(\n      name: String!\n      email: String\n      age: Int\n      register_at: TimeStamp!\n    ): Boolean!\n  }\n  type User {\n    id: Int!\n    name: String!\n    email: String!\n    age: Int!\n    register_at: TimeStamp!\n  }\n`;\n\n```\n\nUserResolver.ts\n\n\n\n```\nexport const resolvers = {\n  TimeStamp: timeStamp,\n  Query: {\n    getUser: async (_: any, args: any) => {\n      const { id } = args;\n\n      return await User.findOne({ where: { id: id } });\n    }\n  },\n  Mutation: {\n    addUser: async (_: any, args: any) => {\n      const { name, email, age, register_at } = args;\n      try {\n        const user = User.create({\n          name,\n          email,\n          age,\n          register_at\n        });\n\n        await user.save();\n\n        return true;\n      } catch (error) {\n        return false;\n      }\n    }\n  }\n};\n\n```\n\nI would like to know how I would initialize my Apollo Server instance if I had additional type definitions and resolvers, for example `BookSchema.ts` and `BookResolver.ts`.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"### Type Definitions\n\n\nThe `ApolloServer` constructor can accept an array instead of just the one `DocumentNode` object. So you can do something like:\n\n\n\n```\nconst server = new ApolloServer({\n  typeDefs: [userTypeDefs, bookTypeDefs],\n  resolvers,\n})\n\n```\n\nNote that if you want to split up an individual type's field definitions as well, you'll need to use type extension syntax. For example:\n\n\n\n```\nconst typeDefsA = gql`\n  type Query {\n    users: [User!]!\n  }\n`\nconst typeDefsB = gql`\n  extend type Query {\n    books: [Book!]!\n  }\n`\nconst typeDefsC = gql`\n  extend type Query {\n    posts: [Post!]!\n  }\n`\n\n```\n\nThe above will be combined into a single `Query` type. You can have as many extensions as you want, but the type you're extending **must** exist (i.e., you can't have just three `extend type Query` definitions). Keeping this in mind, I usually create a \"base\" set of type definitions like:\n\n\n\n```\ntype Query\n\ntype Mutation\n\n```\n\nThen all my other type definitions can extend these types. Notice that because these \"base\" types don't have any fields, we don't use curly brackets at all (an empty set of curly brackets will result in a syntax error!).\n\n\n### Resolvers\n\n\nYour resolver map is a plain JavaScript object, so splitting it it up is trivial.\n\n\n\n```\nconst resolversA = {\n  Query: {\n    users: () => {...},\n  }\n}\n\nconst resolversB = {\n  Query: {\n    books: () => {...},\n  }\n}\n\n```\n\nHowever, if you attempt to combine these resolver maps using `Object.assign` or spread syntax, you'll be hurting because any common properties (like `Query`) will be overridden by each object. So **do not** do this:\n\n\n\n```\nconst resolvers = {\n  ...resolversA,\n  ...resolversB,\n}\n\n```\n\nInstead, you want to *deep merge* the objects, so that any child properties (and their properties, and so on) are merged as well. I recommend using `lodash` but there's any number of utilities you can use.\n\n\n\n```\nconst resolvers = _.merge({}, resolversA, resolversB)\n\n```\n\n### Putting it all together\n\n\nYour code might look something like this:\n\n\nuserTypeDefs.ts\n\n\n\n```\nexport default gql`\ntype User {\n  id: ID!\n  username: String!\n  books: [Book!]!\n}\n\nextend type Query {\n  users: [User!]!\n}\n`\n\n```\n\nbookTypeDefs.ts\n\n\n\n```\nexport default gql`\ntype Book {\n  id: ID!\n  title: String!\n  author: User!\n}\n\nextend type Query {\n  books: [Book!]!\n}\n`\n\n```\n\nuserResolvers.ts\n\n\n\n```\nexport default {\n  Query: {\n    users: () => {...},\n  },\n  User: {\n    books: () => {...},\n  },\n}\n\n```\n\nbookResolvers.ts\n\n\n\n```\nexport default {\n  Query: {\n    books: () => {...},\n  },\n  Book: {\n    author: () => {...},\n  },\n}\n\n```\n\nindex.ts\n\n\n\n```\nimport userTypeDefs from '...'\nimport userResolvers from '...'\nimport bookTypeDefs from '...'\nimport bookResolvers from '...'\n\n\/\/ Note: This is also a good place to put any types that are common to each \"module\"\nconst baseTypeDefs = gql`\n  type Query\n`\n\nconst apollo = new ApolloServer({\n  typeDefs: [baseTypeDefs, userTypeDefs, bookTypeDefs],\n  resolvers: _.merge({}, userResolvers, bookResolvers)\n})\n\n```\n\n"}
{"questionId":"4f40d45dbe9a48fe8fc52263f3c16cb0","question":"How do I convert a Vec> to Result, E>?\nIs there any opinionated and more elegant way to convert `Vec<Result<T, E>>` to `Result<Vec<T>, E>`? I want to get `Ok<Vec<T>>` if all values of vector are `Ok<T>` and `Err<E>` if at least one is `Err<E>`.\n\n\nExample:\n\n\n\n```\nfn vec_of_result_to_result_of_vec<T, E>(v: Vec<Result<T, E>>) -> Result<Vec<T>, E>\nwhere\n    T: std::fmt::Debug,\n    E: std::fmt::Debug,\n{\n    let mut new: Vec<T> = Vec::new();\n\n    for el in v.into_iter() {\n        if el.is_ok() {\n            new.push(el.unwrap());\n        } else {\n            return Err(el.unwrap_err());\n        }\n    }\n\n    Ok(new)\n}\n\n```\n\nI'm looking for a more declarative way to write this. This function forces me to write a `where` clause which will never be used and `Err(el.unwrap_err())` looks useless. In other words, the code does many things just to make the compiler happy. I feel like this is such a common case that there's a better way to do it.\n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"rust"},"answer":"An iterator over `Result<T, E>` can be `collect()`-ed directly into a `Result<Vec<T>, E>`; that is, your entire function can be replaced with:\n\n\n\n```\nlet new: Result<Vec<T>, E> = v.into_iter().collect()\n\n```\n\n"}
{"questionId":"6e95b8d3fbb549ecbb1c57e4e94e2490","question":"VSCode Integrated Terminal Doesn't Load .bashrc or .bash\\_profile\nI have the following files to handle shell configuration:\n\n\n\n```\n#~\/.bash_profile\nif [ -f ~\/.bashrc ]; then\n   source ~\/.bashrc\nfi\n\n```\n\nand\n\n\n\n```\n#~\/.bashrc\n... configure shell\n\n```\n\nIf I open VSCode from the command line using `code`, my `.bashrc` is loaded whenever I add a new instance of the integrated shell.\n\n\nHowever if I open VSCode via its icon, only my `.profile` is loaded. \n\n\n**How can I ensure my `.bashrc` is loaded instead?**\n\n\nI've tried various settings for the `terminal.integrated.shellArgs.osx` setting without any luck.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"Simply add shell arguments to the VsCode `settings.json` file.\n\n\nPaths to the `settings.json` file are as follows:\n\n\n\n```\nWindows: C:\\Users\\<username>\\AppData\\Roaming\\Code\\User\\settings.json`\n\nLinux:   $HOME\/.config\/Code\/User\/settings.json\n\nMac:     $HOME\/Library\/Application\\ Support\/Code\/User\/settings.json\n\n```\n\nAdd one of the following:\n\n\n\n```\n\"terminal.integrated.shellArgs.windows\": [\"-l\"],\n\n\"terminal.integrated.shellArgs.linux\": [\"-l\"],\n\n\"terminal.integrated.shellArgs.osx\": [\"-l\"],\n\n```\n\nThis will launch your shell of choice with the login argument. This will thus execute any user profile that is setup.\n\n\n"}
{"questionId":"3ebe45c2df774cd5b652de0c7201b0e8","question":"No handles with labels found to put in legend\nI'm trying to create a parallelogram in PyPlot. I'm not up to drawing the parallelogram--first I'm putting in the vector arrows--using the following code:\n\n\n\n```\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.spines['left'].set_position('zero')\nax.spines['right'].set_color('none')\nax.spines['bottom'].set_position('zero')\nax.spines['top'].set_color('none')\nplt.axis([-5,5,-5,5])\nax.xaxis.set_ticks_position('bottom')\nax.yaxis.set_ticks_position('left')\nplt.grid()\nplt.arrow(0,0, 3,1, head_width=0.2, color='r', length_includes_head=True, label='u')\nplt.arrow(0,0, 1,3, head_width=0.2, color='r', length_includes_head=True, label='v')\nplt.arrow(0,0, 4,4, head_width=0.2, color='r', length_includes_head=True, label='u+v')\nplt.legend()\n\n```\n\nThis returns the following error:\n\n\n\n```\nNo handles with labels found to put in legend.\n\n```\n\nI'm not sure why, because, based on the documentation for `plt.arrow()`, `label` is an acceptable kwarg, and `plt.legend()` should ostensibly be reading that. The rest of the figure draws fine; it's just missing the legend.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"It might be late but for anyone with the same issue the solution is using the method `legend()` for the corresponding `ax` not as for `plt`\n\n\n\n```\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.spines['left'].set_position('zero')\nax.spines['right'].set_color('none')\nax.spines['bottom'].set_position('zero')\nax.spines['top'].set_color('none')\nplt.axis([-5,5,-5,5])\nax.xaxis.set_ticks_position('bottom')\nax.yaxis.set_ticks_position('left')\nplt.grid()\nplt.arrow(0,0, 3,1, head_width=0.2, color='r', length_includes_head=True, label='u')\nplt.arrow(0,0, 1,3, head_width=0.2, color='r', length_includes_head=True, label='v')\nplt.arrow(0,0, 4,4, head_width=0.2, color='r', length_includes_head=True, label='u+v')\nax.legend()\n\n```\n\n"}
{"questionId":"f73d5168edd94be89524988c22e22a82","question":"How can I run an action when a state changes?\n\n```\nenum SectionType: String, CaseIterable {\n    case top = \"Top\"\n    case best = \"Best\"\n}\n\nstruct ContentView : View {\n    @State private var selection: Int = 0\n\n    var body: some View {\n        SegmentedControl(selection: $selection) {\n            ForEach(SectionType.allCases.identified(by: \\.self)) { type in\n                Text(type.rawValue).tag(type)\n            }\n        }\n    }\n}\n\n```\n\nHow do I run code (e.g `print(\"Selection changed to \\(selection)\")` when the `$selection` state changes? I looked through the docs and I couldn't find anything.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"swift"},"answer":"You can't use `didSet` observer on `@State` but you can on an `ObservableObject` property. \n\n\n\n```\nimport SwiftUI\nimport Combine\n\nfinal class SelectionStore: ObservableObject {\n    var selection: SectionType = .top {\n        didSet {\n            print(\"Selection changed to \\(selection)\")\n        }\n    }\n\n    \/\/ @Published var items = [\"Jane Doe\", \"John Doe\", \"Bob\"]\n}\n\n```\n\nThen use it like this:\n\n\n\n```\nimport SwiftUI\n\nenum SectionType: String, CaseIterable {\n    case top = \"Top\"\n    case best = \"Best\"\n}\n\nstruct ContentView : View {\n    @ObservedObject var store = SelectionStore()\n\n    var body: some View {\n        List {\n            Picker(\"Selection\", selection: $store.selection) {\n                ForEach(FeedType.allCases, id: \\.self) { type in\n                    Text(type.rawValue).tag(type)\n                }\n            }.pickerStyle(SegmentedPickerStyle())\n\n            \/\/ ForEach(store.items) { item in\n            \/\/     Text(item)\n            \/\/ }\n        }\n    }\n}\n\n```\n\n"}
{"questionId":"cfcb56b271b0478ea09e8e21bf5ce6e1","question":"expo version command shows running scripts disabled on this machine\nI am trying to develop a react native project in my Windows 10 machine. I installed node js then expo cli via visual studio code terminal. Then I tried the command `expo --version`, the terminal shows error::\n\n\n\n```\nexpo : File C:\\Users\\saka\\AppData\\Roaming\\npm\\expo.ps1 cannot be loaded because running scripts is disabled on this system. For more \ninformation, see about_Execution_Policies at https:\/go.microsoft.com\/fwlink\/?LinkID=135170.\nAt line:1 char:1\n+ expo --version\n+ ~~~~\n    + CategoryInfo          : SecurityError: (:) [], PSSecurityException\n    + FullyQualifiedErrorId : UnauthorizedAccess\n\n```\n\nthe given link shows many things, but what shall I do to solve my problem? \nThank You!!!\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"bash\/shell"},"answer":"Just try to open Windows PowerShell run as administrator and then run this command line:\n\n\n\n```\nSet-ExecutionPolicy RemoteSigned\n\n```\n\n"}
{"questionId":"2fdb61fca05042f69aebb4f06d3d13e3","question":"What is the difference between Stream.of and IntStream.range?\nPlease, consider this code:\n\n\n\n```\nSystem.out.println(\"#1\");\nStream.of(0, 1, 2, 3)\n        .peek(e -> System.out.println(e))\n        .sorted()\n        .findFirst();\n\nSystem.out.println(\"\\n#2\");\nIntStream.range(0, 4)\n        .peek(e -> System.out.println(e))\n        .sorted()\n        .findFirst();\n\n```\n\nThe output will be:\n\n\n\n```\n#1\n0\n1\n2\n3\n\n#2\n0\n\n```\n\nCould anyone explain, why output of two streams are different?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"java"},"answer":"Well, `IntStream.range()` returns `a sequential ordered IntStream from startInclusive(inclusive) to endExclusive (exclusive) by an incremental step of 1`, which means it's already sorted. Since it's already sorted, it makes sense that the following `.sorted()` intermediate operation does nothing. As a result, `peek()` is executed on just the first element (since the terminal operation only requires the first element).\n\n\nOn the other hand, the elements passed to `Stream.of()` are not necessarily sorted (and the `of()` method doesn't check if they are sorted). Therefore, `.sorted()` must traverse all the elements in order to produce a sorted stream, which allows the `findFirst()` terminal operation to return the first element of the sorted stream. As a result, `peek` is executed on all the elements, even though the terminal operation only needs the first element.\n\n\n"}
{"questionId":"727aa47bfb0645dbafdea7995c1df5b1","question":"What does a yield inside a yield do?\nConsider the following code:\n\n\n\n```\ndef mygen():\n     yield (yield 1)\na = mygen()\nprint(next(a))\nprint(next(a)) \n\n```\n\nThe output yields:\n\n\n\n```\n1\nNone\n\n```\n\nWhat does the interpreter do at the \"outside\" yield exactly?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"python"},"answer":"`a` is a generator object. The first time you call `next` on it, the body is evaluated up to the first `yield` expression (that is, the first to be evaluated: the inner one). That `yield` produces the value `1` for `next` to return, then blocks until the next entry into the generator. That is produced by the second call to `next`, which does *not* send any value *into* the generator. As a result, the first (inner) `yield` evaluates to `None`. That value is used as the argument for the outer `yield`, which becomes the return value of the second call to `next`. If you were to call `next` a third time, you would get a `StopIteration` exception.\n\n\nCompare the use of the `send` method (instead of `next`) to change the return value of the first `yield` expression.\n\n\n\n```\n>>> a = mygen()\n>>> next(a)\n1\n>>> a.send(3)  # instead of next(a)\n3\n>>> next(a)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nStopIteration\n\n```\n\nA more explicit way of writing the generator would have been\n\n\n\n```\ndef mygen():\n    x = yield 1\n    yield x\n\na = mygen()\nprint(a.send(None))  # outputs 1, from yield 1\nprint(a.send(5))     # makes yield 1 == 5, then gets 5 back from yield x\nprint(a.send(3))     # Raises StopIteration, as there's nothing after yield x\n\n```\n\n\n\n---\n\n\nPrior to Python 2.5, the `yield` *statement* provided one-way communication between a caller and a generator; a call to `next` would execute the generator up to the next `yield` statement, and the value provided by the `yield` keyword would serve as the return value of `next`. The generator\nwould also suspend at the point of the `yield` statement, waiting for the next call to `next` to resume.\n\n\nIn Python 2.5, the `yield` statement was replaced\\* with the `yield` *expression*, and generators acquired a `send` method. `send` works very much like `next`, except it can take an argument. (For the rest of this, assume that `next(a)` is equivalent to `a.send(None)`.) A generator starts execution after a call to `send(None)`, at which point it executes up to the first `yield`, which returns a value as before. Now, however, the expression blocks until the *next* call to `send`, at which point the `yield` expression evaluates to the argument passed to `send`. A generator can now *receive* a value when it resumes.\n\n\n\n\n---\n\n\n\\* Not quite replaced; kojiro's answer goes into more detail about the subtle difference between a `yield` statement and `yield` expression.\n\n\n"}
{"questionId":"dd0455234ce2438095b235809c048019","question":"Uninstall Java version 11 mac\nHow to uninstall a specific java version from a mac? \n\n\nWhen I execute the following command: \n\n\n\n```\n\/usr\/libexec\/java_home -V\n\n```\n\nI see the following.\n\n\n\n```\nMatching Java Virtual Machines (2):\n    11.0.2, x86_64: \"OpenJDK 11.0.2\"    \/Library\/Java\/JavaVirtualMachines\/openjdk-11.0.2.jdk\/Contents\/Home\n    1.8.0_202, x86_64:  \"Java SE 8\" \/Library\/Java\/JavaVirtualMachines\/jdk1.8.0_202.jdk\/Contents\/Home\n\n```\n\nI want to uninstall just Java 11.0.2 and keep the second one.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"java"},"answer":"Run this command and it will remove the JDK\n\n\n\n```\nsudo rm -rf \/Library\/Java\/JavaVirtualMachines\/openjdk-11.0.2.jdk\n\n```\n\n"}
{"questionId":"5bf995347434409396e41a928c40aff0","question":"Using Factory Pattern with ASP.NET Core Dependency Injection\nI need the ASP.Net Core dependency injection to pass some parameters to the constructor of my GlobalRepository class which implements the ICardPaymentRepository interface. \n\n\nThe parameters are for configuration and come from the config file and the database, and I don't want my class to go and reference the database and config itself.\n\n\nI think the factory pattern is the best way to do this but I can't figure out the best way to use a factory class which itself has dependencies on config and database.\n\n\nMy startup looks like this currently:\n\n\n\n```\npublic class Startup\n{\n    public IConfiguration _configuration { get; }\n    public IHostingEnvironment _environment { get; }\n\n    public Startup(IConfiguration configuration, IHostingEnvironment environment)\n    {\n        _configuration = configuration;\n        _environment = environment;\n    }\n\n    public void ConfigureServices(IServiceCollection services)\n    {\n        services.AddScoped<IDbRepository, DbRepository>();\n        var connection = _configuration.GetConnectionString(\"DbConnection\");\n        services.Configure<ConnectionStrings>(_configuration.GetSection(\"ConnectionStrings\"));\n        services.AddDbContext<DbContext>(options => options.UseSqlServer(connection));\n        services.AddScoped<ICardPaymentRepository, GlobalRepository>();\n        ...\n    }\n\n    public void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory loggerFactory, IRFDbRepository rFDbRepository)\n    {\n     ...\n    }\n}\n\n```\n\nThe GlobalRepository constructor looks like this:\n\n\n\n```\npublic GlobalRepository(string mode, string apiKey)\n{\n}\n\n```\n\nHow do I now pass the mode from configuration and the apiKey from the DbRepository into the constructor from Startup?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"Use the factory delegate overload when registering the repository\n\n\n\n```\n\/\/...\n\nstring mode = \"get value from config\";\n\nservices.AddScoped<ICardPaymentRepository, GlobalRepository>(sp => {        \n    IDbRepository repo = sp.GetRequiredService<IDbRepository>();\n    string apiKey = repo.GetApiKeyMethodHere();\n\n    return new GlobalRepository(mode, apiKey);\n});\n\n\/\/...\n\n```\n\nAlternative using `ActivatorUtilities.CreateInstance`\n\n\n\n```\n\/\/...\n\nstring mode = \"get value from config\";\n\nservices.AddScoped<ICardPaymentRepository>(sp => {        \n    IDbRepository repo = sp.GetRequiredService<IDbRepository>();\n    string apiKey = repo.GetApiKeyMethodHere();\n\n    return ActivatorUtilities.CreateInstance<GlobalRepository>(sp, mode, apiKey);\n});\n\n\/\/...\n\n```\n\n"}
{"questionId":"f62aab874a8244269fa937c9a6c57e1b","question":"Typescript: onPress type\nI just ported my react-native project to typescript and have a question about functions as props\n\n\nim passing:\n\n\n\n```\n<DisplayCardsWithLikes\ndata={testData}\nlikes={500}\nonPress={() => this.props.navigation.navigate(\"CardDetailScreen\")}\n\/>\n\n```\n\nto\n\n\n\n```\ntype Props = {\n  onPress: Function\n}\n\n\nconst FloatingActionButtonSimple = (props:Props) => {\n  const {onPress} = props\n  return (\n    <View style={styles.containerFab}>\n      <TouchableOpacity style={styles.fab} onPress={onPress}>\n        <Icon name=\"plus\" size={16} color={\"white\"} \/>\n      <\/TouchableOpacity>\n    <\/View>\n  );\n};\n\n```\n\nError:\n\n\n\n```\nError, caused by child onPress:\no overload matches this call.\n  Overload 1 of 2, '(props: Readonly<TouchableOpacityProps>): TouchableOpacity', gave the following error.\n    Type 'Function' is not assignable to type '(event: GestureResponderEvent) => void'.\n      Type 'Function' provides no match for the signature '(event: GestureResponderEvent): void'.\n  Overload 2 of 2, '(props: TouchableOpacityProps, context?: any): TouchableOpacity', gave the following error.\n    Type 'Function' is not assignable to type '(event: GestureResponderEvent) => void'.ts(2769)\nindex.d.ts(5125, 5): The expected type comes from property 'onPress' which is declared here on type 'IntrinsicAttributes & IntrinsicClassAttributes<TouchableOpacity> & Readonly<TouchableOpacityProps> & Readonly<...>'\nindex.d.ts(5125, 5): The expected type comes from property 'onPress' which is declared here on type 'IntrinsicAttributes & IntrinsicClassAttributes<TouchableOpacity> & Readonly<TouchableOpacityProps> & Readonly<...>'\n\n```\n\nSummary:\nonPress passed as prop (is a function). On child type onPress:Function displays an error(the one above), onPress:any works though. I basically don't know which kind of type the onPress prop is\n\n\nSo nothing crazy, but if I define onPress as a function it displays an error, so apparently that's not the correct type. Do you know which type this onPress functions are?\n\n\nThanks a lot!\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"You need to define type as below to get rid of type error from tslint:\n\n\n\n```\ntype Props {\n   onPress: (event: GestureResponderEvent) => void\n}\n\n```\n\nOR\n\n\n\n```\ntype Props {\n   onPress(): void\n}\n\n```\n\nOR\n\n\n\n```\ntype Props {\n   onPress(params: type): void\n}\n\n```\n\n"}
{"questionId":"21ee783e3bf946f997f20b80d23c1b70","question":"How to write 2\\*\\*n - 1 as a recursive function?\nI need a function that takes n and returns **2n - 1** . It sounds simple enough, but the function has to be recursive. So far I have just 2n:\n\n\n\n```\ndef required_steps(n):\n    if n == 0:\n        return 1\n    return 2 * req_steps(n-1)\n\n```\n\nThe exercise states: \"You can assume that the parameter n is always a positive integer and greater than 0\"\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"`2**n -1` is also **1+2+4+...+2n-1** which can made into a single recursive function (without the second one to subtract 1 from the power of 2).\n\n\n**Hint**: 1+2\\*(1+2\\*(...))\n\n\nSolution below, don't look if you want to try the hint first.\n\n\n\n\n---\n\n\nThis works if `n` is guaranteed to be greater than zero (as was actually promised in the problem statement):\n\n\n\n```\ndef required_steps(n):\n    if n == 1: # changed because we need one less going down\n        return 1\n    return 1 + 2 * required_steps(n-1)\n\n```\n\nA more robust version would handle zero and negative values too:\n\n\n\n```\ndef required_steps(n):\n    if n < 0:\n        raise ValueError(\"n must be non-negative\")\n    if n == 0:\n        return 0\n    return 1 + 2 * required_steps(n-1)\n\n```\n\n(Adding a check for non-integers is left as an exercise.)\n\n\n"}
{"questionId":"b4d8dfc4e297446d85be0ec8c1a4a633","question":"What does this mean: a pointer to void will never be equal to another pointer?\nOne of my friends pointed out from \"Understanding and Using C Pointers - Richard Reese, O'Reilly publications\" the second bullet point and I wasn't able to explain the **first** sentence from it. What am I missing?\n\n\n\n> \n> ##### Pointer to void\n> \n> \n> A pointer to void is a general-purpose pointer used to hold references to any data type. An example of a pointer to void is shown below:\n> \n> \n> \n> ```\n> void *pv;\n> \n> ```\n> \n> It has two interesting properties:\n> \n> \n> - A pointer to void will have the same representation and memory alignment as a pointer to `char`.\n> - *A pointer to void will never be equal to another pointer.* However, two void pointers assigned a `NULL` value will be equal.\n> \n> \n> \n\n\nThis is my code, not from the book and all pointers are having the same value and are equal.\n\n\n\n```\n#include <stdio.h>\n\nint main()\n{\n  int a = 10; \n  int *p = &a; \n  void *p1 = (void*)&a;\n  void *p2 = (void*)&a;\n\n  printf(\"%p %p\\n\",p1,p2);\n  printf(\"%p\\n\",p);\n  \n  if(p == p1) \n    printf(\"Equal\\n\");\n  if(p1 == p2) \n    printf(\"Equal\\n\");  \n}\n\n```\n\nOutput:\n\n\n\n```\n 0x7ffe1fbecfec 0x7ffe1fbecfec\n 0x7ffe1fbecfec\n Equal\n Equal\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c"},"answer":"**TL\/DR**: the book is wrong.\n\n\n\n> \n> What am I missing?\n> \n> \n> \n\n\nNothing, as far as I can see. Even the erratum version presented in comments ...\n\n\n\n> \n> A pointer to void will never be equal to another pointer to void.\n> \n> \n> \n\n\n... simply is not supported by the C language specification. To the extent that the author is relying on the language specification, the relevant text would be paragraph 6.5.9\/6:\n\n\n\n> \n> Two pointers compare equal if and only if both are null pointers, both\n> are pointers to the same object (including a pointer to an object and\n> a subobject at its beginning) or function, both are pointers to one\n> past the last element of the same array object, or one is a pointer to\n> one past the end of one array object and the other is a pointer to the\n> start of a different array object that happens to immediately follow\n> the first array object in the address space.\n> \n> \n> \n\n\n`void` is an object type, albeit an \"incomplete\" one. Pointers to `void` that are valid and non-null are pointers to objects, and they compare equal to each other under the conditions expressed by the specification. The usual way that such pointers are obtained is by converting an object pointer of a different (pointer) type to `void *`. The result of such a conversion still points to the same object that the original pointer did.\n\n\nMy best guess is that the book misinterprets the spec to indicate that pointers to void should not be interpreted as pointers to objects. Although there are special cases that apply only to pointers to `void`, that does not imply that general provisions applying to object pointers do not also apply to void pointers.\n\n\n"}
{"questionId":"18c75dbdac8e4a4d8bf34af6ea762f51","question":"What causes this runtime internal error about \"previous declaration at\" when building with Go 1.14 after upgrading from Go 1.13?\nAfter upgrading my `go` installation folder to `Go 1.14`\n\n\n\n```\nsudo tar -C \/usr\/local -xzf go1.14.linux-amd64.tar.gz\n\n```\n\nI am receiving a runtime error every time I try to build a program:\n\n\n\n```\n~\/playground\/go\/src\/hello \ue0b0 go build hello\n# runtime\/internal\/atomic\n\/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64x.go:18:6: Load redeclared in this block\n    previous declaration at \/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64.go:16:24\n\/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64x.go:24:6: Loadp redeclared in this block\n    previous declaration at \/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64.go:22:32\n\/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64x.go:30:6: Load64 redeclared in this block\n    previous declaration at \/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64.go:28:26\n\/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64x.go:36:6: LoadAcq redeclared in this block\n    previous declaration at \/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64.go:34:27\n\/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64x.go:41:6: Xadd redeclared in this block\n    previous declaration at \/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64.go:39:37\n\/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64x.go:44:6: Xadd64 redeclared in this block\n    previous declaration at \/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64.go:42:39\n\/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64x.go:47:6: Xadduintptr redeclared in this block\n    previous declaration at \/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64.go:45:47\n\/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64x.go:50:6: Xchg redeclared in this block\n    previous declaration at \/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64.go:48:36\n\/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64x.go:53:6: Xchg64 redeclared in this block\n    previous declaration at \/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64.go:51:38\n\/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64x.go:56:6: Xchguintptr redeclared in this block\n    previous declaration at \/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64.go:54:45\n\/usr\/local\/go\/src\/runtime\/internal\/atomic\/atomic_amd64x.go:56:6: too many errors\n\n```\n\nI tried to downgrade to version `1.13.8` and `build` and `run` go fine.\n\n\n\n```\nsudo rm -rf \/usr\/local\/go && sudo tar -C \/usr\/local -xzf go1.13.8.linux-amd64.tar.gz\n\n~ \ue0b0 go version\ngo version go1.13.8 linux\/amd64\n\n~\/go \ue0b0 go build hello && go run hello\nhello, world\n\n```\n\nMy OS is Linux Mint 19.2.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"go"},"answer":"This error is reproducible and occurs when the target directory has been already used for older **Go** installations and some files have been renamed (compared to previous version).\n\n\nFor instance:\n\n\nin **Go 1.13.8** file `go\/src\/runtime\/internal\/atomic\/atomic_amd64x.go` has been moved to `go\/src\/runtime\/internal\/atomic\/atomic_amd64.go` in **Go 1.14**.\n\n\nExtracting without cleaning target directory triggers \"**previous declaration**\" error because `func Xchguintptr` is declared both in `go\/src\/runtime\/internal\/atomic\/atomic_amd64x.go` and `go\/src\/runtime\/internal\/atomic\/atomic_amd64.go`\n\n\nTo avoid this type of error remove the installation folder (**\/usr\/local\/go**) and reinstall **Go 1.14**.\n\n\n\n```\nsudo rm -rf \/usr\/local\/go && sudo tar -C \/usr\/local -xzf go1.14.linux-amd64.tar.gz\n\n```\n\nTest again:\n\n\n\n```\n~\/go\/src\/hello \ue0b0 go version\ngo version go1.14 linux\/amd64\n~\/go\/src\/hello \ue0b0 go build hello && go run hello\nhello, world\n\n```\n\n"}
{"questionId":"8a7611886c064cc1a6887ebd489671cc","question":"How can I render plain android ProgressBar with Compose?\nMy application needs a ProgressBar, and I am trying to implement it with Jetpack Compose, so either I need a builtin ProgressBar support (I didn't find it) or there should be a mechanism to display plain Android Widgets with Compose. Is anything of this possible?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"kotlin"},"answer":"Ofcourse, we have Progress Bars in Jetpack Compose:\n\n\n**CircularProgressIndicator:** Displays progress bar as Circle. It is indeterminate. Themed to Primary color set in styles. Another variant is determinate that takes progress in argument as Float (0.0f - 1.0f)\n\n\n**Example:**\n\n\n\n```\n\/\/ Indeterminate\nCircularProgressIndicator()\n\n\/\/ Determinate\nCircularProgressIndicator(progress = 0.5f)\n\n```\n\nLinearProgressIndicator: Displays progress bar as line. It is indeterminate. Themed to Primary color set in styles. Another variant is determinate that takes progress in argument as Float (0.0f - 1.0f)\n\n\n**Example:**\n\n\n\n```\n\/\/ Indeterminate\nLinearProgressIndicator()\n\n\/\/ Determinate\nLinearProgressIndicator(progress = 0.5f)\n\n```\n\n"}
{"questionId":"957dd5a46d2d497083f7c3f75ca52f8e","question":"Return an empty array instead of null with golang for json return with gin\nSo i have a struct :\n\n\n\n```\ntype ProductConstructed struct {\n    Name string `json:\"Name\"`\n    BrandMedals []string `json:\"BRAND_MEDALS\"`\n}\n\n```\n\nWhen i return my object with gin and : \n\n\n\n```\nfunc  contructproduct(c *gin.Context) {\n    var response ProductConstructed \n    response.Name = \"toto\"\n\n    c.JSON(200, response)\n}\n\nfunc main() {\n    var err error\n    if err != nil {\n        panic(err)\n    }\n    \/\/gin.SetMode(gin.ReleaseMode)\n    r := gin.Default()\n    r.POST(\"\/constructProductGo\/v1\/constructProduct\", contructproduct)\n    r.Run(\":8200\") \/\/ listen and serve on 0.0.0.0:8080\n}\n\n```\n\nIt returns me : \n\n\n\n> \n> null\n> \n> \n> \n\n\ninstead of \n\n\n\n> \n> []\n> \n> \n> \n\n\nHow to return an empty array ?\n\n\nRegards\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"go"},"answer":"So the solution was to initialize it with : \n\n\n\n```\nproductConstructed.BrandMedals = make([]string, 0)\n\n```\n\n"}
{"questionId":"94dd17e32cd142ac8ba62155b34f95c1","question":"flake8: Ignore only F401 rule in entire file\nIs there a way to get `flake8` to ignore only a specific rule for an entire file? Specifically, I'd like to ignore just `F401` for an entire file.\n\n\nI have a file like `__init__.py` where I import symbols that are never used within that file. I'd rather not add `# noqa` to each line. I can add `# flake8: noqa` to the beginning of the file, but that ignores **all** rules. I'd like to ignore **just** the `F401` rule.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"there is not currently a way to do what you're asking with only source inside the file itself\n\n\nthe current suggested way is to use the `per-file-ignores` feature in your `flake8` configuration:\n\n\n\n```\n[flake8]\nper-file-ignores =\n    *\/__init__.py: F401\n\n```\n\nNote that `F401` in particular can be *solved* in a better way, any names that are exposed in `__all__` will be ignored by `pyflakes`:\n\n\n\n```\nfrom foo import bar  # would potentially trigger F401\n__all__ = ('bar',)  # not any more!\n\n```\n\n(disclaimer: I'm the current maintainer of `flake8` and one of the maintainers of `pyflakes`)\n\n\n"}
{"questionId":"8fa8c29305f94a628ad8dffa889b53ac","question":"RNGCryptoServiceProvider is obsolete\nI need to generate a token using random numbers and letters. However, the error message is telling me to use RandomNumberGenerator which will only give me random numbers which will not help.\n\n\nerror messsage\n\n\n\n> \n> 'RNGCryptoServiceProvider' is obsolete: 'RNGCryptoServiceProvider is obsolete. To generate a random number, use one of the RandomNumberGenerator static methods instead.'\n> \n> \n> \n\n\nCode currently used\n\n\n\n```\n    private static string RandomString()\n    {\n        int length = 256;\n        const string valid = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\";\n        StringBuilder res = new StringBuilder();\n        using (RNGCryptoServiceProvider rng = new RNGCryptoServiceProvider())\n        {\n            byte[] uintBuffer = new byte[sizeof(uint)];\n\n            while (length-- > 0)\n            {\n                rng.GetBytes(uintBuffer);\n                uint num = BitConverter.ToUInt32(uintBuffer, 0);\n                res.Append(valid[(int)(num % (uint)valid.Length)]);\n            }\n        }\n        return res.ToString();\n    }\n\n```\n\nHow do I change the code above to use a package that is not out of date?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"You can use `RandomNumberGenerator` like this:\n\n\n\n```\nvar randomNumber = new byte[32];\nstring refreshToken = \"\";\n\nusing (var rng = RandomNumberGenerator.Create())\n{\n     rng.GetBytes(randomNumber);\n     refreshToken = Convert.ToBase64String(randomNumber);\n}\n\n```\n\nExample of result: `fHu4SOyv03PhHvqWx5WGmI5xQFnH00CmLCV3IYJd5Dw=`\n\n\nJust adjust this to your case.\n\n\n"}
{"questionId":"23149316814f45659bb2b90e9a476ebe","question":"JSDoc equivalent to Typescript's `as const`?\nI'm in an old project that is too huge to easily convert to Typescript, so I've been using JSDoc instead. The Typescript feature that I can't figure out how to replicate in JSDoc is using `as const` to fully type the property names and values of a static object.\n\n\n\n```\n\/\/ In Typescript\nconst anObject = {hello: 'world'} as const;\n\/\/ (type shows as {hello:'world'} instead of {hello:string}\n\n```\n\nIs there any equivalent for this in JSDoc? I've been completely unable to find anything that does this (`@readonly` and `@const` don't do it), so instead I have to basically copy-paste any static object as a type to be able to properly type these cases, which certainly isn't DRY.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"Since Typescript 4.5:\n\n\n\n```\nconst foo = \/** @type {const} *\/ ({x: 1});\nconst bar = \/** @type {const} *\/ (\"baz\");\n\n```\n\nNote that the parentheses are required; this is cast syntax, not normal type annotation.\n\n\n"}
{"questionId":"02e238f91a3f401aa1f8068a8d397d9b","question":"Crontab never executes in Windows Subsystem for Linux\nI set up some cronjobs a while back using `crontab -e`. My crontab includes the following line:\n\n\n\n```\n* * * * * \/usr\/bin\/touch \/home\/blah\/MADEBYCRON\n\n```\n\nIt's been weeks since I did this. I have never seen `\/home\/blah\/MADEBYCRON`. I set permissions on my home directory so it should be able to create files in this directory, so why does this file never exist?\n\n\n`\/var\/log\/syslog` does not exist.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"Ensure that the cron service is running. I use WSL with cron every day for my local backups using rsync so this should work.\n\n\nUse `which cron` to check its installed, mine says `\/usr\/sbin\/cron`.\n\n\nUse `crontab -l` to list your configured jobs.\n\n\nUse `ps aux | grep cron` to look see if cron is running, you should see `\/usr\/sbin\/cron` if it is.\n\n\nUse `service cron status` to check if the service is started.\n\n\nUse `sudo service cron start` to start the cron service if it is not running.\n\n\n"}
{"questionId":"a6d6fce89b2746fcaf00a3c3adde2281","question":"Error migrations: Cannot declare class X, because the name is already in use\nI do not know why this error occurs when I execute the migrations as I do not have repeated classes.\n\n\nMigrations:\n\n\n\n```\n2014_10_12_100000_create_password_resets_table.php\n2019_01_18_020910_create_roles_table.php\n2019_01_18_025535_create_members_table.php\n2019_01_18_025536_create_users_table.php\n2019_01_18_183649_create_projects_table.php\n2019_01_18_184249_create_member_project_table.php\n2019_01_18_184719_create_sprints_table.php\n2019_01_18_185218_create_tasks_table.php\n2019_01_21_033045_add_shortname_to_project.php\n\n```\n\nError:\n\n\n\n```\nPHP Fatal error:  Cannot declare class CreateRolesTable, because the name is already in use in\noyectos\\database\\migrations\\2019_01_18_020910_create_roles_table.php on line 33\n\nIn 2019_01_18_020910_create_roles_table.php line 33:\n\n  Cannot declare class CreateRolesTable, because the name is already in use\n\n```\n\nClass:\n\n\n\n```\nclass CreateRolesTable extends Migration\n{\n    \/**\n     * Run the migrations.\n     *\n     * @return void\n     *\/\n    public function up()\n    {\n        Schema::create('roles', function (Blueprint $table) {\n            $table->increments('id');\n            $table->string('name',128)->unique();\n            $table->string('description');\n            $table->boolean('system');\n        });\n    }\n\n    \/**\n     * Reverse the migrations.\n     *\n     * @return void\n     *\/\n    public function down()\n    {\n        Schema::dropIfExists('roles');\n    }\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"**First Solution :** \n\n\nIt seems like you have 2 migrations done at different time with essentially same name.\n\n\nfor example : `2019_01_18_020910_create_roles_table.php`\n\n\nand `2019_01_16_020910_create_roles_table.php`\n\n\nLaravel will convert this filename eliminating the date signature and Camel Casing the remaining text.\n\n\nSo both of these migration will have class `CreateRolesTable` even if the time signatures are different. Check if your migrations directory have such 2 files.\n\n\nTo check this run this from terminal in project root : `grep -ri 'createrolestable' database\/migrations`\n\n\n**Second Solution :**\n\n\nSometimes composer's internal class autoloading causes this issue. Do following to check if it resolves : \n\n\nrun `composer install` \n\n\n**Third Solution** : \n\n\nThis is likely to be invalid but a same file should not have same class declaration 2 files by mistake.\n\n\n**Fourth Solution** : \n\n\nThere might be a package you have installed which has a migration with same class name. To find run `grep -ril 'createrolestable' vendor`\n\n\nIf it shows any file then thats what causing 2 classes to have same names.\n\n\nYou can create a new one `php artisan make:migration create_roles_table_custom` . and then copy what you have in current migration to the new one and delete the existing one(not from package but the one you have created). \n\n\nThis will create a class `CreateRolesTableCustom` which is different than what the package already has.\n\n\n"}
{"questionId":"395e18264ae04464971000347270f213","question":"What is the point of \"grep -q\"\nI was reading the grep man page and came across the -q option, which tells grep to \"not write anything to standard output. Exit immediately with zero status if any match is found, even if an error was detected.\"\n\n\nI don't understand why this could be desirable or useful behavior. In a program who's reason d'etre seems to be read from stdin, process, write to stdout, why would I want to silence it completely? \n\n\nIn what case would silencing a program whose goal it is to output things be useful? Why would someone want to entirely ignore errors and force a successful return code?\n\n\nThanks!\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"bash\/shell"},"answer":"The exit status of `grep` doesn't necessarily indicate an *error*\u2009; it indicates *success* or *failure*. `grep` defines success as matching 1 or more lines. Failure includes matching zero lines, *or* some other error that prevented matching from taking place in the first place.\n\n\n`-q` is used when you don't care about which lines matched, only that *some* lines matched.\n\n\n\n```\nif grep -q foo file.txt; then\n    echo \"file.txt contains foo\"\nelse\n    echo \"file.txt does not contain foo\"\nfi\n\n```\n\n"}
{"questionId":"a0eb459e78a04ff3b9d9d473051e4089","question":"Run two Kotlin coroutines inside coroutine in parallel\nI have two suspend functions:\n\n\n\n```\nsuspend fun sendData() : Boolean \n\nsuspend fun awaitAcknowledge() : Boolean\n\n```\n\nand I want to wrap them in a third suspend function in which they should be executed in parallel and I want to calculate the final result by having both return values:\n\n\n\n```\nsuspend fun sendDataAndAwaitAcknowledge() : Boolean {\n    \/\/ TODO execute both in parallel and compare both results\n}\n\n```\n\nHowever, if I write it like that,\n\n\n\n```\nsuspend fun sendDataAndAwaitAcknowledge() : Boolean {\n    val sendResult = sendData()\n    val receiveAck = awaitAcknowledge()\n}\n\n```\n\nthe functions will be executed in a serial order, which will not work in my case. \n\n\nComing from RxJava, I would like to achieve something like the `zip` operator:\n\n\n\n```\nSingle.zip(awaitAcknowledge(), sendData(), {receiveAck, sendResult -> ...})\n\n```\n\nHow can I do this with `Coroutines`?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"kotlin"},"answer":"You can use `awaitAll` for that purpose:\n\n\n\n```\nimport kotlinx.coroutines.*\n\nsuspend fun sendDataAndAwaitAcknowledge() = coroutineScope {\n    awaitAll(async {\n        awaitAcknowledge()\n    }, async {\n        sendData()\n    })\n}\n\nfun sendData() = true\n\nfun awaitAcknowledge() = false\n\nfun main() {\n    runBlocking {\n        println(sendDataAndAwaitAcknowledge()) \/\/ [false, true]\n    }\n}\n\n```\n\n"}
{"questionId":"a6430c09b6024e2193d28f409ec43000","question":"How to copy a directory with symbolic links and resolve them?\nI would like to recursively copy the *contents* of a directory which contains symbolic links (symlinks) as well as normal files with a Bash \/ Shell Script. I don\u2019t know how to copy the symlink-contents. The pseudocode would look something like this:\n\n\n\n```\nfor file in directory do\n  if is symlink\n    resolve symlink and copy its contents\n  else \n    copy the file \/ folder\n\n```\n\nMy directory-structure looks like this:\n\n\n\n```\nbase\/\n  dir1\/\n  symlinkdir1*\/ (--> ..\/somewhere\/else\/dirA)\n    \/file1\n    \/file2\n  symlinkdir2*\/ (--> ..\/somewhere\/else\/dirB)\n    \/file3\n    \/file4\n  \u2026\n\n```\n\nAfter the copy-procedure, I would like to have a directory-structure like this:\n\n\n\n```\nbase\/\n  dir1\/\n  symlinkdir1\/ (no symlink, actual directory)\n    \/file1\n    \/file2\n  symlinkdir2\/ (no symlink, actual directory)\n    \/file3\n    \/file4\n  \u2026\n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"bash\/shell"},"answer":"`cp -rL \/source \/destination`\n\n\nr = recursive\nL = follow and expand symlinks\n\n\n"}
{"questionId":"d41cc5a9ee654f1ead0ab61864c2e08f","question":"Blazor - show confirmation dialog before delete\/update?\nIn the following Blazor (server-side) code snips. How to prompt the confirmation dialog?\n\n\n\n```\n<tbody>\n    @foreach (var r in lists)\n    {\n        var s = r.ID;\n        <tr>\n            <td>@s<\/td>\n            <td><button class=\"btn btn-primary\" @onclick=\"() => DeleteSymbol(s)\">Delete<\/button><\/td>\n        <\/tr>\n    }\n<\/tbody>\n\n@code {\n    async Task DeleteSymbol(string id)\n    {\n        \/\/ Get confirmation response from user before running deletion?\n        \/\/ Delete!\n    }\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"\n```\n@inject IJSRuntime JsRuntime\n\n<tbody>\n...\n<\/tbody>\n\n@code {\n    async Task DeleteSymbol(string id)\n    {\n        bool confirmed = await JsRuntime.InvokeAsync<bool>(\"confirm\", \"Are you sure?\");\n        if (confirmed)\n        {\n            \/\/ Delete!\n        }\n    }\n}\n\n```\n\n"}
{"questionId":"82e5a3a10ecb422aa6a78faf1ddee320","question":"Signed overflow in C++ and undefined behaviour (UB)\nI'm wondering about the use of code like the following\n\n\n\n```\nint result = 0;\nint factor = 1;\nfor (...) {\n    result = ...\n    factor *= 10;\n}\nreturn result;\n\n```\n\nIf the loop is iterated over `n` times, then `factor` is multiplied by `10` exactly `n` times. However, `factor` is only ever used after having been multiplied by `10` a total of `n-1` times. If we assume that `factor` never overflows except on the last iteration of the loop, but may overflow on the last iteration of the loop, then should such code be acceptable? In this case, the value of `factor` would provably never be used after the overflow has happened.\n\n\nI'm having a debate on whether code like this should be accepted. It would be possible to put the multiplication inside an if-statement and just not do the multiplication on the last iteration of the loop when it can overflow. The downside is that it clutters the code and adds an unnecessary branch that would need to check for on all the previous loop iterations. I could also iterate over the loop one fewer time and replicate the loop body once after the loop, again, this complicates the code.\n\n\nThe actual code in question is used in a tight inner-loop that consumes a large chunk of the total CPU time in a real-time graphics application.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"c++"},"answer":"Compilers do assume that a valid C++ program does not contain UB. Consider for example:\n\n\n\n```\nif (x == nullptr) {\n    *x = 3;\n} else {\n    *x = 5;\n}\n\n```\n\nIf `x == nullptr` then dereferencing it and assigning a value is UB. Hence the only way this could end in a valid program is when `x == nullptr` will never yield true and the compiler can assume under the as if rule, the above is equivalent to:\n\n\n\n```\n*x = 5;\n\n```\n\nNow in your code\n\n\n\n```\nint result = 0;\nint factor = 1;\nfor (...) {      \/\/ Loop until factor overflows but not more\n   result = ...\n   factor *= 10;\n}\nreturn result;\n\n```\n\nThe last multiplication of `factor` cannot happen in a valid program (signed overflow is undefined). Hence also the assignment to `result` cannot happen. As there is no way to branch before the last iteration also the previous iteration cannot happen. Eventually, the part of code that is correct (i.e., no undefined behaviour ever happens) is:\n\n\n\n```\n\/\/ nothing :(\n\n```\n\n"}
{"questionId":"7b7711be83b04de1b7961fc5300fcbbe","question":"How to make request body type compatible with RequestInit or BodyInit when using node-fetch?\nI started to use Typescript for my nodejs project. For accessing some external API, I use node-fetch to make requests. While setting up a PUT request, an error pops up, saying that the given body is not assignable to type `RequestInit`:\n\n\nThe error:\n\n\n\n```\nError:(176, 28) TS2345:Argument of type '{ headers: Headers; method: string; body: MyClass; }' is not assignable to parameter of type 'RequestInit'.\n  Types of property 'body' are incompatible.\n    Type 'MyClass' is not assignable to type 'BodyInit'.\n      Type 'MyClass' is not assignable to type 'ReadableStream'.\n        Property 'readable' is missing in type 'MyClass'.\n\n```\n\nMyClass:\n\n\n\n```\nclass MyClass {\n  configId: string;\n  adapterType: string;\n  address: string;\n\n  constructor(configId: string, adapterType: string, address: string) {\n    this.configId = configId;\n    this.adapterType = adapterType;\n    this.address = address;\n  }\n\n  \/\/ some methods\n}\n\n```\n\nInvocation:\n\n\n\n```\nlet body = new MyClass(\"a\", \"b\", \"c\")\nlet url = config.url + \"\/dialog\/\" + dialogId\nlet headers = new Headers()\n\/\/ Append several headers\n\nlet params = {\n  headers: headers,\n  method: \"PUT\",\n  body: body\n}\n\nreturn new Promise((resolve, reject) => {\n  Fetch(url, params) \/\/ <-- error is shown for params variable\n    .then(res => {\n      \/\/ Do stuff\n      resolve(\/*somevalue*\/)\n    })\n}\n\n```\n\nHow should I make the body object compatible?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"You need to `stringify` your body:\n\n\n\n```\nlet params: RequestInit = {\n  headers: headers,\n  method: \"PUT\",\n  body: JSON.stringify(body)\n}\n\n```\n\n"}
{"questionId":"0d0f6694040546128319511be8fce91d","question":"How to fix \"module 'platform' has no attribute 'linux\\_distribution'\" when installing new packages with Python3.8?\nI had Python versions of 2.7 and 3.5. I wanted the install a newer version of Python which is python 3.8. I am using Ubuntu 16.04 and I can not just uninstall Python 3.5 due to the dependencies. So in order to run my scripts, I use `python3.8 app.py`. No problem so far. But when I want to install new packages via pip:\n\n\n\n```\npython3.8 -m pip install pylint\n\n```\n\nIt throws an error:\n\n\n\n```\nAttributeError: module 'platform' has no attribute 'linux_distribution'\n\n```\n\nSo far, I tried:\n\n\n\n```\nsudo update-alternatives --config python3\n\n```\n\nand chose python3.8 and run command by starting with python3 but no luck.\n\n\nThen:\n\n\n\n```\nsudo ln -sf \/usr\/bin\/python3.5 \/usr\/bin\/python3\n\n```\n\nI also tried running the command by starting with python3 but it did not work either.\n\n\nHow can I fix it so that I can install new packages to my new version of Python?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"It looks like at least on my Ubuntu 16.04, pip is shared for all Python versions in `\/usr\/lib\/python3\/dist-packages\/pip`.\n\n\nThis is what I did to get it working again:\n\n\n- `sudo apt remove python3-pip`\n- `sudo python3.8 -m easy_install pip`\n\n\nYou might want to install the python 3.5 version of pip again with `sudo python3.5 -m easy_install pip`.\n\n\n"}
{"questionId":"e7264724255742fcb5f893fece65c809","question":"Netbeans 9\/10\/11 cannot run program \"cmd\"\nI unzipped NetBeans 11 to `C:\\netbeans`, installed several versions of the JDK to `C:\\java` and created a test class within NetBeans containing a main method and one line of code `System.out.print(\"hello world\");`. When I click the *Run Project* button, the output window shows the command:\n\n\n\n```\ncd C:\\Users\\MyUsername\\Documents\\NetBeansProjects\\asdf; \nJAVA_HOME=C:\\\\Java\\\\jdk1.8.0_231 cmd \/c \"\\\"\\\"C:\\\\netbeans-9\\\\java\\\\maven\\\\bin\\\\mvn.cmd\\\" \n  -Dexec.args=\\\"-classpath %classpath com.mycompany.asdf.Test\\\" \n  -Dexec.executable=C:\\\\Java\\\\jdk1.8.0_231\\\\bin\\\\java.exe  \n  -Dmaven.ext.class.path=C:\\\\netbeans-9\\\\java\\\\maven-nblib\\\\netbeans-eventspy.jar  \n  -Dfile.encoding=UTF-8 process-classes org.codehaus.mojo:exec-maven-plugin:1.5.0:exec\\\"\"\n\n```\n\nfollowed by the error:\n\n\n\n> \n> Cannot run program \"cmd\" (in directory \"C:\\Users\\MyUsername\\Documents\\NetBeansProjects\\asdf\"): Malformed argument has embedded quote: \"C:\\netbeans-9\\java\\maven\\bin\\mvn.cmd\" -Dexec.args=\"-classpath %classpath com.mycompany.asdf.Test\" -Dexec.executable=C:\\Java\\jdk1.8.0\\_231\\bin\\java.exe -Dmaven.ext.class.path=C:\\netbeans-9\\java\\maven-nblib\\netbeans-eventspy.jar -Dfile.encoding=UTF-8 process-classes org.codehaus.mojo:exec-maven-plugin:1.5.0:exec\n> \n> \n> \n\n\nWhether the class creates a GUI or not, it gives the same error. I've tried NetBeans 9, 10, and 11 with JDK 7, 8, and 11. Am I missing something during setup, or do I have something in a wrong directory somewhere? I'm lost.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"No need to downgrade the JDK, just **add** this in your **`netbeans.conf`**\n\n\n1. Find your *`netbeans.conf`* file. e.g.: `netbeansIstallDir\/etc\/netbeans.conf`\n2. Find the key `netbeans_default_options` and at the start of the string add the following: `-J-Djdk.lang.Process.allowAmbiguousCommands=true`\n\n\nIt should look like this:\n\n\n\n```\nnetbeans_default_options=\"-J-Djdk.lang.Process.allowAmbiguousCommands=true...\"\n\n```\n\n\n"}
{"questionId":"0771e387c92c4e93aadee80f271ecad2","question":"Message: Return type of CI\\_Session\\_files\\_driver::open($save\\_path, $name) should either be compatible with\nI got this error after install a new xampp version (php8). and clone my codeigniter project.\n\n\n\n```\nMessage: Return type of CI_Session_files_driver::open($save_path, $name)\nshould either be compatible with SessionHandlerInterface::open(string $path, string $name):\nbool, or the #[\\ReturnTypeWillChange] attribute should be used to temporarily suppress the notice\n\n```\n\nFilename: drivers\/Session\\_files\\_driver.php\n\n\nLine Number: 132\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"For anyone else that comes across this error, I also experienced it after upgrading to PHP 8.1. The only way I could find to \"fix\" it was by adding `#[\\ReturnTypeWillChange]` before the `open`, `read`, `write`, `close`, `destroy` and `gc` functions in `\/system\/libraries\/Session\/drivers\/Session_files_driver.php`. For example:\n\n\n\n```\n#[\\ReturnTypeWillChange]\npublic function open($save_path, $name)\n{\n...\n\n```\n\n"}
{"questionId":"5fb50f38fc2c4b45af6d25bb86125006","question":"Renaming multiple columns with dplyr rename(across(\nHey i'm trying to rename some columsn by adding \"Last\\_\" with the new version of dplyr but I keep getting this error\n\n\n\n```\nError: `across()` must only be used inside dplyr verbs.\n\n```\n\nthis is my code\n\n\n\n```\ndata %>% rename(across(everything(), ~paste0(\"Last_\", .)))\n\n```\n\ndplyr version: v1.0.2\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"r"},"answer":"We can use `rename_with` instead of `rename`\n\n\n\n```\nlibrary(dplyr)   \nlibrary(stringr)\ndata %>%\n      rename_with(~str_c(\"Last_\", .), everything())\n\n```\n\nReproducible example\n\n\n\n```\ndata(iris)\nhead(iris) %>% \n    rename_with(~str_c(\"Last_\", .), .cols = everything())\n#  Last_Sepal.Length Last_Sepal.Width Last_Petal.Length Last_Petal.Width Last_Species\n#1               5.1              3.5               1.4              0.2       setosa\n#2               4.9              3.0               1.4              0.2       setosa\n#3               4.7              3.2               1.3              0.2       setosa\n#4               4.6              3.1               1.5              0.2       setosa\n#5               5.0              3.6               1.4              0.2       setosa\n#6               5.4              3.9               1.7              0.4       setosa\n\n```\n\n\n\n---\n\n\nAccording to `?rename`\n\n\n\n> \n> rename() changes the names of individual variables using new\\_name = old\\_name syntax; rename\\_with() renames columns using a function.\n> \n> \n> \n\n\nand in `?across`\n\n\n\n> \n> across() makes it easy to apply the same transformation to multiple\n> columns, allowing you to use select() semantics inside in summarise()\n> and mutate().\n> \n> \n> \n\n\nThe description says its use within `mutate\/summarise` (and `transmute`?), and no indication of usage with any other functions i.e. it would fail with `select`\n\n\n"}
{"questionId":"c247dd26422945b8ba993b357d568baf","question":"Date - Month in bash without leading 0 or space?\nAnyone know how to display the date like this?\n\n\n\n```\n7-1-2019\n\n```\n\nI currently have this which adds leading 0 to the month\n\n\n\n```\n$(LC_ALL=nn_NO.UTF-8 date +'%-d-%m-%Y')\n\n```\n\nlike this\n\n\n\n```\n7-01-2019\n\n```\n\nI use these in lynx dump commands\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"bash\/shell"},"answer":"You have already removed the padding for the day, why not do the same for the month?\n\n\n\n```\n$ date +'%-d-%-m-%Y'\n7-1-2019\n\n```\n\nHere's a list of all padding modifiers from `man date`:\n\n\n\n> \n> By default, date pads numeric fields with zeroes. The following\n>  optional flags may follow '%':\n> \n> \n> `-` (hyphen) do not pad the field\n> \n> \n> `_` (underscore) pad with spaces\n> \n> \n> `0` (zero) pad with zeros\n> \n> \n> `^` use upper case if possible\n> \n> \n> `#` use opposite case if possible\n> \n> \n> After any flags comes an optional field width, as a decimal number;\n>  then an optional modifier, which is either E to use the locale's\n>  alternate representations if available, or O to use the locale's\n>  alternate numeric symbols if available.\n> \n> \n> \n\n\n"}
{"questionId":"d609b008146b40e4a234318abc6a5878","question":"ERROR: Cannot add task 'clean' as a task with that name already exists\nI am facing a problem while building from Gradle. This is the error which I am facing:\n\n\n\n> \n> ERROR: Cannot add task 'clean' as a task with that name already exists.\n> \n> \n> \n\n\nHere is my Gradle \n\n\n\n```\napply plugin: 'com.android.application'android {\ncompileSdkVersion 28\nbuildToolsVersion \"25.0.1\"\ndefaultConfig {\n    applicationId \"net.accedegh.retrofitlibrary\"\n    minSdkVersion 17\n    targetSdkVersion 28\n    versionCode 1\n    versionName \"1.0\"\n    testInstrumentationRunner \"android.support.test.runner.AndroidJUnitRunner\"\n}\nbuildTypes {\n    release {\n        minifyEnabled false\n        proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'\n    }\n}dependencies {\nimplementation fileTree(dir: 'libs', include: ['*.jar'])\nandroidTestCompile('com.android.support.test.espresso:espresso-core:2.2.2', {\n    exclude group: 'com.android.support', module: 'support-annotations'\n})\n\/\/noinspection GradleCompatible\nimplementation 'com.android.support:appcompat-v7:28.0.0'\nimplementation \"com.android.support:recyclerview-v7:28.0.0\"\nimplementation 'com.android.support:cardview-v7:28.0.0'\nimplementation 'com.squareup.retrofit2:retrofit:2.1.0'\nimplementation 'com.squareup.retrofit2:converter-gson:2.1.0'\nimplementation 'com.squareup.picasso:picasso:2.4.0'\nimplementation 'com.github.jd-alexander:LikeButton:0.2.0'\nimplementation'com.github.bumptech.glide:glide:3.7.0'compile 'com.github.varunest:sparkbutton:1.0.3'\nimplementation 'com.android.support:design:28.0.0'\nandroidTestCompile 'junit:junit:4.12'}\n\n```\n\nTop-level `build.gradle` file\n\n\n\n```\n\/\/ Top-level build file where you can add configuration options common to all sub-projects\/modules. \nbuildscript { \n    repositories { \n        jcenter() \n        google() \n    } \n\n   dependencies { \n        classpath 'com.android.tools.build:gradle:3.3.2' \n       \/\/ NOTE: Do not place your application dependencies here; they belong \n       \/\/ in the individual module build.gradle files \n   } \n} \n\nallprojects { \n    repositories { \n        maven { \n            url 'maven.google.com' \n            name 'Google' \n        } \n    } \n\n    task clean(type: Delete) { \n        delete rootProject.buildDir \n    } \n}\n\n```\n\nPlease give me the solution if anyone knows.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"You have this task defined in your project-level `build.gradle` (as posted in your comments):\n\n\n\n```\ntask clean(type: Delete) { \n    delete rootProject.buildDir \n}\n\n```\n\nAndroid Studio's Gradle wrapper already has a `clean` task defined, so there's no need to re-define it. Just delete that task from your project-level `build.gradle` file.\n\n\n"}
{"questionId":"b3cc8dcaa31844dbadd9c7f5deb7ea3e","question":"How to set a nullable database field to NULL with typeorm?\nThis seems like such a simple question to answer, but finding an answer for this seems impossible.\n\n\nI am building a password reset feature for a backend application with Express and Typescript. I am using Postgres for the database and Typeorm for data manipulation. I have a *User* entity with these two columns in my database:\n\n\n\n```\n@Column({\n    unique: true,\n    nullable: true,\n})\nresetPasswordToken!: string;\n\n@Column({ nullable: true, type: 'timestamp with time zone' })\nresetPasswordExpiresAt!: Date;\n\n```\n\nWhen a user requests a password reset token the *resetPasswordToken* and *resetPasswordExpiresAt* fields get both filled with the desired values. With the token that was sent to the user's e-mail address, the user can reset his\/her password. After the user's password is reset, I want to clear these two fields by setting them to *null*:\n\n\n\n```\nuser.resetPasswordToken = null;\nuser.resetPasswordExpiresAt = null;\nuser.save()\n\n```\n\nBut if I do this Typescript complains about the two lines where I assign the *null* value:\n\n\n\n> \n> Type 'null' is not assignable to type 'string'.\n> \n> \n> \n\n\nand\n\n\n\n> \n> Type 'null' is not assignable to type 'Date'.\n> \n> \n> \n\n\nIf I change the columns in my entity to accept *null* like below, the errors disappear:\n\n\n\n```\nresetPasswordToken!: string | null;\n...\nresetPasswordExpiresAt!: Date | null;\n\n```\n\nBut when I start my Express application I get the following error when Typeorm tries to connect to my database:\n\n\n\n> \n> Data type \"Object\" in \"User.resetPasswordToken\" is not supported by \"postgres\" database.\n> \n> \n> \n\n\n\u00a0\n\n\nHow do I set these fields to *null*?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"After a good night rest I managed to solve my problem.\n\n\nTypeorm sets the type of the database fields based on the typing you give the variables for your entities in typescript. Typeorm casts the code below to a *varchar* field in my postgres database because I gave it a *string* as a type in typescript.\n\n\n\n```\n@Column({\n    unique: true,\n    nullable: true,\n})\nresetPasswordToken!: string;\n\n```\n\nThis is also where lies my problem. Typeorm takes the typing of a field and tries to create that database field based on the typing it reads. While the code below is correct, typescript basically encapsulates both types in a single *object* and that object is what is being read by Typeorm causing the error that I got.\n\n\n\n```\nresetPasswordToken!: string | null;\n\n```\n\nTo fix my problem I had to specifiy the database field type explicitly like this:\n\n\n\n```\n@Column({\n    type: 'text',\n    unique: true,\n    nullable: true,\n})\nresetPasswordToken!: string;\n\n```\n\n"}
{"questionId":"a2d37e086623494cb38520fda42d2cc8","question":"What is equivalent to `MapSpaFallbackRoute` for ASP.NET Core 3.0 endpoints?\nIn ASP.NET Core 2.x I used standard routes registation `Configure` method of `Startup` class to **register fallback route** for SPA application using `MapSpaFallbackRoute` extension method from `Microsoft.AspNetCore.SpaServices.Extensions` Nuget package:\n\n\n\n```\npublic void Configure(IApplicationBuilder app)\n{\n    \/\/ ...\n    app.UseMvc(routes =>\n    {\n        routes.MapRoute(\n            name: \"default\",\n            template: \"{controller=Home}\/{action=Index}\/{id?}\");\n        routes.MapSpaFallbackRoute(\n            name: \"spa-fallback\",\n            defaults: new { controller = \"Home\", action = \"Index\" });\n    });\n}\n\n```\n\nI cannot find similar extension method when using ASP.NET Core 3.0 recommended `UseEndpoints` extension method for endpoints registration.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"In ASP.NET Core 3.0 extension method `MapFallbackToController` has same functionality to `MapSpaFallbackRoute` extension method.\n\n\n\n```\npublic void Configure(IApplicationBuilder app)\n{\n    app.UseRouting();\n    app.UseEndpoints(endpoints =>\n    {\n        endpoints.MapControllerRoute(\n            name: \"default\",\n            pattern: \"{controller=Home}\/{action=Index}\/{id?}\");\n\n        endpoints.MapFallbackToController(\"Index\", \"Home\");\n    });\n}\n\n```\n\n"}
{"questionId":"5b13defb2300429f85c1d229c5673675","question":"TSLint marks body-parser as deprecated\nI have the Typescript code below:\n\n\n\n```\nimport * as express from 'express';\nimport * as bodyParser from 'body-parser';\n\n...\nconst app: express.Application = express();\n\napp.use(bodyParser.json());\n\n\n```\n\nIn VSCode the bodyParser on the last line is marked with yellow squiggles saying that body-parser is deprecated.\n\n\nIn the .d.ts file I see the following:\n\n\n\n```\n\/** @deprecated *\/\ndeclare function bodyParser(\n    options?: bodyParser.OptionsJson & bodyParser.OptionsText & bodyParser.OptionsUrlencoded,\n): NextHandleFunction;\n\ndeclare namespace bodyParser {\n...\n    function json(options?: OptionsJson): NextHandleFunction;\n\n```\n\nWhy is the linter complaining about the body-parser function while I do not use it as a function in my code? Am I missing something in a tsconfig.json file to prevent this? Compiling doesn't seem to be a problem.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"BodyParse is built into Express js\n\n\nSo now you don't have to install body-parser, do this instead.\n\n\n\n```\napp.use(express.json());\n\n```\n\n"}
{"questionId":"7874722300b7447ca519c0da8f1a2507","question":"Why are my two tuples containing strings, created the same way, not equal?\nI'm compiling the following program using Microsoft Visual C++, as a C++20 program:\n\n\n\n```\n#include <iostream>\n#include <tuple>\n\nint main()\n{\n    auto t1 = std::make_tuple(\"one\", \"two\", \"three\");\n    auto t2 = std::make_tuple(\"one\", \"two\", \"three\");\n    \n    std::cout << \"(t1 == t2) is \" << std::boolalpha << (t1 == t2) << \"\\n\";\n    std::cout << \"(t1 != t2) is \" << std::boolalpha << (t1 != t2) << \"\\n\";\n\n    return 0;\n}\n\n```\n\nWhen I run it, I see the following output:\n\n\n\n```\n(t1 == t2) is false\n(t1 != t2) is true\n\n```\n\nThe tuples are identical, so why does it have wrong comparison results? How do I fix this?\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"c++"},"answer":"You are comparing pointers to buffers of characters, not strings.\n\n\nSometimes the compiler will turn two different `\"one\"`s into the same buffer, sometimes it will not.\n\n\nIn your case, it isn't. Probably a debug build.\n\n\nAdd `#include <string_view>`, then\n\n\n\n```\nusing namespace std::literals;\n\nauto t1 = std::make_tuple(\"one\"sv, \"two\"sv, \"three\"sv);\nauto t2 = std::make_tuple(\"one\"sv, \"two\"sv, \"three\"sv);\n\n```\n\nand you'll get what you expect. (In pre-[c++17](\/questions\/tagged\/c%2b%2b17 \"show questions tagged 'c++17'\") compilers, use `<string>` and `\"\"s` instead of `<string_view>` and `\"\"sv`).\n\n\n"}
{"questionId":"32d67057a4a64b94ad56ea4de0a29328","question":"E: Package 'python-certbot-nginx' has no installation candidate\nWhen I try to install Certbot for Nginx and run\n\n\n\n```\nsudo apt-get install python-certbot-nginx\n\n```\n\nI get\n\n\n\n```\nE: Package 'python-certbot-nginx' has no installation candidate\n\n```\n\nHow to install Certbot for Nginx?\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"bash\/shell"},"answer":"Since Python2 is no longer supported you just need to ask for Python3.\n\n\nSo\n\n\n\n```\nsudo apt-get install python3-certbot-nginx\n\n```\n\nshould solve your Problem.\n\n\n"}
{"questionId":"35d38d02b93e4b869202aae93a335936","question":"make: \/usr\/bin\/mkdir: Command not found during `gem install nokogiri` in Ubuntu 20.04\nI already installed all neccessary libraries it couldn't found (libxslt-dev, libxml2-dev), specified `-- --with-xml2-include=\/usr\/include\/libxml2 --use-system-libraries`, but it still fails with\n\n\n\n```\nmake \"DESTDIR=\" install\nmake: \/usr\/bin\/mkdir: Command not found\nmake: *** [Makefile:202: .sitearchdir.-.nokogiri.time] Error 127\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"ruby"},"answer":"`sudo ln -s \/bin\/mkdir \/usr\/bin\/mkdir`\n\n\n"}
{"questionId":"de5a159770784fb89e737adbba3f4da0","question":"How to increment and update column in one eloquent query\nIs it possible to update a timestamp (besides `updated_at`) and increment a column in one query? I obviously can\n\n\n\n```\n->increment('count')\n\n```\n\nand separately\n\n\n\n```\n->update(['last_count_increased_at' => Carbon::now()])\n\n```\n\nbut is there an easy way to do both together.\n\n\n\n```\nProduct::where('product_id', $product->id)\n    ->update(['count'=> $count + 1, 'last_count_increased_at' => Carbon::now()];\n\n```\n\nWithout having to query and get the count first?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"php"},"answer":"You can specify additional columns to update during the increment or decrement operation:\n\n\n\n```\nProduct::where('id',$id)\n->increment('count', 1, ['increased_at' => Carbon::now()]);\n\n```\n\nIt is more eloquent solution.\n\n\n"}
{"questionId":"f146d7d4a53c4bb8bac498b88d68eb5b","question":"What is the meaning of set -o pipefail in Bash Script?\nWhat is the meaning of `set -o pipefail` in the beginning of the shell script ?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"bash\/shell"},"answer":"`man bash` says\n\n\n\n> \n> pipefail\n> \n> \n> If set, the return value of a pipeline is the value of the last (rightmost) command to exit with a non-zero status, or zero if all commands in the pipeline exit successfully. This option is disabled by default.\n> \n> \n> \n\n\nWhere \"pipeline\" is\n\n\n\n```\ncommand1 | command2 | command3\n\n```\n\nWithout `pipefail`, the return value of a pipeline is the exit status of the last command in the pipeline, regardless of whether previous commands failed.\n\n\nExample:\n\n\n\n```\n$ grep ^root \/etc\/passwd | cut -f 5 -d :\nSystem Administrator\n$ echo $?\n0\n$ grep ^nonexistant_user \/etc\/passwd | cut -f 5 -d :\n$ echo $?\n0\n$ set -o pipefail\n$ grep ^nonexistant_user \/etc\/passwd | cut -f 5 -d :\n$ echo $?\n1\n\n```\n\n"}
{"questionId":"dc457ade21d544eb8ffd0d7f413f6fa4","question":"Why does std::fs::write(...) use an inner function?\nI'm new to Rust and have been looking through the source code a bit, and found this:\n\n\n\n```\n#[stable(feature = \"fs_read_write_bytes\", since = \"1.26.0\")]\npub fn write<P: AsRef<Path>, C: AsRef<[u8]>>(path: P, contents: C) -> io::Result<()> {\n    fn inner(path: &Path, contents: &[u8]) -> io::Result<()> {\n        File::create(path)?.write_all(contents)\n    }\n    inner(path.as_ref(), contents.as_ref())\n}\n\n```\n\nIs there any reason this function defines an inner function like this? Why not just write:\n\n\n\n```\nFile::create(path.as_ref())?.write_all(contents.as_ref())\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"rust"},"answer":"Monomorphization costs.\n\n\n`write()`, like most filesystem functions in Rust, takes `AsRef<Path>` instead of `Path`, for convenience (to allow you to pass e.g. a `&str`). But that also has a cost: it means that the function will be monomorphized and optimized separately for each type, while there is no real need for that. While it is very likely that LLVM will deduplicate all those instances, the time used for optimizing them is still wasted compile time.\n\n\nTo mitigate this cost, it calls an inner, non-generic function that does all the heavy lifting. The outer function contains only the necessarily-generic code - the conversion to `Path`.\n\n\n"}
{"questionId":"d1c392f0ca5345cea8387fa82d1d7aa2","question":"Python 'No module named' error; 'package' is not a package\nI'm trying to make a simple import and use the emailage third party library.\n\n\nAs per their documentation, the way to use their library is as follows: \n\n\n\n```\npip install emailage-official\n\n```\n\nThen, simply import with: \n\n\n\n```\nfrom emailage.client import EmailageClient\n\n```\n\nThe install works fine with pip - no errors. I double checked to see that the emailage package exists within the proper directory, and it does. \n\n\nPackage exists at:\n\n\n\n```\nC:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python37-32\\Lib\\site-packages\\emailage\n\n```\n\nThis folder has (seemingly) the correct files with an `__init__.py` and everything. However, both pylint and command line interpreter throw me a \n'No module named 'emailage.client'; 'emailage' is not a package' error. \n\n\nThe output of my `sys.path` is:\n\n\n\n```\n[... \n'C:\\\\Users\\\\aaron\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python37-32\\\\lib\\\\site-packages'\n...\n]\n\n```\n\nSo the directory where emailage is installed is a part of the path... and lastly I pip-installed numpy just to test if it worked properly. Numpy installed to the same site-packages folder as emailage, and it works fine when it is imported, so I'm stuck.\n\n\nI don't typically use Python much, so any and all help would be appreciated.\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"python"},"answer":"The issue was in the **naming of my file**. \n\n\nI hastily named my file `emailage.py` and then tried to import from `emailage.client`. \n\n\nI'm assuming that Python looked in my current directory and matched the names of the file I was working on before checking the installed third party libraries. \n\n\nAfter renaming my file everything seems ok.\n\n\nFor others who run into similar problems -- **beware of conflicting naming**. Sometimes the simplest things trip you up the longest.\n\n\n"}
{"questionId":"c9bee6652e684a1c97381d7f7091c0c9","question":"How to disable @typescript-eslint\/no-non-null-assertion rule\nI want to allow `data!.id` \n\n\nError:\n\n\n\n> \n> warning Forbidden non-null assertion \n>  @typescript-eslint\/no-non-null-assertion\n> \n> \n> \n\n\nCurrent config:\n\n\n\n```\nmodule.exports = {\n  parser: '@typescript-eslint\/parser',\n  extends: [\n    'eslint:recommended',\n    'plugin:jsx-a11y\/recommended',\n    'plugin:@typescript-eslint\/eslint-recommended',\n    'plugin:@typescript-eslint\/recommended',\n    'plugin:react\/recommended',\n    'prettier\/@typescript-eslint',\n    'plugin:prettier\/recommended'\n  ],\n  plugins: ['react-hooks'],\n  parserOptions: {\n    ecmaVersion: 2020,\n    sourceType: 'module',\n    ecmaFeatures: {\n      jsx: true\n    }\n  },\n  rules: {\n    '@typescript-eslint\/ban-ts-ignore': 0,\n    '@typescript-eslint\/no-explicit-any': 0,\n    '@typescript-eslint\/consistent-type-assertions': ['warn', { assertionStyle: 'as' }],\n    eqeqeq: 1,\n    'react\/prop-types': 0,\n    '@typescript-eslint\/camelcase': 0,\n    'react-hooks\/rules-of-hooks': 'error',\n    'react-hooks\/exhaustive-deps': 'warn'\n  },\n  globals: {\n    React: 'writable'\n  }\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"Please add `'@typescript-eslint\/no-non-null-assertion': 'off'` to your config file like below.\n\n\n\n```\nmodule.exports = {\n  ...\n  rules: {\n    ...\n    '@typescript-eslint\/no-non-null-assertion': 'off'\n  },\n  ...\n}\n\n```\n\n"}
{"questionId":"8070c702daf04432a3cdb66e42c32fc7","question":"Flatten array of arrays in TypeScript\nI want to flatten `string[][]` into `string[]`.\n\n\nThe advice given in dozens of SO answers is: `[].concat(...arrays)`.\n\n\nBut that gives me this error:\n\n\n\n> \n> Argument of type 'string[]' is not assignable to parameter of type 'ConcatArray'.  \n> \n>  Types of property 'slice' are incompatible.  \n> \n>  Type '(start?: number | undefined, end?: number | undefined) => string[]' is not assignable to type '(start?: number | undefined, end?: number | undefined) => never[]'.  \n> \n>  Type 'string[]' is not assignable to type 'never[]'.  \n> \n>  Type 'string' is not assignable to type 'never'.\n> \n> \n> \n\n\nAnother way I tried is this:\n\n\n\n```\nlet foo: string[][] = [[\"a\", \"b\", \"c\"], [\"a\", \"b\", \"c\"], [\"a\", \"b\", \"c\"]];\nlet bar = [].concat(...foo);\n\n```\n\nWhich gives a similar error:\n\n\n\n> \n> Argument of type 'string[]' is not assignable to parameter of type 'ConcatArray'.\n> \n> \n> \n\n\nWhy does it work for everyone but me?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"Try this:\n\n\n\n\n\n```\nconst a = [[\"a\", \"b\", \"c\"], [\"a\", \"b\", \"c\"], [\"a\", \"b\", \"c\"]]\r\nconst result = a.reduce((accumulator, value) => accumulator.concat(value), []);\r\nconsole.log(result)\n```\n\n\n\n\n\n\n"}
{"questionId":"44cb1f852bfd4ec091979d055b4c53bb","question":"Using rememberCoroutineScope() vs LaunchedEffect\n### Context\n\n\nIn Jetpack compose, we have the option of using `rememberCoroutineScope()` as well as using the `LaunchedEffect` composable in order to use coroutines \/ run suspend functions (show snackbars etc).\n\n\nThe convention I've adopted so far is to remember a single coroutine scope at the top of my compose tree, and pass it down via function arguments to places where it is needed. This vaguely seems like a good practice, but on the other hand it's adding extra noise to my function signatures.\n\n\n### Questions\n\n\n1. Are there any reasons for preferring the use of `LaunchedEffect` over `rememberCoroutineScope()` inside composable functions?\n2. Is it worth the effort to only create \/ remember a coroutine scope once per compose tree, or should I just call `rememberCoroutineScope()` in each function where a coroutine is actually launched?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"kotlin"},"answer":"Leaving my understanding here:\n\n\n**Question 1:**\n`LaunchedEffect` should be used when you want that some action must be taken when your composable is first launched\/relaunched (or when the key parameter has changed). For example, when you want to request some data from your ViewModel or run some sort of animation...  \n\n`rememberCoroutineScope` on the other hand, is specific to store the Coroutine scope allowing the code to launch some `suspend` function...\nimho, the only relation between them is that you can also use a `LaunchedEffect` to launch a coroutine...\n\n\n**Question 2:** As you can see in the docs, `rememberCoroutineScope` will keep the reference of the coroutine's scope in a specific point of the composition. Therefore, if a given composable is removed from the recomposition, that coroutine will be cancelled automatically. For instance, you have the following composable calls `A -> B -> C`. If you remember the coroutine scope in `C` and it is removed from the composition, the coroutine is automatically cancelled. But if you remember from `A`, pass the scope through `B` and `C`, use this scope in `C`, and then `C` is removed, the coroutine will continue running (because it was remembered in `A`)...\n\n\n"}
{"questionId":"fd51d3860b3e483e8c378e6bc16935d1","question":"laravel\/framework[v10.0.0, ..., v10.0.3] require composer-runtime-api ^2.2 -> found composer-runtime-api[2.1.0] but it does not match the constraint\nWhen trying to install laravel 10, I am getting this error-\n\n\n\n> \n> laravel\/framework[v10.0.0, ..., v10.0.3] require composer-runtime-api\n> ^2.2 -> found composer-runtime-api[2.1.0] but it does not match the\n> constraint.\n> \n> \n> \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"To solve the problem, I had to update the composer by running the commands-\n\n\n`composer clearcache`\n\n\n`composer selfupdate`\n\n\n"}
{"questionId":"70db561ca6ee40c696251254b3cc9be6","question":"This expression is not callable. Type 'Number' has no call signatures\nHi I'm new in Typescript, I have an object-type variable in which might be different values from different types or nested object. now my question is that how can I define a model for this object to don't face the example error when call different keys?\n\n\nFor example: \n\n\n\n```\nexport class Controller {\nprotected static response(res: Response, statusCode: number = 200, data: any, user: string = '', dev: string = '', code: number = 200, result: string = 'success'){\n    res.status(statusCode).send({\n        data: data,\n        message: {\n            user: '',\n            dev: ''\n        },\n        code: 403,\n        result: 'Error'\n    })\n}\n\n\n ERROR: res.status ---> This expression is not callable. Type 'Number' has no call signatures\n\n```\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"typescript"},"answer":"I was getting this error as well, and realized I'd just forgotten to import Response. Adding an import line solved the problem for me. \n\n\n\n```\nimport express, {Request, Response} from 'express';\n\n```\n\n"}
{"questionId":"270e0b92975140c9b30a1888b6c81872","question":"Kotlin convert hex string to ByteArray\nI have this string:\n\n\n\n```\nval s = \"00b44a0bc6303782b729a7f9b44a3611b247ddf1e544f8b1420e2aae976003219175461d2bd7\" +\n        \"6e64ba657d7c9dff6ed7b17980778ec0cbf75fc16e52463e2d784f5f20c1691f17cdc597d7a514108\" +\n        \"0809a38c635b2a62082e310aa963ca15953894221ad54c6b30aea10f4dd88a66c55ab9c413eae49c0b\" +\n        \"28e6a3981e0021a7dcb0759af34b095ce3efce78938f2a2bed70939ba47591b88f908db1eadf237a7a\" +\n        \"7100ac87130b6119d7ae41b35fd27ff6021ac928273c20f0b3a01df1e6a070b8e2e93b5220ad0210400\" +\n        \"0c0c1e82e17fd00f6ac16ef37c3b6153d348e470843a84f25473a51f040a42671cd94ffc989eb27fd42\" +\n        \"b817f8173bfa95bdfa17a2ae22fd5c89dab2822bcc973b5b90f8fadc9b074cca8f9365b1e8994ff0bda48\" +            \"b1f7498cce02d4e794915f8a4208de3eaf9fbff5\"\n\n```\n\nWhich is hexadecimal notation of bytes, hardcoded in as string format.\nI need this thing as a bytearray, importantly, not the ASCII representation of it, actually the hexadecimal values that is represents.\n\n\nAll the kotlin methods I can find, such as:\n\n\n\n```\nval b = s.encodeToByteArray()\n\n```\n\nSeem to be taking the actual ASCII value of the string, and converting it to a bytearray.\n\n\nHow do I create a bytearray directly from the values in this string?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"kotlin"},"answer":"You can handle it like this:\n\n\n\n```\nfun String.decodeHex(): ByteArray {\n    check(length % 2 == 0) { \"Must have an even length\" }\n\n    return chunked(2)\n        .map { it.toInt(16).toByte() }\n        .toByteArray()\n}\n\n```\n\n1. Split the string into 2-character pairs, representing each byte.\n2. Parse each hex pair to their integer values.\n3. Convert the parsed `Int`s to `Bytes`.\n\n\n"}
{"questionId":"308638fcc08f4920873246f34fb4a49f","question":"why am I getting Exec format error when I am writing my linux service?\nI am writing a linux service to deploy my springboot web app as a service.\nHere is the service file `springboot.service`\n\n\n\n```\n[Unit]\nDescription=My Webapp Java REST Service\n\n[Service]\nUser=ubuntu\n# The configuration file application.properties should be here:\n\n#change this to your workspace\nWorkingDirectory=\/home\/ubuntu\n\n#path to executable. \n#executable is a bash script which calls jar file\nExecStart=\/home\/ubuntu\/spring-start\n\nSuccessExitStatus=143\nTimeoutStopSec=10\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\n\n```\n\nMy script file `spring-start.sh`\n\n\n\n```\nsudo java -jar \"\/home\/ubuntu\/FirstWebAppWithoutDB.jar\"\n\n```\n\nI also gave execution permission to the spring-start.sh by `chmod u+x spring-start.sh`\n\n\n\n```\nsudo systemctl daemon-reload\n\nsudo systemctl enable springboot.service\n\nsudo systemctl start springboot\n\nsudo systemctl status springboot\n\n```\n\nUnfortunately the service fails with error Exec format error:\n\n\n\n```\nspringboot.service: Failed to execute command: Exec format error\nJul 14 07:39:56 ip-172-31-40-71 systemd[10075]: springboot.service: Failed at step EXEC spawning \/home\/ubuntu\/spring-start.sh: Exec format error\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"bash\/shell"},"answer":"add shebang to the script\n\n\n\n```\n#!\/bin\/bash\nsudo java -jar \"\/home\/ubuntu\/FirstWebAppWithoutDB.jar\"\n\n```\n\nand execution permission\n\n\n\n```\nchmod +x spring-start.sh\n\n```\n\n"}
{"questionId":"1699f34f374b49cabc37de765fab1baa","question":"C# Blazor WebAssembly: Argument 2: cannot convert from 'void' to 'Microsoft.AspNetCore.Components.EventCallback'\nI'm new to blazor C# and trying to make a simple countdown timer website. My website consist of:\n\n\n- Text to display the timer\n- Start and stop button\n- Buttons to set the timer\n\n\nI'm having a problem in the buttons to set the timer. When i click on it, it won't set the timer display and i got an error `Argument 2: cannot convert from 'void' to 'Microsoft.AspNetCore.Components.EventCallback'`. I search on Youtube for the EventCallback topic but the problem is, my component is not seperated while in the video the example code got seperated components linked together. Here's the code.  \n\n**Index.razor**\n\n\n\n```\n@page \"\/\"\n@inherits FrontEnd.Components.Timer\n\n<h1>Timer<\/h1>\n\n<p class=\"timer\">@Hours.ToString(\"00\"):@Minutes.ToString(\"00\"):@Seconds.ToString(\"00\")<\/p>\n@foreach (var timer in timerCollections)\n{\n    \/\/ Here's the problem\n    <button @onclick=\"SetTimer(timer.Id)\">@timer.Hours.ToString(\"00\"):@timer.Minutes.ToString(\"00\"):@timer.Seconds.ToString(\"00\")<\/button>\n}\n<br\/>\n<button @onclick=\"StartTimer\" disabled=@StartButtonIsDisabled>Start<\/button>\n<button @onclick=\"StopTimer\" disabled=@StopButtonIsDisabled>Stop<\/button>\n\n```\n\n**Timer.cs**\n\n\n\n```\nusing System;\nusing System.Timers;\nusing System.Collections.Generic;\nusing Microsoft.AspNetCore.Components;\n\nnamespace FrontEnd.Components\n{\n    public class TimerStructure\n    {\n        public int Id { get; set; }\n        public int Hours { get; set; }\n        public int Minutes { get; set; }\n        public int Seconds { get; set; }      \n    }\n    public class Timer : ComponentBase\n    {\n        public List<TimerStructure> timerCollections = new List<TimerStructure>(){\n            new TimerStructure(){ Id = 1, Hours = 0, Minutes = 30, Seconds = 0 },\n            new TimerStructure(){ Id = 2, Hours = 1, Minutes = 0, Seconds = 0 }\n        };\n\n        public int Index { get; private set; }\n        public int Hours { get; set; } = 0;\n        public int Minutes { get; set; } = 0;\n        public int Seconds { get; set; } = 0;\n        public bool StopButtonIsDisabled { get; set; } = true;\n        public bool StartButtonIsDisabled { get; set; } = false;\n        private static System.Timers.Timer aTimer;\n        \/\/ and this is the function related to the problem\n        public void SetTimer(int value)\n        {\n            this.Index = value - 1;\n            Hours = timerCollections[Index].Hours;\n            Minutes = timerCollections[Index].Minutes;\n            Seconds = timerCollections[Index].Seconds;\n        }\n        public void StopTimer()\n        {\n            aTimer.Stop();\n            aTimer.Dispose();\n\n            StopButtonIsDisabled = true;\n            StartButtonIsDisabled = false;\n\n            Console.WriteLine($\"{Hours}:{Minutes}:{Seconds}\");\n        }\n        public void StartTimer() \n        {\n            aTimer = new System.Timers.Timer(1000);\n            aTimer.Elapsed += CountDownTimer;\n            aTimer.Start(); \n\n            StopButtonIsDisabled = false;   \n            StartButtonIsDisabled = true;\n        }\n        public void CountDownTimer(Object source, ElapsedEventArgs e)\n        {\n            if(Seconds == 0 && Minutes > 0)\n            {\n                Minutes -= 1;\n                Seconds = 59;\n            } else if (Minutes == 0 && Seconds == 0 && Hours > 0)\n            {\n                Hours -= 1;\n                Minutes = 59;\n                Seconds = 59;\n            } else if (Hours == 0 && Minutes == 0 && Seconds == 0)\n            {\n                aTimer.Stop();\n                aTimer.Dispose();\n\n                StopButtonIsDisabled = true;\n                StartButtonIsDisabled = false;\n            } else\n            {\n                Seconds -= 1;\n            }\n            InvokeAsync(StateHasChanged);\n        }\n    }\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"Try: `<button @onclick=\"() => SetTimer(timer.Id)\">`\n\n\n"}
{"questionId":"73b19929a8b84015a0bac5b8eae3cf87","question":"AttributeError: 'NoneType' object has no attribute 'loader'\nhaving an issue today when I started up my laptop (Ubuntu 18.4) and trying to use pip to install packages, I'm met with this error:\n\n\n\n```\nError processing line 3 of \/home\/cjones\/.local\/lib\/python3.6\/site-packages\/googleapis_common_protos-1.5.8-py3.6-nspkg.pth:\n\n  Traceback (most recent call last):\n    File \"\/usr\/lib\/python3.6\/site.py\", line 174, in addpackage\n      exec(line)\n    File \"<string>\", line 1, in <module>\n    File \"<frozen importlib._bootstrap>\", line 568, in module_from_spec\n  AttributeError: 'NoneType' object has no attribute 'loader'\n\nRemainder of file ignored\n\n```\n\nI don't think I changed anything since last successful boot but it seems as though something is missing... can anyone help?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"Delete `\/home\/cjones\/.local\/lib\/python3.6\/site-packages\/googleapis_common_protos-1.5.8-py3.6-nspkg.pth` and retry.\n\n\n"}
{"questionId":"bba2dd15c3be4effb7b53cbb327a5481","question":"Lumen\/Laravel 6: Call to undefined function array\\_except()\nSo my Mailable view is throwing this error - and this is all I have on my hands. It was working fine while I was on Lumen 5.8, so my guess is that it happened after upgrading to Laravel 6.\n\n\n\n> \n> Call to undefined function array\\_except() (View:\n>  \/kunden\/182801\\_60322\/tu\/uat\/api\/resources\/views\/mail\/invite-employee.blade.php)\n> \n> \n> \n\n\nMy blade file looks like this:\n\n\n\n```\n@extends('mail.master')\n\n@section('content')\n<tr>\n    <td align=\"left\" style=\"border: 1px solid #dddee5; border-bottom: 2px solid #cecfd9; padding; 20px;\">\n        <div class=\"padded\">\n            <p>\n            {!! nl2br(e($data->message)) !!}\n            <\/p>\n        <\/div>\n    <\/td>\n<tr>\n<tr>\n    <td align=\"left\" bgcolor=\"#eaeaf2\" class=\"padded\">\n        <p style=\"margin-bottom: 5px;\" class=\"cta-label\">{{ $data->copy->click }}<\/p>\n        <div class=\"cta-link\">\n            <a style=\"color: #337BE9;\" class=\"cta-link--a\" href=\"{{ $data->appUrl }}\/{{ $data->route }}\/{{ $data->verificationCode }}\">{{ $data->appUrl }}\/{{ $data->route }}\/{{ $data->verificationCode }}<\/a>\n        <\/div>\n        <p style=\"font-size: 12px; margin-top: 10px;\">{{ $data->copy->mistake }}<\/p>\n    <\/td>\n<\/tr>\n@endsection\n\n```\n\nwhere obviously no part of the code is trying to call that function.\n\n\nMy composer.json looks like this:\n\n\n\n```\n{\n    \"name\": \"laravel\/lumen\",\n    \"description\": \"The Laravel Lumen Framework.\",\n    \"keywords\": [\"framework\", \"laravel\", \"lumen\"],\n    \"license\": \"MIT\",\n    \"type\": \"project\",\n    \"require\": {\n        \"php\": \">=7.3.9\",\n        \"laravel\/lumen-framework\": \"^6.0\",\n        \"vlucas\/phpdotenv\": \"^3.3\",\n        \"firebase\/php-jwt\": \"^4.0\",\n        \"guzzlehttp\/guzzle\": \"^6.3\",\n        \"illuminate\/mail\": \"6.0.0\",\n        \"phanan\/cascading-config\": \"~2.0\",\n        \"nesbot\/carbon\": \"^2.0\",\n        \"neitanod\/forceutf8\": \"2.0.1\",\n        \"maatwebsite\/excel\": \"^3.1\",\n        \"mpdf\/mpdf\": \"^8.0\",\n        \"tecnickcom\/tcpdf\": \"^6.3\",\n        \"laravel\/helpers\": \"^1.1\"\n    },\n    \"require-dev\": {\n        \"fzaninotto\/faker\": \"~1.4\",\n        \"phpunit\/phpunit\": \"~5.0\",\n        \"mockery\/mockery\": \"~0.9\"\n    },\n    \"autoload\": {\n        \"psr-4\": {\n            \"App\\\\\": \"app\/\"\n        }\n    },\n    \"autoload-dev\": {\n        \"classmap\": [\n            \"tests\/\",\n            \"database\/\"\n        ]\n    },\n    \"scripts\": {\n        \"post-root-package-install\": [\n            \"php -r \\\"copy('.env.example', '.env');\\\"\"\n        ]\n    },\n    \"minimum-stability\": \"dev\",\n    \"prefer-stable\": true\n}\n\n```\n\nwhere the `laravel\/helpers` are also included now, separately. Nothing has helped so far. Any ideas what is causing this error?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"Bit late to the party but:\n\n\nas others have mentioned str\\_ and array\\_ helpers have been moved to a seperate package. If you don't wish to use that package after updating to laravel 6 you have to clear the views that were compiled using the old helper methods.\n\n\n`composer dump-autoload`\n\n\nthen\n\n\n`php artisan view:clear`\n\n\nworked for me\n\n\n"}
{"questionId":"bf954e7fd28f4076b69967e1614c9f6b","question":"difference between RUN cd and WORKDIR in Dockerfile\nIn terms of the way Docker works, is there any difference between `RUN cd \/` and `WORKDIR \/` inside a Dockerfile?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"bash\/shell"},"answer":"`RUN cd \/` does absolutely nothing. `WORKDIR \/` changes the working directory for future commands.\n\n\nEach `RUN` command runs in a new shell and a new environment (and technically a new container, though you won't usually notice this). The `ENV` and `WORKDIR` directives before it affect how it starts up. If you have a `RUN` step that just changes directories, that will get lost when the shell exits, and the next step will start in the most recent `WORKDIR` of the image.\n\n\n\n```\nFROM busybox\nWORKDIR \/tmp\nRUN pwd       # \/tmp\n\nRUN cd \/      # no effect, resets after end of RUN line\nRUN pwd       # still \/tmp\n\nWORKDIR \/\nRUN pwd       # \/\n\nRUN cd \/tmp && pwd  # \/tmp\nRUN pwd       # \/\n\n```\n\n(For the same reason, `RUN export` doesn't do anything that outlives the current Dockerfile instructions, and `RUN .` or the non-standard `RUN source` won't cause environment variables to be set.)\n\n\n"}
{"questionId":"4b91210f47064fe09faff6247ddc3584","question":"How can I get the first day of the next month in Python?\nHow can I get the first date of the next month in Python? For example, if it's now 2019-12-31, the first day of the next month is 2020-01-01. If it's now 2019-08-01, the first day of the next month is 2019-09-01.\n\n\nI came up with this:\n\n\n\n```\nimport datetime\n\ndef first_day_of_next_month(dt):\n    '''Get the first day of the next month. Preserves the timezone.\n\n    Args:\n        dt (datetime.datetime): The current datetime\n\n    Returns:\n        datetime.datetime: The first day of the next month at 00:00:00.\n    '''\n    if dt.month == 12:\n        return datetime.datetime(year=dt.year+1,\n                                 month=1,\n                                 day=1,\n                                 tzinfo=dt.tzinfo)\n    else:\n        return datetime.datetime(year=dt.year,\n                                 month=dt.month+1,\n                                 day=1,\n                                 tzinfo=dt.tzinfo)\n\n\n# Example usage (assuming that today is 2021-01-28):\nfirst_day_of_next_month(datetime.datetime.now())\n# Returns: datetime.datetime(2021, 2, 1, 0, 0)\n\n```\n\nIs it correct? Is there a better way?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"Here is a 1-line solution using nothing more than the standard `datetime` library:\n\n\n\n```\n(dt.replace(day=1) + datetime.timedelta(days=32)).replace(day=1)\n\n```\n\nExamples:\n\n\n\n```\n>>> dt = datetime.datetime(2016, 2, 29)\n>>> print((dt.replace(day=1) + datetime.timedelta(days=32)).replace(day=1))\n2016-03-01 00:00:00\n\n>>> dt = datetime.datetime(2019, 12, 31)\n>>> print((dt.replace(day=1) + datetime.timedelta(days=32)).replace(day=1))\n2020-01-01 00:00:00\n\n>>> dt = datetime.datetime(2019, 12, 1)\n>>> print((dt.replace(day=1) + datetime.timedelta(days=32)).replace(day=1))\n2020-01-01 00:00:00\n\n```\n\n"}
{"questionId":"3dfbd8d94985431996a59a6dfc92d77d","question":"async\/await in Angular `ngOnInit`\nI\u2019m currently evaluating the pros \u2018n\u2019 cons of replacing Angular\u2019s resp. RxJS\u2019 `Observable` with plain `Promise` so that I can use `async` and `await` and get a more intuitive code style.\n\n\nOne of our typical scenarios: Load some data within `ngOnInit`. Using `Observables`, we do:\n\n\n\n```\nngOnInit () {\n  this.service.getData().subscribe(data => {\n    this.data = this.modifyMyData(data);\n  });\n}\n\n```\n\nWhen I return a `Promise` from `getData()` instead, and use `async` and `await`, it becomes:\n\n\n\n```\nasync ngOnInit () {\n  const data = await this.service.getData();\n  this.data = this.modifyMyData(data);\n}\n\n```\n\nNow, obviously, Angular will not \u201cknow\u201d, that `ngOnInit` has become `async`. I feel that this is not a problem: My app still works as before. But when I look at the `OnInit` interface, the function is obviously not declared in such a way which would suggest that it can be declared `async`:\n\n\n\n```\nngOnInit(): void;\n\n```\n\nSo -- bottom line: Is it reasonable what I\u2019m doing here? Or will I run into any unforseen problems?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"typescript"},"answer":"It is no different than what you had before. `ngOnInit` will return a Promise and the caller will ignore that promise. This means that the caller will not wait for everything in your method to finish before it proceeds. In this specific case it means the view will finish being configured and the view may be launched before `this.data` is set.\n\n\nThat is the same situation you had before. The caller would not wait for your subscriptions to finish and would possibly launch the app before `this.data` had been populated. If your view is relying on `data` then you likely have some kind of `ngIf` setup to prevent you from accessing it.\n\n\nI personally don't see it as awkward or a bad practice as long as you're aware of the implications. However, the `ngIf` can be tedious (they would be needed in either way). I have personally moved to using route resolvers where it makes sense so I can avoid this situation. The data is loaded before the route finishes navigating and I can know the data is available before the view is ever loaded.\n\n\n"}
{"questionId":"fb39358ec0754208b537b871203cc527","question":"ReactJS and Typescript : refers to a value, but is being used as a type here (TS2749)\nI'm coding a ReactJS class with Typescript and Material-ui, in a .tsx file. In one of my custom components, I want to create a reference to one of the components that I use in my custom component.\n\n\n\n```\nexport class MyTextField extends React.Component<MyProps, MyState> {\n  private refTextField: React.RefObject<TextField>;\n  constructor(props: MyProps) {\n    super(props);\n    this.refTextField = React.createRef();\n  }\n\n  render(): JSX.Element {\n    const { id, label, value: defaultValue } = this.props;\n    const { value } = this.state;\n    const element = (\n      <TextField ref={this.refTextField} id={id} label={label} defaultValue={defaultValue} value={value} \/>\n    );\n\n    return element;\n  }\n}\n\n```\n\nDuring compilation, I get an error on the declaration of my reference:\n\n\n\n> \n> 'TextField' refers to a value, but is being used as a type here. TS2749\n> \n> \n> \n\n\nI tried to put \"typeof TextField\" in my declaration, but I have another message, when valuing the ref property in my render :\n\n\n\n> \n> Type 'RefObject<(props: TextFieldProps) => Element>' is not assignable\n>  to type '((instance: HTMLDivElement | null) => void) |\n>  RefObject | null | undefined'. Type\n>  'RefObject<(props: TextFieldProps) => Element>' is not assignable to\n>  type 'RefObject'.\n>  Type '(props: TextFieldProps) => Element' is missing the following properties from type 'HTMLDivElement': align, addEventListener,\n>  removeEventListener, accessKey, and 238 more. TS2322\n> \n> \n> \n\n\nAny ideas ?\nthank you so much\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"So I ran into this problem multiple times in my code before but only figured out the reason this was happening today.\n\n\nTL;DR:\nIn your case you just have to write `InstanceType<typeof TextField>` instead of `TextField`.\n\n\nWhen you create a class in TypeScript, the name of that class refers to both the instance type and the Javascript class value. If you reference that class as a type, TypeScript detects that automatically as the instance type of that class. And if you reference that class in the runtime, it just uses it as in the Javascript meaning. And it's all good and dandy till now.\n\n\n\n```\nclass MyClass {}\nlet abc: MyClass; \/\/ ts recognizes as instance type\nabc = new MyClass(); \/\/ completely fine, used here as the javascript value\n\n```\n\nHowever, the real problem is when you export the class from a module *dynamically*. When you export the class in some ways, TypeScript can only export the Javascript value of the class and does not export the type. So if you import it in another module and try to reference it as a type, you will get TS2749.\n\n\n\n```\nlet intervariable = class MyClass{}\nexport const MyClass = intervariable; \/\/ TypeScript does not export type here.\n\n```\n\n\n```\nimport {MyClass} from '.\/myclass';\n\nlet abc: MyClass; \/\/ TypeScript error TS2749\n\n```\n\nWhen this happens, especially if it is out of your control, my solution to get the instance type was simply to use InstanceType and typeof:\n\n\n\n```\nimport {MyClass} from '.\/myclass';\n\nlet abc: InstanceType<typeof MyClass>; \/\/ no error\n\/\/ the rest...\n\n```\n\nThe typeof operator gets you the class constructor type for a class value, and the InstanceType generic gets you the instance type that you want.\n\n\n"}
{"questionId":"259427beffb74950b1af5dacf5413bcc","question":"What Typescript type is a change event? (In Angular)\nI'm trying to figure out what Typescript type a change event is used in an Angular project.\n\n\nThe code is something simple like this:\n\n\n**file-upload.component.html**\n\n\n\n```\n<input type=\"file\" (change)=\"onChange($event)\"\/>\n\n```\n\n**file-upload.ts**\n\n\n\n```\npublic onChange(event: Event): void {\n  if (event.target.files && event.target.files.length) {\n    const [file] = event.target.files;\n    console.log(file);\n  }\n}\n\n```\n\nTyping the event as `Event` gives me the following Typescript linting error:\n\n\n\n```\nProperty 'files' does not exist on type 'EventTarget'\n\n```\n\nWhat should I be typing this if not `Event`?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"It is an event. But you're going to have to cast the const you're using as an HTMLInputElement.\n\n\n\n```\npublic onChange(event: Event): void {\n  if ((event.target as HTMLInputElement).files && (event.target as HTMLInputElement).files.length) {\n    const [file] = event.target.files;\n  }\n}\n\n```\n\nThe only other way is going to be to suppress the error with tsignore. In react, flow, they have a type SyntheticEvent that you can type this particular case as to get around it but angular doesn't have a real equivalent.\n\n\n"}
{"questionId":"0ef15ac0f8a34e41a3058d3d57cb559e","question":"Pytorch fails with CUDA error: device-side assert triggered on Colab\nI am trying to initialize a tensor on Google Colab with GPU enabled.\n\n\n\n```\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nt = torch.tensor([1,2], device=device)\n\n```\n\nBut I am getting this strange error.\n\n\n\n```\nRuntimeError: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\n\n```\n\nEven by setting that environment variable to 1 seems not showing any further details.  \n\nAnyone ever had this issue?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"While I tried your code, and it did not give me an error, I can say that usually the best practice to debug CUDA Runtime Errors: device-side assert like yours is to turn collab to CPU and recreate the error. It will give you a more useful traceback error.\n\n\nMost of the time CUDA Runtime Errors can be the cause of some index mismatching so like you tried to train a network with 10 output nodes on a dataset with 15 labels. And the thing with this CUDA error is once you get this error once, you will recieve it for every operation you do with torch.tensors. This forces you to restart your notebook.\n\n\nI suggest you restart your notebook, get a more accuracate traceback by moving to CPU, and check the rest of your code especially if you train a model on set of targets somewhere.\n\n\n"}
{"questionId":"2a2469b5a5ca40f892ef0fef5eea49b7","question":"When to use Repository vs Service vs Trait in Laravel?\nTo avoid code duplication in Laravel, I want to have a method that is used by multiple Controllers, it inserts some rows in database and also updates some data in another table.\n\n\nI thought of using Repository, but I read somewhere that Repository is better used for retrieving data and shouldn't be used for inserting.\n\n\nSo I'm going to use Traits now. but I'm a bit confused...\n\n\n**Could some one please explain in a simple way what is the best usage for each of these (Repository\/Service\/Trait) and how are they different?**\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"php"},"answer":"**Traits**\n\n\nAre an alternative approach to inheritance that solves some limitations of single class inheritance, which `PHP` uses. Since all Eloquent models `extends Model` it can not extend another class. This is commonly used to share similar logic across objects. Let's imagine a couple of models have a Company relationship.\n\n\n\n```\ntrait HasCompany {\n   public function company() {\n       return $this->belongsTo(Company::class);\n   }\n}\n\n```\n\nNow the user can easily share code from the trait, by the keyword `using`. This is an example and most often a more complex use case would be needed for it to make sense.\n\n\n\n```\nclass User extends Model {\n   use HasCompany;\n}\n\n```\n\n**Repositories**\n\n\nRepositories is a design pattern to abstract data layers from the application. Your logic should not care about how you store data, so if you wanted to change from `Mysql` to `Mongodb`, you would only swap out the repository and not have to change business logic.\n\n\nVery opinionated here, but this is not a fitting design pattern for `Laravel`. `Laravel` has Eloquent and the database layer is therefor already abstracted. Repositories is sometimes used for `Laravel` applications, but rather an outlier, than a common sight. One of the main reasons for repositories, is to be data implementation agnostic, which already exists and you can swap between SQL servers flawlessly. Also `Eloquents` features like `::find()`, `scopes` etc. feels like a replacement for Repositories and is quirky to use at the same time.\n\n\nIf you use Doctrine as the `ORM`, which you can in `Laravel`, it is the core of their architecture and should be used.\n\n\nIt sometimes occur that the repository pattern is used to encapsulate business logic, with a similar approach to a service approach or action pattern.\n\n\n**Services**\n\n\nIs commonly used for a place to store business logic or the building blocks of your actions in your application. In traditional `MVC` design, Controllers should only handle input. Normally you would put your logic in Models, but they get \"fat\" very quickly, when this happens services is a common place to put business logic. Sometimes also named actions or commands, which is similar but a little bit different approaches.\n\n\nOne of the core things it solves, is to make your business logic reusable. Imaging filtering all users by an active flag, when you retrieve it in its controller.\n\n\n\n```\npublic function all() {\n    return User::where('active', true)->get();\n}\n\n```\n\nNow you have your business logic, that enforces that you only work on active users, later you want to notify all active users with an email, by notifications using a command.\n\n\n\n```\nclass NotifyUsers extends Command {\n    public function handle() {\n        foreach (User::where('active', true)->get() as $user) {\n            $user->notify();\n        }\n    }\n}\n\n```\n\nNow you manually have to keep business logic up to date. Next time you add a second condition or change the logic, you have to change the code in two places. In a big application where this code block is used often, it can make it quite hard to maintain the conditions without forgetting one of the places. If you make a service with this logic, you can easily utilize the same business logic across the application. While one have one place to change the code, if this logic had to change.\n\n\n\n```\nclass UserService {\n    public function all() {\n        return User::where('active', true)->get();\n    }\n}\n\n```\n\nEverywhere you want to use this business logic getting active users, you can use the service. Therefor only having one place to maintain the logic. A call can be as simply as `resolve(UserService::class)->all()`. Example of the updated logic with services would be.\n\n\n\n```\n\/\/ controller\npublic function all(UserService $userService) {\n    return $userService->all();\n}\n\n\/\/ command\nclass NotifyUsers extends Command {\n    public function handle(UserService $userService) {\n        $userService->all()->each->notify();\n    }\n}\n\n```\n\n**Conclusion**\n\n\nThe world is not black and white, you have to figure out your own approach. My advice is, do not spend time on Repositories, `Laravel` has a lot of features to handle data related operations `scopes`, `getters` `setters` etc. that conflicts with the Repository design pattern. See if a service like design approach suits you and you can utilize em. `Traits` is not as much an architectural design pattern, as it is a Class inheritance alternative, simply to share logic between classes.\n\n\n"}
{"questionId":"8f6f92f5f707482ba5194f47c2e19ef2","question":"typescript generic type with equal operator means?\nI am learning typescript generic, and come across the following generic type with the equal operator for extends type\n\n\n\n```\nexport interface DataType {\n  [key: string]: FieldValue;\n}\n\nexport interface FormProps<Data extends DataType = DataType> { }\n\n```\n\nWhat does `DataType = DataType` mean in here?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"typescript"},"answer":"If you don't provide a type `Data` (which *must* extend `DataType`), it will default to `DataType`.\n\n\nFrom previous release notes\n\n\n\n> \n> Consider a function that creates a new HTMLElement, calling it with no arguments generates a Div; you can optionally pass a list of children as well. Previously you would have to define it as:\n> \n> \n> \n> ```\n> declare function create(): Container<HTMLDivElement, HTMLDivElement[]>;\n> declare function create<T extends HTMLElement>(element: T): Container<T, T[]>;\n> declare function create<T extends HTMLElement, U extends HTMLElement>(element: T, children: U[]): Container<T, U[]>;\n> \n> ```\n> \n> With generic parameter defaults we can reduce it to:\n> \n> \n> \n> ```\n> declare function create<T extends HTMLElement = HTMLDivElement, U = T[]>(element?: T, children?: U): Container<T, U>;\n> \n> ```\n> \n> A generic parameter default follows the following rules:\n> \n> \n> - A type parameter is deemed optional if it has a default.\n> - Required type parameters must not follow optional type parameters.\n> - Default types for a type parameter must satisfy the constraint for the type parameter, if it exists.\n> - When specifying type arguments, you are only required to specify type arguments for the required type parameters. Unspecified type parameters will resolve to their default types.\n> - If a default type is specified and inference cannot chose a candidate, the default type is inferred.\n> - A class or interface declaration that merges with an existing class or interface declaration may introduce a default for an existing type parameter.\n> - A class or interface declaration that merges with an existing class or interface declaration may introduce a new type parameter as long as it specifies a default.\n> \n> \n> \n\n\n"}
{"questionId":"fd9ab2e2b8c846e88507af610a4b22d2","question":"How to reference System.Windows.Forms in .NET Core 3.0 for WPF apps?\nI'm migrating my WPF desktop app from .NET Framework to Core 3.0.\nI was using System.Windows.Forms.FolderBrowserDialog() and I'm now stuck on how to add this reference to the Core project. There is no \"System.Windows.Forms\" NuGet package available, is there? Is there any alternative way to display the FolderBrowserDialog in Core?\n\n\n**Update**\n\n\nI created the Core project using the default template and then copied and pasted .cs and .xaml files into it. The .csproj file looks like this:\n\n\n\n```\n<Project Sdk=\"Microsoft.NET.Sdk.WindowsDesktop\">\n  <PropertyGroup>\n    <OutputType>WinExe<\/OutputType>\n    <TargetFramework>netcoreapp3.0<\/TargetFramework>\n    <UseWPF>true<\/UseWPF>\n  <\/PropertyGroup>\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"c#"},"answer":"You need to add to csproj an additional switch:\n\n\n\n```\n<UseWindowsForms>true<\/UseWindowsForms>\n\n```\n\nAdd it below **UseWpf**. Then try rebuild. After this, you should be able to use Forms namespace.\n\n\n"}
{"questionId":"17b7a8a853c74835a93896b0a6891061","question":"error when building , getting : \"suspect or \"\nI'm encountering a build issue with go. I'm wondering if it's a bug in the compiler or a problem with the code.\n\n\n\n```\n\/\/ removed the error handling for sake of clarity \n\nfile, _ := c.FormFile(\"file\")\nopenedFile, _ := file.Open()\nbuffer := make([]byte, 512)\nn, _ := openedFile.Read(buffer)\n\ncontentType := http.DetectContentType(buffer[:n])\n\n\/\/ doesn't work\n\nif contentType != \"image\/jpeg\"  || contentType != \"image\/png\" {\n  return \n}\n\n\/\/ works \n\nif contentType != \"image\/jpeg\" {\n    return\n}\nelse if contentType != \"image\/png\" {\n    return\n}\n\n```\n\nerror `suspect or: contentType != \"image\/jpeg\" || contentType != \"image\/png\"`\n\n\nfyi \" c.FormFile(\"file\") \" is form Gin gonic. but it shouldnt really matter.\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"go"},"answer":"What you see is a compiler warning, but the app will run.\n\n\nYour condition is always `true`:\n\n\n\n```\ncontentType != \"image\/jpeg\"  || contentType != \"image\/png\" \n\n```\n\nYou compare a `string` variable to 2 different `string` values (using not equal), so one of them will surely be `true`, and `true || false` is always `true`.\n\n\nMost likely you need logical AND: I assume you want to test if the content type is neither JPEG nor PNG:\n\n\n\n```\nif contentType != \"image\/jpeg\" && contentType != \"image\/png\" {\n    return \n}\n\n```\n\n"}
{"questionId":"de3ec7cb0e1140b4940aa5307c21d295","question":"For Go, ioutil.ReadAll \/ ioutil.ReadFile \/ ioutil.ReadDir deprecated\nStarting from Go 1.16, `ioutil.ReadAll`, `ioutil.ReadFile` and `ioutil.ReadDir` are deprecated, as the package `io\/ioutil` is deprecated.\n\n\nWhat other stdlib packages provide the same functionality?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"go"},"answer":"\n```\nioutil.ReadAll -> io.ReadAll\nioutil.ReadFile -> os.ReadFile\nioutil.ReadDir -> os.ReadDir\n\/\/ others\nioutil.NopCloser -> io.NopCloser\nioutil.TempDir -> os.MkdirTemp\nioutil.TempFile -> os.CreateTemp\nioutil.WriteFile -> os.WriteFile\n\n```\n\n"}
{"questionId":"1311786447f04acab357abcf2d17f160","question":"Simple fetch mock using Typescript and Jest\nWhat would be my absolute *easiest* way of mocking fetch using Typescript?\n\n\nI would just like to do something simple like below. But Typescript tells me that I'm not matching the definition for the full fetch object.\n\n\n\n```\nType 'Mock<Promise<{ json: () => Promise<{ test: number; }>; }>, []>' is not assignable to type '(input: RequestInfo, init?: RequestInit | undefined) => Promise<Response>'.\n   Type 'Promise<{ json: () => Promise<{ test: number; }>; }>' is not assignable to type 'Promise<Response>'.\n     Type '{ json: () => Promise<{ test: number; }>; }' is missing the following properties from type 'Response': headers, ok, redirected, status, and 11 more.\n\n```\n\nWhat would be the simplest solution to get around this? Actually mock out the whole fetch object or other solution?\n\n\n\n```\nglobal.fetch = jest.fn(() =>\n  Promise.resolve({\n    json: () => Promise.resolve({ test: 100 }),\n  }),\n)\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"You can tell TypeScript that you're defining `global.fetch` as a Jest mock.\n\n\n\n```\nglobal.fetch = jest.fn(() =>\n  Promise.resolve({\n    json: () => Promise.resolve({ test: 100 }),\n  }),\n) as jest.Mock;\n\n```\n\n"}
{"questionId":"b7be87ad6f964dfca989aeb15e479b27","question":"Detect Apple Silicon from command line\nHow can I detect from a shell script that it is running on M1 Apple hardware?\n\n\nI want to be able to run a command-line command so that I can write an `if`-statement whose body will only be executed when run on a mac with an M1 processor (and at least macOS Big Sur, naturally).\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"\n\n```\nuname -m\n\n```\n\nwill return `arm64` as opposed to `x86_64`\n\n\n\n```\nif [[ $(uname -m) == 'arm64' ]]; then\n  echo M1\nfi\n\n```\n\nor, as @chepner suggested\n\n\n\n```\nuname -p\n\n```\n\nwill return `arm` as opposed to `i386`\n\n\n\n```\nif [[ $(uname -p) == 'arm' ]]; then\n  echo M1\nfi\n\n```\n\nyet another tool is `arch`:\n\n\n\n```\nif [[ $(arch) == 'arm64' ]]; then\n  echo M1\nfi\n\n```\n\n"}
{"questionId":"201c1ee1d6284c5aa1dfb321825bbb94","question":"No index signature with a parameter of type 'string' was found on type\nI'm coming from mobile app development and do not have much experience with typescript. How one can declare a map object of the form [string:any] ?\n\n\nThe ERROR comes at line: map[key] = value;\n\n\n\n> \n> Element implicitly has an 'any' type because expression of type 'string' can't be used to index type 'Object'.\n> \n> \n> No index signature with a parameter of type 'string' was found on type 'Object'.ts(7053)\n> \n> \n> \n\n\n\n```\n var docRef = db.collection(\"accidentDetails\").doc(documentId);\n\n\n docRef.get().then(function(doc: any) {\n   if (doc.exists) {\n      console.log(\"Document data:\", doc.data());\n      var map = new Object();\n      for (let [key, value] of Object.entries(doc.data())) {\n        map[key] = value;\n\n       \/\/ console.log(`${key}: ${value}`);\n      }\n  } else {\n      \/\/ doc.data() will be undefined in this case\n      console.log(\"No such document!\");\n  } }).catch(function(error: any) {\n      console.log(\"Error getting document:\", error);\n  });\n\n```\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"typescript"},"answer":"You generally don't want to use `new Object()`. Instead, define `map` like so:\n\n\n\n```\nvar map: { [key: string]: any } = {}; \/\/ A map of string -> anything you like\n\n```\n\nIf you can, it's better to replace `any` with something more specific, but this should work to start with.\n\n\n"}
{"questionId":"ee1fb44e9b404c0db870cde6a27c7872","question":"How to test async functions that use Tokio?\nI have an async function that I need to test. This function uses a `mongodb::Database` object to run, so I initialize the connection in the `setup()` function and use `tokio_test::block_on()` to wrap the `await` expression inside.\n\n\n\n```\n#[cfg(test)]\nmod tests {\n    use mongodb::{options::ClientOptions, Client};\n    use tokio_test;\n\n    async fn setup() -> mongodb::Database {\n        tokio_test::block_on(async {\n            let client_uri = \"mongodb:\/\/127.0.0.1:27017\";\n            let options = ClientOptions::parse(&client_uri).await;\n            let client_result = Client::with_options(options.unwrap());\n            let client = client_result.unwrap();\n            client.database(\"my_database\")\n        })\n    }\n\n    #[test]\n    fn test_something_async() {\n        \/\/ for some reason, test cannot be async\n        let DB = setup(); \/\/ <- the DB is impl std::future::Future type\n\n        \/\/ the DB variable will be used to run another\n        \/\/ async function named \"some_async_func\"\n        \/\/ but it doesn't work since DB is a Future type\n        \/\/ Future type need await keyword\n        \/\/ but if I use await-async keywords here, it complains\n        \/\/ error: async functions cannot be used for tests\n        \/\/ so what to do here ?\n        some_async_func(DB);\n    }\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"rust"},"answer":"Just replace `#[test]` with `#[tokio::test]` before any test function. If you use actix-web you can add actix\\_rt into `Cargo.toml` and `#[actix_rt::test]` before the test function\n\n\n\n```\n#[tokio::test]\nasync fn test_something_async() {\n    let DB = setup().await; \/\/ <- the DB is impl std::future::Future type\n\n    \/\/ the DB variable will be used to run another\n    \/\/ async function named \"some_async_func\"\n    \/\/ but it doesn't work since DB is a Future type \n    \/\/ Future type need await keyword\n    \/\/ but if I use await-async keywords here, it complains\n    \/\/ error: async functions cannot be used for tests\n    \/\/ so what to do here ?\n    some_async_func(DB).await;\n}\n\n```\n\n"}
{"questionId":"8bfb5e6d6b9c42ff8cf72f90b1f5fb61","question":"Why do you need to recompile C\/C++ for each OS?\nThis is more of a theoretical question than anything. I'm a Comp sci major with a huge interest in low level programming. I love finding out how things work under the hood. My specialization is compiler design.\n\n\nAnyway, as I'm working on my first compiler, things are occurring to me that are kind of confusing.\n\n\nWhen you write a program in C\/C++, the traditional thing people know is, a compiler magically turns your C\/C++ code into native code for that machine.\n\n\nBut something doesn't add up here. If I compile my C\/C++ program targeting the x86 architecture, it would seem that the same program should run on any computer with the same architecture. But that doesn't happen. You need to recompile your code for OS X or Linux or Windows.(And yet again for 32-bit vs 64-bit)\n\n\nI'm just wondering why this is the case? Don't we target the CPU architecture\/instruction set when compiling a C\/C++ program? And a Mac OS and a Windows Os can very much be running on the same exact architecture.\n\n\n(I know Java and similar target a VM or CLR so those don't count)\n\n\nIf I took a best-shot answer at this, I'd say C\/C++ must then compile to OS-specific instructions. But every source I read says the compiler targets the machine. So I'm very confused.\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"c++"},"answer":"\n> \n> Don't we target the CPU architecture\/instruction set when compiling a C\/C++ program?\n> \n> \n> \n\n\nNo, you don't.\n\n\nI mean yes, you are compiling for a CPU instruction set. But that's not *all* compilation is.\n\n\nConsider the simplest \"Hello, world!\" program. All it does is call `printf`, right? But there's no \"printf\" instruction set opcode. So... what exactly happens?\n\n\nWell, that's part of the C standard library. Its `printf` function does some processing on the string and parameters, then... displays it. How does that happen? Well, it sends the string to standard out. OK... who controls that?\n\n\nThe operating system. And there's no \"standard out\" opcode either, so sending a string to standard out involves some form of OS call.\n\n\nAnd OS calls are not standardized across operating systems. Pretty much every standard library function that does something you couldn't build on your own in C or C++ is going to talk to the OS to do at least some of its work.\n\n\n`malloc`? Memory doesn't belong to you; it belongs to the OS, and you *maybe* are allowed to have some. `scanf`? Standard input doesn't belong to you; it belongs to the OS, and you can maybe read from it. And so on.\n\n\nYour standard library is built from calls to OS routines. And those OS routines are non-portable, so your standard library implementation is non-portable. So your executable has these non-portable calls in it.\n\n\nAnd on top of all of that, different OSs have different ideas of what an \"executable\" even *looks like*. An executable isn't just a bunch of opcodes, after all; where do you think all of those constant and pre-initialized `static` variables get stored? Different OSs have different ways of starting up an executable, and the structure of the executable is a part of that.\n\n\n"}
{"questionId":"b7ed363d16e146e1ab2007b3aed0d8c5","question":"Django UniqueConstraint\n## Context\n\n\nI have the models `AppVersion`, `App` & `DeployApp`. In the `AppVersion` model users can upload APK files to the filesystem. I am using a `pre_save` signal to prevent uploading APK files with the same `version_code` for a specific `App` like this:\n\n\n\n```\n@receiver(pre_save, sender=AppVersion)\ndef prevent_duplicate_version_code(sender, instance, **kwargs):\n    qs = AppVersion.objects.filter(app_uuid=instance.app_uuid, version_code=instance.version_code)\n    if qs.exists():\n        raise FileExistsError(\"Version code has to be unique for a specific app\")\n\n```\n\nThis signal does what I want, except it also raises the error when I am trying to create an object in the bridge-table `DeployApp`.\n\n\n## Models\n\n\n\n```\n# models.py\n\nclass App(models.Model):\n    app_uuid = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False, db_index=True)\n    app_name = models.CharField(max_length=100)\n\n\nclass AppVersion(models.Model):\n    app_version_uuid = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False, db_index=True)\n    app_uuid = models.ForeignKey(App, on_delete=models.CASCADE, related_name='app_versions')\n    app_version_name = models.CharField(max_length=100)\n    version_code = models.IntegerField(blank=True, null=True, editable=False)\n    source = models.FileField(upload_to=get_app_path, storage=AppVersionSystemStorage()) \n\n\nclass DeployApp(models.Model):\n    deploy_app_uuid = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False, db_index=True)\n    app_version = models.ForeignKey(AppVersion, on_delete=models.CASCADE)\n    device_group = models.ForeignKey(DeviceGroup, on_delete=models.CASCADE)\n    release_date = UnixDateTimeField()\n\n```\n\nMy guess is that when creating an object of `DeployApp` the related `AppVersion` is also saved and thus the `pre_save` signal is called and raises the Exception.\n\n\nI also tried to override the `save()` method for the `AppVersion` model but the results are the same.\n\n\nHow do I make sure that the Exception only happens upon creating a new `AppVersion` instance and does not happen when adding or editing a `DeployApp` instance?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"python"},"answer":"Solved it thanks to Bear Brown his suggestion. I removed the signal and added `UniqueConstraint` to the `AppVersion` model like this:\n\n\n\n```\nclass Meta:\n    db_table = 'app_version'\n    constraints = [\n        models.UniqueConstraint(fields=['app_uuid', 'version_code'], name='unique appversion')\n    ]\n\n```\n\n"}
{"questionId":"f910133dc08c4dc8b30d35c88a18fe53","question":"Cannot find name 'it' in Jest TypeScript\nI try to create an intial setup for Jest in React + TypeScript. I have completed the initial setup and try to check whether the test runs.\nWhen I run the test using the command `npm test`, I am getting the following error:\n\n\n\n```\nCannot find name 'it'. Do you need to install type definitions for a test runner? Try `npm i @types\/jest` or `npm i @types\/mocha`.\n\n```\n\nI have installed the types for Jest as well as removed the types in `tsconfig.json`, but still I am getting the same error.\n\n\n\n```\n{\n  \"compilerOptions\": {\n    \"target\": \"es6\",\n    \"lib\": [\"dom\", \"dom.iterable\", \"esnext\"],\n    \"allowJs\": true,\n    \"skipLibCheck\": true,\n    \"esModuleInterop\": true,\n    \"allowSyntheticDefaultImports\": true,\n    \"strict\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"module\": \"esnext\",\n    \"moduleResolution\": \"node\",\n    \"plugins\": [{ \"name\": \"typescript-tslint-plugin\" }],\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"noEmit\": true,\n    \"jsx\": \"preserve\",\n    \"pretty\": true,\n    \"baseUrl\": \"src\",\n    \"types\": [\"jest\"],\n    \"typeRoots\": [\".\/src\/types\"],\n    \"suppressImplicitAnyIndexErrors\": true\n  },\n  \"include\": [\"src\", \"node_modules\/@types\/jest\"],\n  \"exclude\": [\"node_modules\"]\n}\n`\n\n\n```\n\nPackage.json\n\n\n\n```\n\n    \"jest\": {\n        \"transform\": {\n          \".(ts|tsx)\": \"ts-jest\"\n        },\n        \"testRegex\": \"(\/__tests__\/.*|\\\\.(test|spec))\\\\.(ts|tsx|js)$\",\n        \"moduleFileExtensions\": [\n          \"ts\",\n          \"tsx\",\n          \"js\"\n        ]\n      },\n      \"devDependencies\": {\n        \"@babel\/plugin-proposal-export-default-from\": \"^7.2.0\",\n        \"@types\/enzyme\": \"^3.9.3\",\n        \"@types\/jest\": \"^24.0.14\",\n        \"enzyme\": \"^3.10.0\",\n        \"gh-pages\": \"^1.2.0\",\n        \"husky\": \"^2.2.0\",\n        \"jest\": \"^24.8.0\",\n        \"node-sass\": \"^4.11.0\",\n        \"prettier\": \"^1.17.0\",\n        \"react-scripts\": \"2.1.8\",\n        \"react-test-renderer\": \"^16.8.6\",\n        \"stylelint\": \"^9.3.0\",\n        \"stylelint-config-recommended-scss\": \"^3.2.0\",\n        \"stylelint-config-standard\": \"^18.2.0\",\n        \"stylelint-order\": \"^0.8.1\",\n        \"stylelint-scss\": \"^3.1.3\",\n        \"ts-jest\": \"^24.0.2\",\n        \"tslint\": \"^5.16.0\",\n        \"tslint-config-prettier\": \"^1.18.0\",\n        \"tslint-plugin-prettier\": \"^2.0.1\",\n        \"tslint-react\": \"^4.0.0\",\n        \"tslint-react-hooks\": \"^2.1.0\"\n      }\n\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"**Install**\n\n\n\n```\nnpm install -D jest @types\/jest ts-jest\n\n```\n\n**jest.config.js -- at root**\n\n\n\n```\n module.exports = {\n  roots: ['<rootDir>\/src'],\n  transform: {\n    '^.+\\\\.tsx?$': 'ts-jest',\n  },\n  testRegex: '(\/__tests__\/.*|(\\\\.|\/)(test|spec))\\\\.tsx?$',\n  moduleFileExtensions: ['ts', 'tsx', 'js', 'jsx', 'json', 'node'],\n}\n\n```\n\n**tsconfig.json**\n\n\n\n```\n{\n    \"compilerOptions\": {\n     ...\n\n      \"types\": [\"reflect-metadata\", \"jest\"],\n      \"typeRoots\": [\".\/types\", \".\/node_modules\/@types\"]\n     \n     ...\n    },\n    \"exclude\": [\"node_modules\", \"**\/*.spec.ts\", \"**\/*.test.ts\"],\n    \"include\": [\".\/src\/**\/*.tsx\", \".\/src\/**\/*.ts\"]\n  }\n\n```\n\n"}
{"questionId":"61fbff80f7bf4baeb32ce0f1dfa5bcef","question":"This request is not authorized to perform this operation. Azure blobClient\nI have the following code to return a list of containers using the `WindowsAzure.Storage` nuget package:\n\n\n\n```\npublic static class AzureBlobStorageClient\n{\n    public static CloudBlobClient GetClient(string AccountName = \"foo\", string AccountKey = \"bar\" )\n    {\n        try\n        {\n\n            var connectionString = $\"DefaultEndpointsProtocol=https;AccountName={AccountName};AccountKey={AccountKey};EndpointSuffix=core.windows.net\";\n            CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString);\n            CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient();\n            IRetryPolicy exponentialRetryPolicy = new ExponentialRetry(TimeSpan.FromSeconds(2), 10);\n            blobClient.DefaultRequestOptions.RetryPolicy = exponentialRetryPolicy;\n            return blobClient;\n        }\n        catch (StorageException ex)\n        {\n            Console.WriteLine(\"Error returned from the service: {0}\", ex.Message);\n            throw;\n        }\n    }\n\n    public static void DeleteContainer(CloudBlobContainer container)\n    {\n        var result = container.DeleteIfExistsAsync().Result;\n    }\n\n    public static List<CloudBlobContainer> GetContainers()\n    {\n        var client = GetClient();\n        BlobContinuationToken continuationToken = null;\n        List<CloudBlobContainer> results = new List<CloudBlobContainer>();\n        do\n        {\n            var response = client.ListContainersSegmentedAsync(continuationToken).Result;\n            continuationToken = response.ContinuationToken;\n            results.AddRange(response.Results);\n        }\n        while (continuationToken != null);\n\n        return results;\n    }\n\n}\n\n```\n\nwhen i run this, i get the following error on client.ListContainersSegmentedAsync(continuationToken).Result :\n\n\n\n> \n> System.AggregateException: 'One or more errors occurred. (This request is not authorized to perform this operation.)'\n> \n> \n> \n\n\nand I can't see how to set the authorization for the request. \n\n\nMy question is how to get past this error message\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c#"},"answer":"Thanks to @gaurav Mantri for this answer.\n\n\nThe issue was my client IP was not added to the firewall rules for the storage account.\n\n\nTo change this go to :\n\n\nStorage accounts > {yourAccount} > Networking > Firewalls and Virtual networks\n\n\nand add your IP address\n\n\n"}
{"questionId":"c4ec6e19f14447c990ee0356837837f0","question":"How to get all values of an enum in PHP?\nPHP 8.1 is almost getting released, including support for Enumerations. I was testing some of the enum functionality and couldn't find much documentation about it. Hence my question: how do I get all values of an enum?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"php"},"answer":"For basic enums:\n\n\n\n```\n$suits = array_column(Suit::cases(), 'name');\n\n```\n\nFor backed enums where you want the values:\n\n\n\n```\n$suits = array_column(Suit::cases(), 'value');\n\n```\n\nYou could then do something like this:\n\n\n\n```\ntrait EnumToArray\n{\n\n  public static function names(): array\n  {\n    return array_column(self::cases(), 'name');\n  }\n\n  public static function values(): array\n  {\n    return array_column(self::cases(), 'value');\n  }\n\n  public static function array(): array\n  {\n    return array_combine(self::values(), self::names());\n  }\n\n}\n\nenum Suit: string\n{\n\n  use EnumToArray;\n\n  case Hearts = 'H';\n  case Diamonds = 'D';\n  case Clubs = 'C';\n  case Spades = 'S';\n\n}\n\n```\n\n`Suit::array()` will return:\n\n\n\n```\nArray\n(\n    [H] => Hearts\n    [D] => Diamonds\n    [C] => Clubs\n    [S] => Spades\n)\n\n```\n\n"}
{"questionId":"9d6e191eb75b4580b05e83ae50ed1f57","question":"How to fix \"running scripts is disabled on this system\"?\nWhen I try to run ionic commands like `ionic serve` on the VS Code terminal, it gives the following error.\n\n\nHow can I fix this?\n\n\n\n```\nionic : File C:\\Users\\Lakshan\\AppData\\Roaming\\npm\\ionic.ps1 cannot be loaded because running scripts is disabled on this system. For more information, see \nabout_Execution_Policies at https:\/go.microsoft.com\/fwlink\/?LinkID=135170.\nAt line:1 char:1\n+ ~~~~~\n    + CategoryInfo          : SecurityError: (:) [], PSSecurityException\n    + FullyQualifiedErrorId : UnauthorizedAccess\n\n```\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"bash\/shell"},"answer":"I found a way to fix this error.\n\n\nIt is as follows:\n\n\n1. First, **Open PowerShell** with **Run as Administrator**.\n2. Then, run this command in PowerShell\n\n```\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned\n\n```\n3. After that type `Y` and press Enter.\n\n\n"}
{"questionId":"11fa54d6fa9443199122c9cd0e6b2bca","question":"TypeScript - Module '\"\\*.svg\"' has no exported member 'ReactComponent\nI'm trying to import an `.svg` file as a React component with TypeScript.\n\n\nAccording to the React docs, this is done like so:\n\n\n`import { ReactComponent as Icon } from '.\/Icon.svg';`\n\n\nFollowing the TypeScript docs, I've added this:\n\n\n\n```\n\/\/ custom.d.ts\n\ndeclare module '*.svg' {\n    const content: any;\n    export default content;\n}\n\n```\n\nand\n\n\n\n```\n\/\/ tsconfig.json\n\n{\n    ...\n    \"files\": [\n        \"custom.d.ts\"\n    ]\n}\n\n```\n\nThe SVG is rendering. But I'm getting a TypeScript error:\n\n\n`[ts] Module '\"*.svg\"' has no exported member 'ReactComponent'. [2305]`\n\n\nHere is my full `tsconfig` file if that helps:\n\n\n\n```\n{\n  \"compileOnSave\": false,\n  \"compilerOptions\": {\n    \"target\": \"es5\",\n    \"module\": \"commonjs\",\n    \"declaration\": true,\n    \"outDir\": \".\/dist\",\n    \"strict\": true,\n    \"jsx\": \"react\",\n    \"moduleResolution\": \"node\",\n    \"allowSyntheticDefaultImports\": true,\n    \"noUnusedLocals\": true,\n    \"noUnusedParameters\": true,\n    \"removeComments\": false,\n    \"preserveConstEnums\": true,\n    \"sourceMap\": true,\n    \"skipLibCheck\": true,\n    \"esModuleInterop\": true,\n    \"baseUrl\": \".\",\n    \"plugins\": [\n      {\n        \"name\": \"typescript-styled-plugin\"\n      }\n    ],\n    \"typeRoots\": [\".\/node_modules\/@types\"],\n    \"lib\": [\"dom\", \"es2015\", \"es2017\"]\n  },\n  \"exclude\": [\"node_modules\", \"dist\"],\n  \"files\": [\n    \"custom.d.ts\"\n  ]\n}\n\n```\n\nThank you!\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"You have `export default content;` But you are doing a *named* import (not a default import).\n\n\n# Fix\n\n\nChange declaration to export the name you are importing:\n\n\n\n```\ndeclare module '*.svg' {\n  import React = require('react');\n  export const ReactComponent: React.FC<React.SVGProps<SVGSVGElement>>;\n  const src: string;\n  export default src;\n}\n\n```\n\n# Additional\n\n\nRecommend not using `files` in tsconfig.json. Instead just use `include`\n\n\n"}
{"questionId":"790565637d11431eaed50d4430bc1a5c","question":"How can I have IS NULL condition in TypeORM find options?\nIn my queries I'm using TypeORM `find` option.\nHow can I have `IS NULL` condition in the `where` clause?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"sql"},"answer":"Another way is you can use `IsNull()` function, for example:\n\n\n\n```\nimport { IsNull } from \"typeorm\";\nreturn await getRepository(User).findOne({\n    where: { \n      username: IsNull()\n    }\n});\n\n```\n\n"}
{"questionId":"80228906b8e54f11acf6943461359aea","question":"declare type with React.useImperativeHandle()\n\n```\nfunction App(){\n  const cntEl:any = React.useRef(null);  \/\/ I don't know what type should be here.\n  React.useEffect(()=>{\n    if(cntEl.current){ cuntEl.current.start() }\n  }, []);\n  return <Countdown ref={cntEl} \/>\n}\nconst Countdown = React.forwardRef((props,ref) => {\n  React.useImperativeHandle(ref, ()=>({\n    start() {\n      alert('Start');\n    }\n  });\n  return <div>Countdown<\/div>\n});\n\n\n```\n\nI try to use a child method in a parent component using `ref` and `React.useImperativeHandle()`.\n\n\nIt works well. \n\n\nbut I am not satisfied because of `const cntEl:any`.\n\n\nI believe there are many ways to avoid using `any` type I don't know.\n\n\nI just need a type that could be replaced type `any`.\n\n\n**Edited**\n\n\nI can see `(property) React.MutableRefObject<null>.current: null` when I hover at `cntEl.current`\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"I recommend you use type definitions more explicitly\n\n\nFor example, with React DT, you can define ref exotic component with `ForwardRefRenderFunction` instead of `FC`.\n\n\n\n```\ntype CountdownProps = {}\n    \ntype CountdownHandle = {\n  start: () => void,\n}\n    \nconst Countdown: React.ForwardRefRenderFunction<CountdownHandle, CountdownProps> = (\n  props,\n  forwardedRef,\n) => {\n  React.useImperativeHandle(forwardedRef, ()=>({\n    start() {\n      alert('Start');\n    }\n  });\n\n  return <div>Countdown<\/div>;\n}\n\nexport default React.forwardRef(Countdown);\n\n```\n\nand then use React utility `ElementRef`, TypeScript can infer exact ref type of your component\n\n\n\n```\nconst App: React.FC = () => {\n  \/\/ this will be inferred as `CountdownHandle`\n  type CountdownHandle = React.ElementRef<typeof Countdown>;\n\n  const ref = React.useRef<CountdownHandle>(null); \/\/ assign null makes it compatible with elements.\n\n  return (\n    <Countdown ref={ref} \/>\n  );\n};\n\n```\n\n"}
{"questionId":"8752cce043de45e6b2505e9f8fcea1b0","question":"Restart terminal without closing on MacOS\nHow to restart my current MacOS terminal session without closing the window?\n\n\nIn Linux I use `exec bash` but it does not work in this environment. I made a few changes to the `.bash_profile` (prompt, alias etc) I would like to see without closing it and opening again.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"bash\/shell"},"answer":"Just type in the command:\n\n\n\n```\nexec bash -l\n\n```\n\nI guess that should do it.\n\n\nFor `zsh`,\n\n\n\n```\nexec zsh -l\n\n```\n\nThis is needed because every shell on `macOS` by default is a login shell.\n\n\nJusting writing `exec bash` would replace the current shell with a non-login shell which is not the same effect as closing and re-opening the terminal.\n\n\n`exec` would make new `bash -l` process replace the current shell. If `exec` is not used, `bash -l` would spawn a new shell over the current shell incrementing the `$SHLVL`.\n\n\n"}
{"questionId":"1d37357e2f124857997230b8892fb75c","question":"What does asyncio.create\\_task() do?\nWhat does `asyncio.create_task()` do? I have looked at the docs and can't seem to understand it. A bit of code that confuses me is this:\n\n\n\n```\nimport asyncio\n\nasync def counter_loop(x, n):\n    for i in range(1, n + 1):\n        print(f\"Counter {x}: {i}\")\n        await asyncio.sleep(0.5)\n    return f\"Finished {x} in {n}\"\n\nasync def main():\n    slow_task = asyncio.create_task(counter_loop(\"Slow\", 4))\n    fast_coro = counter_loop(\"Fast\", 2)\n\n    print(\"Awaiting Fast\")\n    fast_val = await fast_coro\n    print(\"Finished Fast\")\n\n    print(\"Awaiting Slow\")\n    slow_val = await slow_task\n    print(\"Finished Slow\")\n\n    print(f\"{fast_val}, {slow_val}\")\n\nasyncio.run(main())\n\n```\n\nThis gives the following output:\n\n\n\n```\n001 | Awaiting Fast\n002 | Counter Fast: 1\n003 | Counter Slow: 1\n004 | Counter Fast: 2\n005 | Counter Slow: 2\n006 | Finished Fast\n007 | Awaiting Slow\n008 | Counter Slow: 3\n009 | Counter Slow: 4\n010 | Finished Slow\n011 | Finished Fast in 2, Finished Slow in 4\n\n```\n\nI don't understand quite how this is working.\n\n\n1. Shouldn't the `slow_task` not be able to run until the completion of\nthe `fast_coro` because it was never used in an `asyncio.gather`\nmethod?\n2. Why do we have to `await slow_task`?\n3. Why is `Awaiting Slow` printed after the coroutine appears to have started?\n4. What really is a *task*? I know that what `gather` is doing is scheduling a\n*task*. And `create_task` supposedly creates a task.\n\n\nAn in-depth answer would be greatly appreciated. Thanks!\n\n\nIt also might be worth mentioning that I know very little about Futures.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"python"},"answer":"\n> \n> What does `asyncio.create_task()` do?\n> \n> \n> \n\n\nIt submits the coroutine to run \"in the background\", i.e. concurrently with the current task and all other tasks, switching between them at `await` points. It returns an awaitable handle called a \"task\" which you can also use to cancel the execution of the coroutine.\n\n\nIt's one of the central primitives of asyncio, the asyncio equivalent of starting a thread. (In the same analogy, awaiting the task with `await` is the equivalent of joining a thread.)\n\n\n\n> \n> Shouldn't the `slow_task` not be able to run until the completion of the `fast_coro`\n> \n> \n> \n\n\nNo, because you explicitly used `create_task` to start `slow_task` in the background. Had you written something like:\n\n\n\n```\n    slow_coro = counter_loop(\"Slow\", 4)\n    fast_coro = counter_loop(\"Fast\", 2)\n    fast_val = await fast_coro\n\n```\n\n...indeed `slow_coro` would not run because no one would have yet submitted it to the event loop. But `create_task` does exactly that: submit it to the event loop for execution concurrently with other tasks, the point of switching being any `await`.\n\n\n\n> \n> because it was never used in an `asyncio.gather` method?\n> \n> \n> \n\n\n`asyncio.gather` is not the only way to achieve concurrency in asyncio. It's just a utility function that makes it easier to wait for a number of coroutines to all complete, and submit them to the event loop at the same time. `create_task` does just the submitting, it should have probably been called `start_coroutine` or something like that.\n\n\n\n> \n> Why do we have to await `slow_task`?\n> \n> \n> \n\n\nWe don't *have* to, it just serves to wait for both coroutines to finish cleanly. The code could have also awaited `asyncio.sleep()` or something like that. Returning from `main()` (and the event loop) immediately with some tasks still pending would have worked as well, but it would have printed a warning message indicating a possible bug. Awaiting (or canceling) the task before stopping the event loop is just cleaner.\n\n\n\n> \n> What really is a task?\n> \n> \n> \n\n\nIt's an asyncio construct that tracks execution of a coroutine in a concrete event loop. When you call `create_task`, you submit a coroutine for execution and receive back a handle. You can await this handle when you actually need the result, or you can never await it, if you don't care about the result. This handle is the *task*, and it inherits from `Future`, which makes it awaitable and also provides the lower-level callback-based interface, such as `add_done_callback`.\n\n\n"}
{"questionId":"bc22ecc6d9a64c0d8a1698deb879a248","question":"Getting type of a property of a typescript class using keyof operator\nAs stated in the documentation of Typescript about the `keyof` operator, one can get a property of an object instance using the function below.\n\n\n\n```\nfunction getProperty<T, K extends keyof T>(o: T, name: K) {\n    return o[name];\n}\n\n```\n\nOf course, one can get the type of the property by replacing `return o[name]` into `return typeof o[name]`. Is there a way to retrieve the type of the property **without passing any object instance**?\n\n\n\n```\nfunction getPropertyType<T>(name: keyof T) {\n    \/\/ something like T[name]?\n}\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"typescript"},"answer":"Is this what you're looking for?\n\n\n\n```\ntype PropType<TObj, TProp extends keyof TObj> = TObj[TProp];\n\n```\n\nand get type of an object property by doing:\n\n\n\n```\ntype MyPropType = PropType<ObjType, '<key>'>;\n\n```\n\nwhich is the same as the way of using `Pick` in typescript, and it can report compile error if there's any invalid `key` passed in.\n\n\n**Updates**\n\n\nAs @astoilkov suggested, a simpler alternative is `PropType['key']`.\n\n\n"}
{"questionId":"41c90459f30142a68398835a4066692f","question":"Usage of the TypeScript compiler argument 'skipLibCheck'\nI've been researching around for a further explanation into the *skipLibCheck* TypeScript compiler argument to determine the safety of having this set to **true**. The most in-depth explanation I found is the following:\n\n\n\n> \n> New --skipLibCheck\n>  TypeScript 2.0 adds a new --skipLibCheck compiler option that causes type checking of declaration files (files with extension .d.ts) to be skipped. When a program includes large declaration files, the compiler spends a lot of time type checking declarations that are already known to not contain errors, and compile times may be significantly shortened by skipping declaration file type checks.\n> \n> \n> Since declarations in one file can affect type checking in other files, some errors may not be detected when --skipLibCheck is specified. For example, if a non-declaration file augments a type declared in a declaration file, errors may result that are only reported when the declaration file is checked. However, in practice such situations are rare.\n> \n> \n> \n\n\nI understand that you obviously get a performance benefit from the compiler not having to type check files which are considered not to contain errors but I've seen this flag being used to get around errors being emitted from the compiler in relation to the declaration files having problems.\n\n\nSurely using this flag to get around this decreases the integrity of the typing of your application?\n\n\n","questionMetadata":{"type":"conceptual","level":"advanced","tag":"typescript"},"answer":"To paraphrase the question tersely:\n\n\n\n> \n> Surely [enabling skipLibCheck] decreases the integrity of the typing of your application?\n> \n> \n> \n\n\nI would agree that yes, it does. However, if the alternative is an application that does not compile, then it becomes a handy flag.\n\n\nWhile Typescript itself is fairly mature, the typescript community is still relatively young. There are type definitions available for tons of libraries, and even some native typescript libraries, but they can be incompatible with one another for a variety of reasons.\n\n\nYou may import a library whose typing is built with a less-strict tsconfig than you would like--which your compiler could complain about when you try to use it.\n\n\nYou could find two libraries define the same types, but incompatibly. I've imported some libraries that supplied their own typings for a Polyfill of `Buffer`, and my whole application would fail to compile because of their incompatibility.\n\n\nEnabling `--skipLibCheck` can help work around these issues. Turning it on will prevent Typescript from type-checking the entire imported libraries. Instead, Typescript will only type-check the code you use against these types. This means that as long as you aren't using the incompatible parts of imported libraries, they'll compile just fine.\n\n\ntl;dr, Yes, `--skipLibCheck` degrades type checking, and ideally we wouldn't use it. But not every library provides perfect types yet, so skipping it can be nice.\n\n\n"}
{"questionId":"3fe3b93b355c4fc791fa0fa25cfbce60","question":"TRIM is not a recognized built-in function name\nFor the following code:\n\n\n\n```\nDECLARE @ss varchar(60)\n  SET @ss = 'admin'\n\n  select TRIM(@ss)\n\n```\n\nI've got an error: \n\n\n\n> \n> 'TRIM' is not a recognized built-in function name\n> \n> \n> \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"sql"},"answer":"`TRIM` is introduced in SQL Server (starting with 2017).\n\n\nIn older version of SQL Server to perform trim you have to use `LTRIM` and `RTRIM` like following.\n\n\n\n```\nDECLARE @ss varchar(60)\n  SET @ss = ' admin '\n\n  select RTRIM(LTRIM(@ss))\n\n```\n\nIf you don't like using `LTRIM`, `RTRIM` everywhere, you can create your own custom function like following.\n\n\n\n```\n   CREATE FUNCTION dbo.TRIM(@string NVARCHAR(max))\n    RETURNS NVARCHAR(max)\n     BEGIN\n      RETURN LTRIM(RTRIM(@string))\n     END\n    GO\n\n```\n\n"}
{"questionId":"7ad34ce6dada475ea1847f68c317340f","question":"Method Illuminate\\Translation\\Translator::getFromJson does not exist\nAfter upgrading from laravel 5.8 to laravel 6.0, I found this error.\n\n\n\n> \n> `Method Illuminate\\Translation\\Translator::getFromJson does not exist`\n> \n> \n> \n\n\nAnyone know any work around for this?\n\n\nIt seems that problem is from using @lang for printing translation messages in my blade file\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"Hy Prasanth\n\n\nThe Lang::get and Lang::getFromJson methods have been consolidated in laravel . Calls to the Lang::getFromJson method should be updated to call Lang::get.\n\n\nYou should run the `php artisan view:clear` Artisan command to avoid Blade errors related to the removal of Lang::transChoice, Lang::trans, and Lang::getFromJson.\n\n\nThank you.\n\n\n"}
{"questionId":"3d8103fd421c44aeb12cb758bc4009c5","question":"TypeError: attrib() got an unexpected keyword argument 'convert'\nThis error occurred during automated testing of a python project on the CI server using `pytest`. I'm using `pytest==4.0.2`. This error only just started to occur, previous pipelines seem to work fine.\n\n\nThe full error:\n\n\n\n```\nFile \"\/usr\/local\/lib\/python3.7\/site-packages\/_pytest\/tmpdir.py\", line 35, in TempPathFactory\n    lambda p: Path(os.path.abspath(six.text_type(p)))\nTypeError: attrib() got an unexpected keyword argument 'convert'\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"`pytest` seems to have the package `attrs` as a dependency. `attrs==19.2.0` was released around `2019-10-01 17:00 UTC`. This seems to cause the problem above.\n\n\nSwitching back to `attrs==19.1.0` fixes the problem. Just do the following:\n\n\n\n```\npip install attrs==19.1.0\n\n```\n\n**NOTE**: I expect that the issue will be resolved either by `attrs` or `pytest` soon by releasing a new version. So this fix should only be temporary.\n\n\nFrom the comments: This error does not occur on the newer versions of pytest i.e. `pytest==5.2.0`\n\n\n"}
{"questionId":"c9e4cef604934ee3aca566a806baaf8e","question":"Unable to install java8 with homebrew\nInstalling `java8` with Homebrew seems to no longer be working. After running:\n\n\n\n```\nbrew install caskroom\/cask\/java8\n\n```\n\nI get the following error:\n\n\n\n```\nError: Cask 'java8' is unavailable: '\/usr\/local\/Homebrew\/Library\/Taps\/caskroom\/homebrew-cask\/Casks\/java8.rb' does not exist.\n\n```\n\nSimply doing:\n\n\n\n```\nbrew cask install java8\n\n```\n\nErrors out with:\n\n\n\n```\nError: Cask 'java8' is unavailable: No Cask with this name exists.\n\n```\n\nThis seems like a recent development because I remember installing it this way a few months ago. Any suggestions on how to properly install `java8` on MacOS nowadays?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"java"},"answer":"New command is now :\n\n\n`brew install --cask homebrew\/cask-versions\/adoptopenjdk8`\n\n\n"}
{"questionId":"b5f24a0fa58646d9b8febf96146c0ded","question":"Dart: convert Map to List of Objects\nDid several google searches, nothing helpful came up. Been banging my head against some errors when trying to do something that should be pretty simple. Convert a map such as `{2019-07-26 15:08:42.889861: 150, 2019-07-27 10:26:28.909330: 182}` into a list of objects with the format:\n\n\n\n```\nclass Weight {\n  final DateTime date;\n  final double weight;\n  bool selected = false;\n\n  Weight(this.date, this.weight);\n}\n\n```\n\nI've tried things like: `List<Weight> weightData = weights.map((key, value) => Weight(key, value));`\n\n\nThere's no `toList()` method for maps, apparently. So far I'm not loving maps in dart. Nomenclature is confusing between the object type map and the map function. Makes troubleshooting on the internet excruciating. \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"dart"},"answer":"Following on Richard Heap's comment above, I would:\n\n\n\n```\nList<Weight> weightData =\n  mapData.entries.map( (entry) => Weight(entry.key, entry.value)).toList();\n\n```\n\nDon't forget to call `toList`, as Dart's `map` returns a kind of Iterable.\n\n\n"}
{"questionId":"886170bf13af42e78e116f4a56bffa80","question":"Check If a Column Exists in Laravel Migration File\nAlready I have a table name `table_one.` Now I want to add two more columns to it. Everything works fine so far. But in my method, I want to check a column exists or not in my table like `dropIfExists('table').`\n\n\n\n```\n\/**\n * Run the migrations.\n *\n * @return void\n *\/\npublic function up()\n{\n    Schema::table('table_one', function (Blueprint $table) {\n        $table->string('column_one')->nullable();\n        $table->string('column_two')->nullable();\n    });\n}\n\n\/**\n * Reverse the migrations.\n *\n * @return void\n *\/\npublic function down()\n{\n    Schema::table('table_one', function (Blueprint $table) {\n        \/\/ in here i want to check column_one and column_two exists or not\n        $table->dropColumn('column_one');\n        $table->dropColumn('column_two');\n    });\n}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"php"},"answer":"You need something just like this\n\n\n\n```\n  public function down()\n    {\n        if (Schema::hasColumn('users', 'phone'))\n        {\n            Schema::table('users', function (Blueprint $table)\n            {\n                $table->dropColumn('phone');\n            });\n        }\n    }\n\n```\n\n"}
{"questionId":"0212c3bebe524463ab360ffd041619eb","question":"Typescript: Type X is missing the following properties from type Y length, pop, push, concat, and 26 more. [2740]\nI have this Product interface:\n\n\n\n```\nexport interface Product{\n  code: string;\n  description: string;\n  type: string;\n}\n\n```\n\nService with method calling product endpoint:\n\n\n\n```\n  public getProducts(): Observable<Product> {\n    return this.http.get<Product>(`api\/products\/v1\/`);\n  }\n  \n\n```\n\nAnd component where I use this service to get the Products.\n\n\n\n```\nexport class ShopComponent implements OnInit {\n    public productsArray: Product[];\n    \n    ngOnInit() {\n        this.productService.getProducts().subscribe(res => {\n          this.productsArray = res;\n        });\n    }\n}\n\n```\n\nWith this state I'm getting error:\n\n\n\n> \n> [ts] Type 'Product' is missing the following properties from type\n> 'Product[]': length, pop, push, concat, and 26 more. [2740]\n> \n> \n> \n\n\nRemoving typing on `productsArray` variable removes the error, but don't get why this is not working, since server response is an array of objects in the type of `Products`?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"You are returning `Observable<Product>` and expecting it to be `Product[]` inside `subscribe` callback.\n\n\nThe Type returned from `http.get()` and `getProducts()` should be `Observable<Product[]>`\n\n\n\n```\npublic getProducts(): Observable<Product[]> {\n    return this.http.get<Product[]>(`api\/products\/v1\/`);\n}\n\n```\n\n"}
{"questionId":"a5a027ff8aca45ca8bd48ba9b1653289","question":"How to remove last n characters from a string in Bash?\nI have a variable `var` in a Bash script holding a string:\n\n\n\n```\necho $var\n\"some string.rtf\"\n\n```\n\nI want to remove the last four characters of this string and assign the result to a new variable `var2`, so that\n\n\n\n```\necho $var2\n\"some string\"\n\n```\n\nHow can I do this?\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"bash\/shell"},"answer":"First, it's usually better to be explicit about your intent. So if you know the string ends in a `.rtf` that you want to remove, you can just use `var2=${var%.rtf}`. One potentially-useful aspect of this approach is that if the string *doesn't* end in `.rtf`, it is not changed at all; `var2` will contain an unmodified copy of `var`.\n\n\nIf you want to remove a filename suffix but don't know or care exactly what it is, you can use `var2=${var%.*}` to remove everything starting with the last `.`. Or, if you only want to keep everything up to but not including the *first* `.`, you can use `var2=${var%%.*}`. Those options have the same result if there's only one `.` in the string, but if there might be more than one, you get to pick which end of the string to work from. On the other hand, if there's no `.` in the string at all, `var2` will again be an unchanged copy of `var`.\n\n\nIf you really want to always remove a specific *number* of characters, here are some options.\n\n\nYou tagged this `bash` specifically, so we'll start with bash builtins. The one which has worked the longest is the same suffix-removal syntax I used above: to remove four characters, use `var2=${var%????}`. Or to remove four characters only if the first one is a dot, use `var2=${var%.???}`, which is like `var2=${var%.*}` but only removes the suffix if the part after the dot is exactly three characters. As you can see, to count characters this way, you need one question mark per unknown character removed, so this approach gets unwieldy for larger substring lengths.\n\n\nAn option in newer shell versions is substring extraction: `var2=${var:0:${#var}-4}`. Here you can put any number in place of the `4` to remove a different number of characters. The `${#var}` is replaced by the length of the string, so this is actually asking to extract and keep (length - 4) characters starting with the first one (at index 0). With this approach, you lose the option to make the change only if the string matches a pattern. As long as the string has at least four characters, no matter what its actual value is, the copy will include all but its last four characters.\n\n\nYou can leave the start index out; it defaults to 0, so you can shorten that to just `var2=${var::${#var}-4}`. In fact, newer versions of bash (specifically 4+, which means the one that ships with MacOS won't work) recognize negative lengths as the index of the character to stop at, counting back from the end of the string. So in those versions you can get rid of the string-length expression, too: `var2=${var::-4}`. This interpretation is also triggered if you leave the string length in but the string is shorter than four characters, since then `${#var}-4` is negative. For example, if the string has three characters, `${var:0:${#var}-4}` becomes `${var:0:-1}` and removes only the last character.\n\n\nIf you're not actually using bash but some other POSIX-type shell, the pattern-based suffix removal with `%` will still work \u2013 even in plain old dash, where the index-based substring extraction won't. Ksh and zsh do both support substring extraction, but require the explicit 0 start index; zsh also supports the negative end index, while ksh requires the length expression. Note that zsh, which indexes *arrays* starting at 1, nonetheless indexes *strings* starting at 0 *if you use this bash-compatible syntax*. But zsh also allows you to treat scalar parameters as if they were arrays of characters, in which case the substring syntax uses a 1-based count and places the start and (inclusive) end positions in brackets separated by commas: `var2=$var[1,-5]`.\n\n\nInstead of using built-in shell parameter expansion, you can of course run some utility program to modify the string and capture its output with command substitution. There are several commands that will work; one is `var2=$(sed 's\/.\\{4\\}$\/\/' <<<\"$var\")`.\n\n\n"}
{"questionId":"28f2e7cb1d954fb8b4da34daedcc8e76","question":"Type 'MutableRefObject' is not assignable to type 'LegacyRef | undefined'\nGiven this very simple component :\n\n\n\n```\nconst InputElement => React.forwardRef((props:any, ref) => {\n    const handleRef = React.useRef<HTMLInputElement|undefined>()\n    React.useImperativeHandle(ref, () => ({\n        setChecked(checked:boolean) {\n            if (handleRef.current) {\n                handleRef.current.checked = checked;\n            }\n        }\n    }), []);\n    return (\n        <input ref={ handleRef } type=\"checkbox\" \/>  {\/* <-- error here *\/}\n    )\n})\n\n```\n\nI have this error :\n\n\n\n```\nType 'MutableRefObject<HTMLInputElement | undefined>' is not assignable to type 'LegacyRef<HTMLInputElement> | undefined'.\n  Type 'MutableRefObject<HTMLInputElement | undefined>' is not assignable to type 'RefObject<HTMLInputElement>'.\n    Types of property 'current' are incompatible.\n      Type 'HTMLInputElement | undefined' is not assignable to type 'HTMLInputElement | null'.\n        Type 'undefined' is not assignable to type 'HTMLInputElement | null'.ts(2322)\n\n```\n\nWhat does this mean? How to fix this error?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"To fix the error, you should pass `null` as an initial value to the `useRef` hook. You don't need to add `| undefined`:\n\n\n\n```\nReact.useRef<HTMLInputElement>(null)\n\n```\n\nThe error message says that the `useRef` function expects either an `HTMLInputElement` or `null`, but if you pass in nothing, it evaluates to `undefined` and that's why get the error message:\n\n\n\n```\nType 'undefined' is not assignable to type 'HTMLInputElement | null'\n\n```\n\n"}
{"questionId":"9c8bc59069214afbb550b7ed007c299c","question":"Class-validator - validate array of objects\nI am using class-validator package with NestJS and I am looking to validate an array of objects that need to have exactly 2 objects with the same layout:\n\n\nSo far I have:\n\n\n\n```\nimport { IsString, IsNumber } from 'class-validator';\n\nexport class AuthParam {\n  @IsNumber()\n  id: number;\n\n  @IsString()\n  type: string;\n\n  @IsString()\n  value: string;\n}\n\n```\n\nand \n\n\n\n```\nimport { IsArray, ValidateNested } from 'class-validator';\nimport { AuthParam } from '.\/authParam.model';\n\nexport class SignIn {\n  @IsArray()\n  @ValidateNested({ each: true })\n  authParameters: AuthParam[];\n}\n\n```\n\nper @kamilg response (I am able to enforce exacly 2 elements):\n\n\n\n```\nimport { IsArray, ValidateNested, ArrayMinSize, ArrayMaxSize } from 'class-validator';\nimport { AuthParam } from '.\/authParam.model';\n\nexport class SignInModel {\n  @IsArray()\n  @ValidateNested({ each: true })\n  @ArrayMinSize(2)\n  @ArrayMaxSize(2)\n  authParameters: AuthParam[];\n}\n\n```\n\nI still can pass an empty array or an array with some other objects not related to AuthParam.\n\n\nHow I should modify it get validation? \n\n\nAlso how I can enforce mandatory 2 elements in the array? MinLength(2) seems to be regarding string... (resolved)\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"Add `@Type(() => AuthParam)` to your array and it should be working. `Type` decorator is required for nested objects(arrays). Your code becomes\n\n\n\n```\nimport { IsArray, ValidateNested, ArrayMinSize, ArrayMaxSize } from 'class-validator';\nimport { AuthParam } from '.\/authParam.model';\nimport { Type } from 'class-transformer';\n\nexport class SignInModel {\n  @IsArray()\n  @ValidateNested({ each: true })\n  @ArrayMinSize(2)\n  @ArrayMaxSize(2)\n  @Type(() => AuthParam)\n  authParameters: AuthParam[];\n}\n\n```\n\nBe careful if you are using any exception filter to modify the error reponse. Make sure you understand the structure of the class-validator errors.\n\n\n"}
{"questionId":"ec6f725587fa4213a95cddac742529e7","question":"Cannot find module 'react-dom\/client' from 'node\\_modules\/@testing-library\/react\/dist\/pure.js'\nWhile running `npm test` I got following error:\n\n\n\n```\nCannot find module 'react-dom\/client' from 'node_modules\/@testing-library\/react\/dist\/pure.js'\nRequired stack:\nnode_modules\/@testing-library\/react\/dist\/pure.js\nnode_modules\/@testing-library\/react\/dist\/index.js\n\n```\n\nAll necessary packages seem to be installed. I reinstalled `react-dom`, but it didn't help. Below providing imports used in my test file:\n\n\n\n```\nimport React from \"react\";\nimport { render, screen } from '@testing-library\/react';\nimport userEvent from '@testing-library\/user-event';\nimport '@testing-library\/jest-dom';\n\n```\n\nAdditionally providing my `package.json`:\n\n\n\n```\n{\n  \"name\": \"fe\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"dependencies\": {\n    \"@fontsource\/roboto\": \"^4.5.3\",\n    \"@material-ui\/core\": \"^4.12.3\",\n    \"@material-ui\/icons\": \"^4.11.2\",\n    \"@mui\/icons-material\": \"^5.5.0\",\n    \"@mui\/material\": \"5.5.3\",\n    \"@mui\/styles\": \"^5.5.1\",\n    \"@reduxjs\/toolkit\": \"^1.8.0\",\n    \"@testing-library\/jest-dom\": \"5.16.3\",\n    \"@testing-library\/react\": \"13.0.0\",\n    \"@testing-library\/user-event\": \"14.0.4\",\n    \"axios\": \"^0.26.1\",\n    \"react\": \"^17.0.2\",\n    \"react-dom\": \"^17.0.2\",\n    \"react-hook-form\": \"^7.28.1\",\n    \"react-redux\": \"^7.2.6\",\n    \"react-router-dom\": \"^6.2.2\",\n    \"react-scripts\": \"5.0.0\",\n    \"redux\": \"^4.1.2\",\n    \"styled-components\": \"^5.3.5\",\n    \"web-vitals\": \"^2.1.4\",\n    \"yup\": \"^0.32.11\"\n  },\n  \"scripts\": {\n    \"start\": \"react-scripts start\",\n    \"build\": \"react-scripts build\",\n    \"test\": \"react-scripts test\",\n    \"eject\": \"react-scripts eject\"\n  },\n  \"browserslist\": {\n    \"production\": [\n      \">0.2%\",\n      \"not dead\",\n      \"not op_mini all\"\n    ],\n    \"development\": [\n      \"last 1 chrome version\",\n      \"last 1 firefox version\",\n      \"last 1 safari version\"\n    ]\n  },\n  \"devDependencies\": {\n    \"@types\/jest\": \"^27.4.0\",\n    \"@types\/node\": \"^16.11.25\",\n    \"@types\/react\": \"^17.0.39\",\n    \"@types\/react-dom\": \"^17.0.11\",\n    \"@types\/styled-components\": \"^5.1.24\",\n    \"@typescript-eslint\/eslint-plugin\": \"^5.12.0\",\n    \"@typescript-eslint\/parser\": \"^5.12.0\",\n    \"eslint\": \"^8.9.0\",\n    \"eslint-config-airbnb\": \"^19.0.4\",\n    \"eslint-config-prettier\": \"^8.4.0\",\n    \"eslint-import-resolver-typescript\": \"^2.5.0\",\n    \"eslint-plugin-import\": \"^2.25.4\",\n    \"eslint-plugin-jsx-a11y\": \"^6.5.1\",\n    \"eslint-plugin-prettier\": \"^4.0.0\",\n    \"eslint-plugin-react\": \"^7.28.0\",\n    \"eslint-plugin-react-hooks\": \"^4.3.0\",\n    \"prettier\": \"2.5.1\",\n    \"typescript\": \"^4.5.5\"\n  }\n}\n\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"I think it's because your @testing-library\/react using the newer version, just test with version of 12.1.2\n\n\n"}
{"questionId":"c499143c8def4d0e849ba6db29e52c7a","question":"How to fix Binding element 'children' implicitly has an 'any' type.ts(7031)?\nI'm missing something here with the validation how to add types validation? Having error \"element 'children' implicitly has an 'any' type\".\n\n\n\n```\nimport * as React from 'react';\nimport Button from '.\/Styles';\n\nconst Button1 = ({ children, ...props }) => (\n  <Button {...props}>{children}<\/Button>\n);\n\nButton1.propTypes = {};\n\nexport default Button1;\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"typescript"},"answer":"Edit 2022:\nwith react 18, FC no longer provides children, so you have to type it yourself, and you can drop FC:\n\n\n\n```\nimport React, { ReactNode } from \"react\";\n\ninterface Props {\n    children?: ReactNode\n    \/\/ any props that come into the component\n}\n\nconst Button1 = ({ children, ...props }: Props) => (\n    <Button {...props}>{children}<\/Button>\n);\n\n```\n\nYes you are missing a type for Props as whole, which means typescript sees it as `any` and your ts rules dont allow it.\n\n\nYou have to type your props as:\n\n\n\n```\nimport React, { FC } from \"react\";\n\ninterface Props {\n    \/\/ any props that come into the component\n}\n\nconst Button1: FC<Props> = ({ children, ...props }) => (\n    <Button {...props}>{children}<\/Button>\n);\n\n```\n\n"}
{"questionId":"1719d022689b4ec087800296ca2d77f1","question":"How to solve \"AttributeError: module 'google.protobuf.descriptor' has no attribute '\\_internal\\_create\\_key\"?\nI encountered it while executing `from object_detection.utils import label_map_util` in jupyter notebook. It is actually the tensorflow object detection tutorial notebook(it comes with the tensorflow object detection api)\nThe complete error log:\n\n\n\n```\nAttributeError                            Traceback (most recent call last)\n<ipython-input-7-7035655b948a> in <module>\n      1 from object_detection.utils import ops as utils_ops\n----> 2 from object_detection.utils import label_map_util\n      3 from object_detection.utils import visualization_utils as vis_util\n\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\object_detection\\utils\\label_map_util.py in <module>\n     25 import tensorflow as tf\n     26 from google.protobuf import text_format\n---> 27 from object_detection.protos import string_int_label_map_pb2\n     28 \n     29 \n\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\object_detection\\protos\\string_int_label_map_pb2.py in <module>\n     19   syntax='proto2',\n     20   serialized_options=None,\n---> 21   create_key=_descriptor._internal_create_key,\n     22   serialized_pb=b'\\n2object_detection\/protos\/string_int_label_map.proto\\x12\\x17object_detection.protos\\\"\\xc0\\x01\\n\\x15StringIntLabelMapItem\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\n\\n\\x02id\\x18\\x02 \\x01(\\x05\\x12\\x14\\n\\x0c\\x64isplay_name\\x18\\x03 \\x01(\\t\\x12M\\n\\tkeypoints\\x18\\x04 \\x03(\\x0b\\x32:.object_detection.protos.StringIntLabelMapItem.KeypointMap\\x1a(\\n\\x0bKeypointMap\\x12\\n\\n\\x02id\\x18\\x01 \\x01(\\x05\\x12\\r\\n\\x05label\\x18\\x02 \\x01(\\t\\\"Q\\n\\x11StringIntLabelMap\\x12<\\n\\x04item\\x18\\x01 \\x03(\\x0b\\x32..object_detection.protos.StringIntLabelMapItem'\n     23 )\n\nAttributeError: module 'google.protobuf.descriptor' has no attribute '_internal_create_key'\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"python"},"answer":"The protoc version I got through `pip show protobuf` and `protoc --version` were different. The version in pip was a bit outdated.\n\n\nAfter I upgraded the pip version with\n\n\n\n```\npip install --upgrade protobuf\n\n```\n\nthe problem was solved.\n\n\n"}
{"questionId":"3fc54b5b5a7f4a97811b50dfdbbb8ce9","question":"Can't Install ffi -v '1.9.18' on macos Catalina\nCan't install specific ffi version. If I run gem install ffi, everything works just fine but I need specific version 1.9.18 and it doesn't work.\n\n\n\n```\nFetching ffi 1.9.18\nInstalling ffi 1.9.18 with native extensions\nGem::Ext::BuildError: ERROR: Failed to build gem native extension.\n\ncurrent directory: \/Users\/chille\/.rbenv\/versions\/2.6.5\/lib\/ruby\/gems\/2.6.0\/gems\/ffi- \n1.9.18\/ext\/ffi_c\n\/Users\/chille\/.rbenv\/versions\/2.6.5\/bin\/ruby -I \n\/Users\/chille\/.rbenv\/versions\/2.6.5\/lib\/ruby\/2.6.0 -r\n.\/siteconf20200928-22154-72wopi.rb extconf.rb\nchecking for ffi_call() in -lffi... yes\nchecking for ffi_prep_closure()... yes\nchecking for ffi_raw_call()... yes\nchecking for ffi_prep_raw_closure()... yes\nchecking for shlwapi.h... no\nchecking for rb_thread_blocking_region()... no\nchecking for rb_thread_call_with_gvl()... yes\nchecking for rb_thread_call_without_gvl()... yes\nchecking for ffi_prep_cif_var()... yes\ncreating extconf.h\ncreating Makefile\n\ncurrent directory: \/Users\/chille\/.rbenv\/versions\/2.6.5\/lib\/ruby\/gems\/2.6.0\/gems\/ffi- \n1.9.18\/ext\/ffi_c\nmake \"DESTDIR=\" clean\n\ncurrent directory: \/Users\/chille\/.rbenv\/versions\/2.6.5\/lib\/ruby\/gems\/2.6.0\/gems\/ffi- \n1.9.18\/ext\/ffi_c\nmake \"DESTDIR=\"\ncompiling AbstractMemory.c\ncompiling ArrayType.c\ncompiling Buffer.c\ncompiling Call.c\nCall.c:355:5: error: implicit declaration of function 'rb_thread_call_without_gvl' is \ninvalid in C99\n[-Werror,-Wimplicit-function-declaration]\nrbffi_thread_blocking_region(call_blocking_function, data, (void *) -1, NULL);\n^\n.\/Thread.h:78:39: note: expanded from macro 'rbffi_thread_blocking_region'\n# define rbffi_thread_blocking_region rb_thread_call_without_gvl\n                                  ^\n1 error generated.\nmake: *** [Call.o] Error 1\n\nmake failed, exit code 2\n\nGem files will remain installed in \n\n```\n\n\/Users\/chille\/.rbenv\/versions\/2.6.5\/lib\/ruby\/gems\/2.6.0\/gems\/ffi-1.9.18 for inspection.\nResults logged to \/Users\/chille\/.rbenv\/versions\/2.6.5\/lib\/ruby\/gems\/2.6.0\/extensions\/x86\\_64-\ndarwin-19\/2.6.0\/ffi-1.9.18\/gem\\_make.out\n\n\nAn error occurred while installing ffi (1.9.18), and Bundler cannot continue.\nMake sure that `gem install ffi -v '1.9.18'` succeeds before bundling.\n\n\nIn Gemfile:\nbootstrap-sass was resolved to 3.3.7, which depends on\nsass was resolved to 3.5.5, which depends on\nsass-listen was resolved to 4.0.0, which depends on\nrb-inotify was resolved to 0.9.10, which depends on\nffi\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"ruby"},"answer":"To get past that error, install ffi using:\n\n\n`gem install ffi -v '1.9.18' -- --with-cflags=\"-Wno-error=implicit-function-declaration\"`\n\n\n**NOTE**: make sure to use the correct version of `ffi` in example above.\n\n\n"}
{"questionId":"566a2787de6c4327b5f7c7cc6b411475","question":"Flutter: Outline input border\nI was trying to build a border for my text field like:\n\n\n\n```\nreturn TextField(\n  ...\n border: OutlineInputBorder(\n  borderSide: BorderSide(\n   color: Colors.red, \n    width: 5.0),\n    )\n  )\n)\n\n```\n\nBut it always return a black border with 1.0 as width. \nThe only way that I found to change the color was to create a ThemeData where I specify the hint color, but I could not find a way to change my width.\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"dart"},"answer":"What your looking for is - `enabledBorder` property of `InputDecoration`.\n\n\nIf you want to Change Border on focus use - `focusedBorder`\n\n\n\n```\n    TextField(\n        decoration: new InputDecoration(\n            focusedBorder: OutlineInputBorder(\n                borderSide: BorderSide(color: Colors.greenAccent, width: 5.0),\n            ),\n            enabledBorder: OutlineInputBorder(\n                borderSide: BorderSide(color: Colors.red, width: 5.0),\n            ),\n            hintText: 'Mobile Number',\n        ),\n    ),\n\n```\n\n"}
{"questionId":"2c995d5f60444606983705f3cc9ff3a0","question":"Argument of type 'HTMLElement | null' is not assignable to parameter of type 'Element'. Type 'null' is not assignable to type 'Element'.ts(2345)\nI have index.html\n\n\n\n```\n<body>\n    <div id=\"portal\"><\/div>\n    <div id=\"root\"><\/div>\n<\/body>\n\n```\n\nand want to use the component below in separate `portal div` than `root div`,\n\n\n\n```\nimport React from 'react';\n\nconst portalDiv = document.getElementById('portal');\n\nfunction Portal1(props) {\n  return ReactDOM.createPortal(\n    <div>\n      {props.children}\n    <div\/>, \n  portalDiv); \/\/here\n}\n\nexport default Portal1;\n\n```\n\nBut I am getting this error, **Argument of type 'HTMLElement | null' is not assignable to parameter of type 'Element'.\nType 'null' is not assignable to type 'Element'.ts(2345)** in VScode.\n\n\nI am using Typescript.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"Other people have answered that you should add a null-check, but Typescript also has a non-null assertion that you can use when you are *sure* that the value is never null by adding the `!` operator to the end of your statement:\n\n\n\n```\nconst portalDiv = document.getElementById('your-element')!;\n\n```\n\n"}
{"questionId":"a013d3454dc0447bb0a9bd4b918e8263","question":"What does this tensorflow message mean? Any side effect? Was the installation successful?\nI just installed tensorflow v2.3 on anaconda python. I tried to test out the installation using the python command below;\n\n\n\n```\n$ python -c \"import tensorflow as tf; x = [[2.]]; print('tensorflow version', tf.__version__); print('hello, {}'.format(tf.matmul(x, x)))\"\n\n```\n\nI got the following message;\n\n\n\n```\n2020-12-15 07:59:12.411952: I tensorflow\/core\/platform\/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nhello, [[4.]]\n\n```\n\nFrom the message, it seems that the installation was installed successfully. But what does `This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX AVX2` mean exactly?\n\n\nAm I using a tensorflow version with some limited features? Any side effects?\n\n\nI am using Windows 10.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"python"},"answer":"An important part of Tensorflow is that it is supposed to be fast. With a suitable installation, it works with CPUs, GPUs, or TPUs. Part of going fast means that it uses different code depending on your hardware. Some CPUs support operations that other CPUs do not, such as vectorized addition (adding multiple variables at once). Tensorflow is simply telling you that the version you have installed can use the AVX and AVX2 operations and is set to do so by default in certain situations (say inside a forward or back-prop matrix multiply), which can speed things up. This is not an error, it is just telling you that it can and will take advantage of your CPU to get that extra speed out.\n\n\nNote: AVX stands for Advanced Vector Extensions.\n\n\n"}
{"questionId":"eac14c6df5d446d7a5d961284a61f3f8","question":"Do I need to explicitly handle negative numbers or zero when summing squared digits?\nI recently had a test in my class. One of the problems was the following:\n\n\n\n> \n> Given a number **n**, write a function in C\/C++ that returns the sum of the digits of the number *squared*. (The following is important). The *range* of **n** is [ -(10^7), 10^7 ]. Example: If **n** = 123, your function should return 14 (1^2 + 2^2 + 3^2 = 14).\n> \n> \n> \n\n\nThis is the function that I wrote:\n\n\n\n```\nint sum_of_digits_squared(int n) \n{\n    int s = 0, c;\n\n    while (n) {\n        c = n % 10;\n        s += (c * c);\n        n \/= 10;\n    }\n\n    return s;\n}\n\n```\n\nLooked right to me. So now the test came back and I found that the teacher didn't give me all the points for a reason that I do not understand. According to him, for my function to be complete, I should've have added the following detail:\n\n\n\n```\nint sum_of_digits_squared(int n) \n {\n    int s = 0, c;\n\n    if (n == 0) {      \/\/\n        return 0;      \/\/\n    }                  \/\/\n                       \/\/ THIS APPARENTLY SHOULD'VE \n    if (n < 0) {       \/\/ BEEN IN THE FUNCTION FOR IT\n        n = n * (-1);  \/\/ TO BE CORRECT\n    }                  \/\/\n\n    while (n) {\n        c = n % 10;\n        s += (c * c);\n        n \/= 10;\n    }\n\n    return s;\n}\n\n```\n\nThe argument for this is that the number **n** is in the range [-(10^7), 10^7], so it can be a negative number. But I don't see where my own version of the function fails. If I understand correctly, the meaning of `while(n)` is `while(n != 0)`, **not** `while (n > 0)`, so in my version of the function the number **n** wouldn't fail to enter the loop. It would work just the same. \n\n\nThen, I tried both versions of the function on my computer at home and I got exactly the same answers for all the examples that I tried. So, `sum_of_digits_squared(-123)` is equal to `sum_of_digits_squared(123)` (which again, is equal to `14`) (even without the detail that I apparently should've added). Indeed, if I try to print on the screen the digits of the number (from least to greatest in importance), in the `123` case I get `3 2 1` and in the `-123` case I get `-3 -2 -1` (which is actually kind of interesting). But in this problem it wouldn't matter since we square the digits. \n\n\nSo, who's wrong?\n\n\n**EDIT**: My bad, I forgot to specify and didn't know it was important. The version of C used in our class and tests has to be C99 or **newer**. So I guess (by reading the comments) that my version would get the correct answer in any way.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"c"},"answer":"Summarizing a discussion that's been percolating in the comments:\n\n\n- There is no good reason to test in advance for `n == 0`. The `while(n)` test will handle that case perfectly.\n- It's likely your teacher is still used to earlier times, when the result of `%` with negative operands was differently defined. On some old systems (including, notably, early Unix on a PDP-11, where Dennis Ritchie originally developed C), the result of `a % b` was *always* in the range `[0 .. b-1]`, meaning that -123 % 10 was 7. On such a system, the test in advance for `n < 0` would be necessary.\n\n\nBut the second bullet applies only to earlier times. In the current versions of both the C and C++ standards, integer division is defined to truncate towards 0, so it turns out that `n % 10` is guaranteed to give you the (possibly negative) last digit of `n` even when `n` is negative.\n\n\nSo the answer to the question *\"What is the meaning of `while(n)`?\"* is *\"Exactly the same as `while(n != 0)`\"*, and the answer to *\"Will this code work properly for negative as well as positive `n`?\"* is *\"Yes, under any modern, Standards-conforming compiler.\"* The answer to the question *\"Then why did the instructor mark it down?\"* is probably that they're not aware of a significant language redefinition that happened to C in 1999 and to C++ in 2010 or so.\n\n\n"}
{"questionId":"17d677754879445486fc10ca36d3c1f0","question":"NPM package cannot be used as a JSX Component - Type errors\nIve been getting these strange type errors on my typescript project for certain packages.\nEx:\n\n\n\n```\n'TimeAgo' cannot be used as a JSX component.\n  Its instance type 'ReactTimeago<keyof IntrinsicElements | ComponentType<{}>>' is not a valid JSX element.\n    The types returned by 'render()' are incompatible between these types.\n      Type 'React.ReactNode' is not assignable to type 'import(\"\/home\/user\/app\/node_modules\/@types\/react-bootstrap-table-next\/node_modules\/@types\/react\/index\").ReactNode'.\n        Type '{}' is not assignable to type 'ReactNode'.\n\n```\n\nI don't get these type errors on my local windows machine but they keep occurring in my linux virtual machine. I've deleted the project many times, cloned my repo and installed packages again in different versions of node and I still get the same type errors.\n\n\nChecked node 12.18.3, 16.13.1\n\n\nHere is some quick package json info:\n\n\n\n```\n\"react-timeago\": \"^6.2.1\",\n\"react-custom-scrollbars\": \"^4.2.1\",\n\"react-custom-scrollbars-2\": \"^4.4.0\",\n\"react\": \"^17.0.2\",\n\"next\": \"^12.1.1\",\n\"@types\/react-custom-scrollbars\": \"^4.0.10\",\n\"@types\/react-timeago\": \"^4.1.3\",\n\"@types\/react\": \"^17.0.44\",\n\"typescript\": \"^4.3.5\"\n\"@types\/node\": \"^14.18.12\",\n\n```\n\nThis happens on basic custom components:\n\n\n\n```\nMyTst.tsx\nimport TimeAgo from \"react-timeago\";\n\nconst Mytst = () => {\n  return (\n    <div>\n      <TimeAgo date={\"02\/02\/2022\"} \/>\n    <\/div>\n  );\n};\n\nexport default Mytst;\n\n```\n\nI get this error for react-custom-scrollbars-2 as well. There seems to be an issue with matching the types correctly between the library which contains the component and the @types files associated with them. Anyone have any ideas on how to resolve these type errors?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"Had the same issue.\nJust add this\n\n\n\n```\n\"resolutions\": {\n  \"@types\/react\": \"17.0.2\",\n  \"@types\/react-dom\": \"17.0.2\"\n},\n\n```\n\nto `package.json` file and run `yarn` command.\n\n\n**UPD:** Just a bit detailed answer:\n\n\n`@types\/react-dom` has its own dependencies and one of them is `@types\/react` with a version set to `'*'` - a major release, that now, probably, refers to `18`.\n\n\nEven if you specify some strict versions in your `package.json` (without `^`) parent package might install its own duplicates of packages that you are already using for its own purposes.\n\n\nBy using `resolutions` we are specifying strict restrictions for dependencies of dependencies.\n\n\n"}
{"questionId":"c19ae76613c64e74a4f943a8af9062be","question":"What typescript type do I use with useRef() hook when setting current manually?\nHow can I use a React ref as a mutable instance, with Typescript? The current property appears to be typed as read-only.\n\n\nI am using React + Typescript to develop a library that interacts with input fields that are NOT rendered by React. I want to capture a reference to the HTML element and then bind React events to it. \n\n\n\n```\n  const inputRef = useRef<HTMLInputElement>();\n  const { elementId, handler } = props;\n\n  \/\/ Bind change handler on mount\/ unmount\n  useEffect(() => {\n    inputRef.current = document.getElementById(elementId);\n    if (inputRef.current === null) {\n      throw new Exception(`Input with ID attribute ${elementId} not found`);\n    }\n    handler(inputRef.current.value);\n\n    const callback = debounce((e) => {\n      eventHandler(e, handler);\n    }, 200);\n\n    inputRef.current.addEventListener('keypress', callback, true);\n\n    return () => {\n      inputRef.current.removeEventListener('keypress', callback, true);\n    };\n  });\n\n```\n\nIt generates compiler errors: `semantic error TS2540: Cannot assign to 'current' because it is a read-only property.`\n\n\nI also tried `const inputRef = useRef<{ current: HTMLInputElement }>();` This lead to this compiler error:\n\n\n\n```\nType 'HTMLElement | null' is not assignable to type '{ current: HTMLInputElement; } | undefined'.\n\n  Type 'null' is not assignable to type '{ current: HTMLInputElement; } | undefined'.\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"Yeah, this is a quirk of how the typings are written: \n\n\n\n```\nfunction useRef<T>(initialValue: T): MutableRefObject<T>;\nfunction useRef<T>(initialValue: T|null): RefObject<T>;\n\n```\n\nIf the initial value includes `null`, but the specified type param doesn't, it'll be treated as an immutable `RefObject`. \n\n\nWhen you do `useRef<HTMLInputElement>(null)`, you're hitting that case, since `T` is specified as `HTMLInputElement`, and `null` is inferred as `HTMLInputElement | null`.\n\n\nYou can fix this by doing:\n\n\n\n```\nuseRef<HTMLInputElement | null>(null)\n\n```\n\nThen `T` is `HTMLInputElement | null`, which matches the type of the first argument,\u00a0so you\u00a0hit the first override and get a mutable ref instead.\n\n\n"}
{"questionId":"4d8e732166f745949f64c2e871eafaeb","question":"node\\_modules\/rxjs\/internal\/types.d.ts(81,44): error TS1005: ';' expected error after installation of Angular 6\nI got an error of \n\n\n\n> \n> node\\_modules\/rxjs\/internal\/types.d.ts(81,44): error TS1005: ';' expected.\n> \n> \n> \n\n\nafter the installation of Angular 6. \n\n\nCheck the error:\n\n\n\n```\nERROR in node_modules\/rxjs\/internal\/types.d.ts(81,44): error TS1005: ';' expected.\nnode_modules\/rxjs\/internal\/types.d.ts(81,74): error TS1005: ';' expected.\nnode_modules\/rxjs\/internal\/types.d.ts(81,77): error TS1109: Expression expected.\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"This problem might arise due to version mismatch. To solve your problem you need to do following changes in your **package.json** file.\n\n\n**Step 1** : Go to `package.json` and modify `\"rxjs\": \"^6.0.0\"` to `\"rxjs\": \"6.0.0\"`\n\n\n**Step 2** Run `npm install` in your project.\n\n\nThere is no need to change the typescript version. (Mine: `\"typescript\": \"~2.7.2\"`)\n\n\n**Edit:** If you are using `rxjs-compat` then you also need to do following in order to fixed the issue. change the `rxjs-compat` version from `\"rxjs-compat\": \"^6.2.2\"` to `\"rxjs-compat\": \"6.2.2\"`\n\n\nHope this will help!\n\n\n"}
{"questionId":"906757e09bde4b6a848e092720c1ba58","question":"Why is --isolatedModules error fixed by any import?\nIn a create-react-app typescript project, I tried to write this just to test some stuff quickly:\n\n\n\n```\n\/\/ experiment.test.ts\nit('experiment', () => {\n  console.log('test');\n});\n\n```\n\nBut it gives me the following error, with a red squiggly beneath `it`:\n\n\n\n> \n> All files must be modules when the '--isolatedModules' flag is provided.\n> \n> \n> \n\n\nHowever, if I change the file to the following, then everything apparently is fine (except for the unused import of course):\n\n\n\n```\n\/\/ experiment.test.ts\nimport { Component} from 'react'; \/\/ literally anything, don't even have to use it\n\nit('test', () => {\n  console.log('test');\n});\n\n```\n\nWhy? What is happening here? What does `--isolatedModules` actually mean\/do?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"typescript"},"answer":"Typescript treats files without import\/exports as legacy script files. As such files are not modules and any definitions they have get merged in the global namespace. `isolatedModules` forbids such files.\n\n\nAdding any import or export to a file makes it a module and the error disappears.\n\n\n**Also `export {}` is a handy way** to make a file a module without importing anything.\n\n\n"}
{"questionId":"00275d26af0d4c2ca0f03a7fcbe2e436","question":"dyld: Library not loaded: \/usr\/local\/opt\/icu4c\/lib\/libicui18n.62.dylib error running php after installing node with brew on Mac\nI installed node using homebrew (Mojave), afterwards php stoped working and if I try to run `php -v` I get this error:\n\n\n\n```\nphp -v\ndyld: Library not loaded: \/usr\/local\/opt\/icu4c\/lib\/libicui18n.62.dylib\n  Referenced from: \/usr\/local\/bin\/php\n  Reason: image not found\n\n```\n\nI tried to uninstall both node and icu4c but the problem persists\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"php"},"answer":"\n> \n> Update - As stated in some of the comments, running `brew cleanup` could possibly fix this error, if that alone doesn't fix it, you might try upgrading individual packages or all your brew packages.\n> \n> \n> \n\n\nI just had this same problem. Upgrading Homebrew and then cleaning up worked for me. This error likely showed up for me because of a mismatch in package versions. None of the above solutions resolved my error, but running the following homebrew commands did.\n\n\n\n> \n> **Caution** - This will upgrade all your brew packages, including, but not limited to PHP. If you only want to upgrade specific packages make sure to be specific.\n> \n> \n> \n\n\n\n```\nbrew upgrade icu4c\n\nbrew upgrade \/\/ or upgrade all packages\n\n```\n\nand finally\n\n\n\n```\nbrew cleanup\n\n```\n\n"}
{"questionId":"7bb86ef1774549a1bc11992ee205a46e","question":"TypeError: react\\_\\_WEBPACK\\_IMPORTED\\_MODULE\\_2\\_\\_\\_default(...) is not a function. How do I solve this?\nI was using React when I got the following error.\n\n\n`TypeError: react__WEBPACK_IMPORTED_MODULE_2___default(...) is not a function`\n\n\nHow do I solve this?\nI have added the code below.\n\n\n\n```\nimport React from 'react';\nimport LoginForm from '.\/loginForm'\nimport useState from \"react\"\n\nfunction LoginPage(){\n    const[details, setDetails] = useState({\n        username: '',\n        password: ''\n    });\n    function handleChange(event){\n        const updatedDetails = {...details, [event.target.name]: event.target.value};\n        setDetails(updatedDetails)\n    }\n    return(<LoginForm details={details} onChange={handleChange}\/>)\n};\n\nexport default LoginPage;\n\n```\n\nThe other component is :-\n\n\n\n```\nimport React from \"react\";\nimport 'bootstrap\/dist\/css\/bootstrap.min.css';\nimport { Jumbotron, Container } from 'reactstrap';\nimport { Button, Form, FormGroup, Label, Input, FormText } from 'reactstrap';\n\nfunction FormPage(props){\n  return (\n    <Jumbotron fluid>\n    <Container fluid>\n    <center><h1 className=\"display-3\"> Log IN<\/h1><\/center>\n    <p className=\"lead\">Please Enter your Details Here<\/p>\n    <\/Container><br \/>\n    <Container fluid>\n    <Form>\n        <FormGroup>\n            <Label for=\"username\">Username<\/Label>\n            <Input type=\"textarea\" name=\"username\" id=\"username\" placeholder=\"Enter your Username Here\" value={props.details.username} onChange={props.onChange}><\/Input>\n        <\/FormGroup>\n        <FormGroup>\n            <Label for=\"password\">Password<\/Label>\n            <Input type=\"password\" name=\"password\" id=\"password\" placeholder=\"Enter your Password Here\" value={props.details.username} onChange={props.onChange}><\/Input>\n        <\/FormGroup>\n        <Button color=\"success\">Submit<\/Button>\n    <\/Form>\n    <\/Container>\n    <\/Jumbotron>\n  );\n};\n\nexport default FormPage;\n\n```\n\nPS: I am typing this because StackOverflow is asking me to add some more details as my question is mostly code. Sorry.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"I am just going to point out you are importing from \"react\" lib twice\n\n\n\n```\n\/\/ You have: (look closely at libs you are importing from and how)\nimport React from 'react';\nimport LoginForm from '.\/loginForm'\nimport useState from \"react\"\n\n\/\/ Should be:\nimport React, { useState } from 'react';\nimport LoginForm from '.\/loginForm'\n\n\/\/ Why?\n\/\/ Because useState is one of React libs' export default's apis \/ methods\n\/\/ aka useState is just a part of the React default export but also is itself, an export\n\/\/ React.useState() is how it would look if you just imported the React lib itself and nothing else\n\/\/ how I personally handle any react apis is via ==> \nimport React, { useState } from 'react\n\n\/\/ apart from loving seeing all libraries and individual apis imported \n\/\/ as soon as I see a file to get a sense of its potential functionalities, \n\/\/ read my detailed explanation below\n\n\n```\n\nHere, you are literally importing (`export default react`) from the entire React lib and simply naming it a random string useState and then trying to use that how you use the real `React.useState()` api\/method.\n\n\nSo you trying to use useState like the actual React `useState` api in this manner will absolutely cause an error because you are basically saying this `require('react')()` when you import the default import of react lib versus an api that is part of that default export, or it is itself an export in which you need to deconstruct from the lib in the import statement, not sure related but you 100% have malformed code I cannot believe no one even mentioned this?\n\n\nFor further example, for it to work as you have it (although eslint should be screaming about duplicate imports by now before you even hit save) you would have to do `useState.useState()`, which clearly is not what you want. Some people don't mind `React.useState()` but I personally will shun you haha jk, but destruct from import!!! please (:\n\n\n***Using Best Coding Standards*** is key to being a great software engineer on a team and even in your own personal projects for all of the reasons, and will absolutely increase your DX and output overall.\n\n\nHope this helped my dude. We all have gone through these little quirks that once you learn, add that to your \"things I know for sure\" list and keep trucking\n\n\n"}
{"questionId":"f3071b30fc3a4c368e7a6c6aaac4972b","question":"Change array in javascript into simpler object\nI have a simple JSON with an array that contains further objects, etc. like this:\n\n\n\n```\nlanguagePack:\n[\n  {\n    'key': 'Username',\n    'value': 'Benutzername',\n    'group': 'default'\n  },\n  {\n    'key': 'Password',\n    'value': 'Passwort',\n    'group': 'default'\n  }\n]\n\n```\n\nBut what I really want is an object like this:\n\n\n\n```\nlanguagePack: \n{\n    'Username': 'Benutzername',\n    'Password': 'Passwort'\n}\n\n```\n\nSo, I want to reduce the array to simple key-value-pairs that are inside an array or even an object (keys are unique). Does anyone have an idea how to reduce this with some of these cool array functions? I only came up with something like an for each and building the object \"by hand\" property for property, but I remember there were some cool things for array like 'reduce', the spread operator (...), map, every, some, etc.\n\n\nI tried it with something like:\n\n\n\n```\nvar temp = this.languagePack.map(([key, value]) => ({key,value}))\nconsole.log(temp)\n\n```\n\nBut that only got me an error message `TypeError: Invalid attempt to destructure non-iterable instance` \n\n\nEdit: All three answers are working perfectly fine. Thanks.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"Basically you need to use `forEach` instead of `map` function and then you can build that object to whatever key, value pair you want to keep.\n\n\n**Try this, it will solve your problem.**\n\n\n\n```\nvar temp = {};\n\nthis.languagePack.forEach(({key,value}) => {\n    temp[key] = value\n})\n\nconsole.log(temp)\n\n```\n\n**Note:** Here we are not using `map` because we want to return object not an array, so, we can use `reduce` function here to do so, but I thought this would be simple and easy to understand what we want and what are we doing here.\n\n\n"}
{"questionId":"ffa38169b6934ae28b36e44496f4ba5e","question":"querySelectorAll to include self\n\n```\n<div class=\"a\">\n    <span class=\"a\">a<\/span>\n    <span class=\"a\">b<\/span>\n    <span class=\"a\">c<\/span>\n<\/div>\n\n```\n\nAssuming I have a variable called `divA` representing the top level div node. `divA.querySelectorAll('.a')` will return a list of the 3 `span.a`s. I wonder if there's an easy way to return a list of 4 elements including the divA itself?\n\n\nI know I could start from a higher level node, but let's assume there might be other `.a` elements that I don't want to mess with them.\n\n\nIn reality I still need to test whether `divA` matches my selector or not. So is there a way for css selector to test an element itself?\n\n\nI could create a parent node and run querySelectorAll from there. But if there's an easier way, I don't need to go that far.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"\n> \n> I still need to test whether divA matches my selector or not. Is there\n>  a way for css selector to test an element itself?\n> \n> \n> \n\n\n`querySelector()` cannot return the context element it's running on.\n\n\nWhat you can do is use @Andreas' solution followed by a `filter()`\/`matches()` combo.\n\n\n\n```\n[divA, ...divA.querySelectorAll('.a')].filter(el => el.matches('.a'));\n\n```\n\n"}
{"questionId":"6d96517b00fb4d69aafe01f9b49ba673","question":"How to make onclick function execute only once?\nI have this code for Google analytics on a button. I need it to be executed only once, so that the user can't change statistics by pressing the button many times. I tried solutions from similar topics, but they don't work. Please help. This is my code.\n\n\n\n```\n<script>\n    function klikaj(i) {\n        gtag('event', 'first-4', {\n            'event_category' : 'cat-4',\n            'event_label' : 'site'\n        });\n    }\n\n    document.body.addEventListener(\"click\", klikaj(i), {once:true})\n<\/script>\n\n\n<div id=\"thumb0\" class=\"thumbs\" onclick=\"klikaj('rad1')\">My button<\/div>\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"you could use `removeAttribute()` like this: `document.getElementById('thumb0').removeAttribute(\"onclick\");`\n\n\nor you could let the function return false like this: `document.getElementById('thumb0').onclick = ()=> false`\n\n\n"}
{"questionId":"07f85b3f40c7461c9e65ae977a1aabe4","question":"How do print the console output of the page in puppeter as it would appear in the browser?\nI keep seeing this **WRONG CODE**\n\n\n\n```\npage.on('console', msg => console.log(msg.text()));\n\n```\n\nThat **FAILS**\n\n\n\n```\nconsole.log('Hello %s', 'World');\n\n```\n\nproduces\n\n\n\n```\nHello World     \/\/ browser\nHello %s World  \/\/ puppeteer\n\n```\n\nOk, So I thought maybe I could do this\n\n\n\n```\npage.on('console', msg => console.log(...msg.args()));\n\n```\n\nNOPE: That dumps out some giant `JSHandle` thing.\n\n\nOk, So maybe\n\n\n\n```\npage.on('console', msg => console.log(...msg.args().map(a => a.toString());\n\n```\n\nNOPE: That prints\n\n\n\n```\nJSHandle: Hello %s JSHandle: World\n\n```\n\nI suppose I can hack it by removing the first 9 characters.\n\n\nI also tried\n\n\n\n```\npage.on('console', msg => console.log(...msg.args().map(a => a.jsonValue())));\n\n```\n\nNOPE: That prints\n\n\n\n```\nPromise { <pending> } Promise { <pending> }\n\n```\n\nOkay how about\n\n\n\n```\npage.on('console', async(msg) => {\n  const args = Promise.all(msg.args().map(a => a.jsonValue()));\n  console.log(...args);\n});\n\n```\n\nNope, That prints \n\n\n\n```\nUnhandledPromiseRejectionWarning: TypeError: Found non-callable @@iterator\nUnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 95)\n\n```\n\nAnother go\n\n\n\n```\npage.on('console', async(msg) => {\n  const args = Promise.all(msg.args().map(async(a) => {\n    return await a.jsonValue();\n  }));\n  console.log(...args);\n});\n\n```\n\nsame as before\n\n\n\n```\nUnhandledPromiseRejectionWarning: TypeError: Found non-callable @@iterator\nUnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a \n\n```\n\nI even traced through `msg.text()` but all it does is return a premade string so it's too late to use that. \n\n\nHow do I get the same output as the browser's console.log in puppeteer?\n\n\n\n\n---\n\n\nPS: as mentioned above, this hack works in puppeteer 1.20.0\n\n\n\n```\n  page.on('console', (msg) => {\n    console.log(...msg.args().map(v => v.toString().substr(9)));\n  });\n\n```\n\nbut it's clearly a hack and I expect it will break at some point so looking for the correct solution.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"\n```\n  page.on('console', async e => {\n    const args = await Promise.all(e.args().map(a => a.jsonValue()));\n    console.log(...args);\n  });\n\n```\n\nor\n\n\n\n```\n  page.on('console', async e => {\n    const args = await Promise.all(e.args().map(a => a.jsonValue()));\n    console[e.type() === 'warning' ? 'warn' : e.type()](...args);\n  });\n\n```\n\nworks\n\n\n"}
{"questionId":"547a927c006f42c4b0103c20bd97f090","question":"Sequelize: Naming collision between attribute 'playlist' and association 'playlist'?\nI am using node.js, Sequelize and MariaDB and I am running into the following error, which I am not sure how to resolve?\n\n\n\n> \n> Error: Naming collision between attribute 'playlist' and association\n>  'playlist' on model playlist\\_entry. To remedy this, change either foreignKey\n>  or as in your association definition\n> \n> \n> \n\n\nMy Javascript:\n\n\n\n```\nEntities = function (settings, context) {\n\n    sequelize = context.sequelize;\n\n    var entities = {\n\n        Playlist: this.sequelize.define('playlist', {\n            name: Sequelize.STRING,\n            description: Sequelize.STRING\n        }),     \n\n        PlaylistEntry: this.sequelize.define('playlist_entry', {\n            playlist: Sequelize.INTEGER\n            \/\/track: Sequelize.INTEGER\n        })\n\n    };  \n\n     entities.PlaylistEntry.belongsTo(\n         entities.Playlist,\n         { foreignKey: { name: 'fk_playlist' }});\n\n    return entities;                    \n}\n\n```\n\nMy tables:\n\n\n\n```\nCREATE TABLE `playlist` (\n  `id` int(11) unsigned NOT NULL,\n  `name` varchar(255) NOT NULL,\n  `description` varchar(255) DEFAULT NULL,\n  `createdAt` timestamp NULL DEFAULT NULL,\n  `updatedAt` timestamp NULL DEFAULT NULL,\n  `external_id` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `id` (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\nCREATE TABLE `playlist_entry` (\n  `id` int(11) unsigned NOT NULL AUTO_INCREMENT,\n  `playlist` int(11) unsigned DEFAULT NULL,\n  `track` int(11) unsigned DEFAULT NULL,\n  `createdAt` timestamp NULL DEFAULT NULL,\n  `updatedat` timestamp NULL DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `track_idx` (`track`),\n  KEY `playlist_idx` (`playlist`),\n  CONSTRAINT `fk_playlist` FOREIGN KEY (`playlist`) REFERENCES `playlist` (`id`) ON DELETE NO ACTION ON UPDATE NO ACTION\n) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"I faced exactly this problem while play with Sequelize, It's happened because the `column name` and `reference name` are same\n\n\n**Wrong Implementation**\n\n\n\n```\n\nmodule.exports = (sequelize, DataTypes) => {\n  const session = sequelize.define('session', {\n    menteeId: DataTypes.INTEGER,\n  }, {});\n\n  session.associate = (models) => {\n    session.belongsTo(models.user, {\n      foreignKey: 'menteeId',\n      as: 'menteeId',\n      onDelete: 'CASCADE',\n    });\n  };\n  return session;\n};\n\n\n```\n\nHere the `column name` (`menteeId`) and `alias name` (`menteeId`) are the same, To resolve this you just need to change `alias name` \n\n\n**Correct Implementation**\n\n\n\n```\n\nmodule.exports = (sequelize, DataTypes) => {\n  const session = sequelize.define('session', {\n    menteeId: DataTypes.INTEGER,\n  }, {});\n\n  session.associate = (models) => {\n    session.belongsTo(models.user, {\n      foreignKey: 'menteeId',\n      as: 'MenteeId', \/\/ Changes applied here\n      onDelete: 'CASCADE',\n    });\n  };\n  return session;\n};\n\n\n```\n\nIn your case, you can do this\n\n\n\n```\nentities.PlaylistEntry.belongsTo(\n    entities.Playlist,\n    { \n    foreignKey: { name: 'fk_playlist' },\n    as: 'PlayListAlias', \/\/ Appropriate name\n    },\n);\n\n```\n\n"}
{"questionId":"d7777a4282274a368c54ed8ed8571641","question":"Is it possible to use different angular elements built with different versions of Angular\nI would like to know if it is possible to use different angular elements (custom elements) built with different versions of Angular.\nI have heard that zone.js was polluting the global scope.\n\n\nThanks for your answer.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"Yes, you have heard it correct. We cannot use multiple angular elements if each angular element created from a specific version is trying to load zonejs.\n\n\nHaving said that it is 100% possible to have multiple angular elements of different versions on a single page. All we need to take care of is loading zone js only once and sharing it across all the web-components(Angular Elements).\n\n\nWhile bootstrapping multiple elements we can add the logic of not loading\/patching zonejs if already loaded as below:\n\n\n**Remove zonejs polyfill from polyfill.ts for all Angular Elements**\n\n\n**Create a file in `main.ts` level. Let's say bootstraper.ts :**\n\n\n\n```\nexport class Bootstrapper {\n  constructor(\n    private bootstrapFunction: (bootstrapper: Bootstrapper) => void\n  ) {}\n\n  \/**\n   * Before bootstrapping the app, we need to determine if Zone has already\n   * been loaded and if not, load it before bootstrapping the application.\n   *\/\n  startup(): void {\n    console.log('NG: Bootstrapping app...');\n\n    if (!window['Zone']) {\n      \/\/ we need to load zone.js\n      console.group('Zone: has not been loaded. Loading now...');\n      \/\/ This is the minified version of zone\n      const zoneFile = `\/some\/shared\/location\/zone.min.js`;\n\n      const filesToLoad = [zoneFile];\n\n      const req = window['require'];\n      if (typeof req !== 'undefined') {\n        req(filesToLoad, () => {\n          this.bootstrapFunction(this);\n          console.groupEnd();\n        });\n      } else {\n        let sequence: Promise<any> = Promise.resolve();\n        filesToLoad.forEach((file: string) => {\n          sequence = sequence.then(() => {\n            return this.loadScript(file);\n          });\n        });\n\n        sequence.then(\n          () => {\n            this.bootstrapFunction(this);\n            console.groupEnd();\n          },\n          (error: any) => {\n            console.error('Error occurred loading necessary files', error);\n            console.groupEnd();\n          }\n        );\n      }\n    } else {\n      \/\/ zone already exists\n      this.bootstrapFunction(this);\n    }\n  }\n\n  \/**\n   * Loads a script and adds it to the head.\n   * @param fileName\n   * @returns a Promise that will resolve with the file name\n   *\/\n  loadScript(fileName: string): Promise<any> {\n    return new Promise(resolve => {\n      console.log('Zone: Loading file... ' + fileName);\n      const script = document.createElement('script');\n      script.src = fileName;\n      script.type = 'text\/javascript';\n      script.onload = () => {\n        console.log('\\tDone');\n        resolve(fileName);\n      };\n      document.getElementsByTagName('head')[0].appendChild(script);\n    });\n  }\n}\n\n```\n\nAnd in `main.ts` we can change the bootstrap logic to the below one :\n\n\n\n```\nimport { enableProdMode } from '@angular\/core';\nimport { platformBrowserDynamic } from '@angular\/platform-browser-dynamic';\n\nimport { AppModule } from '.\/app\/app.module';\nimport { Bootstrapper } from '.\/bootstraper';\nconst bootstrapApp = function(): void {\n  platformBrowserDynamic()\n    .bootstrapModule(AppModule)\n    .then(() => {})\n    .catch(err => console.error(err));\n};\n\nconst bootstrapper = new Bootstrapper(bootstrapApp);\nbootstrapper.startup();\n\n```\n\nThis way we can definitely create multiple Angular Elements (Web Components) and use in a SPA.\n\n\nThanks\n\n\n"}
{"questionId":"826cc39b303349cab756eb5889102867","question":"Please change the parent  to \nI'm getting this warning in React app:\n\n\n\n```\nYou rendered descendant <Routes (or called `useRoutes()`) at \"\/\" (under <Route path=\"\/\">) \nbut the parent route path has no trailing \"*\". This means if you navigate deeper, \nthe parent won't match anymore and therefore the child routes will never render.\n\nPlease change the parent <Route path=\"\/\"> to <Route path=\"*\">.\n\n```\n\nHere is my code:\n\n\n\n```\n<Router>\n        <Routes>\n            <Route exact path=\"\/login\" element={<Login \/>} \/>\n\n            <Route exact path=\"\/\" element={<AppBody \/>} >\n              <Route exact path=\"\/add-edit-profile\" element={<PageContent \/>} \/>\n              <Route exact path=\"\/profile-list\" element={<ProfileList \/>} \/>\n              \n            <\/Route>\n        <\/Routes>\n    <\/Router>\n\n```\n\nAppBody.js:\n\n\n\n```\n                <Sidebar\/>\n                <div className='page-content'>\n                    <Header \/>\n                <\/div>\n                \n                <Routes>\n                    <Route exact path=\"\/add-edit-profile\" element={<PageContent \/>} \/>\n                    <Route exact path=\"\/profile-list\" element={<ProfileList \/>} \/>\n                    \n                <\/Routes>\n                \n\n\n```\n\nWhat I've to change in my code to fix the warning?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"It means that `AppBody` is rendering more deeply nested routes and the path needs to specify the wildcard `*` character to indicate it can match more generic\/nested paths. `react-router-dom` route paths are ***always*** exactly matched, so if sub-routes are rendered the path needs to allow for them. Change `path=\"\/\"` to `path=\"\/*\"`.\n\n\nSince `AppBody` is rendering the routes and no `Outlet` for the nested `Route` components, they can be safely removed.\n\n\n\n```\n<Router>\n  <Routes>\n    <Route exact path=\"\/login\" element={<Login \/>} \/>\n    <Route exact path=\"\/*\" element={<AppBody \/>} > \/>\n  <\/Routes>\n<\/Router>\n\n```\n\n"}
{"questionId":"153caa702ffd46ecbf210496ff7c07ba","question":"Error: Exception in HostFunction: Malformed calls from JS: field sizes are different. In an Animated View\nAs I understand this error can occur in a number of different use cases. Here is what happened in this use case\n\n\n- An Animated View is being controlled by a `PanResponder` and this is reset at certain intervals to create an infinite scroll effect.\n- Compiles and runs perfectly and functions as intended.\n- Small gestures (almost like a tap) ie. pixel movements of about +- 4dx\/ 4 dy the code crashes with the error in the title.\n\n\nThe error is thrown in the Child View of the `PanResponsder` with the mismatch resulting from the `translate: [{transform}]` I believe.\n\n\nWhy does the code function fine except for smaller gestures? What casuses the error?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"I ended up resolving the issue.\nIn this case, it was specific to a `PanResponder`, but I believe this can occur in other situations as well, the error tracking should be similar. A **moveY** variable on the `PanResponder` went beyond a threshold set elsewhere. This resulted in `translateY` being set to **NaN** which threw the above error. This results in a mismatch in props.\n\n\n1. If others experience this issue my advice would be to identify the specific component experiencing the mismatch. (In this case `PanResponder`)\n2. Isolate the props (set defaults\/ dummy values) and ensure that each prop is resolved correctly (especially in `Animated` `transform`: `translatex`\/`translateY` )\n3. Trace the prop responsible and adjust the logic specific to that prop to avoid **NaNs** and **undefined** being passed.\n\n\n"}
{"questionId":"73e303c3ecb74bec83a2e6375ac5e15d","question":"How to update ApolloClient authorization header after successful login?\nBasically, we have this in our `index.js` file to set-up the `ApolloProvider` authorization to make queries \/ mutations.\n\n\n\n```\nimport React from 'react';\nimport ReactDOM from 'react-dom';\n\nimport App from '.\/App';\n\nimport ApolloClient from \"apollo-boost\";\nimport { ApolloProvider } from \"react-apollo\";\n\nlet session = localStorage.getItem(\"session\");\nlet authorization = \"\";\nif(session) {\n    let sessionObj = JSON.parse(session);\n    authorization = sessionObj.access_token\n}\n\nconst graphQLServerURL = process.env.REACT_APP_API_URL;\nconst client = new ApolloClient({\n    uri: graphQLServerURL + \"\/graphql\",\n    headers: {\n       authorization,\n    }\n});\n\nReactDOM.render(\n    <ApolloProvider client={client}>\n        <App \/>\n    <\/ApolloProvider>\n    , document.getElementById('root'));\n\n```\n\nWhen the app first loads, the `authorization` header would be `null`. However, within the `<App>` component, we have a `<Login>` component which basically does a post request with a `username` and `password`. Upon successful request in the `.then()` method, we have:\n\n\n\n```\n.then(res => {\nif (res === 200) {\n    localStorage.setItem(\"session\", JSON.stringify(res.data));\n    history.push(\"\/dashboard\");\n});\n\n```\n\nSo what happens is the user is redirected to a `<Dashboard>` component which has a `<Query>` component (to list some data). However, the `authorization` in `ApolloClient` is still `null` until I hit refresh. Using `push` doesn't reload the `<App>` component (so that it gets the updated `session` from localstorage).\n\n\nHow should I do this in a way that after successful post request on login, the authorization from `index.js` gets the latest `session` object without having to reload the entire application?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"You can use the `request` function if you use `apollo-boost`\n\n\n\n```\nconst getToken = () => {\n  const token = localStorage.getItem('token');\n  return token ? `Bearer ${token}` : '';\n};\n\nconst client = new ApolloClient({\n  uri: `${graphQLServerURL}\/graphql`,\n  request: (operation) => {\n    operation.setContext({\n      headers: {\n        authorization: getToken(),\n      },\n    });\n  },\n});\n\n```\n\n"}
{"questionId":"d365b50793ff4fe4b68fc2118fd93ebf","question":"Find all values by specific key in a deep nested object\nHow would I find all values by specific key in a deep nested object?\n\n\nFor example, if I have an object like this:\n\n\n\n```\nconst myObj = {\n  id: 1,\n  children: [\n    {\n      id: 2,\n      children: [\n        {\n          id: 3\n        }\n      ]\n    },\n    {\n      id: 4,\n      children: [\n        {\n          id: 5,\n          children: [\n            {\n              id: 6,\n              children: [\n                {\n                  id: 7,\n                }\n              ]\n            }\n          ]\n        }\n      ]\n    },\n  ]\n}\n\n```\n\nHow would I get an array of all values throughout all nests of this obj by the key of `id`.\n\n\nNote: `children` is a consistent name, and `id`'s won't exist outside of a `children` object.\n\n\nSo from the obj, I would like to produce an array like this:\n\n\n\n```\nconst idArray = [1, 2, 3, 4, 5, 6, 7]\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"You could make a recursive function like this:\n\n\n\n```\nidArray = []\n\nfunction func(obj) {\n  idArray.push(obj.id)\n  if (!obj.children) {\n    return\n  }\n\n  obj.children.forEach(child => func(child))\n}\n\n```\n\nSnippet for your sample:\n\n\n\n\n\n```\nconst myObj = {\r\n  id: 1,\r\n  children: [{\r\n      id: 2,\r\n      children: [{\r\n        id: 3\r\n      }]\r\n    },\r\n    {\r\n      id: 4,\r\n      children: [{\r\n        id: 5,\r\n        children: [{\r\n          id: 6,\r\n          children: [{\r\n            id: 7,\r\n          }]\r\n        }]\r\n      }]\r\n    },\r\n  ]\r\n}\r\n\r\nidArray = []\r\n\r\nfunction func(obj) {\r\n  idArray.push(obj.id)\r\n  if (!obj.children) {\r\n    return\r\n  }\r\n\r\n  obj.children.forEach(child => func(child))\r\n}\r\n\r\nfunc(myObj)\r\nconsole.log(idArray)\n```\n\n\n\n\n\n\n"}
{"questionId":"e71ead5029474d55ac2d901643cedab5","question":"Should I use console.error() or throw new Error()\nI've seen both:\n\n\n\n```\nthrow new Error(error);\n\n```\n\n&\n\n\n\n```\nconsole.error(error);\n\n```\n\n**E.G:** \n\n\n**jQuery**: \n\n\n\n```\n                if ( !w.document ) {\n                    throw new Error( \"jQuery requires a window with a document\" );\n                }\n\n```\n\n&  \n\n**Vue.js**:\n\n\n\n```\n      if (config.warnHandler) {\n        config.warnHandler.call(null, msg, vm, trace);\n      } else if (hasConsole && (!config.silent)) {\n        console.error((\"[Vue warn]: \" + msg + trace));\n      }\n\n```\n\nBoth error handling ways seem reliable and used. But my question is: \n\n\n\n> \n> Is there a difference between them? And if there is, when should I use which?\n> \n> \n> \n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"javascript"},"answer":"The key difference: throwing halts the execution, while `console.error` does not.\n\n\nMost of the time, it's better to throw an error.\n\n\nThat's a built-in way to signal that something failed and normal execution cannot continue unless the error is expected, caught and handled properly.\n\n\nIn the most platforms, an uncaught exception will be logged to the console as well to warn the developer about it, but caught exceptions won't be logged, since they are assumed to be handled by the code.\n\n\nUsing `console.error` can be good for cases where the error happened isn't fatal, but you'd like to warn the developer.\n\n\nHowever, overusing this feature can easily cause other errors and harder-to-debug code. For example, consider this code:\n\n\n\n```\nconst elem = document.querySelector('.elem')\nif(!elem) \n  console.error('elem cannot be found!')\nconst returnValue = functionThatDoesSomethingWithElem(elem)\nif(!returnValue.success) \n  console.error('Doing something with elem has failed!')\nif(!returnValue.doSomethingElse()) \n  console.error('Doing something else with elem has failed!')\n\n```\n\nThe above code will log three errors if there's no elem, but execution still continues, maybe causing even more errors.\n\n\nBy throwing an exception, this is avoidable:\n\n\n\n```\nconst elem = document.querySelector('.elem')\nif(!elem) \n  throw new Error('elem cannot be found!')\nconst returnValue = functionThatDoesSomethingWithElem(elem)\nif(!returnValue.success) \n  throw new Error('Doing something with elem has failed!')\nif(!returnValue.doSomethingElse()) \n  throw new Error('Doing something else with elem has failed!')\n\n```\n\nThis will print only the first error message, and execution halts, unless you put it inside a `try..catch` structure, like:\n\n\n\n```\ntry{\n  const elem = document.querySelector('.elem')\n  if(!elem) \n    throw new Error('elem cannot be found!')\n  const returnValue = functionThatDoesSomethingWithElem(elem)\n  if(!returnValue.success) \n    throw new Error('Doing something with elem has failed!')\n  if(!returnValue.doSomethingElse()) \n    throw new Error('Doing something else with elem has failed!')\n}catch(exception){\n  console.error(exception)\n  fallbackMethod()\n}\n\n```\n\n\n\n---\n\n\nThere's another difference: `throw`n errors can be caught by the caller of your function, so it can programmatically handle them (causing the execution to continue, and the error isn't displayed). On the other hand, if you use `console.error`, the caller can **not** decide if an error is expected, causing to log the error even if it is normal, so the console can become messy (you can't decide what's a real error and what's not.\n\n\n"}
{"questionId":"dc6bf69379d84dc0b0d1f706888d54ec","question":"How to access current value in a text input field with puppeteer\nI'm trying to automate retrieving form values from *an already filled out form* with puppeteer and xpath.\n\n\nI've already automated FILLING a text input field as follows, but doing the reverse with .evaluate() doesn't work:\n\n\n\n```\n[fieldHandle] = await page.$x(\"\/\/label[text() = 'My Label']\/..\/following-sibling::td[1]\/\/input\")\nawait page.evaluate((x, y) => x.value = y, fieldHandle, 'newValue')\n\n```\n\nThis is my most recent attempt - still no success...\n\n\n\n```\nlet [fieldHandle] = await page.$x(\"\/\/label[text() = 'My Label']\/..\/following-sibling::td[1]\/\/input\")\nlet fieldRaw = await fieldHandle.getProperty('textContent')\nlet fieldValue = await fieldRaw.jsonValue()\n\n```\n\nHopefully someone out there knows how to achieve this!\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"Using evaluate should work:\n\n\n\n```\nconsole.log(await page.evaluate(x => x.value, fieldHandle)));\n\n```\n\n"}
{"questionId":"62f5682986314c42aec1bf60401f13d0","question":"Jest global teardown runs before tests finish?\nI'm trying to make sure my app gets properly destroyed after all my Jest tests have run, but I'm running into some very strange behaviour trying to use Jest's global teardown config value.\n\n\nHere's the situation: my app creates a database connection. It also has a `destroy` method that closes the database connection. This works.\n\n\nI have a single test that starts the server, runs a query against the database connection. In my global teardown function, I call `app.destroy()`, but the process hangs.\n\n\nIf I comment out the `destroy` call in the global teardown function and put `app.destroy()` in my test after the query, Jest doesn't hang and closes like it's supposed to. I can also put `afterAll(() => app.destroy())` in my test file and things work properly.\n\n\nHere is my `jest.config.js`\n\n\n\n```\nmodule.exports = {\n  testEnvironment: 'node',\n  roots: [\n    '<rootDir>\/src'\n  ],\n  transform: {\n    '^.+\\\\.tsx?$': 'ts-jest'\n  },\n  testRegex: '(\/__tests__\/.*|(\\\\.|\/)(test|spec))\\\\.tsx?$',\n  moduleFileExtensions: [\n    'ts',\n    'tsx',\n    'js',\n    'jsx',\n    'json',\n    'node'\n  ],\n  globalSetup: '<rootDir>\/src\/testSetup.ts',\n  globalTeardown: '<rootDir>\/src\/testTeardown.ts'\n};\n\n```\n\nHere is the test (it's currently the only test in the app):\n\n\n\n```\nimport app from '..\/..';\n\ndescribe('User router', () => {\n  it('Should respond with an array of user objects', async () => {\n    await app.models.User.query();\n  });\n});\n\n```\n\nAnd here is the global teardown in `<rootDir>\/src\/testTeardown.ts`:\n\n\n\n```\nimport app from '.\/index';\n\nmodule.exports = async function testTeardown() {\n  await app.destroy();\n};\n\n```\n\nUsing the code above, the process hangs after tests finish. I've tried adding a `console.log` to `testTeardown` and the end of the test, and the logs happen in the correct order: test logs, then teardown logs. However if I move `app.destroy()` into my test it works perfectly:\n\n\n\n```\nimport app from '..\/..';\n\ndescribe('User router', () => {\n  it('Should respond with an array of user objects', async () => {\n    await app.models.User.query();\n    await app.destroy();\n  });\n});\n\n```\n\nThis also works:\n\n\n\n```\nimport app from '..\/..';\n\nafterAll(() => app.destroy());\n\ndescribe('User router', () => {\n  it('Should respond with an array of user objects', async () => {\n    await app.models.User.query();\n  });\n});\n\n```\n\nWhy is this happening?\n\n\nAlso just for shits and giggles I tried setting a `global._app` in the test and then checking it in the teardown handler, but it was `undefined`. Do Jest's setup\/teardown functions even get run in the same process as the tests?\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"javascript"},"answer":"No, jest globalSetup and globalTeardown files don't necessarily get run in the same process as your tests. This is because jest parallelises your tests and runs each test file in a separate process, but there is only one global setup\/teardown phase for the combined set of test files.\n\n\nYou can use `setupFiles` to add a file that gets run in process with each test file. In the `setupFiles` file you can put:\n\n\n\n```\nafterAll(() => app.destroy());\n\n```\n\nYour jest config is just\n\n\n\n```\nmodule.exports = {\n  testEnvironment: 'node',\n  roots: [\n    '<rootDir>\/src'\n  ],\n  transform: {\n    '^.+\\\\.tsx?$': 'ts-jest'\n  },\n  testRegex: '(\/__tests__\/.*|(\\\\.|\/)(test|spec))\\\\.tsx?$',\n  moduleFileExtensions: [\n    'ts',\n    'tsx',\n    'js',\n    'jsx',\n    'json',\n    'node'\n  ],\n  setupFiles: ['<rootDir>\/src\/testSetup.ts']\n};\n\n```\n\n"}
{"questionId":"83ddd307fa8e44cb94ea0aa464c2c06e","question":"accessing changed value of exported variable\nI have a module \"a\" which exports a variable initiated as `null`. This variable is imported into module \"b\".\n\n\nFollowing some changes to the initial variable, I try to access it again from module \"b\", only to find I get the original `null` value.\n\n\nAren't these variables being imported as reference? Meaning, they should reflect any changes made to them at a later point in runtime.\n\n\n\n```\n\/\/ main.js\nimport * as a from '.\/a.js'\nimport * as b from '.\/b.js'\n\n\/\/ a.js\nlet test = null\nexport default test\n\nexport function change() {\n  test = 'test'\n  console.log(['in a.js change()', test])\n}\nconsole.log(['in a.js global', test])\n\n\/\/ b.js\nimport test, { change } from '.\/a.js'\n\nconsole.log(['in b.js, pre a.change()', test])\nchange()\nconsole.log(['in b.js, post a.change()', test])\n\n\n\/*\noutput:\nArray [ \"in a.js global\", null ]\nArray [ \"in b.js, pre a.change()\", null ]\nArray [ \"in a.js change()\", \"test\" ]\nArray [ \"in b.js, post a.change()\", null ]   WHY ISN'T THIS = \"test\" ?\n*\/\n\n```\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"javascript"},"answer":"\n> \n> Aren't these variables being imported as reference? Meaning, they should reflect any changes made to them at a later point in runtime.\n> \n> \n> \n\n\nNo. When you imported the variable, you made a copy of it into a new variable. That copy will not change if the original variable gets something new assigned to it.\n\n\nWhen you do this:\n\n\n\n```\nimport test, { change } from '.\/a.js'\n\n```\n\nYou're assigning the exported default value into a new variable named `test`. That new variable has no connection with the other variable any more.\n\n\nThere are several ways to provide access to the changed variable:\n\n\n1. **Export an object where the variable is a property on that object.** Then, in your original module, change the value of that property. In this case, the imported module will have a pointer to the same object so when you access the property on that object, you will see the new value.\n2. **Export a method** that retrieves the current value of the variable from within the module.\n3. **Create an event** which an outside module can listen for and fire that event anytime the value of the variable changes.\n\n\nRemember that plain values are assigned by making a copy of the value and inserting it into the new variable. Objects, on the other hand, are assigned by making a copy of the pointer to the same object and putting that pointer into the new variable. So, with objects the new variable contains a pointer to the same object.\n\n\n"}
{"questionId":"d933a0d6804b41c8814f22993c25e5d4","question":"useState and callback function\nIn the class component, the `setState()` method can take a callback function, but in a functional component when I give a callback to costume setState this warning occurs:\nWarning: State updates from the `useState()` and `useReducer()` Hooks don't support the second callback argument. To execute a side effect after rendering, declare it in the component body with `useEffect()`.\nI need my state set, and then the page will redirect. But I don't have any idea.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"Instead of passing a `callback` function, use `useEffect` hook, and do something like this to achieve the desired result.\n\n\n\n```\n useEffect(() => {\n    console.log('state changed', your-state-variable)\n    \/\/ write your callback function here\n  }, [your-state-variable]);\n\n```\n\n"}
{"questionId":"13dc39bb312345689217408846d27c13","question":"How to Reverse a string with numbers, but don't reverse 1 and 0?\nI am learning random algorithms, and I am currently stock in one, where I have to reverse a string that contains numbers, but I am not to reverse 1 and 0 in the string e.g, 2345678910 would be 1098765432.\n\n\nHere's what I've done so far:\n\n\n\n\n\n```\nfunction split(str) {\r\n  let temp = [];\r\n  temp = str.split('');\r\n  const backwards = [];\r\n  const totalItems = str.length - 1;\r\n  for (let i = totalItems; i >= 0; i--) {\r\n    backwards.push(temp[i]);\r\n\r\n  }\r\n  return backwards.join('').toString();\r\n\r\n}\r\nconsole.log(split(\"10 2 3 U S A\"));\r\nconsole.log(split(\"2345678910\"));\n```\n\n\n\n\n\n\nI am currently having the issue of not reversing the 10. \n\n\nWhat am I doing wrong?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"You can replace `10` with a specified character which does not exist in the text, and after running the implemented algorithm replace it back with `10`.\n\n\n\n\n\n```\nlet out_of_alphabet_character = '#';\r\nvar reg_for_the_alphabet = new RegExp(out_of_alphabet_character, \"g\");\r\n\r\nfunction specific_revert(str) {\r\n  str = str.replace(\/(10)\/g, out_of_alphabet_character);\r\n  let temp = [];\r\n  \r\n  temp = str.split('');\r\n  const backwards = [];\r\n  const totalItems = str.length - 1;\r\n  for (let i = totalItems; i >= 0; i--) {\r\n    backwards.push(temp[i]);\r\n  }\r\n  return backwards.join('').toString().replace(reg_for_the_alphabet, '10');\r\n}\r\nconsole.log(specific_revert(\"10 2 3 U S A\"));\r\nconsole.log(specific_revert(\"234567891010\"));\n```\n\n\n\n\n\n\n"}
{"questionId":"7bfd4fab336d4592a20974f62c02ad69","question":"How to change nested property of an object using spread operator?\nThis is a clean version of the my situation:\n\n\n\n```\nconst person1 = {\n    name: \"Person1 Name\",\n    hairColor: \"Brown\",\n\n    backpack: {\n        color: \"Army-Green\",\n        content: [\n            \"item1\",\n            \"item2\",\n            \"...\"\n        ]\n    }\n}\n\n```\n\nAnd I'm trying to change only the **backpack color**\n\n\nI already tried this code below but not success:\n\n\n\n```\nperson = {...person1, backpack.color: \"New backpack color\"}};\n\nperson = {...person1, backpack: {...backpack, color: \"New backpack color\"}};\n\nperson = {...person1, backpack: {color: \"New backpack color\"}};\n\nperson = {...person1, backpack = {...backpack, color: \"New backpack color\"}};\n\n```\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"javascript"},"answer":"\n```\nconst person2 = {...person1, backpack: {...person1.backpack, color: 'Red' }}\n\n```\n\n"}
{"questionId":"8fca7f44945441689d181f2252aae71e","question":"Invalid key length in crypto.createCipheriv\nI generated a base64-encoded key using this code in NodeJS v8.11.0:\n\n\n\n```\nconst secret = 'shezhuansauce';\nconst key = crypto.createHash('sha256').update(String(secret)).digest('base64');\n\/\/output is REtgV24bDB7xQYoMuypiBASMEaJbc59nJWChoXbbmsA=\n\n```\n\nUsing the key, I try to encrypt a string:\n\n\n\n```\nvar tobeEncrypted = 'some secret string';\nconst iv = crypto.randomBytes(16).toString('hex').slice(0, 16);\nconst cipher = crypto.createCipheriv('aes-256-ctr', key, iv);\nconst encrypted = cipher.update(String(tobeEncrypted), 'utf8', 'hex') + cipher.final('hex');\nconsole.log(encrypted);\n\n```\n\nHowever, I received an error: \n\n\n\n```\ncrypto.js:219\nthis._handle.initiv(cipher, toBuf(key), toBuf(iv));\n           ^\nError: Invalid key length\n\n```\n\nThe key needs to be base64 string as I will store it in a Cloud service and it only receives base64 string.\n\n\nAny help is appreciated.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"You said you stored a key in BASE 64 and the key is 256 bits (or 32 bytes) (which we see that you computed sha256), so simply get that base64 key, then you can get the bytes easily like this:\n\n\n\n```\nconst key_in_bytes = Buffer.from(BASE_64_KEY, 'base64')\n\n```\n\nAnd you can use this key in bytes as your key as:\n\n\n\n```\nconst cipher = crypto.createCipheriv('aes-256-ctr', key_in_bytes, iv);\n\n```\n\n"}
{"questionId":"40fadc4ae3ea494ba4ba834f84f0bf5c","question":"what is difference between strapi content type and component?\nI'm new user of strapi. While reading docs I got confused between content type and component.What is the difference between these two?\n\n\n","questionMetadata":{"type":"conceptual","level":"beginner","tag":"javascript"},"answer":"A **Content Type** is a data structure that is used as a collection of specific content.\n\n\nCould be Articles, Restaurants, Products, etc...\n\n\nA **Component** is a data structure (also) that could be used and re-used in many different **Content Type**. \n\n\nExample if you have Article and Product **Content Types** in your app, and you want to manage SEO tags for both of these **Content Types** you will create a SEO **Component** and use it in your Article AND Product **Content Type**\n\n\nYou will not have to create the same SEO data structure in both **Content Types**.\n\n\n"}
{"questionId":"5839220293cb43a080d4c950f2050b7b","question":"How to select all elements with a class in JS\nI'd like to modify all classes with JS.\nIs there way to select them without manually setting the array index (for example [0] or [1] or [184])?\n\n\nExample code:\n\n\n\n```\n<div class='message'>something:<\/div>\n<div class='message'>something<\/div>\n\n```\n\n\n```\nconst element = document.querySelectorAll(\".message\");\nelement.classList.add(\"color\");\n\n```\n\nIt works only when I add `[0]` and only for the first element that has the class.\nBut I'd like to modify all the elements with the class.\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"javascript"},"answer":"It's important to learn what basic language syntax does first. The `[0]` is selecting the `0` index of an array (or array-like object). So to operate on them all, you can use a loop with a variable that is incremented starting at `0` and continuing until it goes out of bounds of the array.\n\n\n\n```\nfunction replaceEmotes() {\n    var messages = document.querySelectorAll(\".message\");\n    for (var i = 0; i < messages.length; i++) {\n        var str = messages[i].innerHTML.replace(\":smile:\", \"<i class='em em-smile'><\/i>\");\n        messages[i].innerHTML = str;\n    }\n}\n\n```\n\nThere are other ways too, but this is a fundamental syntax that should probably be learned first.\n\n\n"}
{"questionId":"b6041bbbd9174d719444309505923b56","question":"Having mounted() only run once on a component Vue.js\nI have two components that conditionally render with `v-if`:\n\n\n\n```\n<Element v-if=\"this.mode === 'mode'\"\/>\n<OtherElement v-if=\"this.mode !== 'mode'\"\/>\n\n```\n\nI have load-in animations for both components that I have under `mounted()`, that I only want to run the *first* time they are loaded. But with mounted, each time the component is recreated when `this.mode` changes, the animations trigger again. How can I avoid this?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"You could wrap your components within a `keep-alive` element ..\n\n\n\n```\n<keep-alive>\n    <Element v-if=\"this.mode === 'mode'\"\/>\n    <OtherElement v-else \/>\n<\/keep-alive>\n\n```\n\n"}
{"questionId":"414fc0a08b5f40748e55705a1bbc0170","question":"Multi-line string insert using jQuery\n\n```\n$(\"#addSelect\").click(function() {\n        $(\"#optionsForm\").after(\"Hello world.\");\n} );\n\n```\n\nThis works.\n\n\n\n```\n$(\"#addSelect\").click(function() {\n        $(\"#optionsForm\").after(\"<tr>\n    <td><input type=\"text\" class=\"optionField\" value=\"Options\" \/><\/td>\n    <td>\n        <ul class=\"option\">\n            <li><select><option>Value..<\/option><\/select><\/li>\n        <\/ul>\n    <\/td>\n<\/tr>\");\n} );\n\n```\n\nSomething like this doesn't. \n\n\nIn Chrome, I get the error \"**Unexpected token ILLEGAL**\". After Googling I've discovered my teeny brain doesn't know much about javascript and multi-lines. So I added '\\' to then end of each line. Yet, I now get the error \"**Unexpected identifier**\".\n\n\nI'd like this to not be as difficult as I'm making it :)\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"javascript"},"answer":"Use back ticks ` to start and finish the string\n\n\n\n```\n$(\"#addSelect\").click(function() {\r\n    $(\"#optionsForm\").after(`<tr>\r\n    <td><input type=\"text\" class=\"optionField\" value=\"Options\" \/><\/td>\r\n    <td>\r\n        <ul class=\"option\">\r\n            <li><select><option>Value..<\/option><\/select><\/li>\r\n        <\/ul>\r\n    <\/td>\r\n<\/tr>`);\r\n} );\n```\n\n\n\n\n\n\n"}
{"questionId":"569195e58a194ad2a7129e8c24bd894c","question":"Can I prevent Cypress cy.get from failing if no elements are found?\nI am using Cypress `cy.get` to grab elements, but if there are none, my test is failing.\nI do not want it to fail. I want it to continue. The test is simply to list the items that are there, if any.\n\n\n\n```\nconst listItemTitle = '[data-cy-component=list-item-title]';\ncy.get(listItemTitle).each(($el, index, $list) => {\n  cy.wrap($el).then(($span) => {\n    const spanText = $span.text();\n    cy.log(`index: ` + index + ' ' + spanText);\n  });\n});\n\n```\n\nI would have thought, if there are no elements there - that this code would still be ok, but not so. When I run it, I get this error: `CypressError: Timed out retrying: Expected to find element: '[data-cy-component=list-item-title]', but never found it.`\n\n\nIt works fine where elements are present. If no elements are found, I want to go on & do a different test.\n\n\nHere is an experiment I tried:\n\n\n\n```\nlet count: number = Cypress.$(listItemTitle).length;\ncy.log('before:' + count);\ncy.get(actionsBarActionsAdd).click();\ncy.get(singlePickerSearch).type('Assets' + '{enter}');\ncy.get(listItemCheckboxTitle)\n  .first()\n  .click();\ncy.toast({\n  type: 'Success',\n});\ncount = Cypress.$(listItemTitle).length;\ncy.log('after:' + count);\ncy.get(listItemTitle).each(($li, index, $lis) => {\n  cy.log('interface no. ' + (index + 1) + ' of ' + $lis.length);\n  cy.wrap($li).click();\n});\n\n```\n\nThis was the outcome:\n\n\n\n```\n18 LOG        before:0\n19 GET       [data-cy-component-key=\"actions-add\"] input\n20 CLICK\n21 GET       [data-cy-component=single-picker-search] input\n22 TYPE      Assets{enter}\n23 GET       [data-cy-component=\"list-item-checkbox-title\"]\n24 FIRST\n25 CLICK\n26 GET       .iziToast    toast2\n27 ASSERT    expected [ <div.iziToast.iziToast-opening.fadeInUp.iziToast-theme- \nalloy.iziToast-color-green.iziToast-animateInside>, 1 more... ] to have class iziToast-color-green\n28 LOG       after:0\n29 GET       [data-cy-component=list-item-title]\n30 LOG       interface no. 1 of 1\n\n```\n\nConclusively shows that `Cypress.$(listItemTitle).length` does not count number of elements with selector: listItemTitle.\n\n\nUpdate:\n\n\nBy putting a `cy.wait(1000)`; after the Add had been executed (in my experiment) - giving the DOM time to update - the new element was found. With more specific selectors, the wait is not required\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"You can use jquery via `Cypress.$` to check if any exist.\n\n\n\n```\nconst listItemTitle = '[data-cy-component=list-item-title]';\nif (Cypress.$(listItemTitle).length > 0) {\n  cy.get(listItemTitle).each(($el, index, $list) => {\n    cy.wrap($el).then(($span) => {\n    const spanText = $span.text();\n    cy.log(`index: ` + index + ' ' + spanText);\n    });\n  });\n}\n\n```\n\n"}
{"questionId":"036f0ad7a26c424aa451a91383816f11","question":"How to mock a delay in my api call function?\nI have a function to update an user with an api post request. The backend is not done yet. The function will thus always return an error. In order to test the loading and error states, I would like to temporarily add a fake delay before returning the result. How to do so? Here is the function:\n\n\n\n\n\n```\nconst updateProfile = async (form) => {\n  try {\n    const res = await api.post(\"\/update-profile\", form);\n    return res;\n  } catch (err) {\n    throw new Error(\"error.unknown\");\n  }\n};\n```\n\n\n\n\n\n\nWriting this didn't work:\n\n\n\n\n\n```\nconst updateProfile = async (form) => {\n  try {\n    let fakeCallDone = false\n    setTimeout(()=> fakeCallDone = true, 2000)\n    const res = await api.post(\"\/update-profile\", form);\n    fakeCallDone && res;\n  } catch (err) {\n    throw new Error(\"error.unknown\");\n  }\n};\n```\n\n\n\n\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"javascript"},"answer":"You can create a simple sleep function.\n\n\n\n```\nconst sleep = ms => new Promise(\n  resolve => setTimeout(resolve, ms));\n\n```\n\nAnd then use like\n\n\n\n```\n await sleep(2000);\n\n```\n\n"}
{"questionId":"90c11d4b1ba249568672bc460ec1c707","question":"Intercept a certain request and get its response (puppeteer)\nOnce that puppeteer goes to a certain url, I want that it listens to all the requests that are made, then find a specific request and return its response. The response should be a json object.\n\n\nI managed in listening to all the requests and intercepting the desired one, but I don't know how to get its response. Here's my attempt: I get the error `TypeError: Cannot read property 'then' of null`.\n\n\nAny suggestion? \n\n\n\n```\npage.on('request',async(request)=>{\n    console.log(request.url())\n\n    if (request.url().includes('desiredrequest.json')){\n        console.log('Request Intercepted')\n        request.response().then(response => {\n            return response.text();\n        }).then(function(data) {\n        console.log(data); \/\/ this will be a string\n        alert(data)\n        });\n    }\n\n    request.continue()\n})\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"Since the response may have not arrived yet, the better method would be listening on the `response` event and get the request object from it.\n\n\n\n```\npage.on('response', async(response) => {\n    const request = response.request();\n    if (request.url().includes('desiredrequest.json')){\n        const text = await response.text();\n        console.log(text);\n    }\n})\n\n```\n\n"}
{"questionId":"b2d835e74e1c42c1b3aa69c258d14548","question":"discord.js MessageEmbed fields.flat is not a function\nI am making a Discord bot using JavaScript and discord.js. There, I want to send a RichEmbed\/MessageEmbed (I don't know how it's called) to a channel. Instead of sending an Embed though, it threw an error inside discord.js.\n\n\n\n```\nTypeError: fields.flat is not a function\n    at Function.normalizeFields (D:\\discord-bot\\node_modules\\discord.js\\src\\structures\\MessageEmbed.js:436:8)\n    at MessageEmbed.addFields (D:\\discord-bot\\node_modules\\discord.js\\src\\structures\\MessageEmbed.js:252:42)\n    at commands.forEach.command (D:\\discord-bot\\src\\js\\core\\commands\\commandManager.js:55:19)\n    at Array.forEach (<anonymous>)\n    at helloWorldEmbed (D:\\discord-bot\\src\\js\\core\\commands\\commandManager.js:54:18)\n    at Object.call (D:\\discord-bot\\src\\js\\core\\commands\\commandManager.js:29:13)\n    at Client.client.on (D:\\discord-bot\\src\\js\\core\\bot.js:16:49)\n    at Client.emit (events.js:182:13)\n    at MessageCreateAction.handle (D:\\discord-bot\\node_modules\\discord.js\\src\\client\\actions\\MessageCreate.js:31:14)\n    at Object.module.exports [as MESSAGE_CREATE] (D:\\discord-bot\\node_modules\\discord.js\\src\\client\\websocket\\handlers\\MESSAGE_CREATE.js:4:32)\n\n```\n\nI searched already for an answer, but it seems like I'm the only person having trouble with it.\n\n\nHere's the code I used:\n\n\n\n```\nconst embed = new MessageEmbed()\n    .setTitle('Hello World')\n    .setDescription('This is a test.')\n    .setColor('#3498db')\nquotes.forEach(quote => {\n    embed.addField(quote.name, quote.description, true)\n})\nmessage.channel.send('Hello world.', embed)\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"As discussed in comments, updating Node.js solves the issue. Discord.js v12 requires 12.0.0 or newer because of the methods (like `Array#flat()` in the error) it uses for efficiency which don't exist in older versions.\n\n\n"}
{"questionId":"29fe68d11057404c97c757471885315d","question":"How to do `cy.notContains(text)` in cypress?\nI can check if text exists in cypress with `cy.contains('hello')`, but now I delete hello from the page, I want to check hello doesn't exist, how do I do something like `cy.notContains('hello')`?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"For the simple problem of checking 'hello' doesn't exist, you can use `.contains('hello')` followed a `.should()`. So it would look something like this for the whole page:\n\n\n\n```\n\/\/ code to delete hello\n\ncy.contains('.selector', 'hello').should('not.exist')\n\n```\n\nOr you can further narrow it down to a particular area of the app:\n\n\n\n```\n\/\/ code to delete hello\n\ncy.get('.element-had-hello').should('not.include.text', 'hello')\n\n```\n\n"}
{"questionId":"14702c3fd7154583ab27cefb77e983b6","question":"NestJS Injecting request or execution context in services\nHow can I inject the request or the execution context in a service?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"Since version 6.0, current request can be injected into a service, with `REQUEST` token : \n\n\n\n```\nexport class AppService {\n  constructor(@Inject(REQUEST) private request) {}\n\n  load() {\n    const user = this.request.user;\n  }\n}\n\n```\n\n"}
{"questionId":"848d57240a2944ec8703a3fac010f21b","question":"If classlist contains more than one specific class\nI need a function to trigger if the element `recordplayerstick` contains either the `pinplace` or `pinsongplay` class. The code I currently have returns a syntax error. What is the correct way to do this?\n\n\n\n```\nif (document.getElementById('recordplayerstick').classList.contains('pinplace pinsongplay')) {\n    removepin();\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"javascript"},"answer":"You are going to have to do two checks if you are going to use classList.\n\n\n\n\n\n```\nfunction removepin() {\r\n  console.log(\"yep\");\r\n}\r\nvar cList = document.getElementById('recordplayerstick').classList;\r\nif (\r\n  cList.contains('pinplace') ||\r\n  cList.contains('pinsongplay')) {\r\n  removepin();\r\n}\n```\n\n\n```\n<div id=\"recordplayerstick\" class=\"pinplace pinsongplay\"><\/div>\n```\n\n\n\n\n\n\n"}
{"questionId":"adf9e507ef2449999620a56ce4052859","question":"Open new tab with useNavigate hook in React\nI am trying to navigate with the useNavigate hook and everything is good, but I want it to open the URL in a new tab, and not the current one. Is that possible?\n\n\nMy code:\n\n\n\n```\nimport { useNavigate } from \"react-router-dom\";\n...\n...\nconst navigate = useNavigate();\n...\n...\n<Button onClick={()=>{navigate('\/someURL')}}>Open URL<\/Button>\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"you can use `window.open()` method instead of using `navigate()` to open the URL in the new tab. Pass `'_blank'` as a second argument in the function for opening it in new tab.\n\n\nImportantly!\nIf the `rel=\"noopener noreferrer\"` attribute is added to the link, the referrer information will not be leaked. The end goal is not to miss the HTTP referrer title when a person clicks on a hyperlink. If there is no information in the title, it will not be tracked by analytical tools.\n\n\nExample:\n\n\n\n```\n<Button onClick={()=>window.open('\/someURL','_blank', 'rel=noopener noreferrer')}>Open URL<\/Button>\n\n```\n\n"}
{"questionId":"dbbe0646a8064179ac2ae209c5d8f60f","question":"vue-test-utils: could not overwrite property $route, this is usually caused by a plugin that has added the property as a read-only value\nI've looked at other answers with this problem, and it seems to be caused by trying to import `vue-router` into the test. This however, is not the case for my problem. Here is my test code:\n\n\n\n```\nimport { mount, shallowMount, createLocalVue } from '@vue\/test-utils'\nimport ListDetails from '..\/components\/homepage\/ListDetails'\nimport EntityList from '..\/components\/homepage\/EntityList'\nimport BootstrapVue from 'bootstrap-vue'\nimport Vuex from 'vuex'\nimport faker from 'faker'\n\nconst localVue = createLocalVue()\n\nlocalVue.use(Vuex)\nlocalVue.use(BootstrapVue)\n\ndescribe('ListDetails.vue', () => {\n    it('gets the correct page list when router list id param present', () => {\n        const selected_list = {\n            id: faker.random.number(),\n            name: faker.lorem.words(),\n            entries: []\n        }\n\n        testRouteListIdParam(selected_list)\n    })\n})\n\n```\n\nThen in `testRouteListIdParam`, I have:\n\n\n\n```\nfunction testRouteListIdParam(selected_list) {\n    \/\/ just assume this sets up a mocked store properly.\n    const store = setUpStore(selected_list, true)\n\n    const $route = {\n        path: `\/homepage\/lists\/${selected_list.id}`\n    }\n\n    const wrapper = mount(ListDetails, {\n        mocks: {\n            $route\n        },\n        store,\n        localVue\n    })\n}\n\n```\n\nAs soon as `mount()` happens, I get the error:\n\n\n\n```\n[vue-test-utils]: could not overwrite property $route, this is usually caused by a plugin that has added the property as a read-only value\n\n```\n\nAny ideas why this would be happening? Again, I'm not using VueRouter anywhere in the unit tests, so I'm not sure why I'm getting the error. Could it be BootstrapVue or Vuex that are messing things up?\n\n\n","questionMetadata":{"type":"debugging","level":"advanced","tag":"javascript"},"answer":"So this is a bug with vue-test-utils. If you are using VueRouter *anywhere* (even if it's not used in any unit test), you will get the above error.\n\n\nA work around is to use process.env.NODE\\_ENV in your unit tests and set it to 'test', and wherever you're using VueRouter, check process.env.NODE\\_ENV like so:\n\n\n\n```\nif (!process || process.env.NODE_ENV !== 'test') {\n    Vue.use(VueRouter)\n}\n\n```\n\nat least until vue-test-utils bug is fixed, this should fix this problem\n\n\n"}
{"questionId":"e7fa46ace8de40a292fb15782d041d14","question":"Cannot update a project from Angular 13 to 14\nI have a project which uses Angular 13 and I want to update it to Angular 14.\n\n\nWhen I try to update with:\n\n\n\n```\nng update @angular\/core@14 @angular\/cli@14\n\n```\n\nI get:\n\n\n\n```\nPackage \"@angular-eslint\/schematics\" has an incompatible peer dependency to \"@angular\/cli\" (requires \">= 13.0.0 < 14.0.0\", would install \"14.0.1\").\n\n```\n\nAny ideas on how to avoid this error?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"I have two suggestions:\n\n\n\n\n---\n\n\n## Suggestion 1:\n\n\n1. First upgrade the `@angular-eslint\/schematics` package\n\n\n\n```\nng update @angular-eslint\/schematics@14\n\n```\n\n2. Upgrade Angular\n\n\n\n```\nng update @angular\/core@14 @angular\/cli@14\n\n```\n\n\n\n---\n\n\n## Suggestion 2:\n\n\n1. Remove all `@angular-eslint` packages from `package.json` file and run `npm install` so the packages would be deleted\n\n\n\n```\nnpm install\n\n```\n\n2. Upgrade Angular\n\n\n\n```\nng update @angular\/core@14 @angular\/cli@14\n\n```\n\n3. Add all `@angular-eslint` packages with `ng add @angular-eslint\/schematics` command:\n\n\n\n```\nng add @angular-eslint\/schematics\n\n```\n\n"}
{"questionId":"c2e1830d5ba94b3484c7f1a3145aacf5","question":"converting RegExp to String then back to RegExp\nSo I have a RegExp `regex = \/asd\/`\n\n\nI am storing it as a as a key in my key-val store system.\n\n\nSo I say `str = String(regex)` which returns `\"\/asd\/\"`.\n\n\nNow I need to convert that string back to a RegExp.\n\n\nSo I try: `RegExp(str)` and I see `\/\\\/asd\\\/\/`\n\n\nthis is not what I want. It is not the same as `\/asd\/`\n\n\nShould I just remove the first and last characters from the string before converting it to regex? That would get me the desired result in this situation, but wouldn't necessarily work if the RegExp had modifiers like `\/i` or `\/g`\n\n\nIs there a better way to do this?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"\n```\nconst regex = \/asd\/gi;\n\n```\n\n## converting RegExp to String\n\n\n\n```\nconst obj = {flags: regex.flags, source: regex.source};\nconst string = JSON.stringify(obj);\n\n```\n\n## then back to RegExp\n\n\n\n```\nconst obj2 = JSON.parse(string);\nconst regex2 = new RegExp(obj2.source, obj2.flags);\n\n```\n\nRequires ES6+.\n\n\n"}
{"questionId":"47dcf5bc63f94c45bed501395fbffdd3","question":"String replacements in index.html in vite\nI am trying to inject some strings into the index.html of a Vite app (using vue3 template). In a vue-cli project for example we would have\n\n\n`<link rel=\"icon\" href=\"<%= BASE_URL %>favicon.ico\">`\n\n\nWhat is the Vite way to do that? (I know that BASE\\_URL is just '\/' in this case. I am asking for the general solution) I would be fine with a solution that covers environment variables only, but it would be great to know an even more general solution that can use JS code as in\n\n\n`<title><%= htmlWebpackPlugin.options.title %><\/title>`\n\n\nAnd I would really appreciate a solution that doesn't require installing an npm package\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"Had to lower my expectations considerably:\n\n\n1. I install a package\n2. I \"cheat\" and use process.env\n\n\n\n```\n\/\/ vite.config.js\nimport vue from '@vitejs\/plugin-vue'\n\nimport { loadEnv } from 'vite'\nimport { createHtmlPlugin } from 'vite-plugin-html'\n\nexport default ({ mode }) => {\n  const env = loadEnv(mode, process.cwd())\n  return {\n    plugins: [\n      vue(),\n      createHtmlPlugin({\n        minify: true,\n        inject: {\n          data: {\n            title: env.VITE_MY_FOO,\n          }\n        }\n      }),\n    ],\n  }\n}\n\n\n```\n\nthen in .env\n\n\n\n```\nVITE_MY_FOO=\"Hello vite ejs\"\n\n```\n\nand in index.html\n\n\n\n```\n<title><%= title %><\/title>\n\n```\n\nCan't say I like it, but it works\n\n\n"}
{"questionId":"83a97dfd52c941ea99e6921093bdcd30","question":"TypeScript - ts(7053) : Element implicitly has an 'any' type because expression of type 'string' can't be used to index\nIn TypeScript, I declare an interface like this:\n\n\n\n```\nexport default interface MyDTO {\n    readonly num: string;\n    readonly entitle: string;\n    readonly trb: string;\n    readonly ucr: string;\n    readonly dcr: string;\n    readonly udm?: string;\n    readonly ddm?: string;\n}\n\n```\n\nWith a function, I would like to access the value of a property, whose name is contained in a variable.\n\n\n\n```\nprivate doSomething(dto: MyDTO, property: string): any {\n    let label: any;\n\n    if (['dcr', 'ddm'].includes(property)) {\n        label = doSomethingElse(dto[property]);\n    } else {\n        label = dto[property];\n    }\n    \n    return label;\n}\n\n```\n\nUnfortunately, TypeScript gives me the following error message :\n\n\n\n> \n> Element implicitly has an 'any' type because expression of type\n> 'string' can't be used to index type 'MyDTO'. No index signature\n> with a parameter of type 'string' was found on type\n> 'MyDTO'.ts(7053)\n> \n> \n> \n\n\nAnyone have an idea, please ?\n\n\nThank you\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"The reason for this is because `MyDTO` has explicitly named properties, but you're using a generic string as an index, so TypeScript is saying that it can't guarantee that whatever string is passed into your `doSomething` function will actually match a property name on your interface.\n\n\nAn excellent workaround for this that was introduced in TypeScript 2.1 is `keyof`. This allows you to explicitly type something as a key of a certain class\/interface.\n\n\nThis will A. get rid of the TS error you're seeing, and B. also check to make sure that any callers of your function actually pass a valid key.\n\n\n\n```\nexport default interface MyDTO {\n    readonly num: string;\n    readonly entitle: string;\n    readonly trb: string;\n    readonly ucr: string;\n    readonly dcr: string;\n    readonly udm?: string;\n    readonly ddm?: string;\n}\n\nfunction doSomething(dto: MyDTO, property: keyof MyDTO): any {\n    let label: any;\n\n    if (['dcr', 'ddm'].includes(property)) {\n        label = doSomethingElse(dto[property]);\n    } else {\n        label = dto[property];\n    }\n    \n    return label;\n}\n\ndoSomething(obj, \"foo\") \/\/ is a TS error\ndoSomething(obj, \"num\") \/\/ is valid\n\n```\n\n"}
{"questionId":"ed138f6b165f4b0c920ecb303ff1a0e6","question":"How can I get the current Leaflet map zoom level?\nI'm trying to get the zoom level of a map in real time to make a button that locks the zoom with the current value. I have tried using getMapZoom and getZoom but both give me an undefined value. I think I'm not using the correct ref but I haven't been able to find much documentation about it. Here's the code:\n\n\n\n```\n<Map className=\"map-layer\" \n          center={center} \n          onoverlayadd={this.overlayadd} \n          onoverlayremove={this.overlayremove}\n          ondragend={this.zoomChange}\n          onzoomend={console.log('Zoom: ' + this.mapRef.leafletElement.getMapZoom())}\n          zoom={this.state.zoom}\n          ref={this.mapRef}\n          preferCanvas={false}\n          animate={true}\n          scrollWheelZoom={this.state.zoomLock ? false : true}\n          doubleClickZoom={this.state.zoomLock ? false : true}\n          touchZoom={this.state.zoomLock ? false : true}\n          maxZoom={7}\n          minZoom={7}\n\n                    >\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"In pure leaflet if you defined map as `const map = L.map(\"map\", options)` than you just call `map.getZoom()`. \n\n\nIn constructor `this.mapRef = React.createRef()`\nIn Map element:\n\n\n\n```\n    ref={this.mapRef}\n    onzoomend={() => console.log(this.mapRef.current.leafletElement.getZoom()}\n\n```\n\n"}
{"questionId":"ad2145ac574d420a8060d88261c73f58","question":"Get key with minimum value\nI have an array like this: `arr = {lst1: 300, lst2: 381, lst3: 4, lst4: 4, lst5: 49, \u2026}`\n\n\nAnd I'm trying get the lowest value with key using Javascript.\n\n\nWhat I've tried:\n\n\n`alert(Math.min.apply(Math, arr));` returns `Infinity` I don't know why\n\n\nI got this on Google, just for try:\n\n\n\n```\nvar keys = Object.keys(arr).map(Number).filter(function(a){\n    return arr[a];\n}); alert(Math.min.apply(Math, keys));\n\n```\n\nreturns `Infinity` too \n\n\nI want something more complete, like this output: \"The lowest value is 2 from lst9\".\n\n\nI really tried fix it myself before asking here, but without success!\nCan you help me fix this \"Infinity\" issue? Thank you.\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"javascript"},"answer":"You can get the key and value using `Object.entries`:\n\n\n\n\n\n```\nvar arr = {\r\n  lst1: 300,\r\n  lst2: 381,\r\n  lst3: 4,\r\n  lst4: 4,\r\n  lst5: 49\r\n};\r\n\r\nfunction lowestValueAndKey(obj) {\r\n  var [lowestItems] = Object.entries(obj).sort(([ ,v1], [ ,v2]) => v1 - v2);\r\n  return `Lowest value is ${lowestItems[1]}, with a key of ${lowestItems[0]}`;\r\n}\r\n\r\nvar lowest = lowestValueAndKey(arr);\r\nconsole.log(lowest);\n```\n\n\n\n\n\n\n"}
{"questionId":"ce5d9061432643f984e34a6ee0f078c7","question":"Environment variables not found during Mocha unit test Node.js\nI am trying to run a mocha unit test but one of the modules used by the module I am testing on requires environment variables such as process.env.CLIENT\\_ID through dotenv. When I run my Mocha test, these environment variables are not found. How can I can include environment variables from a .env file in my mocha unit tests?\n\n\ntest.js:\n\n\n\n```\n    var messenger = require(__dirname + \"\/..\/routes\/messenger.js\");\nvar assert = require(\"assert\") \n\n\ndescribe(\"Return Hello\", function(){\n    it('Should return hello',function(done){\n        messenger.testFunction(function(value){\n            assert(value === \"Hello\", 'Should return Hello')\n            done()\n        })\n    })\n})\n\n```\n\nSection of file that contains the problem that goes through unit test:\n\n\n\n```\n    var express = require(\"express\")\nvar router = express.Router();\n\nrequire('dotenv').config()\n\nvar plaid = require('plaid');\nvar mysql = require('mysql');\n\nvar fs = require(\"fs\");\n\n\nconst plaidClient = new plaid.Client(\n    process.env.PLAID_CLIENT_ID, \/\/ these are all not found\n    process.env.PLAID_SECRET,\n    process.env.PLAID_PUBLIC_KEY,\n    plaid.environments.sandbox);\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"to me the most elegant way of setting your env before the tests is **inside package.json**.\n\n\nHere is an example to adapt to your own npm test command:\n\n\n\n```\n\"scripts\": {\n  \"test\": \"mocha -r dotenv\/config\"\n}\n\n```\n\nThe main idea is to add the **-r dotenv\/config**.\n\n\nThe method works as well with **dotenv-flow**, do not forget to add **NODE\\_ENV=test** at the beginning of the command.\n\n\nIt works as well with **nodemon**.\n\n\n"}
{"questionId":"9e5078d33f704df48570f36c100b2d59","question":"What causes NextJS Warning: \"Extra attributes from the server: data-new-gr-c-s-check-loaded... \"\nI am getting the following warning from my NextJS Application:\n\n\n\n> \n> Warning: Extra attributes from the server: data-new-gr-c-s-check-loaded,data-gr-ext-installed,cz-shortcut-listen,data-lt-installed\n> \n> \n> \n\n\nI don't know why it happens, what is the explanation for this?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"Disabling the Grammarly extension solved the problem for me.\n\n\n"}
{"questionId":"90ff878a09f444169509a46c7027c009","question":"How to use promise.allSettled with typescript?\nTypescript build is failing as it does not seem to like `Promise.allSetttled` even though I have set ts config comilerOptions with `\"lib\": [ \"ES2020.Promise\" ],`\n\n\nIt seems as though the response for `promise.allSettled` does not include `result` or `reason`.\n\n\nWhen running typescript build I get the following error:\n\n\n\n```\nProperty 'reason' does not exist on type 'PromiseSettledResult<IMyPromiseResult>'.\n\n```\n\nand\n\n\n\n```\nProperty 'value' does not exist on type 'PromiseRejectedResult'.\n\n```\n\nMy code block looks like this and as you can see, I am trying to access `reason` and `result` from eaech of the promises that get resolved.\n\n\n\n```\nconst myPromise = async () : Promise<IMyPromiseResult> {\n  return new Promise((resolve) => {\n    resolve(\"hello world\")\n  })\n}\n\nconst data = await Promise.allSettled([\n  myPromise()\n]);\n\nconst response = data.find(res => res.status === 'fulfilled')?.result;\n\nif(!response) {\n  const error = data.find(res => res.status === 'rejected')?.reason;\n  throw new Error(error);\n}\n\n```\n\nHow can I update the Promise.allSettled declaration to include the correct interfaces?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"Like Bergi mentioned TypeScript does not know if the type is `PromiseFulfilledResult` \/ `PromiseRejectedResult` when checking types.\n\n\nThe only way is to cast the promise result. This can be done because you already verified that the resolved promise is either fulfilled or rejected.\n\n\nSee this example:\n\n\n\n```\nconst myPromise = async (): Promise<string> => {\n  return new Promise((resolve) => {\n    resolve(\"hello world\");\n  });\n};\n\nconst data = await Promise.allSettled([myPromise()]);\n\nconst response = (data.find(\n  (res) => res.status === \"fulfilled\"\n) as PromiseFulfilledResult<string> |\u00a0undefined)?.value;\n\nif (!response) {\n  const error = (data.find(\n    (res) => res.status === \"rejected\"\n  ) as PromiseRejectedResult |\u00a0undefined)?.reason;\n  throw new Error(error);\n}\n\n```\n\n"}
{"questionId":"2d54d77c0337439f973be5d9eba67abb","question":"UseSelector State is Undefined\nThe page does not render, citing `TypeError: state is undefined`, tracing back to this line in **SelectForm.js**: `const filter = useSelector(state => state.filter);`.\n\n\nI've spent hours trying to figure out what I'm doing wrong. I've tried createSelector but that didn't work. I've tried dispatching a \"Fetch Initial State\" action, and that didn't work. The component is wrapped in provider tags. I'm not sure why I don't have access to the state. At this point I'm unable to see any flaws I've been looking at it for so long.\n\n\nCode Snippets\n\n\n**reducer.js**\n\n\n\n```\nlet initialState = {\n    filter: {\n        country: null,\n        state: null,\n        level: null,\n        team: null\n    },\n\n    isDBConnecting: false, \n    isDBConnected: false, \n    isDBError: false \n}\n\nconst SelectorReducer = (state=initialState, action) => {\n    switch (action.type) {        \n        case 'DB_CONNECT_INIT':\n            return {\n                ...state,\n                isDBConnecting: true,\n                isDBConnected: false,\n                isDBError: false,\n            };\n...\n...\n}\n\nexport default SelectorReducer;\n\n```\n\n**actions.js**\n\n\n\n```\nexport const initializeDBConnection = () => {\n    return {\n        type: 'DB_CONNECT_INIT'\n    }\n};\n\n```\n\n**ParentComponent.js**\n\n\n\n```\nimport React from 'react';\nimport { createStore } from 'redux';\nimport { Provider } from 'react-redux'; \/\/import provider to provide component access to the state\n\n\/\/Component imports\nimport SelectForm from '.\/components\/SelectForm'\nimport SelectorReducer from '...\/reducer.js'\n\nconst SelectorStore = createStore(SelectorReducer);\n\nconst ParentComponent = () => {\n\n    return (\n        <div className=\"page-container\">\n            <div id=\"carousel\">\n                <div id=\"wrapper\">\n                    <Provider store={SelectorStore}>\n                        <SelectForm \/>\n                    <\/Provider>\n                <\/div>\n            <\/div>\n    <\/div>\n    )\n}\n\n```\n\n**SelectForm.js (Child Component, wrapped in Provider tags above)**\n\n\n\n```\n\/\/IMPORTS\nimport React from 'react'; \/\/import react\nimport { useSelector, useDispatch } from 'react-redux';\n\n\/\/COMPONENT IMPORTS\nimport FormGroup from '..\/FormGroup';\nimport { * as actions } from '...\/actions.js';\n\nconst SelectForm = (props) => {\n\n    \/\/STATEFUL IMPORTS\n    \/\/filter\n    const filter = useSelector(state => state.filter);\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"Credit to @NicholasTower for the answer in the comments. My reducer did not have a default case in which \n\n\n\n```\ndefault: return state\n\n```\n\nPutting that in solved the issue.\n\n\n"}
{"questionId":"b59bc1ecf626471da322eca13d223daf","question":"React: Deep requiring is deprecated as of uuid, Please require the top-level module\nmy React app successfully shows the button however receiving this error.\n\n\nindex.js:1 Deep requiring like `const uuidv4 = require('uuid\/v4');` is deprecated as of uuid@7.x. Please require the top-level module when using the Node.js CommonJS module or use ECMAScript Modules when bundling for the browser\n\n\n\n```\nimport React, { Component } from 'react';\nimport { Container, ListGroup, ListGroupItem, Button } from 'reactstrap';\nimport { CSSTransition, TransitionGroup } from 'react-transition-group';\nimport uuid from 'uuid\/v4';\n\nclass ShoppingList extends Component {\n    state = {\n        items: [\n            { id: uuid(), name: 'Eggs' },\n            { id: uuid(), name: 'Milk' },\n            { id: uuid(), name: 'Steak' },\n            { id: uuid(), name: 'Water' },\n        ]\n    }\n\n    render() {\n        const { items } = this.state;\n        return (\n            <Container>\n                <Button \n                 color=\"dark\"\n                 style={{marginBottom: '2rem'}}\n                 onClick={() => {\n                    const name = prompt('Enter Item');\n                    if (name) {\n                        this.setState(state => ({\n                           items: [...state.items, { id: uuid(), name }] \n                        }));\n                    }\n                }}\n                >Add Item<\/Button>\n            <\/Container>\n        );\n    }\n}\n\nexport default ShoppingList;\n\n```\n\n- I tried to use 'import { v4 as uuidv4 } from 'uuid'; uuidv4();'\n- however my button would not show up and i would get error:\n- Uncaught ReferenceError: uuid is not defined\n- Perhaps i am meant to be getting this error? and everything is currently working fine?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"This has changed after recent update to library and now you may import uuid, as per library description :\n\n\n\"As of uuid@7 this library now provides ECMAScript modules builds, which allow packagers like Webpack and Rollup to do \"tree-shaking\" to remove dead code. Instead, use the import\"\n\n\n\n```\nimport { v4 as uuid_v4 } from \"uuid\";\nuuid_v4()\n\n```\n\n... or for CommonJS:\n\n\n\n```\nconst { v4: uuid_v4 } = require('uuid');\nuuid_v4();\n\n```\n\n"}
{"questionId":"ac0663a2384247df8e38679e4da9f529","question":"useRef Typescript error: Property 'current' does not exist on type 'HTMLElement'\nI'm making a Datepicker with React and Typescript, and I can't seem to get over the errors with `useRef` and the ref's `.current` property. I'm trying to get the `div` I'm assigning the ref to to close when the document is clicked outside of it.\n\n\nI just can't quite seem to figure out what I'm doing wrong. Here is my code:\n\n\n\n```\nfunction Datepicker({label, placeholder}: DatepickerProps){\n  const [open, setOpen] = React.useState(false)\n  const [day, setDay] = React.useState(0)\n  const [month, setMonth] = React.useState(new Date().getMonth())\n  const [year, setYear] = React.useState(new Date().getFullYear())\n  const picker = React.useRef(null) as HTMLElement | null\n\n  React.useEffect(() => {\n    document.addEventListener(\"mousedown\", toggle)\n    return () => {\n      document.removeEventListener(\"mousedown\", toggle)\n    };\n  }, []);\n\n  const toggle = (e: MouseEvent) => {\n    if (picker && picker.current){\n      if (picker.current.contains(e.target) === false){\n        setOpen(false)\n      }\n    }\n  }\n\n  \/\/...\n\n  return(\n    \n    \/\/...\n\n    <div \n      ref={picker}\n      className={\"datepicker-picker\" + (open ? \" open\" : \"\")}\n    >\n\n      \/\/...\n\n  )\n}\n\n\n\n```\n\n`React.useRef(null) as HTMLElement | null` is giving me the following problem:\n\n\n\n```\nConversion of type 'MutableRefObject<null>' to type 'HTMLElement' may be a mistake because neither type sufficiently overlaps with the other. If this was intentional, convert the expression to 'unknown' first.\n  Type 'MutableRefObject<null>' is missing the following properties from type 'HTMLElement': accessKey, accessKeyLabel, autocapitalize, dir, and 234 more.ts(2352)\n\n```\n\n`.current` is giving me the following:\n\n\n\n```\nProperty 'current' does not exist on type 'HTMLElement'.\n\n```\n\nand when I try to apply the ref to a `div` element, its says the following:\n\n\n\n```\nType 'HTMLElement | null' is not assignable to type 'string | ((instance: HTMLDivElement | null) => void) | RefObject<HTMLDivElement> | null | undefined'.\n  Type 'HTMLElement' is not assignable to type 'string | ((instance: HTMLDivElement | null) => void) | RefObject<HTMLDivElement> | null | undefined'.\n    Type 'HTMLElement' is not assignable to type 'string'.ts(2322)\nindex.d.ts(143, 9): The expected type comes from property 'ref' which is declared here on type 'DetailedHTMLProps<HTMLAttributes<HTMLDivElement>, HTMLDivElement>'\n\n```\n\nI'm using VSCode as my IDE, if that helps at all.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"`useRef` does not return the value in the ref - typing the return value as `HTMLElement | null` is inaccurate. Use a generic argument instead:\n\n\n\n```\nconst picker = React.useRef<HTMLElement>(null);\n\n```\n\nYou'll also need to change `toggle` so that the type narrowing occurs as desired:\n\n\n\n```\nconst toggle = (e: MouseEvent) => {\n  const { current } = picker;\n  if (current && !current.contains(e.target)) {\n    setOpen(false);\n  }\n}\n\n```\n\n"}
{"questionId":"d273023b26e5458e958f5e8670988359","question":"form-data | axios: Unable to get headers from FormData, Error: getHeaders is not a function\nI am trying to post text and file fields using `form-data` and `axios`, but I am getting an error: `getHeaders()` is not a function. Below is my `submit` code, note that I am using `React` with `Typescript`.\n\n\n\n```\nimport * as FormData from 'form-data'\nimport axios from 'axios'\n\nsubmit(event: React.FormEvent<HTMLFormElement>) {\n  event.preventDefault()\n\n  const { title, description, pictureFile } = this.state\n\n  let data = new FormData()\n  data.append('title', title)\n  data.append('description', description)\n  data.append('picture', pictureFile)\n\n  axios.post('\/api\/route', data, {\n    headers: data.getHeaders() \/\/ ERROR: getHeaders is not a function\n  })\n    .then(res => handle(res))\n    .catch(err => handle(err))\n}\n\n```\n\nThe particular header I am interested in is the `Authorization`, I can set it manually, but then the boundaries are required so ... I better try to get that `getHeaders()` function to work.\n\n\nI don't get the problem here, `getHeaders` seems to be part of `form-data` API.\n\n\nPlease help.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"`form-data` is used only on `Node`, if you run it on the browser, it will switch to the `window's` version of `FormData`. I saw this in their code.\n\n\n\n```\nmodule.exports = typeof self == 'object' ? self.FormData : window.FormData;\n\n```\n\n"}
{"questionId":"33f4bf46e38642969b1ee388a150bef3","question":"How to write console.log to a file instead\nNow I show the information using:\n\n\n\n```\nconsole.log (kraken.id, markets)\n\n```\n\nHowever, I want to write all the information that goes to the console to a file instead. How can that be done by completing the below code?\n\n\n\n```\n'use strict';\nvar ccxt = require('ccxt');\n\n(async () => {\n  let kraken = new ccxt.kraken()\n  let markets = await kraken.load_markets()\n  \/\/console.log (kraken.id, markets)\n\n\n  \/\/How to write above console.log to file?\n  const fs = require('fs');\n  fs.writeFile(\"\/Users\/Andreas\/Desktop\/NODE\/myproject\/files\/test.txt\", \"allinfoAsstring\", function (err) {\n    if (err) {\n      return console.log(err);\n    }\n\n    console.log(\"The file was saved!\");\n  });\n})()\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"You can try to create an Object out of your variables and format them as a JSON string.\n\n\n\n\n\n```\n\/* ... *\/\r\nconst obj = {kraken, markets}\r\n\r\nconst fs = require('fs');\r\nfs.writeFile(\"\/Users\/Andreas\/Desktop\/NODE\/myproject\/files\/test.txt\", JSON.stringify(obj), function(err) {\r\n    if(err) {\r\n        return console.log(err);\r\n    }\r\n\r\n    console.log(\"The file was saved!\");\r\n}); \n```\n\n\n\n\n\n\nLater, you can retrieve the values from the file, by running\n\n\n\n\n\n```\nfs.readFile('\/Users\/Andreas\/Desktop\/NODE\/myproject\/files\/test.txt', 'utf8', function(err, data) {\r\n const obj = JSON.parse(data)\r\n\r\n console.log(\"The data from the file is: \" + obj)\r\n})\n```\n\n\n\n\n\n\n"}
{"questionId":"5f07ad296c314c9d82575cbaa861a69e","question":"Next.js, how to submit a form to another page?\n(*Next.js*) I have a GET form on one page. I want to submit it to another page. I know I can set the `action` property to the other page. That works. However, it does a page reload instead of just rendering the new page; the same as would happen if you had a link on the page without wrapping it in a `Link` component.\n\n\nI could catch the submit event, build a query, and push it onto the router. But that seems like a lot of extra work for something I assume has already been figured out.\n\n\nAny ideas how to do this without reinventing the wheel?\n\n\n\n```\n<form method='get' action='\/search'>\n  <input name='q' placeholder='Search' arial-label='Search' \/>\n<\/form>\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"I ended up catching the submit event and pushing a URL onto the router.\n\n\n\n```\nimport {useState} from 'react'\nimport {useRouter} from 'next\/router'\n\nconst preventDefault = f => e => {\n  e.preventDefault()\n  f(e)\n}\n\nexport default ({action = '\/search'}) => {\n   const router = useRouter()\n   const [query, setQuery] = useState('')\n\n   const handleParam = setValue => e => setValue(e.target.value)\n\n   const handleSubmit = preventDefault(() => {\n     router.push({\n       pathname: action,\n       query: {q: query},\n     })\n   })\n\n   return (\n     <form onSubmit={handleSubmit}>\n       <input\n         type='text'\n         name='q'\n         value={query}\n         onChange={handleParam(setQuery)}\n         placeholder='Search'\n         aria-label='Search'\n       \/>\n     <\/form>\n   )\n}\n\n\n```\n\n"}
{"questionId":"d85d73c8ad8b4deda4d70486500dbb03","question":"Accessing something inside the object when you don't know the key\nI am getting a following object \n\n\n\n```\n{\n  IuW1zvaSABwH4q: {\n    label: 'Random Image of TypeScript not relavent to coworking',\n    thumbId: 'd501-f-b601-c8b1-4bd995e',\n    schemaType: 'xman-assets-image-set'\n  }\n}\n\n```\n\nNow, I want to access the value of thumbID inside it i.e. **d501-f-b601-c8b1-4bd995e**\n\n\nBut my root key seems to be dynamic\/random (IuW1zvaSABwH4q), How can I access the value inside it?\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"javascript"},"answer":"You can get the values from object and than access the desired key.\n\n\n\n```\nlet obj =  {\r\n    IuW1zvaSABwH4q: \r\n      {\r\n        label: 'Random Image of TypeScript not relavent to coworking', \r\n        thumbId: 'd501-f-b601-c8b1-4bd995e',\r\n        schemaType: 'xman-assets-image-set' \r\n      } \r\n    }\r\n    \r\nlet op = Object.values(obj)[0].thumbId\r\n\r\nconsole.log(op)\n```\n\n\n\n\n\n\n"}
{"questionId":"ffa512bbeb304053b6f8c2757ffc4e27","question":"Do we still need functional setState way in react hooks?\n\n```\n  const [count, setCount] = useState(0);\n\n  const handleClick = () =>\n    setCount(prevCount => {\n      return prevCount + 1;\n    });\n\n```\n\n\n```\n  const [count, setCount] = useState(0);\n\n  const handleClick = () => setCount(count + 1);\n\n```\n\nComing from class-based component background, it becoming a habit where we use functional `setState`. I'm wondering if do we still need to rely on prevState in functional hooks? Or the current state is always \"trustable\" and most \"updated\"?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"javascript"},"answer":"**Yes**, the behavior is similar.\n\n\nReact is batching the updates calls.\nWhen Writing:\n\n\n\n```\nconst handleClick = () => setCount(count + 1)\nhandleClick()\nhandleClick()\nhandleClick()\n\n```\n\nthe `count` in state will be 1\n\n\nWhen Writing:\n\n\n\n```\nconst handleClick = () =>\n  setCount(prevCount => {\n    return prevCount + 1;\n});\nhandleClick()\nhandleClick()\nhandleClick()\n\n```\n\nthe `count` in state will be 3\n\n\n"}
{"questionId":"63ddd1e380f74816ab1b657bfd16f28d","question":"Get event target inside a web component\nAnchor elements (`<a>`) are created when the user interacts with a web component. The problem is, that I cannot get the anchor element returned from the \"outside\" of the web component when an anchor is clicked.\n\n\nI add an event listener to `document` listening for click events. When an element somewhere in the DOM is clicked I expect the `e.target` to be the clicked element.\nIn the case of a click somewhere inside the web component the custom element (`<fancy-list><\/fancy-list>`) will be returned - not the clicked element.\n\n\nWhen the mode of the shadow DOM is set to `open` the DOM should be accessible.\n\n\n\n\n\n```\nclass Fancylist extends HTMLElement {\n  constructor() {\n    super();\n    const shadow = this.attachShadow({ mode: 'open' });\n\n    const wrapper = document.createElement('div');\n    wrapper.innerHTML = `<ul><\/ul><button>Add item<\/button>`;\n\n    shadow.appendChild(wrapper);\n\n    this.on_root_click = this.on_root_click.bind(this);\n  }\n\n  connectedCallback() {\n    this.ul_elm = this.shadowRoot.querySelector('ul');\n    this.shadowRoot.addEventListener('click', this.on_root_click, false);\n  }\n\n  on_root_click(e){\n    switch(e.target.nodeName){\n      case 'BUTTON':\n        this.ul_elm.innerHTML += '<li><a href=\"p1\">List item<\/a><\/li>';\n        break;\n      case 'A':\n        e.preventDefault();\n        console.log('You clicked a link!');\n        break;\n    }\n  }\n}\n\ncustomElements.define('fancy-list', Fancylist);\n```\n\n\n```\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>List<\/title>\n    <meta charset=\"utf-8\" \/>\n    <script type=\"text\/javascript\">\n      document.addEventListener('DOMContentLoaded', e => {\n        document.body.addEventListener('click', e => {\n          \/\/console.log(e.composedPath());\n          console.log(e.target); \/\/ why is this not returning an anchor element when an anchor is clickend inside the <fancy-list>?\n        }, false);\n      }, false);\n    <\/script>\n  <\/head>\n  <body>\n  <h1>List<\/h1>\n  <fancy-list><\/fancy-list>\n  <\/body>\n<\/html>\n```\n\n\n\n\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"The purpose of the *Shadow* DOM is precisely *to mask* the HTML content the Shadow DOM from the containter point of view.\n\n\nThat's also why inner events are retargeted in order to expose the Shadow DOM host.\n\n\nHowever, you can still get the real target by getting the first item of the `Event.path` Array property.\n\n\n\n```\nevent.path[0]\n\n```\n\nNB: of course it will work only with `open` Shadow DOM.\n\n\n\n\n\n```\nclass Fancylist extends HTMLElement {\n  constructor() {\n    super();\n    const shadow = this.attachShadow({ mode: 'open' });\n\n    const wrapper = document.createElement('div');\n    wrapper.innerHTML = `<ul><\/ul><button>Add item<\/button>`;\n\n    shadow.appendChild(wrapper);\n\n    this.on_root_click = this.on_root_click.bind(this);\n  }\n\n  connectedCallback() {\n    this.ul_elm = this.shadowRoot.querySelector('ul');\n    this.shadowRoot.addEventListener('click', this.on_root_click, false);\n  }\n\n  on_root_click(e){\n    switch(e.target.nodeName){\n      case 'BUTTON':\n        this.ul_elm.innerHTML += '<li><a href=\"p1\">List item<\/a><\/li>';\n        break;\n      case 'A':\n        e.preventDefault();\n        break;\n    }\n  }\n}\n\ncustomElements.define('fancy-list', Fancylist);\n```\n\n\n```\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>List<\/title>\n    <meta charset=\"utf-8\" \/>\n    <script type=\"text\/javascript\">\n      document.addEventListener('DOMContentLoaded', e => {\n        document.body.addEventListener('click', e => {\n          console.log(e.path[0]);\n        }, false);\n      }, false);\n    <\/script>\n  <\/head>\n  <body>\n  <h1>List<\/h1>\n  <fancy-list><\/fancy-list>\n  <\/body>\n<\/html>\n```\n\n\n\n\n\n\n**Update 2021**\n\n\nAs commented now you should use `event.composedPath()`.\n\n\n"}
{"questionId":"360e4692ebc443dfb49eb5411ceb5753","question":"Type error on response of Promise.allSettled()\nI've been trying to use Promise.allSettled on NodeJS with Typescript recently, and I'm facing issues with the response. the allSettled method returns an array with `status: \"rejected\" | \"fulfilled\"` and a value, in case it's fulfilled. The problem is, when I try to access the value of the response, I get the following errors:\n\n\n\n```\nProperty 'value' does not exist on type 'PromiseSettledResult<unknown>'.\nProperty 'value' does not exist on type 'PromiseRejectedResult'.ts(2339)\n\n```\n\nBelow I'll leave a simple example so you can copy the code and try yourself:\n\n\n\n```\nconst p1 = Promise.resolve(50); \nconst p2 = Promise.resolve(100); \n\nconst promiseArray = [p1, p2]; \n  \nPromise.allSettled( promiseArray ). \n  then( results => results.forEach( result =>  \n    console.log(result.status, result.value)));\n\n```\n\nIf I run this code on my project, I get an error because of `result.value` at the end.\n\n\nI'm running my node on version 12.18.3 on Windows, and I've set my target on the `tsconfig.json` as `ES2020` to be able to use the method itself.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"@jonrsharpe answered it:\nYou only have a value attribute where the status is fulfilled, and you're not checking for that.\n\n\nSo using my own example, it can be fixed as the following:\n\n\n\n```\nconst p1 = Promise.resolve(50); \nconst p2 = Promise.resolve(100); \n\nconst promiseArray = [p1, p2]; \n  \nPromise.allSettled( promiseArray ). \n  then( results => results.forEach( result =>  \n    console.log(result.status,\n                result.status === 'fulfilled' && result.value\n    );\n  ));\n\n```\n\nIt now verifies if the promise was fulfilled and then prints the value, if it's the case.\n\n\n"}
{"questionId":"86bfeef39c564fa5af62535e6c7859fc","question":"How to create and fire a custom event in angular\nI'm new in Angular and I've read about event binding so I can do something like this:\n\n\n\n```\n<button (click)=\"doSomething()\"><\/button>\n\n```\n\nI'd like to know if it's possible to create a custom event and do the same thing. Let's say that I want to have a custom event like: `deleteItem`, is it possible to do something like this? And how?\n\n\n\n```\n<my-component (deleteItem)=\"doSomething()\"><\/my-component>\n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"javascript"},"answer":"Of course, you can use an `eventEmitter`\nin my-component ts file add this\n\n\n\n```\n @Output() deleteItem= new EventEmitter();\n\n```\n\nand when you want to rise the event do this\n\n\n\n```\n  this.deleteItem.emit();\n\n```\n\nalso you can pass data like this\n\n\n\n```\n  this.countUpdate.emit({value: some data });\n\n```\n\nthen catch it in the parent component like this\n\n\n\n```\n<my-component (deleteItem)=\"doSomething($event)\"><\/my-component>\n\n```\n\nand in the parent ts file\n\n\n\n```\n    doSomething(event)\n    { \n       console.log(event);\n    }\n\n```\n\n"}
{"questionId":"279906ad998640ecbf0e5a4b65c5e14c","question":"In Vue3 composition API make the watcher work immediately\nUsing Vue3 composition API. How do I make watched to work immediately. The following code doesn't work.\n\n\n\n```\nwatch((immediate=true) => props.isOpen, () => {\n        if (props.isOpen && props.preventBackgroundScrolling) {\n          document.body.style.setProperty('overflow', 'hidden')\n        } else {\n          document.body.style.removeProperty('overflow')\n        }\n\n          });\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"It should placed as option :\n\n\n\n```\nwatch(() => props.isOpen, () => {\n        if (props.isOpen && props.preventBackgroundScrolling) {\n          document.body.style.setProperty('overflow', 'hidden')\n        } else {\n          document.body.style.removeProperty('overflow')\n        }\n\n          },{immediate:true});\n\n\n```\n\nor\n\n\n\n```\nwatch('props.isOpen', () => {\n        if (props.isOpen && props.preventBackgroundScrolling) {\n          document.body.style.setProperty('overflow', 'hidden')\n        } else {\n          document.body.style.removeProperty('overflow')\n        }\n\n          },\n       {immediate:true}\n   );\n\n\n```\n\n"}
{"questionId":"c794f56d54a843c8906f092df2323b7e","question":"What's the purpose of a leading pipe when declaring a type on Typescript\nUsing Prettier I noticed that this code block is formatted to contain an extra leading pipe, see the following example:\n\n\n\n```\n\/\/ before Prettier\nlet foo: {\n    [k: string]: any\n} | boolean;\n\n\/\/ after Prettier\nconst actions:\n  | {\n      [k: string]: any;\n    }\n  | boolean = true;\n\n```\n\nNotice the pipe added by Prettier on the type declaration.\n\n\nThis could also be declared in a single line, and prettier keeps the format without adding the extra pipe:\n\n\n\n```\nconst actions: { [k: string]: any } | boolean = true;\n\n```\n\nMy doubt is why is this pipe added? Does it change anything at the Typescript level?\n\n\n","questionMetadata":{"type":"conceptual","level":"beginner","tag":"javascript"},"answer":"It's purely stylistic, there is no functional difference.\n\n\nConsider the following:\n\n\n\n```\ntype Foo = Bar\n  | Baz\n  | Bap\n\n```\n\ncompared to this: \n\n\n\n```\ntype Foo =\n  | Bar\n  | Baz\n  | Bap\n\n```\n\nThe second example is a lot cleaner, and it's immediately clear that the three things on the right side of the `|`s are the constituents of the union.\n\n\nClearly, you wouldn't add a leading `|` when defining everything on one line:\n\n\n\n```\ntype T = A | B\n\n```\n\n"}
{"questionId":"465f820d9d83429dbe953ddd6ebef421","question":"Zod Schema: How to make a field optional OR have a minimum string contraint?\nI have a field where I want the value to either be optional OR have the field have a minimum length of `4`.\n\n\nI've tried the following:\n\n\n\n```\nexport const SocialsSchema = z.object({\n  myField: z.optional(z.string().min(4, \"Please enter a valid value\")),\n});\n\n```\n\nThis passes if I used a value like: `\"good\"`, but if I've got an empty value then it fails.\n\n\nHow do I correctly implement a constraint using zod schemas to make an optional value with a minimum constraint if the value is not empty?\n\n\nIs it possible to do this without using regex or a regex solution the only way?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"In your case, you consider `\"\"` to be the same as `undefined` (i.e.: when the string is empty, it's like there's no string at all).\n\n\nIt's implementable in Zod this way:\n\n\n\n```\nimport { z } from \"zod\";\nimport { strict as assert } from \"node:assert\";\n\n\/\/ `myString` is a string that can be either optional (undefined or missing),\n\/\/ empty, or min 4\nconst myString = z\n  .union([z.string().length(0), z.string().min(4)])\n  .optional()\n  .transform(e => e === \"\" ? undefined : e);\n\nconst schema = z.object({ test: myString });\n\nassert( schema.parse({}).test === undefined ); \/\/ missing string\nassert( schema.parse({ test: undefined }).test === undefined ); \/\/ string is undefined\nassert( schema.parse({ test: \"\" }).test === undefined ); \/\/ string is empty\nassert( schema.parse({ test: \"1234\" }).test === \"1234\" ); \/\/ string is min 4\n\n\/\/ these successfully fail\nassert( schema.safeParse({ test: \"123\" }).success !== true );\nassert( schema.safeParse({ test: 3.14 }).success !== true );\n\n```\n\n"}
{"questionId":"8c30df0435434c3ab53e86e69f268b53","question":"AWS NodeJS SDK V3 DynamoDB UpdateItem - TypeError: Cannot read property '0' of undefined\nI am trying to get a basic database update operation to work in nodejs using the new AWS SDK V3 for NodeJS.\n\n\nThe data object that I am trying to update looks like this:\n\n\n\n```\n{\n  auth: { BOOL: false },\n  username: { S: 'siegbert' },\n  secondsLeft: { N: 49985 },\n  userid: { S: '123456' }\n}\n\n```\n\nIn the same file I have already succesfully done a GetItemCommand using the SDK V3.\n\n\nUnfortunately I keep getting a really weird error when using the AWS SDK v3, when using the SDK v2, the exact same params seem to work. I tried looking into the docs but the update operation is not really well documented yet.\n\n\n\n```\nvar params = {\n    TableName: \"tableXYZ\",\n    Key: {\n        userid: user.userid.S,\n    },\n    UpdateExpression: \"SET secondsLeft = :newsecondsLeft\", \n    ExpressionAttributeValues: { \n        \":newsecondsLeft\": user.secondsLeft.N,\n    },\n    ReturnValues: \"UPDATED_NEW\"\n};\n\n\ntry {\n        const data = await dbclient.send(new UpdateItemCommand(params));\n        console.log(\"data:\" + JSON.stringify(data));\n        return true;\n    } catch (error) {\n        console.error(error);\n        return false;\n    }\n\n```\n\nThis basically throws\n\n\n\n```\nTypeError: Cannot read property '0' of undefined\n    at Object.AttributeValue.visit (XX\\node_modules\\@aws-sdk\\client-dynamodb\\dist\\cjs\\models\\models_0.js:1101:40)\n    at XX\\node_modules\\@aws-sdk\\client-dynamodb\\dist\\cjs\\protocols\\Aws_json1_0.js:5074:20\n    at Array.reduce (<anonymous>)\n    at serializeAws_json1_0ExpressionAttributeValueMap (XX\\node_modules\\@aws-sdk\\client-dynamodb\\dist\\cjs\\protocols\\Aws_json1_0.js:5068:34)\n    at serializeAws_json1_0UpdateItemInput (XX\\node_modules\\@aws-sdk\\client-dynamodb\\dist\\cjs\\protocols\\Aws_json1_0.js:6067:40)\n    at Object.serializeAws_json1_0UpdateItemCommand (XX\\node_modules\\@aws-sdk\\client-dynamodb\\dist\\cjs\\protocols\\Aws_json1_0.js:474:27)\n    at serialize (XX\\node_modules\\@aws-sdk\\client-dynamodb\\dist\\cjs\\commands\\UpdateItemCommand.js:42:30)\n    at XX\\node_modules\\@aws-sdk\\middleware-serde\\dist\\cjs\\serializerMiddleware.js:5:27\n    at XX\\node_modules\\@aws-sdk\\middleware-logger\\dist\\cjs\\loggerMiddleware.js:6:28\n\n```\n\nWhen using the exact same params but with the SDK v2, it works:\n\n\n\n```\nvar docClient = new AWS.DynamoDB.DocumentClient();\n\ndocClient.update(params, function (err, data) {\n    if (err) {\n        console.error(\"Unable to update item. Error JSON:\", JSON.stringify(err, null, 2));\n    } else {\n        console.log(\"UpdateItem succeeded:\", JSON.stringify(data, null, 2));\n    }\n});\n\n```\n\nAny help on how to use the SDK V3 for the update would be appreciated!\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"Couple of corrections:\n\n\n- when passing values we need to pass the object with type. so, instead of `user.userid.S` pass entire `user.userid`. Since its not able to determine the type, it is assuming as an array and trying to get the first element of the array and resulting in that error.\n- Numeric values, should simply be passed as String value of type 'N',like `secondsLeft: { N: \"49985\" }`\n\n\nHere is the updated code.\n\n\n\n```\nconst { DynamoDB, UpdateItemCommand } = require(\"@aws-sdk\/client-dynamodb\");\nconst dbclient = new DynamoDB({ region: \"us-east-1\" });\nconst user = {\n  auth: { BOOL: false },\n  username: { S: \"siegbert\" },\n  secondsLeft: { N: \"49985\" },\n  userid: { S: \"123456\" },\n};\nvar params = {\n  TableName: \"tableXYZ\",\n  Key: {\n    id: user.userid,\n  },\n  UpdateExpression: \"SET secondsLeft = :newsecondsLeft\",\n  ExpressionAttributeValues: {\n    \":newsecondsLeft\": user.secondsLeft,\n  },\n  ReturnValues: \"UPDATED_NEW\",\n};\n\ndbclient\n  .send(new UpdateItemCommand(params))\n  .then((result) => {\n    console.log(\"data:\" + result);\n  })\n  .catch((err) => {\n    console.log(\"err\", err);\n  });\n\n```\n\n"}
{"questionId":"04d69eb46ae74840b062d23381abc23b","question":"Material Autocomplete does not work with InputProps\nI am trying to change the border of my `TextField` that is rendered through my `Autocomplete`, but when I add the `InputProps` prop, the `Autocomplete` no longer renders `Chip`s\n\n\n\n```\n<Autocomplete\n    multiple\n    freeSolo\n    options={options}\n    renderTags={(value, { className, onDelete }) =>\n        value.map((option, index) => (\n            <Chip\n                key={index}\n                variant=\"outlined\"\n                data-tag-index={index}\n                tabIndex={-1}\n                label={option}\n                className={className}\n                color=\"secondary\"\n                onDelete={onDelete}\n            \/>\n        ))\n    }\n    renderInput={(params) => (\n        <TextField\n            {...params}\n            id={id}\n            className={textFieldStyles.searchField}\n            label={label}\n            value={value}\n            onChange={onChange}\n            variant=\"outlined\"\n            \/\/InputProps={{\n            \/\/     classes: {\n            \/\/         input: textFieldStyles.input,\n            \/\/         notchedOutline: textFieldStyles.notchedOutline\n            \/\/     }\n            \/\/}}\n            InputLabelProps={{\n                classes: {\n                    root: textFieldStyles.label\n                }\n            }}\n        \/>\n    )}\n\/>\n\n```\n\nThe above code works, and once I uncomment the `InputProps` line, the input no longer renders `Chip`s when an item is selected or entered.\n\n\nThanks\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"This happens because the InputProps attribute is overriding the InputProps parameter of params, you have to merge InputProps property of params:\n\n\n\n```\nInputProps={{\n    ...params.InputProps,\n    classes: {\n        input: textFieldStyles.input,\n        notchedOutline: textFieldStyles.notchedOutline\n    }\n}}\n\n```\n\n"}
{"questionId":"5d8a3c48dbbc4f17a7c5e62ec49ce226","question":"Select attributes on repository.find() with relations (TypeORM)\nMy method returns a a bill object with all of User object.\nI would like that I return only bill object and User with two attributes in entity. I use TypeORM\n\n\n\n```\n  \/**\n   * Returns a bills by account bill\n   *\/\n  async getByAccountBill(\n    accountBill: string,\n    id?: number\n  ): Promise<Object | undefined> {\n    const userService = new UserService();\n    const user = await userService.getById(id);\n\n    const bills = await this.billRepository.find({\n      select: [\"accountBill\"],\n      where: {\n        accountBill: Like(`${accountBill}%`),\n        user: Not(`${user.id}`)\n      },\n      relations: [\"user\"] \/\/ I get All object Entity (userId, password, login...) I want to only name and surname\n    });\n\n    if (bills) {\n      return bills;\n    } else {\n      return undefined;\n    }\n  }\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"You can use querybuilder which is one of the most powerful tool of TypeOrm, to do so.\n\n\n\n```\nconst values = this.billRepository.createQueryBuilder(\"bill\")\n    .leftJoinAndSelect(\"bill.user\", \"user\")\n    .where(\"bill.accountBill LIKE :accountBill\", {accountBill})\n    .andWhere(\"user.id = :userId\", {userId: user.id})\n    .select([\"user.name\", \"user.surname\"])\n    .execute();\n\n```\n\n\n```\n\/\/ NOTE\n\/\/ .execute() will return raw results.\n\/\/ To return objects, use .getMany()\n\n```\n\n"}
{"questionId":"126ba96a28324f0888756ce5b2165a37","question":"Get tests running time with Jest\nIs there any way to know how long my tests take without doing it programmatically with Jest?\n\n\nTo be clear, I know that if I add a variable to get the current time before each test and then log this when my test completes I'll get this information, but I want this automatically, maybe with some Jest configuration.\n\n\n","questionMetadata":{"type":"conceptual","level":"beginner","tag":"javascript"},"answer":"You shouldn't need any configuration to get the running time for your tests\n\n\n\n```\nPASS  src\/containers\/Dashboard\/Dashboard.test.tsx (12.902s)\n\n```\n\nThat 12.902s in the brackets is the total time from when the `test` command was run.\n\n\nIf you want to see the running time per test you can run jest with the --verbose flag and it will show you the time for each test as well as the whole suite.\n\n\n\n```\n  Dashboard Container\n    \u2713 render without crashing (1090ms)\n\n```\n\n"}
{"questionId":"1620f7a9a51f43cb978b28ee6134be5b","question":"How to add jquery third party plugin in rails 6 webpacker\nI know its simple but with update of rails 6. there is new syntax in rails 6 for manage javascript assets which is maintained by webpacker.\n\n\n\n```\n\/\/application.js\nrequire(\"@rails\/ujs\") \/\/.start()\nrequire(\"turbolinks\").start()\nrequire(\"@rails\/activestorage\").start()\nrequire('jquery').start()\nrequire('jquery_ujs').start()\nrequire('bootstrap-daterangepicker').start()\nrequire(\"custom\/custom\").start()\nrequire(\"bootstrap\").start()\nrequire(\"channels\")\n\n```\n\ni am able to add `custom\/custom` but bootstrap and jquery is not working\ni have install jquery and bootstrap via npm\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"run below command to add jQuery.\n\n\n\n```\n$ yarn add jquery\n\n```\n\nAdd below code in `config\/webpack\/environment.js`\n\n\n\n```\nconst webpack = require('webpack')\nenvironment.plugins.prepend('Provide',\n  new webpack.ProvidePlugin({\n    $: 'jquery\/src\/jquery',\n    jQuery: 'jquery\/src\/jquery'\n  })\n)\n\n```\n\nRequire jquery in application.js file.\n\n\n\n```\nrequire('jquery')\n\n```\n\nNo more need to add jquery-rails gem!\n\n\n"}
{"questionId":"9160b21e7079458a85b0d82aead2b7a9","question":"How to fix Warning: validateDOMNesting(...):  cannot appear as a child of \nim passing users list as a props to UserItem Component to make iterate on user list and displaying them on table. the list is displayed correctly and i dont have any divs in my render return but i still get the error :\nindex.js:1446 Warning: validateDOMNesting(...): cannot appear as a child of .\n\n\ntried many solutions found online but none of them worked\n\n\nUsersManagement code :\n\n\n\n```\nimport React, { Component } from 'react';\nimport PropTypes from 'prop-types';\nimport { connect } from 'react-redux';\nimport Spinner from '.\/common\/Spinner';\nimport { getUsers } from '..\/actions\/userActions';\nimport UserItem from '.\/UserItem';\n\nclass UsersManagement extends Component {\n  componentDidMount() {\n    if (!this.props.auth.isAuthenticated) {\n      this.props.history.push('\/login');\n    }\n    this.props.getUsers();\n  }\n\n  render() {\n    const { users, loading } = this.props.user;\n    let usersList;\n    if (users === null || loading) {\n      usersList = <Spinner \/>\n    } else {\n      if (users.length > 0) {\n        usersList = users.map(user => (\n          <UserItem key={user._id} user={user} \/>\n        ))\n      } else {\n        usersList = <h2>No users<\/h2>\n      }\n    }\n\n    return (\n      <div className=\"row\">\n        <div className=\"col-12\">\n          <h1 className=\"text-center mb-2\">Users Management<\/h1>\n          <button type=\"button\" className=\"btn btn-success mb-4\">New User<\/button>\n          <table className=\"table\">\n            <thead>\n              <tr>\n                <th scope=\"col\">Options<\/th>\n                <th scope=\"col\">Username<\/th>\n                <th scope=\"col\">Email<\/th>\n                <th scope=\"col\">Phone Number<\/th>\n              <\/tr>\n            <\/thead>\n            <tbody>\n              {usersList}\n            <\/tbody>\n          <\/table>\n        <\/div>\n      <\/div>\n    )\n  }\n}\n\nUsersManagement.propTypes = {\n  getUsers: PropTypes.func.isRequired,\n  auth: PropTypes.object.isRequired,\n  user: PropTypes.object.isRequired\n}\n\nconst mapStateToProps = state => ({\n  auth: state.auth,\n  user: state.user\n})\n\nexport default connect(mapStateToProps, {\n  getUsers\n})(UsersManagement);\n\n```\n\nUserItem code :\n\n\n\n```\nimport React, { Component } from 'react';\nimport PropTypes from 'prop-types';\n\nclass UserItem extends Component {\n  render() {\n    const { user } = this.props;\n    console.log(user);\n    return (\n      <tr>\n        <th scope=\"row\">\n          <button type=\"button\" className=\"btn btn-primary fa-xs mr-1\"><i className=\"fas fa-pencil-alt\"><\/i><\/button>\n          <button type=\"button\" className=\"btn btn-danger fa-xs\"><i className=\"far fa-trash-alt\"><\/i><\/button>\n        <\/th>\n        <td>{user.username}<\/td>\n        <td>{user.email}<\/td>\n        <td>{user.phone}<\/td>\n      <\/tr>\n    )\n  }\n}\n\nUserItem.propTypes = {\n  user: PropTypes.object.isRequired\n}\n\nexport default UserItem;\n\n```\n\ni expect to to fix the warning message\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"Most likely the component `Spinner` renders a `<div>` as the outermost node. Check the implementation of it.\n\n\nYou implicitly render it inside `<tbody>` through the lines \n\n\n\n```\n<tbody>\n    {usersList}\n<\/tbody>\n\n```\n\nwhere `usersList` defaults to `<Spinner \/>` when there are no users or `loading` is `true`. This is why get the error.\n\n\nA fix would be to wrap the `Spinner` into a `td` that spans the whole row:\n\n\n\n```\nif (users === null || loading) {\n    usersList = <tr><td colSpan=\"4\"><Spinner \/><\/td><\/tr>;\n} else {\n    \/\/ ...\n}\n\n```\n\n"}
{"questionId":"7270c9e38c234336a5bfb6f6892f0074","question":"Return an empty Observable\nThe function `more()` is supposed to return an `Observable` from a get request\n\n\n\n```\nexport class Collection {\n  public more = (): Observable<Response> => {\n    if (this.hasMore()) {\n      return this.fetch();\n    } else {\n      \/\/ return empty observable\n    }\n  };\n\n  private fetch = (): Observable<Response> => {\n    return this.http.get(\"some-url\").map((res) => {\n      return res.json();\n    });\n  };\n}\n\n```\n\nIn this case I can only do a request if `hasMore()` is true, else I get an error on `subscribe()` function `subscribe is not defined`, how can I return an empty Observable?\n\n\n\n```\nthis.collection.more().subscribe(\n  (res) => {\n    console.log(res);\n  }, (err) => {\n    console.log(err);\n  }\n);\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"Since all the answers are outdated, I will post the up to date answer here\n\n\nIn RXJS >= 6\n\n\n\n```\nimport { EMPTY } from 'rxjs'\nreturn EMPTY;\n\n```\n\n"}
{"questionId":"8c2a632fc53f42db9be972c9ce9469bf","question":"How to create arrays from array\nI am a JavaScript beginner and I am trying make two different arrays with values from one main array.\n\n\nMy main array looks like:\n\n\n\n```\n0: Array(3) [ 2011, 127072.7, 51584 ]\n1: Array(3) [ 2012, 125920.3, 59974 ]\n2: Array(3) [ 2013, 129305.4, 15468 ]\n3: Array(3) [ 2014, 135364, 84554 ]\n4: Array(3) [ 2015, 136757, 98754 ]\n5: Array(3) [ 2016, 155653.5, 155548 ]\n6: Array(3) [ 2017, 164130.5, 284848 ]\n\n```\n\nAnd i need create two arrays, first looking like:\n\n\n\n```\n0: Array(2) [ 2011, 127072.7]\n1: Array(2) [ 2012, 125920.3]\n2: Array(2) [ 2013, 129305.4]\n3: Array(2) [ 2014, 135364]\n4: Array(2) [ 2015, 136757]\n5: Array(2) [ 2016, 155653.5]\n6: Array(2) [ 2017, 164130.5]\n\n```\n\n(first and second value)\n\n\nand second like:\n\n\n\n```\n0: Array(2) [ 2011, 51584]\n1: Array(2) [ 2012, 59974]\n2: Array(2) [ 2013, 15468]\n3: Array(2) [ 2014, 84554]\n4: Array(2) [ 2015, 98754]\n5: Array(2) [ 2016, 155548]\n6: Array(2) [ 2017, 284848]\n\n```\n\n(first and third value)\n\n\nI trying splice, filter etc. but I don't know, how to start.\n\n\n**It is not necessary to write me an exact solution, but only steps how to do it.**\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"javascript"},"answer":"You could take a dynamic approach and get all items of the array after the key value for each new array.\n\n\n\n\n\n```\nvar data = [[2011, 127072.7, 51584], [2012, 125920.3, 59974], [2013, 129305.4, 15468], [2014, 135364, 84554], [2015, 136757, 98754], [2016, 155653.5, 155548], [2017, 164130.5, 284848]],\r\n    [first, second] = data.reduce(\r\n        (r, [k, ...a]) => {\r\n            a.forEach((v, i) => r[i].push([k, v]));\r\n            return r;\r\n        },\r\n        Array.from({ length: Array.isArray(data[0]) ? data[0].length - 1 : 0 }, () => [])\r\n    );\r\n\r\nconsole.log(first);\r\nconsole.log(second);\n```\n\n\n```\n.as-console-wrapper { max-height: 100% !important; top: 0; }\n```\n\n\n\n\n\n\n"}
{"questionId":"eb3b4cc29ae24339813bbf1998a2667c","question":"How do I avoid unused setState functions? Can React useState be created without a setter?\nI'm currently reducing \/ removing npm warnings on a React site.\n\n\nA large number of these warnings are caused by the setState function as seen below, being 'unused'.\n\n\n\n```\nconst [state, setState] = useState('some state');\n\n```\n\nWhich of the following would be a better way to remove these warnings? Or is there a better way to approach the issue?\n\n\n1.\n\n\n\n```\nconst[state] = useState('some state');\n\n```\n\n2. \n\n\n\n```\nconst state = 'some state';\n\n```\n\n","questionMetadata":{"type":"optimization","level":"beginner","tag":"javascript"},"answer":"If `setState` isn't being used at all, then it's a value that never changes so can be a constant (2.). You could probably move it out of the component as well.\n\n\n"}
{"questionId":"5bca539a032a4120bd0863b5d3a23447","question":"FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory when processing large files with fs\nI have a nodeJs script that process a bunch of large .csv files (1.3GB for all). It run for a moment and throw this error:\n\n\n`FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory`\n\n\nI have tried to put `#!\/usr\/bin\/env node --max-old-space-size=4096` at the beginning of my file but this didn't solve my problem...\n\n\nTried to google it but nearly no pertinent info about my issue..\n\n\nCan I flush memory once I do not need file content ? Do I need to allocate more memory to nodeJs ?\n\n\nThanks ;)\n\n\nHere is my code sample:\n\n\n\n```\nfs.readdir(dirName, function(err, filenames) {\n    if (err) console.error(err);\n    else {\n        filenames.forEach(function(filename) {\n            fs.readFile(dirName + filename, 'utf-8', function(err, content) {\n                if (err) console.error(err);\n                else processFile(content);\n            });\n        });\n    }\n});\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"I have finally found the solution to the problem ! I needed to launch the process adding the `--max-old-space-size=8192` param to node process like so:\n\n\n\n```\nnode --max-old-space-size=8192 .\/myScript.js\n\n```\n\nI processed my 1.3GB without a problem !!\n\n\n"}
{"questionId":"d2af1e2e0e4744c4bb80db9115e13e62","question":"Mocking axios with Jest throws error \u201cCannot read property 'interceptors' of undefined\u201d\nI\u2019m having trouble mocking axios with Jest and react-testing-library. I\u2019m stuck on an error around axios interceptors and can\u2019t make my way around it.\n\n\nThis is my `api.js` file:\n\n\n\n```\nimport axios from 'axios';\n\nconst api = axios.create({\n  baseURL: window.apiPath,\n  withCredentials: true,\n});\n\napi.interceptors.request.use(config => {\n  const newConfig = Object.assign({}, config);\n  newConfig.headers.Accept = 'application\/json';\n\n  return newConfig;\n}, error => Promise.reject(error));\n\n```\n\nThe call to api in my component:\n\n\n\n```\nconst fetchAsync = async endpoint => {\n  const result = await api.get(endpoint);\n  setSuffixOptions(result.data.data);\n};\n\n```\n\nThen in my spec file:\n\n\n\n```\njest.mock('axios', () => {\n  return {\n    create: jest.fn(),\n    get: jest.fn(),\n    interceptors: {\n      request: { use: jest.fn(), eject: jest.fn() },\n      response: { use: jest.fn(), eject: jest.fn() },\n    },\n  };\n});\n\ntest('fetches and displays data', async () => {\n  const { getByText } = render(<Condition {...props} \/>);\n  await expect(getByText(\/Current milestone\/i)).toBeInTheDocument();\n});\n\n```\n\nThe test fails with this message:\n\n\n\n```\n    TypeError: Cannot read property 'interceptors' of undefined\n\n       6 | });\n       7 |\n    >  8 | api.interceptors.request.use(config => {\n         |                ^\n       9 |   const newConfig = Object.assign({}, config);\n      10 |   newConfig.headers.Accept = 'application\/json';\n      11 |\n\n```\n\nWhat am I doing wrong here?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"the `create` method is what creates the api which has the `get` and `interceptors` methods. So you need to create a dummy api object:\n\n\n\n```\n\njest.mock('axios', () => {\n  return {\n    create: jest.fn(() => ({\n      get: jest.fn(),\n      interceptors: {\n        request: { use: jest.fn(), eject: jest.fn() },\n        response: { use: jest.fn(), eject: jest.fn() }\n      }\n    }))\n  }\n})\n\n```\n\n"}
{"questionId":"7000db4dfd704a9f89ba0d4843551d19","question":"Cannot Set Property ... Which Only Has Getter (javascript es6)\nSo I have a simple Javascript class\n\n\n\n```\nclass MyClass {\n    constructor(x) {\n        this.x = x === undefined ? 0 : x;\n    }\n\n    get x() {\n        return this.x;\n    }\n}\n\n```\n\nWhen a MyClass is created, I want it's x to be set to the value passed in as a parameter. After this, I do not want it to be able to be changed, so I have intentionally not made a set x() method.\n\n\nHowever, I guess I must be missing something fundamental, as this is giving me the \"Cannot set property ... which only has getter\" error.\n\n\nHow do I assign a value to x without creating a setter method? \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"There are a couple problems here.\n\n\nWhen you make a getter via `get x()` you are causing `this.x` to result in calling the getter, which will recurse indefinitely due to your `get x()` doing `this.x`.\n\n\nReplace your references to `this.x` with `this._x` in this code like this:\n\n\n\n```\nclass MyClass {\n    constructor(x) {\n        this._x = x === undefined ? 0 : x;\n    }\n\n    get x() {\n        return this._x;\n    }\n}\n\n```\n\nNow your encapsulated `x` which is now `_x` will not be confused for a call to the getter via `this.x`.\n\n\n"}
{"questionId":"9ab37c4570dc4993a8a13153b3bda152","question":"Invert a Map object\nI was wondering, what is the most convenient way to invert keys and values in a Map. Is there any builtin method or should it be done by iterating over keys and values?\n\n\n\n```\nconst map: Map<string, number> = new Map()\n\n```\n\n\n```\nconst inverse: Map<number, string>\n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"javascript"},"answer":"You could pass the inverse tuples to the constructor, using `Array.from` and `Array#reverse`:\n\n\n\n```\nnew Map(Array.from(origMap, a => a.reverse()))\n\n```\n\nSee it run on an example:\n\n\n\n\n\n```\nconst origMap = new Map([[1, \"a\"],[2, \"b\"]]);\r\nconsole.log(...origMap);\r\n\r\n\/\/ Reverse:\r\nconst inv = new Map(Array.from(origMap, a => a.reverse()));\r\nconsole.log(...inv);\n```\n\n\n\n\n\n\n"}
{"questionId":"92a3355adca14826911f153f3b969c0f","question":"ReactJS vs NodeJS - Why do I need to create both?\nI understand that React is frontend, and NodeJS is the backend that allows Javascript code to function outside of a browser. What I don't understand (and this is after following tutorials online on setting up a React project and a NodeJS project) is why I have to create an instance of each.\n\n\nFor example, in my React project, I managed to create a website. But because I needed a backend, I decided to use NodeJS. But I'm doing NodeJS tutorials, and I can create a website using NodeJS too. I'm confused because right now, it's appearing to be that React and NodeJS do the *SAME THING*. \n\n\nI have never worked with NodeJS before so I am a bit confused. I was under the impression that I would just use NodeJS to host the backend, but after seeing that I'm literally having to create an entire website with NodeJS, I don't understand how I'm supposed to use React and NodeJS together. \n\n\nHow do the two, React and NodeJS, integrate together to create a fully-functioning web app? I have yet to see something online that clearly breaks down how the two interact.\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"javascript"},"answer":"React is front-end library. It provides great tooling for creatiing user interfaces. And it creates a single page application. Which means when you open a react app. It does not reload and its really fast.\n\n\nWhile you could also use nodejs and something like handlebars to create a website. But that website would be rendered on server and then served to the user. But its alot more than that. There are a lot of things that you want to do on the server. Like authentication. You want that part to be secure. So you keep it on the server.\n\n\nNow the answer to the final part of your question.\n\n\nFor a fully functional app. You would use react to create user interfaces. And nodejs for creating an API which your react app will call.\n\n\n"}
{"questionId":"29c0a17115f5415697a24048e996a79b","question":"React Routing Redirect onClick\nIve been trying to do a simple redirect to another component on button click, but for some reason it doesnt work.I want to redirect to '\/dashboard' and display AdminServices from login as follows: \n\n\n\/\/index.js\n\n\n\n```\nReactDOM.render(<BrowserRouter><App \/><\/BrowserRouter>, \n    document.getElementById(\"root\"));\n\n```\n\n\/\/App.js\n\n\n\n```\n     <Switch>\n          <Route path=\"\/\" component={Login} \/>\n          <Route path=\"\/dashboard\" component={AdminServices} \/>\n        <\/Switch>\n\n```\n\n\/\/Login.js\n\n\n\n```\n<Button\nonClick={this.login}\n>\n<\/Button>\n\nlogin = () =>{\n    <Redirect to=\"\/dashboard\" \/>\n  }\n\n```\n\n\/\/AdminServices.js\n\n\n\n```\nrender(){\n        return(\n            <div>Test<\/div>\n        );\n    }\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"Instead of using the props.history, a better way is using the useHistory hook like below:\n\n\n\n```\nimport { useHistory } from 'react-router-dom'\n\nconst MyComponent = (props) => {\n  const history = useHistory();\n\n  handleOnSubmit = () => {\n    history.push(`\/dashboard`);\n  };\n};\n\n```\n\n"}
{"questionId":"909deaded03f4279adcd62253161c166","question":"How should I use Svelte Reactivity with DOM getElementById?\nI have a div element where it is scrollable\n\n\n\n```\n    <script>\n        let scrollBoxObj;\n        $: scrollBoxObj = document.getElementById(\"chat-box\");\n        \n        $: if (!(scrollBoxObj === undefined) && scrollBoxObj.scrollTop < scrollBoxObj.scrollHeight) {\n            scrollBoxObj.scrollTop = scrollBoxObj.scrollHeight;\n        }\n    <\/script>\n    <div id=\"scrollBox\" class=\"h-screen w-auto chat-box border border-orange rounded\">\n        <div id=\"chat-box\" style=\"margin: 0\" class=\"chat-box\">\n            {#each chatBox as { user, content, type}}\n                <MessageBox {user} message={content} {type} \/>\n            {\/each}\n        <\/div>\n    <\/div>\n\n    <style>\n        .chat-box {\n            overflow-y: auto;\n        }\n    <\/style>\n\n```\n\ni am trying to auto scroll down when a new message is added.\nbut it is not reactive.\nor i didn't understand how reactivity works in svelte.\ni also tried to assign scrollBoxObj in onMount but it was still the same result didn't work.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"In Svelte, the reactivity from `$: x = y` kicks in when there is a change on the left side from the equation (the `y`-part).\n\n\nIn your code you have\n\n\n\n```\n$: scrollBoxObj = document.getElementById(\"chat-box\");\n\n```\n\nOn the left side we have a string (*\"chat-box\"*) which is a constant and will never change, there is also the *document* which will also never change, this means the reactivity will not work as you expect.\n\n\nWhen working with Svelte using `document.getElementById` (or any of the other selector methods) is considered bad practice because you are interacting directly with the DOM, while your DOM should rather be a reflection of your state. In short: don't do it.\n\n\nSvelte provides you with an easy way to bind a DOM-node (like a *div*) to a variable by simply adding `bind:this={variable}`\n\n\nIn your case this would become:\n\n\n\n```\n<script>\n let scrollbox\n<\/script>\n\n<div bind:this={scrollbox}>\n ...\n<\/div>\n\n```\n\nThe *reactive declaration* (the *if* block) will execute whenever anything inside it changes, which is scrollbox in your case:\n\n\n\n```\n$: if (scrollBoxObj && scrollBoxObj.scrollTop < scrollBoxObj.scrollHeight) {\n  scrollBoxObj.scrollTop = scrollBoxObj.scrollHeight;\n}\n\n```\n\nJust note here again that this will only trigger when *scrollbox* changes, once it is bound you will not retrigger (for instance when the user scrolls, it will not do anything) for that you should probably use an `on:scroll` event.\n\n\n"}
{"questionId":"9b808f7e3e2447c9bfc1b9130f35da4d","question":"Destructuring Nested objects in javascript | Destructure second level parent and child Objects\nI need to destructure and get values of title, child, childTitle from this object\n\n\n\n```\nconst obj1 = {\n   title : 'foo',\n   child : {\n       title2 : 'bar'\n   }\n}\n\nlet {title, child} = obj1;\nconsole.log(title)   \/\/ 'foo'\nconsole.log(child)   \/\/ { title : 'bar' } \n\n\/\/ but couldn't get child object this way\n\nlet { title , child : { title2 } } = obj1;\nconsole.log(title)   \/\/ 'foo'\nconsole.log(child)   \/\/ undefined\nconsole.log(title2)  \/\/ 'bar'\n\n```\n\nHow could I get the child object?\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"javascript"},"answer":"`child: { title2 }` is just destructuring the child property. If you want to pick up the child property itself simply specify it in the statement: `let { title, child, child: { title2 } } = obj1;`\n\n\n\n\n\n```\nconst obj1 = {\r\n  title: 'foo',\r\n  child: {\r\n    title2: 'bar'\r\n  }\r\n}\r\n\r\nlet { title, child, child: { title2 } } = obj1;\r\n\r\nconsole.log(title);\r\nconsole.log(child); \r\nconsole.log(title2);\n```\n\n\n\n\n\n\n"}
{"questionId":"b323cbe3fccf4eacb7d59bd28bdd9c6b","question":"How to pass a prop to {children} in React?\nI have a Parent component that acts as a wrapper to its children. How do I pass a prop to a child that will be rendered using the format below?\n\n\n\n```\nimport React, { useEffect, useState } from 'react';\n\nconst Parent = ({ children }) => {\n  const [completeState, setCompleteState] = useState(false);\n\n  useEffect(\n    () => {\n      \/* .. code that runs then sets completeState to true *\/\n    setCompleteState(true);\n     }, []\n  );\n\n  return (\n     <section>\n       \/* \n          how to pass a 'didComplete' prop to children?\n           didComplete={completeState}\n       *\/\n       {children} \/\/ Child component below would be rendered here with the didComplete prop passed in\n    <\/section>\n\n  )\n}\n\n```\n\n\n```\nimport React from 'react';\n\nconst Child = ({ didComplete }) => (<h1>The function completed? {didComplete}<\/h1>); \n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"javascript"},"answer":"The `props.children` is a React *Element*, which is nothing but a plain JS object having the description of what needs to be rendered in the DOM.\n\n\nTo provide additional details, we need to *clone* the existing React Element object and provide the additional details.\n\n\nTo do this we can use the `React.cloneElement` API to pass the additional prop to the `children` :\n\n\n\n```\nreturn (\n     <section>\n       {React.cloneElement(children, {didComplete: completeState})}\n    <\/section>\n);\n\n```\n\n"}
{"questionId":"59d6815db9a64416bbf3e381c917a9f0","question":"How to find by nested property in mongoose\nI'm trying to find an object in my database by a nested property, I can't seem to find any way to do it. My schema is below and I have shown how I've attempted to query.\n\n\n\n```\nvar stations = {\n    Alpha: Number,\n    Beta: Number\n};\nvar systemSchema = new mongoose.Schema({\n    name: String,\n    location: String,\n    nodes: {\n        main: stations,\n        secondary: stations,\n        tertiary: stations\n    }\n});\n\nvar System = mongoose.model(\"System\", systemSchema);\n\nSystem.findOne({ nodes: { main: {Alpha: 23000}}}, function(err, system){\n    if(err){console.log(err);}\n    else{console.log(system);}\n});\n\n```\n\nEvery time I run this, nothing gets returned. I was expecting that I would have the corresponding object in my database returned. \n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"Change this\n\n\n\n```\nSystem.findOne({ nodes: { main: {Alpha: 23000}}}, function(err, system){\n if(err){console.log(err);}\n  else{console.log(system);}\n});\n\n```\n\nto \n\n\n\n```\n System.findOne({ 'nodes.main.Alpha': 23000}, function(err, system){\n   if(err){console.log(err);}\n   else{console.log(system);}\n });\n\n```\n\nThis will work\n\n\n"}
{"questionId":"5cb9a76a870143529d9c6174af370245","question":"How to divide two native JavaScript BigInt's and get a decimal result\nHere's what I've tried so far. I'm looking to get a `12.34`:\n\n\n`BigInt('12340000000000000000') \/ BigInt('1000000000000000000')`\n\n\n\n> \n> 12n\n> \n> \n> \n\n\n`Number(BigInt('12340000000000000000') \/ BigInt('1000000000000000000'))`\n\n\n\n> \n> 12\n> \n> \n> \n\n\nFWIW, when I use the JSBI lib, it's working how I'd like:\n\n\n`JSBI.BigInt('12340000000000000000') \/ JSBI.BigInt('1000000000000000000');`\n\n\n\n> \n> 12.34\n> \n> \n> \n\n\nIs that not possible natively?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"You should multiply the numerator to accommodate the number of digits you need, perform the division and then divide with normal floating point division.\n\n\n(Run in browser that supports BigInt, like Chrome)\n\n\n\n\n\n```\nvar a = 12340000000000000000n;\nvar b =  1000000000000000000n;\n\nconsole.log(Number(a * 100n \/ b) \/ 100);\n```\n\n\n\n\n\n\nBy only converting to Number at the \"end\", you will lose the least precision.\n\n\n### More precision\n\n\nIf you need more than 16 digits precision *and* need decimals, then you'll need to throw your own implementation of a kind of `BigDecimal` API, or use an existing one.\n\n\nHere is a simple one using `BigInt` as its base type, combined with a configuration that determines how many digits (from the right) of each such BigInt should be interpreted as decimals (digits in the fractional part). That last information will for instance be used to insert a decimal separator when outputting the number as a string.\n\n\n\n\n\n```\nclass BigDecimal {\n    constructor(value) {\n        let [ints, decis] = String(value).split(\".\").concat(\"\");\n        decis = decis.padEnd(BigDecimal.decimals, \"0\");\n        this.bigint = BigInt(ints + decis);\n    }\n    static fromBigInt(bigint) {\n        return Object.assign(Object.create(BigDecimal.prototype), { bigint });\n    }\n    divide(divisor) { \/\/ You would need to provide methods for other operations\n        return BigDecimal.fromBigInt(this.bigint * BigInt(\"1\" + \"0\".repeat(BigDecimal.decimals)) \/ divisor.bigint);\n    }\n    toString() {\n        const s = this.bigint.toString().padStart(BigDecimal.decimals+1, \"0\");\n        return s.slice(0, -BigDecimal.decimals) + \".\" + s.slice(-BigDecimal.decimals)\n                .replace(\/\\.?0+$\/, \"\");\n    }\n}\nBigDecimal.decimals = 18; \/\/ Configuration of the number of decimals you want to have.\n\n\/\/ Demo\nvar a = new BigDecimal(\"123456789123456789876\");\nvar b = new BigDecimal( \"10000000000000000000\");\n\nconsole.log(a.divide(b).toString());\n```\n\n\n\n\n\n\nAgain, this needs a browser that supports `BigInt` (Chrome at the time of writing).\n\n\n"}
{"questionId":"a7167187917d47018065095567cdcb9f","question":"How to parse dates in JSON request with NestJs @Body\nI have a DTO that looks like this:\n\n\n\n```\nclass PersonDto {\n   readonly name: string;\n   readonly birthDate: Date;\n}\n\n```\n\nMy NestJs controller method looks like this:\n\n\n\n```\n@Post\ncreate(@Body() person: PersonDto) {\n    console.log(\"New person with the following data:\", person);\n    \/\/ more logic here\n}\n\n```\n\nThe JSON data that gets posted has `birthDate` as a string: `\"2020-01-15\"`. How can I convert this string to a JavaScript `Date` object? I'd like to add the `@IsDate` class-validation to `PersonDto` but currently that would fail.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"I figured out how to use the global `ValidationPipe` with a Date property and the `@IsDate()` annotation:\n\n\nThe first step is to allow transformations like this (my bootstrap file as an example):\n\n\n\n```\nasync function bootstrap() {\n  const app = await NestFactory.create(AppModule);\n  app.useGlobalPipes(new ValidationPipe({transform: true}));\n  await app.listen(3000);\n}\nbootstrap();\n\n```\n\nThen you need to annotate the DTO with the `@Type()` annotation:\n\n\n\n```\nimport { IsDate } from 'class-validator';\nimport { Type } from 'class-transformer';\n\nclass PersonDto {\n   readonly name: string;\n   @Type(() => Date)\n   @IsDate()\n   readonly birthDate: Date;\n}\n\n```\n\n"}
{"questionId":"70acd4ed405b4d809feaa5a725981a71","question":"CSS' calc() in styled-components\nTrying this:\n\n\n\n```\nconst styledDiv = styled.div`\n  ${props => props.takeViewportHeight && `min-height: calc(100vh-16px);`}\n`\n\n```\n\nIt's not working. Am I missing something about calc in styled components?\n\n\n","questionMetadata":{"type":"debugging","level":"beginner","tag":"javascript"},"answer":"A funny tip about CSS `calc`:\n\n\n\n> \n> whitespaces are required between +\/-\n> \n> \n> \n\n\nmeaning:\n\n\n\n```\nconst styledDiv = styled.div`\n  ${props => props.takeViewportHeight && `min-height: calc(100vh - 16px);`}\n`\n\n```\n\nshould work\n\n\n"}
{"questionId":"7835844d96404a169de0bf6e768165da","question":"Should e2e tests persist data in real databases?\nI've been reading a lot about e2e testing and one thing I cannot understand is how \"real\" should e2e tests be.\n\n\nRegardless of the tools I use for the e2e tests, I've seen that most of the time they hit either local, development or alpha environments.\n\n\nIf my application has authentication, should I create a \"test\" user with valid credentials in the database? Should I do that for Alpha or even Production environments? How else would this test user login into my application?\n\n\nSay I have the infamous TODO app. I have a test that logs the user in. After logging in, I want to test that the user is able to create a TODO. This TODO is saved in a Database. \n\n\nAfter running the tests, should I run something to remove the data created during e2e tests? Or should I intercept the request just before saving it and mock the response (would this be an antipattern for e2e testing)? \n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"javascript"},"answer":"I'm currently working at a large well-known company on our test tools and frameworks team. So while I'm no expert, it is something that's part of my job. I'm going to be talking specifically about web testing. Testing is somewhat different for native apps like iOS and Android and I'm not super familiar with those aspects.\n\n\nThe terminology between e2e (end to end) and integration tests is somewhat interchangeable, while unit tests have a more specific definition.\n\n\nGenerally e2e\/integration tests should be runnable in dev and production environments. Depending on your setup, your dev environment is probably using some semi-frequently updated snapshot of your production db. In other cases, your local environment may be hitting the actual production db. There are pros\/cons to both approaches, but it largely depends on the scale of your company or project. For example, if you're at a large company with dedicated teams you can see many changes a day hitting production databases vs a small team where a weekly snapshot of the prod db is probably good enough for testing locally.\ni\nAt the base level, all integration tests should be treated as real. When dealing with web apps there are lots of other factors we have to take into account like different web browsers, network activity \/ availability, etc. So mocking out data for api calls would allow for super fast testing, but then adds another level of complexity with making sure the mocks stay up to date with the real-world database.\n\n\nRunning integration tests locally should more or less be doing the same thing against your dev server that they will be doing against staging and production. With the exception of having the app detecting whether its running in a dev, staging, or production environment to switch out URL's and various credentials, the app should be expected to behave exactly the same way.\n\n\nIn regards to your question about authentication, the answer is yes. Lets look at 2 examples that show different considerations.\n\n\nSuppose your project is very small. You create some real accounts on production and your db gets snapshotted weekly for use in your local dev environment. You just run your integration tests with one or more of those users as needed. Since the local tests are only hitting your local db, you don't need to worry about the data being generated since it won't affect production. Other engineers on your team can use the same user(s) and not worry about it. If one engineer makes some changes to the db schema, ORM, etc then everyone just gets a new copy of the db snapshot and continues working.\n\n\nNow for the other extreme. Suppose your project is very big. Millions of users and hundreds of employees all collectively making changes to the codebase and db every day. There are all kinds of ways that infrastructures are setup to handle various engineering tasks. There's too much data and the db changes too often to make using local snapshots feasible. At this scale, you're probably doing continuous integration and running your tests on every commit. You want to do that so that incoming changes don't make it to production and cause major problems. You're probably running your local dev environments against either a constantly updated staging database, or perhaps even against your production db itself. (Try planning for the staging db as it avoids a lot of other problems.) \n\n\nNow, having just a small set of dedicated test users starts to be a problem. Tests are running all the time, both automated and by dozens of engineers all working on their own bits of work. Since the staging db is probably shared, you easily start getting weird conflicts as the same test user is doing all kinds of things and starts causing tests to fail. A good solution I've seen for this is a kind of test account checkout server. You create say 100 or 1000 (or more) test user accounts. When your integration tests run, they literally check out a test user account from the server. When the tests are done, the integration tests clean up whatever changes they made on that user and tell the checkout server that the user is free again. Then it randomly gets checked out by someone\/something else and the cycle continues.\n\n\nSo the take aways that related directly to your question:\n\n\n1. You should always have dedicated test user accounts that are exactly the same as regular user accounts, just dedicated to testing.\n2. Depending on scale of team and project, if small a few dedicated accounts is fine. If working on a much larger scale, you need many more dedicated test accounts and probably want an automated service that allows individual test runs to checkout users as needed.\n3. Tests should always clean up after themselves. If a test creates a TODO that gets stored in the db. When the test is done running, that TODO should be deleted from the db. If you aren't constant about this, you'll eventually run into bugs and issues where data is inconsistent. God forbid this happens in production.\n4. Only worry about mocking data for unit tests, unless you're working in a very good and dedicated engineering environment where you have people dedicated to keeping the db mocks up to date all the time. If you *can* do that, your integration tests will be very fast and you don't really have to worry about the db stuff as much. But its hard to maintain this over time without dedicated support.\n\n\n"}
{"questionId":"486440240d094e798bc9f66d499a388b","question":"NextJS getServerSideProps() with multiple fetch requests\nIs there a way to fetch data from multiple API routes in a single `getServerSideProps()`?\n\n\nI have a table that I need to show data from multiple MongoDB collections and trying to figure out how to pull that data in.\n\n\nEssentially, I need to combine these two functions but can't seem to find the best way to go about it.\n\n\n\n```\nexport async function getServerSideProps() {\n  const res = await fetch(`${process.env.APP_DOMAIN}\/api\/${apiRoute}`);\n  const { data } = await res.json();\n  return { props: { operations: data } };\n}\n\nexport async function getServerSideProps() {\n  const res = await fetch(`${process.env.APP_DOMAIN}\/api\/${apiRoute2}`);\n  const { data } = await res.json();\n  return { props: { incidents: data } };\n}\n\n```\n\nI may be attempting something dumb so a pointer in the right direction is greatly appreciated!\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"Did you try the following?\n\n\n\n```\nexport async function getServerSideProps() {\n  const [operationsRes, incidentsRes] = await Promise.all([\n    fetch(`${process.env.APP_DOMAIN}\/api\/${apiRoute}`), \n    fetch(`${process.env.APP_DOMAIN}\/api\/${apiRoute2}`)\n  ]);\n  const [operations, incidents] = await Promise.all([\n    operationsRes.json(), \n    incidentsRes.json()\n  ]);\n  return { props: { operations, incidents } };\n}\n\n```\n\n`Promise.all` will trigger both requests and they will return the resolved value for both fetch calls when completed\n\n\n"}
{"questionId":"1b56b24cebd443e6989006565285e916","question":"console log the state after using useState doesn't return the current value\nusing `console.log()` after using reactjs `useState()` hook, doesn't return the current value of this state, How can I handle this?\n\n\nHere's code for the case, try to figure out what's the console log display.\n\n\n\n```\nimport React, { useState } from \"react\";\nimport ReactDOM from \"react-dom\";\n\nfunction Weather() {\n  const [weather, setWeather] = useState();\n\n  return (\n    <input\n      value={weather}\n      onChange={(e) => {\n        setWeather(e.target.value);\n        console.log(weather);\n      }}\n    \/>\n  );\n}\n\nconst rootElement = document.getElementById(\"root\");\nReactDOM.render(<Weather \/>, rootElement);\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"`useState` by default simply does one thing and one thing only, set the new state and cause a re-render of the function. It is asynchronous in nature so by default, methods running after it usually run.\n\n\nFrom your example, on a fresh load of the page, typing 's' causes useState to change the state, but because it is asynchronous, `console.log` will be called with the old state value, i.e. `undefined` (since you didn't set a value. You should consider setting an initial state, if you want to)\n\n\n\n```\nconst [weather, setWeather] = useState('');    \/\/ Set the intial state\n\n```\n\nThe only way to truly read the value of the state is to use `useEffect`, which is called when there is a re-render of the component. Your method simply becomes:\n\n\n\n```\nimport React, { useEffect, useState } from 'react';\nimport ReactDOM from 'react-dom';\n\nfunction Weather() {\n    const [weather, setWeather] = useState('');\n\n    useEffect(() => console.log(weather), [weather]);\n\n    const changeValue = event => setWeather(event.target.value);\n\n    return <input value={weather} onChange={changeValue} \/>;\n}\n\nconst rootElement = document.getElementById('root');\nReactDOM.render(<Weather \/>, rootElement);\n\n```\n\n"}
{"questionId":"beec85cefc43470cae26234d74c7fc57","question":"'yield' expression implicitly results in an 'any' type because its containing generator lacks a return-type annotation\nThe first snippet is the code im working with and below is the error it throws and it happens on every \"yield select\" portion that is in the code and im not sure what my next step is.\n\n\n\n```\nfunction* onLoadingDomainsresult() {\n  const pathname = yield select(getPathname);\n\n  interface Params {\n    hastag: string;\n  }\n\n'yield' expression implicitly results in an 'any' type because its containing generator lacks a return-type annotation.  TS7057\n\n    113 | \n    114 | function* onLoadingDomainsresult() {\n  > 115 |   const pathname = yield select(getPathname);\n        |                    ^\n    116 | \n    117 |   interface Params {\n    118 |     hastag: string;\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"The literal type of `select(getPathname)` doesn't relate to the value you get back from the `yield`. `select(getPathname)` is the value yielded by your co-routine to its iterating context.\n\n\nThe value injected into your generator by its running context (through the `next()` call) DOES matter to the type you get back from the `yield` expression.\n\n\nEither way, currently Typescript has no metadata about what it's going to get at all, since your generator function has no type annotation.\n\n\nI'm guessing this is redux-saga.\n\n\nA typical Generator function type annotation is something like...\n\n\n\n```\ntype WhatYouYield=\"foo\"\ntype WhatYouReturn=\"bar\"\ntype WhatYouAccept=\"baz\"\n\nfunction* myfun(): Generator<\n  WhatYouYield,\n  WhatYouReturn,\n  WhatYouAccept\n> {\nconst myYield = \"foo\" \/\/type of myYield is WhatYouYield\nconst myAccepted = yield myYield; \/\/type of myAccepted is WhatYouAccept\nreturn \"baz\" \/\/type of this value is WhatYouReturn \n}\n\n```\n\n...and the error you're getting is from Typescript having to guess the `WhatYouAccept` type without the Generator type annotation on your function.\n\n\n"}
{"questionId":"dc60b48a69754efd9d4f493f1b9912b4","question":"How to separate web components to individual files and load them?\nI have a web component `x-counter`, which is in a single file.\n\n\n\n```\nconst template = document.createElement('template');\ntemplate.innerHTML = `\n  <style>\n    button, p {\n      display: inline-block;\n    }\n  <\/style>\n  <button aria-label=\"decrement\">-<\/button>\n    <p>0<\/p>\n  <button aria-label=\"increment\">+<\/button>\n`;\n\nclass XCounter extends HTMLElement {\n  set value(value) {\n    this._value = value;\n    this.valueElement.innerText = this._value;\n  }\n\n  get value() {\n    return this._value;\n  }\n\n  constructor() {\n    super();\n    this._value = 0;\n\n    this.root = this.attachShadow({ mode: 'open' });\n    this.root.appendChild(template.content.cloneNode(true));\n\n    this.valueElement = this.root.querySelector('p');\n    this.incrementButton = this.root.querySelectorAll('button')[1];\n    this.decrementButton = this.root.querySelectorAll('button')[0];\n\n    this.incrementButton\n      .addEventListener('click', (e) => this.value++);\n\n    this.decrementButton\n      .addEventListener('click', (e) => this.value--);\n  }\n}\n\ncustomElements.define('x-counter', XCounter);\n\n```\n\nHere the template is defined as using JavaScript and html contents are added as inline string. Is there a way to separate template to an `x-counter.html` file, css to say, `x-counter.css` and corresponding JavaScript code to `xcounter.js` and load them in index.html?\n\n\nEvery example I lookup has web components mixed. I would like to have separation of concerns, but I am not sure how to do that with components. Could you provide a sample code? Thanks.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"In the main file, use `<script>` to load the Javascript file `x-counter.js`.\n\n\nIn the Javascript file, use `fetch()` to load the HTML code `x-counter.html`.\n\n\nIn the HTML file, use `<link rel=\"stylesheet\">` to load the CSS file `x-counter.css`.\n\n\n**CSS file : `x-counter.css`** \n\n\n\n```\nbutton, p {\n    display: inline-block;\n    color: dodgerblue;\n}\n\n```\n\n**HTML file : `x-counter.html`**\n\n\n\n```\n<link rel=\"stylesheet\" href=\"x-counter.css\">\n<button aria-label=\"decrement\">-<\/button>\n    <p>0<\/p>\n<button aria-label=\"increment\">+<\/button>\n\n```\n\n**Javascript file : `x-counter.js`**\n\n\n\n```\nfetch(\"x-counter.html\")\n    .then(stream => stream.text())\n    .then(text => define(text));\n\nfunction define(html) {\n    class XCounter extends HTMLElement {\n        set value(value) {\n            this._value = value;\n            this.valueElement.innerText = this._value;\n        }\n\n        get value() {\n            return this._value;\n        }\n\n        constructor() {\n            super();\n            this._value = 0;\n\n            var shadow = this.attachShadow({mode: 'open'});\n            shadow.innerHTML = html;\n\n            this.valueElement = shadow.querySelector('p');\n            var incrementButton = shadow.querySelectorAll('button')[1];\n            var decrementButton = shadow.querySelectorAll('button')[0];\n\n            incrementButton.onclick = () => this.value++;\n            decrementButton.onclick = () => this.value--;\n        }\n    }\n\n    customElements.define('x-counter', XCounter);\n}\n\n```\n\n**Main file : `index.html`**\n\n\n\n```\n<html>\n<head>\n    <!-- ... -->\n    <script src=\"x-counter.js\"><\/script>\n<\/head>\n<body>\n    <x-counter><\/x-counter>\n<\/body>\n<\/html>\n\n```\n\n"}
{"questionId":"a2c139feaf8d4b5c99596bc1567da982","question":"RxJS combineLatest without waiting for source observables to emit?\nI have two source observables from where I need to calc some data as soon as one source observable emits. I'm trying to use the `combineAll()` operator but it only emits a value when each of the source observables emits for the first time.\n\n\nIs there any operator similar to `combineAll()` that emits as soon as any of the source observables emits for the first time? If not, what's the clearest way of doing it?\n\n\nWhat I've tried:\n\n\n\n```\nconst source1$ = service.getSomeData();\nconst source2$ = service.getOtherData();\n\ncombineLatest(\n  source1$,\n  source2$\n).pipe(\n  map([source1Data, source2Data] => {\n    \/\/ this code only gets executed when both observables emits for the first time\n    return source1Data + source2Data;\n  })\n)\n\n```\n\n","questionMetadata":{"type":"implementation","level":"advanced","tag":"javascript"},"answer":"One way is prefixing all sources with `startWith`:\n\n\n\n```\ncombineLatest([\n  source1$.pipe(startWith(?)),\n  source2$.pipe(startWith(?)),\n])\n\n```\n\n\n> \n> that emits as soon as any of the source observables emits for the first time?\n> \n> \n> \n\n\nThis looks like you might be looking for `race(source1$, source2$)` Observable creation method or maybe just `merge(source1$, source2$).pipe(take(1))`. But it really depends on what you want to do.\n\n\n"}
{"questionId":"1e69295ae41c48c691abda11e761cb0e","question":"Puppeteer Execution context was destroyed, most likely because of a navigation\nI am facing this problem in puppeteer in a for loop when i go on another page to get data, then when i go back it comes me this error line:\n\n\n\n```\nError \"We have an error Error: the execution context was destroyed, probably because of a navigation.\"\n\n```\n\nIt's a directory page that contains 15 companies per page and then I want to visit each company to get information.\n\n\n\n```\ntry {\n    const browser = await pupputer.launch({\n        headless: false,\n        devtools: true,\n        defaultViewport: {\n            width: 1100,\n            height: 1000\n        }\n    });\n\n    const page = await browser.newPage();\n    await page.goto('MyLink');\n\n    await page.waitForSelector('.list-firms');\n\n    for (var i = 1; i < 10; i++) {\n\n        const listeCompanies = await page.$$('.list-firms > div.firm');\n\n        for (const companie of listeCompanies) {\n\n            const name = await companie.$eval('.listing-body > h3 > a', name => name.innerText);\n            const link = await companie.$eval('.listing-body > h3 > a', link => link.href);\n\n            await Promise.all([\n                page.waitForNavigation(),\n                page.goto(link),\n                page.waitForSelector('.firm-panel'),\n            ]);\n\n            const info = await page.$eval('#info', e => e.innerText);\n\n            const data = [{\n                name: name,\n                information: info,\n            }];\n\n            await page.goBack();\n\n        }\n        await Promise.all([\n            page.waitForNavigation(),\n            page.click('span.page > a[rel=\"next\"]')\n        ]);\n    }\n} catch (e) {\n    console.log('We have error', e);\n}\n\n```\n\nI managed to only get the data of the first company.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"## Problem\n\n\nThe error means that you are accessing data which has become obsolete\/invalid because of navigation. In your script the error references the variable `listeCompanies`:\n\n\n\n```\nconst listeCompanies = await page.$$('.list-firms > div.firm');\n\n```\n\nYou first, use this variable in a loop, then you navigate via `page.goto` and after that your loop tries to get the next item out of the variable `listeCompanies`. But after the navigation happened the element handles in that variable are not present anymore and therefore the error is thrown. That's also why the first iteration works.\n\n\n## Solution\n\n\nThere are multiple ways to fix this.\n\n\n1. Extract the data from your page at once (before using the loop)\n2. Use a second pageto do the \"loop navigation\" so that your main page does not need to navigate\n3. \"Refresh\" your variable by re-executing the selector after calling `page.goBack`\n\n\n\n\n---\n\n\n### Option 1: Extract the data before entering the loop\n\n\nThis is the cleanest way to do it. You extract the information in the first page at once and then iterate over your extracted data. The `nameLinkList` will be an array with the `name` and `link` values (e.g. `[{name: '..', link: '..'}, {name: '..', link: '..'}]`). There is also no need to call `page.goBack` at the end of the loop as the data is already extracted.\n\n\n\n```\nconst nameLinkList = await page.$$eval(\n    '.list-firms > div.firm',\n    (firms => firms.map(firm => {\n        const a = firm.querySelector('.listing-body > h3 > a');\n        return {\n            name: a.innerText,\n            link: a.href\n        };\n    }))\n);\n\nfor (const {name, link} of arr) {\n    await Promise.all([\n        page.waitForNavigation(),\n        page.goto(link),\n        page.waitForSelector('.firm-panel'),\n    ]);\n\n    const info = await page.$eval('#info', e => e.innerText);\n\n    const data = [{\n        name: name,\n        information: info,\n    }];\n}\n\n```\n\n### Option 2: Use a second page\n\n\nIn this case your browser will have two open pages. The first one will only be used to read the data, the second one is used for navigation.\n\n\n\n```\nconst page2 = await browser.newPage();\nfor (const companie of listeCompanies ){\n    const name = await companie.$eval('.listing-body > h3 > a', name => name.innerText);\n    const link = await companie.$eval('.listing-body > h3 > a', link => link.href);\n\n    await Promise.all([\n        page2.goto(link),\n        page2.waitForSelector('.firm-panel'),\n    ]);\n\n    const info = await page2.$eval('#info', e => e.innerText);\n    \/\/ ...\n}\n\n```\n\n### Option 3: \"Refresh\" selectors\n\n\nHere you simply re-execute your selector after going back to your \"main page\". Note, that the `for..of` has to be change to an iterator-loop as we are replacing the array.\n\n\n\n```\nlet listeCompanies  = await page.$$('.list-firms > div.firm');\nfor (let i = 0; i < listeCompanies.length; i++){\n    \/\/ ...\n\n    await page.goBack();\n    listeCompanies = await page.$$('.list-firms > div.firm');\n}\n\n```\n\nI recommend to go with option 1 as this also reduced the number of necessary navigation requests and will therefore speed up your script.\n\n\n"}
{"questionId":"18f86c9b47c1460fbcdfe856a0358574","question":"xlsx to json with empty cells\nI'm converting a file from xlsx format to json, I can do it correctly, but I could not get it to show the cells that are empty, just ignore them.\nI am using the XLSX library.\nThis is the code with which I do the parsing.\n\n\n\n```\nconst workbook = XLSX.readFile(filename);\n        const sheet_name_list = workbook.SheetNames;\n        let jsonPagesArray = [];\n        sheet_name_list.forEach(function(sheet) {\n                const jsonPage = {\n                    name: sheet,\n                    content: XLSX.utils.sheet_to_json(workbook.Sheets[sheet])\n                };\n                jsonPagesArray.push(jsonPage);\n            });\n        res.json(\n            {\n                data:jsonPagesArray\n            }\n        );\n        });\n\n```\n\nactually if a give this:\n\n\n\n```\nxxx1 | xxx2 | xxx3\n------------------\nyyyy | yyyy | \nzzzz | zzzz | zzzz\n\n```\n\nit return me:\n\n\n\n```\n    [\n        {\n            xxx1:yyyy,\n            xxx2:yyyy\n        }\n    ],\n    [\n        {\n            xxx1:zzzz,\n            xxx2:zzzz,\n            xxx3:zzzz\n        }\n    ]\n\n```\n\ni want it return me something like this:\n\n\n\n```\n    [\n        {\n            xxx1:yyyy,\n            xxx2:yyyy,\n            xxx3:\"\"\n        }\n    ],\n    [\n        {\n            xxx1:zzzz,\n            xxx2:zzzz,\n            xxx3:zzzz\n        }\n    ]\n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"javascript"},"answer":"can you replace `content: XLSX.utils.sheet_to_json(workbook.Sheets[sheet])` with `content: XLSX.utils.sheet_to_json(workbook.Sheets[sheet], {defval:\"\"})` in your code and try again? \n\n\n"}
{"questionId":"f38aa8142cb74345a9cb7258729a2e19","question":"Axios.get().then() in a for loop\nHow would I go about running Axios in a for loop, each with a corresponding `.then()` function. Then after the for loop ends, run another function.\n\n\nExample:\n\n\n\n```\nconst array = ['asdf', 'foo', 'bar'];\nlet users = [];\nfor (i = 0; i < array.length; i++) {\n  axios.get('\/user\/' + array[i].id).then(response => {\n    \/\/ do something with response\n    users.push(response);\n  });\n}\n\nconsole.log(users);\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"\n```\nconst array = [{ id: 'asdf'}, { id: 'foo' }, { id: 'bar' }]; \/\/ changed the input array a bit so that the `array[i].id` would actually work - obviously the asker's true array is more than some contrived strings\nlet users = [];\nlet promises = [];\nfor (i = 0; i < array.length; i++) {\n  promises.push(\n    axios.get('\/user\/' + array[i].id).then(response => {\n      \/\/ do something with response\n      users.push(response);\n    })\n  )\n}\n\nPromise.all(promises).then(() => console.log(users));\n\n```\n\nThe `.then()` method of a Promise itself returns a Promise; so you can collect those and await all of them with `Promise.all()`.\n\n\nNote that even if you're doing this within an `async` function, you don't want to `await` inside the for-loop, because then each request will wait for the previous one to finish before it even starts, and presumably you want to run these requests in parallel.\n\n\nDepending on your use case, a concise async \/ await function might look like this:\n\n\n\n```\nasync function getMultiple(...objectsToGet) {\n  let users = [];\n  await Promise.all(objectsToGet.map(obj =>\n    axios.get('\/user\/' + obj.id).then(response => {\n      \/\/ do something with response\n      users.push(response);\n    })\n  ));\n  return users;\n}\n\n\/\/ some other async context\nconsole.log(await getMultiple({ id: 'asdf'}, { id: 'foo' }, { id: 'bar' }));\n\n```\n\n"}
{"questionId":"c25fd9fff6d9485ab59d39b8135436a6","question":"Vue, Vuetify is not properly initialized\nI have setup `Vuetify` on my `Vue` webpack application.\n\n\nMy project is setup with `vue init webpack my-project` running `Vue 2.5.2` and using `Vuetify 2.0.2`.\n\n\nI have installed `Vuetify` in my `App.js`\n\n\n\n```\nimport Vue from 'vue'\nimport '..\/node_modules\/vuetify\/dist\/vuetify.min.css';\nimport App from '.\/App'\nimport router from '.\/router'\nimport store from '.\/store'\nimport Vuetify from 'vuetify'\n\nVue.use(Vuetify)\n\n\/* eslint-disable no-new *\/\nnew Vue({\n  el: '#app',\n  router,\n  store,\n  render: h => h(App)\n})\n\n```\n\nEverything seems to be working fine. I'm able to call `Vuetify`components in one of my components.\n\n\n\n```\n<template>\n  <v-container>\n        <v-card width=\"400\" height=\"150\" raised>\n          <h4>Hello<\/h4>\n        <\/v-card>\n  <\/v-container>\n<\/template>\n\n```\n\nI then read that I need to wrap my `App.js` with the v-app component, but when I do that I get an error saying: `Error: Vuetify is not properly initialized`.\n\n\n\n```\n<template>\n  <div id=\"app\">\n    <v-app>\n      <NavigationBar \/>\n      <v-content>\n        <router-view \/>\n      <\/v-content>\n    <\/v-app>\n  <\/div>\n<\/template>\n\n```\n\nMaybe `Vuetify` isn't installed correctly, I hope some of you can bring some light on my issue.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"There is lot of changes with new version.\n\n\ntry this\n\n\n\n```\nimport Vue from 'vue';\nimport Vuetify from 'vuetify';\nVue.use(Vuetify);\n\nnew Vue({\nvuetify : new Vuetify(),\n...\n});\n\n```\n\ngood luck\n\n\n"}
{"questionId":"2c00ab0fc02b41cba8a2fd3ff75b9615","question":"react-router v5.1.0 Cannot read property 'location' of undefined ,\nI am trying to change the background color of my container based on page URL, so if user navigate to URL `'\/movie'` it should change the background eg to red else it should set the background to green\n\n\nHere is my index.js\n\n\n\n```\nimport React from 'react';\nimport { BrowserRouter, Switch, Route, useLocation} from 'react-router-dom';\n\nimport styled from 'styled-components';\nimport Movies from 'pages\/Movies\/Movies'\nimport Templates from 'pages\/Movies\/Templates'\n;\n\nexport default () => {\n    \n    const location = useLocation();\n\n    return (\n        <>\n            <BrowserRouter>\n                <Container style={{backgroundColor:location.pathname === '\/movies' ? 'red' : 'green'}}>\n                    <Main>\n                        <App>\n                            <Switch>\n                                <Route path='\/templates' component={Templates} \/>\n                                <Route path='\/movies'  component={Movies} \/>\n                            \n                            <\/Switch>\n                        <\/App>\n                    <\/Main>\n                <\/Container>\n            <\/BrowserRouter>\n        <\/>\n    );\n}\n\nconst Container = styled.div`\n    min-height: 100vh;\n    \n`;\n\n```\n\nUnfortunately, I am getting the following error\n\n\n\n```\n  Uncaught TypeError: Cannot read property 'location' of undefined\n    at useLocation (app.js:54283)\n    at app.js:72792\n    at renderWithHooks (app.js:37714)\n    at mountIndeterminateComponent (app.js:40129)\n    at beginWork$1 (app.js:41478)\n    at HTMLUnknownElement.callCallback (app.js:21756)\n    at Object.invokeGuardedCallbackDev (app.js:21805)\n    at invokeGuardedCallback (app.js:21860)\n    at beginWork$$1 (app.js:47124)\n    at performUnitOfWork (app.js:46032)\n\n```\n\nWhat do I need to do to solve this problem?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"You need to move `BrowserRouter` out of that component. The best is to move it to `index.js` and enclose `<App \/>`\n\n\n"}
{"questionId":"3e20f9cc4a7f4c41a7f74b04f0cf73c7","question":"How to mock a File with a big size in JavaScript for testing purposes?\nI've implemented a test case for my upload component which shows error if `file.size` more than 1 mb.\n\n\nIs there a way to monkeypatch file size with `jest` or just js for getting test failing without creating the file like this?\n\n\n\n```\nconst file = new File(\n  [new Blob(['1'.repeat(1024 * 1024 + 1)], { type: 'image\/png' })],\n  'darthvader.png'\n)\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"You can simply use `Object.defineProperty` to override the original getter, it's configurable:\n\n\n\n\n\n```\nconst file = new File([\"\"], 'darthvader.png');\r\nObject.defineProperty(file, 'size', { value: 1024 * 1024 + 1 })\r\nconsole.log( file.size ); \/\/ 1048577\n```\n\n\n\n\n\n\n"}
{"questionId":"25e3e49eb4704071a97d3fcd623a0d29","question":"history.replace in react-router-dom v6\nI've previously used `react-router-dom v5.2.0`. There I used `history.replace('\/path)` to redirect the page to another page. (Hence it will not store in the address history). Now I have to use `react-router-dom v6.0.0-beta.0`. In version 6, I have to use `useNavigate` hook instead of `useHistory` hook. I can use it as below.\n\n\n\n```\nconst navigate = useNavigate();\nnavigate('\/path')\n\n```\n\nBut I don't know how to use it for redirect. (Like `history.replace`)\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"If you need to replace the current location instead of push a new one onto the history stack, use `navigate(to, { replace: true })`. If you need state, use `navigate(to, { state })`.\n\n\n"}
{"questionId":"a0438c01fd4b48748f871581fca83861","question":"NuxtJs Cannot read property '\\_normalized' of undefined\nthis error show when I try to fetch some data in nuxtjs\n\n\n\n```\nCannot read property '_normalized' of undefined\n\n```\n\nThis is my axios request in nuxtjs :\n\n\n\n```\n async asyncData({app}){\n        var response = await app.$axios.get('\/store\/get-services',{params:{id:app.$auth.user.store.id}});\n        return {services:response.data.services};\n    }\n\n```\n\nthis is a backend controller laravel:\n\n\n\n```\npublic function store_ads(Request $req){\n    if($req->user()->store->id != $req->id){\n        abort(403);\n    }\n    $services = Store::select('id')->with('ads:id,store_id,price,title')->where('id',$req->id)->first();\n    return response()->json(['services'=>$services],200);\n}\n\n```\n\nand here is how i fetch them in my template:\n\n\n\n```\n<div class=\"services-list\">\n                        <h4 class=\"is-vcentered title has-text-centered has-text-grey-light\" v-if=\"services.ads.length==0\">No Services<\/h4>\n                        <ul>\n                            <li v-for=\"service in services.ads\" :key=\"service.id\"><nuxt-link>{{service.title}}<\/nuxt-link><\/li>\n                        <\/ul>\n                    <\/div>\n\n```\n\nWhat is the reason?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"The error was I don't pass `to` property for `<nuxt-link><\/nuxt-link>` and this is what causes this error\n\n\n"}
{"questionId":"78d2796139a647df866878b27e79d8ff","question":"React: Prevent scroll when modal is open\nI have a custom modal component. When it's open, there is no scrolling whatsoever in the background.\n\n\nI tried this code below:\n\n\n\n```\ncomponentDidMount() {\n    document.body.style.overflow = 'hidden';\n}\n\ncomponentWillUnmount() {\n    document.body.style.overflow = 'unset';\n}\n\n```\n\nWhich seems to work at first, but when I use the modal component, in another page, there is no scroll even when the modal is closed.\n\n\nIs there a better solution for this?\n\n\nMy modal component:\n\n\n\n```\nexport class Modal extends React.Component {\n\nconstructor(props) {\n    super(props);\n}\n\ncomponentDidMount() {\n    document.body.style.overflow = 'hidden';\n}\n\ncomponentWillUnmount() {\n    document.body.style.overflow = 'unset';\n}\n\nrender() {\n    return (\n        <React.Fragment>\n            {this.props.showAt ?\n                this.props.show ?\n                    <div style={style} className={`${this.props.sectionName} ${modalTypeStyle ? modalTypeStyle : styles.modalWhite} ${modalTypeSize ? modalTypeSize : styles.modalSmall} ${!this.props.showAt ? styles.modalWhiteFixed : \"\"}`}>\n                        {this.props.arrowShape ? <div className={arrowTypeStyle ? arrowTypeStyle : styles.triangleToprightWhite} \/> : null}\n                        {this.props.children}\n                    <\/div>\n                    : null\n                :\n                this.props.show ?\n                    <div className={`${this.props.className} ${styles.modal}`}>\n                        <div style={style} className={`${this.props.sectionName} ${modalTypeStyle ? modalTypeStyle : styles.modalWhite} ${modalTypeSize ? modalTypeSize : styles.modalSmall} ${!this.props.showAt ? styles.modalWhiteFixed : \"\"}`}>\n                            {this.props.arrowShape ? <div className={arrowTypeStyle ? arrowTypeStyle : styles.triangleToprightWhite} \/> : null}\n                            {this.props.children}\n                        <\/div>\n                    <\/div> :\n                    null}\n        <\/React.Fragment>\n    )\n  }\n}\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"Use state to track if the Modal is open and only hide scroll if it's true. Since you're using `document.body.style.overflow = 'hidden'` in `componentDidMount`, the component still gets mounted which calls the lifecycle method that hides the scroll on body.\n\n\n\n```\nexport class Modal extends React.Component {\n\nconstructor(props) {\n    super(props);\n    this.state = {\n      open:false\n    }\n}\n\ncomponentDidMount() {    \n  if(this.state.open){\n    document.body.style.overflow = 'hidden';\n  }    \n}\n\ncomponentWillUnmount() {\n    document.body.style.overflow = 'unset';\n}\n\nrender() {\n    return (\n        <React.Fragment>\n            {this.props.showAt ?\n                this.props.show ?\n                    <div style={style} className={`${this.props.sectionName} ${modalTypeStyle ? modalTypeStyle : styles.modalWhite} ${modalTypeSize ? modalTypeSize : styles.modalSmall} ${!this.props.showAt ? styles.modalWhiteFixed : \"\"}`}>\n                        {this.props.arrowShape ? <div className={arrowTypeStyle ? arrowTypeStyle : styles.triangleToprightWhite} \/> : null}\n                        {this.props.children}\n                    <\/div>\n                    : null\n                :\n                this.props.show ?\n                    <div className={`${this.props.className} ${styles.modal}`}>\n                        <div style={style} className={`${this.props.sectionName} ${modalTypeStyle ? modalTypeStyle : styles.modalWhite} ${modalTypeSize ? modalTypeSize : styles.modalSmall} ${!this.props.showAt ? styles.modalWhiteFixed : \"\"}`}>\n                            {this.props.arrowShape ? <div className={arrowTypeStyle ? arrowTypeStyle : styles.triangleToprightWhite} \/> : null}\n                            {this.props.children}\n                        <\/div>\n                    <\/div> :\n                    null}\n        <\/React.Fragment>\n    )\n  }\n}\n\n```\n\n"}
{"questionId":"ec8df89d345649d688ea2f8f6ed3ee56","question":"Mixed default and named exports in Node with ES5 syntax\nAll my experience with exporting\/importing modules has come in ES6 using `export` and `import`, where you can do something like this to have a single module export a default function as well as separate named functions.\n\n\n\n```\n\/\/ module.js\nexport default mainFunction\nexport { namedFunction }\n\n\/\/ main.js\nimport mainFunction from 'functions'\nmainFunction()\n\nimport { namedFunction } from 'function'\nnamedFunction()\n\n```\n\nHowever I can't figure out how to do this with ES5 style imports using `module.exports` and `require`. As far as I understand, I can export either a single default:\n\n\n\n```\n\/\/ module.js\nmodule.exports = function mainFunction() {}\n\n\/\/ main.js\nconst mainFunction = require('module.js')\n\n```\n\nOr I can create named exports:\n\n\n\n```\n\/\/ module.js\nmodule.exports = {\n  namedFunction: function() {}\n}\n\n\/\/ main.js\nconst namedFunction = require('module.js').namedFunction\n\n```\n\nBut I can't do both. I thought I could maybe name one of the exports \"default\" like this, but it doesn't work\n\n\n\n```\n\/\/ module.js\nmodule.exports = {\n  default: function() {},\n  namedFunction: function() {}\n}\n\n\/\/ main.js\nconst mainFunction = require('module.js') \/\/ does not work\nconst mainFunction = require('module.js').default \/\/ works, but not what I want\nconst namedFunction = require('module.js').namedFunction\n\n```\n\nHow can I accomplish this dual default\/named export with ES5?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"You want to assign the value of `module.exports` to be your default function, and then put all the named exports as properties on that function.\n\n\n\n```\nconst defaultFunction = () => { console.log('default!'); };\nconst namedFunction1 = () => { console.log('1!'); };\nconst namedFunction2 = () => { console.log('2!'); };\n\nconst myModule = module.exports = defaultFunction;\nmyModule.namedFunction1 = namedFunction1;\nmyModule.namedFunction2 = namedFunction2;\n\n```\n\nLet's say that was in `myModule.js`. Then you can do this:\n\n\n\n```\nconst myModule = require('.\/myModule');\nmyModule(); \/\/ Prints: 'default!'\nmyModule.namedFunction1(); \/\/ Prints: '1!'\n\n```\n\n"}
{"questionId":"711796fad6b4449f8f7cf3ea24464361","question":"Difference between || and ?? operators\nWhat is the difference between `??` and `||` in JS\n\n\n\n\n\n```\nconst a = {}\n\nconst b = a.name ?? 'varun 1'\n\nconsole.log(b)\n\nconst d = a.name || 'varun 2'\n\nconsole.log(d)\n```\n\n\n\n\n\n\nIn the above code they both work the same. I know `||` is OR i.e if the first statement is `false` then set the value for the next one. Is it same for `??`?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"javascript"},"answer":"The main difference is that `nullish coalescing(??)` operator will only give the result as the right operand only if the left operand is either `null` or `undefined`.\n\n\nWhereas the `OR(||)` operator will give the result as right operand for all the falsy values of the left operand.\n\n\nBelow are some examples\n\n\n- Snippet 1: With `0` as input\n\n\n\n\n\n```\nconst a = 0;\n\/\/ a || 10 --> Will result in 10, as || operator considers 0 as falsy value and resulting the right side operand\nconsole.log(`a || 10 = ${a || 10}`);\n\/\/ a ?? 10 --> Will result in 0, as ?? operator considers 0 as truthy value and resulting the left side operand\nconsole.log(`a ?? 10 = ${a ?? 10}`);\n```\n\n\n\n\n\n\n- Snippet 2: With `''` as input\n\n\n\n\n\n```\nconst a = ''\nconsole.log(`a || \"ABC\" = ${a || \"ABC\"}`); \/\/ ABC\nconsole.log(`a ?? \"ABC\" = ${a ?? \"ABC\"}`); \/\/ ''\n```\n\n\n\n\n\n\n- Snippet 3: With `null` as input\n\n\n\n\n\n```\nconst a = null;\nconsole.log(`a || 10 = ${a || 10}`); \/\/ 10\nconsole.log(`a ?? 10 = ${a ?? 10}`); \/\/ 10\n```\n\n\n\n\n\n\n- Snippet 4: With `undefined` as input\n\n\n\n\n\n```\nconst a = {}\n\/\/ Here a.name will be undefined, hence both of the operands results the right side operand\nconsole.log(`a.name ?? 'varun 1' = ${a.name ?? 'varun 1'}`); \/\/ 'varun 1'\nconsole.log(`a.name || 'varun 2' = ${a.name || 'varun 2'}`); \/\/ 'varun 2'\n```\n\n\n\n\n\n\n\n\n\n```\nconst a = {name: ''}\n\/\/ Here a.name will be '', then\n\n\/\/ ?? will result ''\nconsole.log(`a.name ?? 'varun 1' = ${a.name ?? 'varun 1'}`);\n\/\/ || will result in 'varun 2'\nconsole.log(`a.name || 'varun 2' = ${a.name || 'varun 2'}`);\n```\n\n\n\n\n\n\n- Snippet 5: With `false` as input\n\n\n\n\n\n```\nconst a = false;\nconsole.log(`a || 10 = ${a || 10}`); \/\/ 10\nconsole.log(`a ?? 10 = ${a ?? 10}`); \/\/ false\n```\n\n\n\n\n\n\nAs mentioned above, both the operators behave similarly when the input is either `null` or `undefined`. The real difference we'll get to see when we provide `falsy` values such as `0`, `''`, `false`, `NaN`.\n\n\n"}
{"questionId":"773767a266734d0ca2671902edf6a4a2","question":"Module not found: Error: Can't resolve 'react-dom\/client'\nI am using react with the following packages:\n\n\n\n```\n{\n  \"name\": \"demo\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"dependencies\": {\n    \"@testing-library\/jest-dom\": \"^5.16.4\",\n    \"@testing-library\/react\": \"^13.0.1\",\n    \"@testing-library\/user-event\": \"^13.5.0\",\n    \"h3-js\": \"^3.7.2\",\n    \"leaflet\": \"^1.7.1\",\n    \"react\": \"^17.0.1\",\n    \"react-dom\": \"^17.0.1\",\n    \"react-leaflet\": \"3.0.2\",\n    \"react-scripts\": \"5.0.1\",\n    \"web-vitals\": \"^2.1.4\"\n  },\n  \"scripts\": {\n    \"start\": \"react-scripts start\",\n    \"build\": \"react-scripts build\",\n    \"test\": \"react-scripts test\",\n    \"eject\": \"react-scripts eject\"\n  },\n  \"eslintConfig\": {\n    \"extends\": [\n      \"react-app\",\n      \"react-app\/jest\"\n    ]\n  },\n  \"browserslist\": {\n    \"production\": [\n      \">0.2%\",\n      \"not dead\",\n      \"not op_mini all\"\n    ],\n    \"development\": [\n      \"last 1 chrome version\",\n      \"last 1 firefox version\",\n      \"last 1 safari version\"\n    ]\n  }\n}\n\n```\n\nMy `index.js` looks like the following:\n\n\n\n```\nimport React from 'react';\nimport ReactDOM from 'react-dom\/client';\nimport '.\/index.css';\nimport App from '.\/App';\nimport reportWebVitals from '.\/reportWebVitals';\n\nconst root = ReactDOM.createRoot(document.getElementById('root'));\nroot.render(\n  <React.StrictMode>\n    <App \/>\n  <\/React.StrictMode>\n);\n\nreportWebVitals();\n\n\n```\n\nMy `App.js` is like the following:\n\n\n\n```\nimport React from \"react\";\nimport { render } from \"react-dom\";\nimport LeafletMap from \".\/Map\";\n\nclass App extends React.Component {\n  state = { resolution: 8, kRing: 0 };\n\n  constructor(props) {\n    super(props);\n    this.state = { resolution: 8, kRing: 0 };\n  }\n\n  render() {\n    return (\n      <div>\n        <LeafletMap\n          resolution={this.state.resolution}\n          kRing={this.state.kRing}\n        \/>\n        Resolution:\n        <input\n          type=\"number\"\n          min={0}\n          max={15}\n          onChange={this.onChangeResolution}\n          defaultValue={8}\n        \/>\n        <br \/>\n        K Rings:\n        <input\n          type=\"number\"\n          min={0}\n          max={100}\n          onChange={this.onChangeKRings}\n          defaultValue={0}\n        \/>\n      <\/div>\n    );\n  }\n\n  onChangeResolution = (e) => {\n    this.setState({ resolution: Number.parseInt(e.target.value) });\n  };\n  onChangeKRings = (e) => {\n    this.setState({ kRing: Number.parseInt(e.target.value) });\n  };\n}\n\nexport default App;\n\n\n```\n\nWhen I run my app with `npm run start` I get the following error:\n\n\n\n```\nCompiled with problems:X\n\nERROR in .\/src\/index.js 5:0-40\n\nModule not found: Error: Can't resolve 'react-dom\/client' in '\/home\/Desktop\/Code\/demo_app\/src'\n\n```\n\nI reinstalled all packages and its also listed in `npm list`:\n\n\n\n```\n>  npm list\ndemo_app@0.1.0 \/home\/Desktop\/Code\/demo_app\n\u251c\u2500\u2500 @testing-library\/jest-dom@5.16.4\n\u251c\u2500\u2500 @testing-library\/react@13.0.1\n\u251c\u2500\u2500 @testing-library\/user-event@13.5.0\n\u251c\u2500\u2500 h3-js@3.7.2\n\u251c\u2500\u2500 leaflet@1.7.1\n\u251c\u2500\u2500 react-dom@17.0.1\n\u251c\u2500\u2500 react-leaflet@3.0.2\n\u251c\u2500\u2500 react-scripts@5.0.1\n\u251c\u2500\u2500 react@17.0.1\n\u2514\u2500\u2500 web-vitals@2.1.4\n\n```\n\nAny suggestions why I have problems compiling my application?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"The final solution that worked for me was simply to change the React 18 `index.js` file to the following:\n\n\n\n```\nimport React from 'react';\nimport ReactDOM from 'react-dom';\nimport '.\/index.css';\nimport App from '.\/App';\nimport reportWebVitals from '.\/reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App \/>\n  <\/React.StrictMode>,\n  document.getElementById('root')\n);\n\n\/\/ If you want to start measuring performance in your app, pass a function\n\/\/ to log results (for example: reportWebVitals(console.log))\nreportWebVitals();\n\n\n```\n\n"}
{"questionId":"d2f0f802b2be4706b54079ac63407180","question":"How to post multiple Axios requests at the same time?\nAt this moment I have a webpage in which a long list of Axios POST calls are being made. Now, the requests seem to be sent in parallel (JavaScript continues sending the next request before the result is received). \n\n\nHowever, the results seem to be returned one by one, not simultaneously. Let's say one POST call to the PHP script takes 4 seconds and I need to make 10 calls. It would currently take 4 seconds per call, which would be 40 seconds in total. I hope to find a solution to both and receive all results at approximately the same time (~4 seconds) instead of ~40 seconds.\n\n\nNow I've read about threads, multithreading in NodeJS using Workers. I've read that JavaScript itself is only single-threaded, so it may not allow this by itself.\n\n\nBut I'm not sure where to go from here. All I have are some ideas. I'm not sure whether or not I'm heading into the right direction and if I am, I am not sure how to use Workers in NodeJS and apply it in my code. Which road should I take? Any guidance would be highly appreciated!\n\n\nHere is a small piece of example code:\n\n\n\n```\nfor( var i = 0;  i < 10;  i++ )\n{\n    window.axios.post(`\/my-url`, {\n        myVar: 'myValue'\n    })\n    .then((response) => {\n        \/\/ Takes 4 seconds, 4 more seconds, 4 more seconds, etc\n        \/\/ Ideally: Takes 4 seconds, returns in the same ~4 seconds, returns in the same ~4 seconds, etc\n        console.log( 'Succeeded!' );\n    })\n    .catch((error) => {\n        console.log( 'Error' );\n    });\n\n    \/\/ Takes < 1 second, < 1 more second, < 1 more second, etc\n    console.log( 'Request sent!' );\n}\n\n```\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"javascript"},"answer":"There are three cases via you can achieve your goal.\n\n\n1. For simultaneous requests with Axios, you can use `Axios.all()`\n\n\n\n```\n axios.all([\n   axios.post(`\/my-url`, {\n     myVar: 'myValue'\n   }), \n   axios.post(`\/my-url2`, {\n     myVar: 'myValue'\n   })\n ])\n .then(axios.spread((data1, data2) => {\n   \/\/ output of req.\n   console.log('data1', data1, 'data2', data2)\n }));\n\n```\n2. you can use `Promise.allSettled()`. The Promise.allSettled() method returns a promise that resolves after all of the given promises have either resolved or rejected,\n3. You can try to use `Promise.all()` but it has the drawback that if any **1** req failed then it will fail for all and give o\/p as an error(or in catch block)\n\n\nbut the best case is the first one.\n\n\n"}
{"questionId":"45d0b10eb08043cf8002a5182dbc57b9","question":"Next.JS \"Link\" vs \"router.push()\" vs \"a\" tag\nNewbie here, sorry if this is extremely basic question but I'm not able to wrap my head around which technique should be followed to navigate across various pages.\n\n\nAs of now I know three different methods through which I can achieve this:\n\n\n1. Link component exported by `next\/link`\n2. `router.push()` using useRouter exported ny next\/router\n3. Simple HTML `<a><\/a>`\n\n\n\n```\n<Link href=\"\/about\"><a>About me<\/a><\/Link>\n<a href=\"\/about\">About me<\/a>\n<button onClick={() => router.push(\"\/about\")}>About me<\/button>\n\n```\n\nAll of the methods work and achieve the same exact thing. I just wanted to know what are the differences, if any, among these approaches. Thank you so much in advance!\n\n\n","questionMetadata":{"type":"conceptual","level":"beginner","tag":"javascript"},"answer":"## router.push\n\n\n`router.push('\/push')` behaves similarly to window.location. It does not create a `<a>` tag, which means - if you are concern with SEO, your links will not be detected by crawlers.\n\n\n## `<Link>`\n\n\nHowever, `<Link>` will create a `<a>` tag, which means your links will be detected when crawlers scrape your site. End users will still navigate without reloading the page, creating the behavior of a Single Page App.\n\n\n## `<a>`\n\n\n`<a>` tag without using next\/link's `<Link>` creates a standard hyperlink which directs end user to the url as a new page. (standard behavior).\n\n\nYou should be using `<Link>` throughout all your website, and use router.push for places where you need redirect in order to retain the behaviour of a Single Page App.\n\n\n"}
{"questionId":"e28060ddbd1842a68cc696491052e34f","question":"Why React Hook useState uses const and not let\nThe standard way to use a React useState Hook is the following:\n\n\n\n```\nconst [count, setCount] = useState(0);\n\n```\n\nHowever this `const count` variable is clearly going to be reassigned to a different primitive value.\n\n\nWhy then is the variable not defined as `let count`?\n\n\n","questionMetadata":{"type":"conceptual","level":"intermediate","tag":"javascript"},"answer":"\n> \n> clearly going to be reassigned to a different primitive value\n> \n> \n> \n\n\nNot really. When the component is rerendered, the function is executed again, creating a new scope, creating a new `count` variable, which has nothing to do with the previous variable.\n\n\nExample:\n\n\n\n\n\n```\nlet _state;\r\nlet _initialized = false;\r\nfunction useState(initialValue) {\r\n  if (!_initialized) {\r\n    _state = initialValue;\r\n    _initialized = true;\r\n  }\r\n  return [_state, v => _state = v];\r\n}\r\n\r\nfunction Component() {\r\n  const [count, setCount] = useState(0);\r\n\r\n  console.log(count);\r\n  setCount(count + 1);\r\n}\r\n\r\nComponent();\r\nComponent(); \/\/ in reality `setCount` somehow triggers a rerender, calling Component again\r\nComponent(); \/\/ another rerender\n```\n\n\n\n\n\n\n**Note:** Hooks are way more sophisticated and are not actually implemented like this. This is just to demonstrate a similar behavior.\n\n\n"}
{"questionId":"02e569118b5e4863a6e0774a8a903368","question":"React suspense\/lazy delay?\nI am trying to use the new React Lazy and Suspense to create a fallback loading component. This works great, but the fallback is showing only a few ms. Is there a way to add an additional delay or minimum time, so I can show animations from this component before the next component is rendered? \n\n\nLazy import now\n\n\n\n```\nconst Home = lazy(() => import(\".\/home\"));\nconst Products = lazy(() => import(\".\/home\/products\"));\n\n```\n\nWaiting component: \n\n\n\n```\nfunction WaitingComponent(Component) {\n\n    return props => (\n      <Suspense fallback={<Loading \/>}>\n            <Component {...props} \/>\n      <\/Suspense>\n    );\n}\n\n```\n\nCan I do something like this?\n\n\n\n```\nconst Home = lazy(() => {\n  setTimeout(import(\".\/home\"), 300);\n});\n\n```\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"`lazy` function is supposed to return a promise of `{ default: ... }` object which is returned by `import()` of a module with default export. `setTimeout` doesn't return a promise and cannot be used like that. While arbitrary promise can:\n\n\n\n```\nconst Home = lazy(() => {\n  return new Promise(resolve => {\n    setTimeout(() => resolve(import(\".\/home\")), 300);\n  });\n});\n\n```\n\nIf an objective is to provide *minimum* delay, this isn't a good choice because this will result in *additional* delay.\n\n\nA minimum delay would be:\n\n\n\n```\nconst Home = lazy(() => {\n  return Promise.all([\n    import(\".\/home\"),\n    new Promise(resolve => setTimeout(resolve, 300))\n  ])\n  .then(([moduleExports]) => moduleExports);\n});\n\n```\n\n"}
{"questionId":"eb7c4637d67845d59ea7d71553f329a8","question":"Get HTML5 localStorage keys\nI'm just wondering how to get all key values in `localStorage`.\n\n\n\n\n---\n\n\nI have tried to retrieve the values with a simple JavaScript loop\n\n\n\n```\nfor (var i=1; i <= localStorage.length; i++)  {\n   alert(localStorage.getItem(i))\n}\n\n```\n\nBut it works only if the keys are progressive numbers, starting at 1.\n\n\n\n\n---\n\n\nHow do I get all the keys, in order to display all available data?\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"javascript"},"answer":"in ES2017 you can use:\n\n\n\n```\nObject.entries(localStorage)\n\n```\n\n"}
{"questionId":"516d22d7c65f463db1d8a679a8246daf","question":"Error: Network error: Error writing result to store for query (Apollo Client)\nI am using Apollo Client to make an application to query my server using Graphql. I have a python server on which I execute my graphql queries which fetches data from the database and then returns it back to the client. \n\n\nI have created a custom NetworkInterface for the client that helps me to make make customized server request (by default ApolloClient makes a POST call to the URL we specify). The network interface only has to have a query() method wherein we return the promise for the result of form `Promise<ExecutionResult>`. \n\n\nI am able to make the server call and fetch the requested data but still getting the following error.\n\n\n\n```\nError: Network error: Error writing result to store for query \n{\n   query something{\n      row{\n         data\n      }\n   }\n}\nCannot read property 'row' of undefined\n    at new ApolloError (ApolloError.js:32)\n    at ObservableQuery.currentResult (ObservableQuery.js:76)\n    at GraphQL.dataForChild (react-apollo.browser.umd.js:410)\n    at GraphQL.render (react-apollo.browser.umd.js:448)\n    at ReactCompositeComponent.js:796\n    at measureLifeCyclePerf (ReactCompositeComponent.js:75)\n    at ReactCompositeComponentWrapper._renderValidatedComponentWithoutOwnerOrContext (ReactCompositeComponent.js:795)\n    at ReactCompositeComponentWrapper._renderValidatedComponent (ReactCompositeComponent.js:822)\n    at ReactCompositeComponentWrapper._updateRenderedComponent (ReactCompositeComponent.js:746)\n    at ReactCompositeComponentWrapper._performComponentUpdate (ReactCompositeComponent.js:724)\n    at ReactCompositeComponentWrapper.updateComponent (ReactCompositeComponent.js:645)\n    at ReactCompositeComponentWrapper.performUpdateIfNecessary (ReactCompositeComponent.js:561)\n    at Object.performUpdateIfNecessary (ReactReconciler.js:157)\n    at runBatchedUpdates (ReactUpdates.js:150)\n    at ReactReconcileTransaction.perform (Transaction.js:140)\n    at ReactUpdatesFlushTransaction.perform (Transaction.js:140)\n    at ReactUpdatesFlushTransaction.perform (ReactUpdates.js:89)\n    at Object.flushBatchedUpdates (ReactUpdates.js:172)\n    at ReactDefaultBatchingStrategyTransaction.closeAll (Transaction.js:206)\n    at ReactDefaultBatchingStrategyTransaction.perform (Transaction.js:153)\n    at Object.batchedUpdates (ReactDefaultBatchingStrategy.js:62)\n    at Object.enqueueUpdate (ReactUpdates.js:200)\n\n```\n\nI want to know the possible cause of the error and solution if possible.\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"I had a similar error.\nI worked it out by adding id to query.\nfor example, my current query was\n\n\n\n```\nquery  {\n  service:me {\n    productServices {\n      id\n      title\n    }\n  }\n}\n\n```\n\nmy new query was\n\n\n\n```\nquery  {\n  service:me {\n    id \/\/ <-------\n    productServices {\n      id\n      title\n    }\n  }\n}\n\n```\n\n"}
{"questionId":"1bed9ef24953470182de9eafaaa2a6c0","question":"Svelte 3 - How to loop each block X amount of times\nI'm hoping to find a way to iterate over an #each block a set amount of times in Svelte 3. In Vue I would do something like this:\n\n\n\n```\n<li v-for=\"i in 3\"><!-- somecontent --><\/li>\n\n```\n\nBut as I understand Svelte handles loops much differently using the .length property of the array being #eached. Is there some way to pull off something like this in Svelte?\n\n\n\n```\n{#each 3 as i}\n  <li><!-- somecontent --><\/li>\n{\/if}\n\n```\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"javascript"},"answer":"You can use `{#each ...}`, like:\n\n\n\n```\n{#each Array(3) as _, i}\n    <li>{i + 1}<\/li>\n{\/each}\n\n```\n\n"}
{"questionId":"d6c9948575e64cd39b66f1bae39c4299","question":"How to align a component to the center\/right with Material UI?\nI want to align my button to the right of the parent.\n\n\nI was wondering if there is a *proper* way to do it in Material UI. I could use:\n\n\n\n```\n<Grid container justify=\"flex-end\">\n\n```\n\nBut then I would have to use another `<Grid item \/>`. Seems like too much work.\n\n\nOr maybe I am just better off using plain old CSS, messing around with `float: right`, and dealing with the apparent zero height of the element.\n\n\n","questionMetadata":{"type":"implementation","level":"beginner","tag":"javascript"},"answer":"### BEFORE Material UI 5:\n\n\nTry this\n\n\n\n```\n<Grid container justify=\"flex-end\">\n  <Button>Example<\/Button>\n<\/Grid>\n\n```\n\n### UPDATE Material UI 5: (Thanks to Dipak)\n\n\n\n> \n> The prop `justify` of `ForwardRef(Grid)` is deprecated. Use `justifyContent` instead, the prop was renamed.\n> \n> \n> \n\n\n\n```\n<Grid container justifyContent=\"flex-end\">\n  <Button>Example<\/Button>\n<\/Grid>\n\n```\n\n### Update: The best solutions are the answers of NearHuscarl or Eduardo Oviedo Blanco, you'd rather use Stack or Box than Grid.\n\n\n"}
{"questionId":"576605fdf5b94b91a6c29d0245a4b6ae","question":"react-select: How do I resolve \u201cWarning: Prop `id` did not match\u201d\nI have a web app with ReactJs and NextJs. In a functional component, I have used **react-select** then, I'm receiving the following console warning:\n\n\n\n> \n> Warning: Prop `id` did not match. Server: \"react-select-7-input\" Client: \"react-select-2-input\"\n> \n> \n> \n\n\nMy code is the following:\n\n\n\n```\nimport { Row, Col, Card, Form, Button } from 'react-bootstrap';\nimport Select from 'react-select';\n\nconst priorityOptions = [\n  { value: 'p1', label: 'Top level - P1' },\n  { value: 'p2', label: 'Mid level - P2' },\n  { value: 'p3', label: 'Low level - P3' }\n];\nconst PostView = () => {\n  return (\n    <div className=\"DashboardSla-ContentBody__Form\">\n      <Row>\n        <Col md=\"10\">\n          <Card className=\"shadow-sm\">\n            <Card.Body>\n              <Form>\n                <h5 className=\"text-secondary mb-3\">Booking details<\/h5>\n                <Form.Group controlId=\"postType\">\n                  <Form.Label>Booking priority<\/Form.Label>\n                  <Select \n                    id=\"postType\"\n                    placeholder=\"Make a selection\"\n                    options={priorityOptions}\n                  \/>\n                <\/Form.Group>\n                <Button\n                  type=\"submit\"\n                  variant=\"primary\"\n                >Add Booking<\/Button>\n              <\/Form>\n            <\/Card.Body>\n          <\/Card>\n        <\/Col>\n      <\/Row>\n    <\/div>\n  )\n}\n\nexport default PostView;\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"try to add prop `instanceId` set as unique string and should work\n\n\n"}
{"questionId":"59e9bb55fb614ab08f53451f4a0517dd","question":"eslint throws `no-undef` errors when linting Jest test files\nI'm using Jest to write some specs and ESLint to lint the styling. \n\n\nFor my `foo.spec.js` tests, eslint keeps throwing the following errors. It seems to think that `jest`, `beforeEach`, `afterEach`, etc... are not defined in that file. \n\n\n\n```\n   11:1   error  'beforeEach' is not defined  no-undef\n   12:3   error  'jest' is not defined        no-undef\n   14:13  error  'jest' is not defined        no-undef\n   18:1   error  'afterEach' is not defined   no-undef\n   20:1   error  'describe' is not defined    no-undef\n   21:3   error  'it' is not defined          no-undef\n   25:5   error  'expect' is not defined      no-undef\n   28:5   error  'expect' is not defined      no-undef\n   31:5   error  'expect' is not defined      no-undef\n   34:3   error  'it' is not defined          no-undef\n   38:5   error  'expect' is not defined      no-undef\n   41:5   error  'jest' is not defined        no-undef\n   42:5   error  'expect' is not defined      no-undef\n   43:5   error  'expect' is not defined      no-undef\n   46:3   error  'it' is not defined          no-undef\n   54:5   error  'expect' is not defined      no-undef\n   58:5   error  'jest' is not defined        no-undef\n\n```\n\nI believe those are included by jest automatically and so they don't need to be explicitly imported in my spec files. In fact the only thing I import via my `jest.setup.js` file is\n\n\n\n```\nimport \"react-testing-library\/cleanup-after-each\";\nimport \"jest-dom\/extend-expect\";\n\n```\n\nIs there a way to eliminate these errors without having to disable eslint rules at the top of each individual file or inline? \n\n\nThanks!\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"Please, add the following to your .eslintrc file:\n\n\n\n```\n{\n  \"overrides\": [\n    {\n      \"files\": [\n        \"**\/*.spec.js\",\n        \"**\/*.spec.jsx\"\n      ],\n      \"env\": {\n        \"jest\": true\n      }\n    }\n  ]\n}\n\n```\n\n"}
{"questionId":"562a30191a004d6db252d1d9e581e5db","question":"Why do some variables declared using let inside a function become available in another function, while others result in a reference error?\nI can't understand why variables act so strange when declared inside a function.\n\n\n1. In the `first` function I declare with `let` the variables `b` and `c` with the value *10*:\n\n\n\n```\nb = c = 10;\n\n```\n\nIn the `second` function I show:\n\n\n\n```\nb + \", \" + c\n\n```\n\nAnd this shows:\n\n\n\n```\n10, 10\n\n```\n2. Also in `first` function I declare `a` with value *10*:\n\n\n\n```\nlet a = b = c = 10;\n\n```\n\nBut in the `second` function it shows an error:\n\n\n\n> \n> Can't find variable: `a`\n> \n> \n>\n3. Now in the `first` function I declare `d` with value *20*:\n\n\n\n```\nvar d = 20;\n\n```\n\nBut in the `second` function it shows the same error as before, but with the variable `d`:\n\n\n\n> \n> Can't find variable: `d`\n> \n> \n>\n\n\nExample:\n\n\n\n\n\n```\nfunction first() {\r\n  let a = b = c = 10;\r\n  var d = 20;\r\n  second();\r\n}\r\n\r\nfunction second() {\r\n  console.log(b + \", \" + c); \/\/shows \"10, 10\"\r\n\r\n  try{ console.log(a); }  \/\/ Rreference error\r\n  catch(e){ console.error(e.message) }\r\n\r\n  try{ console.log(d); } \/\/ Reference error\r\n  catch(e){ console.error(e.message) }\r\n}\r\nfirst()\n```\n\n\n\n\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"It's because you're actually saying: \n\n\n\n```\nc = 10;\nb = c;\nlet a = b;\n\n```\n\nAnd not what you think you are saying, which is:\n\n\n\n```\nlet a = 10;\nlet b = 10;\nlet c = 10;\n\n```\n\nYou'll notice that no matter how many variables you add to your chain, it will only be the first (a) that causes the error. \n\n\nThis is because \"let\" scopes your variable to the block (or, \"locally\", more or less meaning \"in the brackets\") in which you declare it. \n\n\nIf you declare a variable without \"let\", it scopes the variable globally. \n\n\nSo, in the function where you set your variables, everything gets the value 10 (you can see this in the debugger if you put a breakpoint). If you put a console log for a,b,c in that first function, all is well. \n\n\nBut as soon as you leave that function, the first one (a)--and again, keep in mind, technically in the order of assignment, it is the last one-- \"disappears\" (again, you can see this in the debugger if you set a breakpoint in the second function), but the other two (or however many you add) are still available.\n\n\nThis is because, \"let\" ONLY APPLIES TO (so only locally scopes) THE FIRST VARIABLE--again, which is technically the last to be declared and assigned a value--in the chain. The rest technically do not have \"let\" in front of them. So those are technically declared globally (that is, on the global object), which is why they appear in your second function. \n\n\nTry it: remove the \"let\" keyword. All your vars will now be available. \n\n\n\"var\" has a similar local-scope effect, but differs in how the variable is \"hoisted\", which is something you should definitely understand, but which is not directly involved with your question. \n\n\n(BTW, this question would stump enough pro JS devs to make it a good one).\n\n\nStrongly suggest you spend time with the differences in how variables can be declared in JS: without a keyword, with \"let\", and with \"var\". \n\n\n"}
{"questionId":"b7b973d0f62240259ead03e1660b75e3","question":"How do I check if a string is entirely made of the same substring?\nI have to create a function which takes a string, and it should return `true` or `false` based on whether the input consists of a repeated character sequence. The length of the given string is always greater than `1` and the character sequence must have at least one repetition.\n\n\n\n```\n\"aa\" \/\/ true(entirely contains two strings \"a\")\n\"aaa\" \/\/true(entirely contains three string \"a\")\n\"abcabcabc\" \/\/true(entirely containas three strings \"abc\")\n\n\"aba\" \/\/false(At least there should be two same substrings and nothing more)\n\"ababa\" \/\/false(\"ab\" exists twice but \"a\" is extra so false)\n\n```\n\nI have created the below function:\n\n\n\n\n\n```\nfunction check(str){\r\n  if(!(str.length && str.length - 1)) return false;\r\n  let temp = '';\r\n  for(let i = 0;i<=str.length\/2;i++){\r\n    temp += str[i]\r\n    \/\/console.log(str.replace(new RegExp(temp,\"g\"),''))\r\n    if(!str.replace(new RegExp(temp,\"g\"),'')) return true;\r\n  }\r\n  return false;\r\n}\r\n\r\nconsole.log(check('aa')) \/\/true\r\nconsole.log(check('aaa')) \/\/true\r\nconsole.log(check('abcabcabc')) \/\/true\r\nconsole.log(check('aba')) \/\/false\r\nconsole.log(check('ababa')) \/\/false\n```\n\n\n\n\n\n\nChecking of this is part of the real problem. I can't afford a non-efficient solution like this. First of all, it's looping through half of the string.\n\n\nThe second problem is that it is using `replace()` in each loop which makes it slow. Is there a better solution regarding performance?\n\n\n","questionMetadata":{"type":"optimization","level":"intermediate","tag":"javascript"},"answer":"There\u2019s a nifty little theorem about strings like these.\n\n\n\n> \n> A string consists of the same pattern repeated multiple times if and only if the string is a nontrivial rotation of itself.\n> \n> \n> \n\n\nHere, a rotation means deleting some number of characters from the front of the string and moving them to the back. For example, the string `hello` could be rotated to form any of these strings:\n\n\n\n```\nhello (the trivial rotation)\nelloh \nllohe \nlohel \nohell \n\n```\n\nTo see why this works, first, assume that a string consists of k repeated copies of a string w. Then deleting the first copy of the repeated pattern (w) from the front of the string and tacking it onto the back will give back the same string. The reverse direction is a bit trickier to prove, but the idea is that if you rotate a string and get back what you started with, you can apply that rotation repeatedly to tile the string with multiple copies of the same pattern (that pattern being the string you needed to move to the end to do the rotation).\n\n\nNow the question is how to check whether this is the case. For that, there\u2019s another beautiful theorem we can use:\n\n\n\n> \n> If x and y are strings of the same length, then x is a rotation of y if and only if x is a substring of yy.\n> \n> \n> \n\n\nAs an example, we can see that `lohel` is a rotation of `hello` as follows:\n\n\n\n```\nhellohello\n   ^^^^^\n\n```\n\nIn our case, we know that every string x will always be a substring of xx (it\u2019ll appear twice, once at each copy of x). So basically we just need to check if our string x is a substring of xx without allowing it to match at the first or halfway character. Here\u2019s a one-liner for that:\n\n\n\n```\nfunction check(str) {\n    return (str + str).indexOf(str, 1) !== str.length;\n}\n\n```\n\nAssuming `indexOf` is implemented using a fast string matching algorithm, this will run in time O(n), where n is the length of the input string.\n\n\n"}
{"questionId":"5783fb60cf754b7fb46be15f2d4f38c7","question":"Destructuring and rename property\n\n```\nconst a = {\n b: {\n  c: 'Hi!'\n }\n};\n\nconst { b: { c } } = a;\n\n```\n\nIs it possible rename `b` in this case? I want get `c` and also rename `b`.\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"You could destructure with a renaming and take the same property for destructuring.\n\n\n\n\n\n```\nconst a = { b: { c: 'Hi!' } };\r\nconst { b: formerB, b: { c } } = a;\r\n\r\nconsole.log(formerB)\r\nconsole.log(c);\n```\n\n\n\n\n\n\n"}
{"questionId":"02ae1d4e2556436f8e57ffe81abe2b7f","question":"ReactJS Bootstrap Navbar and Routing not working together\nI am trying to create a simple Webapp using ReactJS, and I wanted to use the `Navbar` provided by React-Bootstrap. \n\n\nI created a `Navigation.js` file containing a class `Navigation` to separate the `Navbar` and the Routing from the `App.js` file. However, both parts do not seem to work. When I load the page, it is just empty, there is no Navbar. Can anyone spot a mistake?\n\n\nNavigation.js:\n\n\n\n```\nimport React, { Component } from 'react';\nimport { Navbar, Nav, Form, FormControl, Button, NavItem } from 'react-bootstrap';\nimport { Switch, Route } from 'react-router-dom';\nimport { Home } from '.\/Page';\n\nclass Navigation extends Component {\n    render() {\n        return (\n            <div>\n                <div>\n                    <Navbar>\n                        <Navbar.Brand href=\"\/\">React-Bootstrap<\/Navbar.Brand>\n                        <Navbar.Collapse>\n                            <Nav className=\"mr-auto\">\n                                <NavItem eventkey={1} href=\"\/\">\n                                    <Nav.Link href=\"\/\">Home<\/Nav.Link>\n                                <\/NavItem>\n                            <\/Nav>\n                            <Form inline>\n                                <FormControl type=\"text\" placeholder=\"Search\" className=\"mr-sm-2\" \/>\n                                <Button variant=\"outline-success\">Search<\/Button>\n                            <\/Form>\n                        <\/Navbar.Collapse>\n                    <\/Navbar>\n                <\/div>\n                <div>\n                    <Switch>\n                        <Route exact path='\/' component={Home} \/>\n                        <Route render={function () {\n                            return <p>Not found<\/p>\n                        }} \/>\n                    <\/Switch>\n                <\/div>\n            <\/div>\n        );\n    }\n}\n\nexport default Navigation;\n\n```\n\nApp.js:\n\n\n\n```\nimport React, { Component } from 'react';\nimport Navigation from '.\/components\/routing\/Navigation';\n\n\n\nclass App extends Component {\n  render() {\n    return (\n      <div id=\"App\">\n        <Navigation \/>\n      <\/div>\n    );\n  }\n}\n\nexport default App;\n\n```\n\nI tried using a `NavItem` containing a `LinkContainer` from `react-router-bootstrap` already, which led to the same result. \n\n\nJust for completeness, Page.js:\n\n\n\n```\nimport React, { Component } from 'react';\nimport { Link } from 'react-router-dom';\n\nexport const Page = ({ title }) => (\n    <div className=\"App\">\n      <div className=\"App-header\">\n        <h2>{title}<\/h2>\n      <\/div>\n      <p className=\"App-intro\">\n        This is the {title} page.\n      <\/p>\n      <p>\n        <Link to=\"\/\">Home<\/Link>\n      <\/p>\n      <p>\n        <Link to=\"\/about\">About<\/Link>\n      <\/p>\n      <p>\n        <Link to=\"\/settings\">Settings<\/Link>\n      <\/p>\n    <\/div>\n);\n\n\nexport const About = (props) => (\n    <Page title=\"About\"\/>\n);\n\nexport  const Settings = (props) => (\n    <Page title=\"Settings\"\/>\n);\n\nexport const Home = (props) => (\n    <Page title=\"Home\"\/>\n);\n\n```\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"First of all, in your snippets it doesn't seem like you're wrapping your code in a `Router`, so you should make sure that you're doing that inside `App` or in `ReactDOM.render`:\n\n\n\n```\nimport { BrowserRouter } from 'react-router-dom';\n\nReactDOM.render(\n  <BrowserRouter>\n    <App \/>\n  <\/BrowserRouter>, \n  rootElement\n  );\n\n```\n\nNext, your specific problem is that you're rendering react-bootstrap's `Nav.Link` instead of react-router's `Link` component, so the router is not picking up your route changes. Fortunately, react-bootstrap provides a render prop in most of its components to specify which component or element you want to render if you don't want the default. Switch to something like this:\n\n\n\n```\nimport { Switch, Route, Link } from 'react-router-dom';\n\nclass Navigation extends Component {\n  render() {\n    return (\n      <div>\n        <div>\n          <Navbar>\n            <Navbar.Brand as={Link} to=\"\/\" >React-Bootstrap<\/Navbar.Brand>\n            <Navbar.Collapse>\n              <Nav className=\"mr-auto\">\n                <NavItem eventkey={1} href=\"\/\">\n                  <Nav.Link as={Link} to=\"\/\" >Home<\/Nav.Link>\n                <\/NavItem>\n              <\/Nav>\n              <Form inline>\n                <FormControl type=\"text\" placeholder=\"Search\" className=\"mr-sm-2\" \/>\n                <Button variant=\"outline-success\">Search<\/Button>\n              <\/Form>\n            <\/Navbar.Collapse>\n          <\/Navbar>\n        <\/div>\n        <div>\n          <Switch>\n            <Route exact path='\/' component={Home} \/>\n            <Route render={function () {\n              return <p>Not found<\/p>\n            }} \/>\n          <\/Switch>\n        <\/div>\n      <\/div>\n    );\n  }\n}\n\n```\n\n"}
{"questionId":"ce70991777b44853872caf25455d8804","question":"How to Set port in next.js\none application is running on port 3000 and I want to run another application on a different port of the default port. How I change this in React Next.js.\nMy **package.js** script is \n\n\n\n```\n\"scripts\": {\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\",\n    \"dev\": \"next\",\n    \"build\": \"next build\",\n    \"start\": \"next start\"\n  },\n\n```\n\nand start script command is\n`npm run dev`\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"This work for me\n\n\n\n```\n \"scripts\": { \n       \"dev\": \"next dev -p 8080\",\n       \"start\": \"next start -p 8080\",\n},\n\n```\n\n"}
{"questionId":"60c527d98d154db99f56b19f38cbc984","question":"How to convert file to base64 in JavaScript?\n**UPD** TypeScript version is also available in answers\n\n\nNow I'm getting File object by this line:\n\n\n\n```\nfile = document.querySelector('#files > input[type=\"file\"]').files[0]\n\n```\n\nI need to send this file via json in base 64. What should I do to convert it to base64 string?\n\n\n","questionMetadata":{"type":"implementation","level":"intermediate","tag":"javascript"},"answer":"**Modern ES6 way** (async\/await)\n\n\n\n```\nconst toBase64 = file => new Promise((resolve, reject) => {\n    const reader = new FileReader();\n    reader.readAsDataURL(file);\n    reader.onload = () => resolve(reader.result);\n    reader.onerror = reject;\n});\n\nasync function Main() {\n   const file = document.querySelector('#myfile').files[0];\n   console.log(await toBase64(file));\n}\n\nMain();\n\n```\n\nUPD:\n\n\nIf you want to catch errors\n\n\n\n```\nasync function Main() {\n   const file = document.querySelector('#myfile').files[0];\n   try {\n      const result = await toBase64(file);\n      return result\n   } catch(error) {\n      console.error(error);\n      return;\n   }\n   \/\/...\n}\n\n```\n\n"}
{"questionId":"9bb6446044e2462987a80682de496e19","question":"Expected validator to return Promise or Observable\nI'm trying to do a custom validation on Angular 5 but I'm facing the following error\n\n\n\n```\nExpected validator to return Promise or Observable\n\n```\n\nI just want to return an error to the form if the value doesn't match the required, here's my code:\n\n\nThis is the component where my form is\n\n\n\n```\n  constructor(fb: FormBuilder, private cadastroService:CadastroService) {\n    this.signUp = fb.group({\n      \"name\": [\"\", Validators.compose([Validators.required, Validators.minLength(2)])],\n      \"email\": [\"\", Validators.compose([Validators.required, Validators.email])],\n      \"phone\": [\"\", Validators.compose([Validators.required, Validators.minLength(5)])],\n      \"cpf\": [\"\", Validators.required, ValidateCpf]\n    })     \n   }\n\n```\n\nThis code is in the file with the validation I want to implement:\n\n\n\n```\nimport { AbstractControl } from '@angular\/forms';\n\nexport function ValidateCpf(control: AbstractControl){\n    if (control.value == 13445) {\n        return {errorCpf: true}\n    }\n    return null;\n}\n\n```\n\nDoes that type of validation only work with observables or can I do it without being a promise or observable?\n\n\n","questionMetadata":{"type":"debugging","level":"intermediate","tag":"javascript"},"answer":"It means that you have to **add multiple validators in array**\n\n\n.\nExample:\n\n\n**With Error**\n\n\n\n```\nprofileFormGroup = {\n  budget: [null, Validators.required, Validators.min(1)]\n};\n\n```\n\nAbove one throws error that **validator to return Promise or Observable**\n\n\n**Fix:**\n\n\n\n```\nprofileFormGroup = {\n  budget: [null, [Validators.required, Validators.min(1)]]\n};\n\n```\n\n**Explanation:**\n\n\nIn angular Reactive form validation done by using in-built validators which could given in array in 2nd postion, when **multiple validators used**.\n\n\n\n> \n> FIELD\\_KEY: [INITIAL\\_VALUE, [LIST\\_OF\\_VALIDATORS]]\n> \n> \n> \n\n\n"}
